
\subsection{Optimal reparametrization}
As we have seen, the analysis of unparametrized curves in the shape analysis framework results in an optimization problem. Specifically, using SRVT on a pair of curves \(c_1, c_2\) produces a pair of curves in SRV form \(q, r\). We then want to find the optimal reparametrization \(\varphi \in \DiffI\) minimizing: 
\begin{equation}\label{eq:optimal_reparam}
  E(\varphi):=\norm{q- \sqrt{\dot\varphi}r \circ \varphi}_{L_2}^2.
\end{equation}
Methods to solve this optimization problem are mainly divided into two categories; gradient based and dynamic programming. \todo{repraze and give citatiosn}.

\subsubsection{Optimizaiton with dyanmic programming}
\citeauthor{bauer2017dp} \cite{bauer2017dp} consider an algorithm for finding an optimal parametrization to \eqref{eq:optimal_reparam} which is based on dynamic programming. The method first discretizes the interval \(I\) into  \(\mathcal{I} = \{\tau_0, \ldots, \tau_M\}\). Then the reparametrizations considered is restricted increasing piecewise linear functions \(\varphi\) such that
\begin{equation*}
 \varphi(\mathcal{I}) \subset \mathcal{I}. 
\end{equation*}
Finding the optimal reparametrization in this restricted search space reduces to finding the optimal path through the grid \(\mathcal{I}\times\mathcal{I}\). Thus, the problem of minimizing the cost function \(E(\varphi)\) will have the subproblems 
\begin{equation}
  H(i, j) = \min_{\varphi(i) = j} \int_0^i \vert q- \sqrt{\dot\varphi }r \circ \varphi \vert^2 \, dt,
\end{equation}
where \(i,j \in \mathcal{I}\). To relate the subproblems define for any \(k <i, l<j \in \mathcal{I} \) the local cost function 
\begin{equation}
  \overline E(k,l,i,j) =   \int_k^l \vert q- \sqrt{\dot\varphi_{k,l,i,j} }r \circ \varphi_{k,l,i,j} \vert^2 \, dt ,
\end{equation}
where \(\varphi_{k,l,i,j} \) is the linear function such that \(\varphi_{k,l,i,j}(k)=l\) and \(\varphi_{k,l,i,j}(i)=j\). Now, the subproblems \(E(i,j)\) will be related by 
\begin{equation}
  H(i,j)  = \min_{k<i,l<j\in \mathcal{I}}\overline{E}(k,l,i,j) + H(k,l), 
\end{equation}
Thus, the optimal reparametrization \(\varphi_\text{opt}\) can be found by recursively computing \(E(\varphi_\text{opt})= H(M,M)\) while storing the intermediate values \(H(i,j), (k,l)\). The optimal reparametrization will then be the path through \(\mathcal{I}\times\mathcal{I}\) given recursively by the intermediate values. \todo{comment on depth}

\subsubsection{Deep reparametrization}
\citeauthor{jørgen2021} \cite{jørgen2021} presents a new gradient based algorithm for finding the optimal reparametrization which he calls deep reparametrization. The algorithm is inspired by another gradient based algorithm where the optimal reparametrization \(\overline \gamma\) is found by a composition of diffeomorphisms
\begin{equation}
  \overline \gamma = \gamma_1 \circ \gamma_2 \circ \cdots  \circ \gamma_m, 
\end{equation}
In this algorithm a basis \({(v_i)}_{i=1}^{N}\) for a subspace \(V\) of \( T_{\id}\DiffI\) is needed. Then, for each iteration \(n\), the next diffeomorphisms \(\gamma_{n+1}\) is is computed by projecting the Riemannian gradient of 
\begin{equation}
  E^{r_n}(\gamma) = E(\gamma_1 \circ \gamma_2 \circ \cdots  \circ \gamma_n \circ \gamma),   
\end{equation}
onto \(V\). Denoting this projected gradient by \(\nabla E_{\id}^{r_r}\) the next diffeomorphism is given by 
\begin{equation}
  \gamma_{n+1} = \id + \eta \nabla E_{\id}^{r_r} 
\end{equation}
, where \(\eta\) is a step size determined such that \(\overline \gamma_{n+1} \in \DiffI\) 

The deep reparametrization algorithm produces similar solutions to the method previously described, but the optimization procedure is different. Instead of determining each diffeomorphism sequentially, one considers the composition of diffeomorphisms 
\begin{equation}
  F_{C} = f_{c^{[L]}}\circ f_{c^{[L-1]}} \circ \cdots \circ f_{c^{[2]}} \circ f_{c^{[1]}},
\end{equation}
where \(C = \{c^\}{[i]}_{i=1}\). As before, each diffeomorphism \(f_{c}\) has the form 
\begin{equation}
  f_{c}= \id +  \sum_{i=1}^{L} c_i v_i, 
\end{equation}
where \(c \in \R^N \). 

Futhermore, analytical solutions of the integral \eqref{eq:srvt_reparam} is generally not available. Therefore, the integral is discretized with the composite trapezoid rule resulting in the cost function 
\begin{equation}\label{eq:discretized_cost}
  \hat E(C) = \frac{1}{N} \sum_{i = 1}^N {\vert r(t_i)- \sqrt{ F_C'(t_i)}r \circ F_C(t_i) \vert^2}, 
\end{equation}
where \({(t_i)}_{i=1}^N\) denotes the \(N\) equidistant points discretizing \(I\). 

The reparametrization problem has thereby been reduced to a usual parameter optimization problem. This problem can then be solved by a standard gradient-based optimization algorithm such as BFGS. Notably, deep reparametrization has been implemented using automatic differentiation. In this framework, the gradients of functions and their compositions are available. Thus gradient-based algorithms can be implemented efficiently.

Finally, the method implements a scaling of parameters ensuring that \(F_C' \) is strictly positive. Specifically, if at any computed derivative of layer  \(f_{c^{[l]}}\) is greater than some predetermined \(\epsilon\), then \(c^{[l]}\) are scaled. 