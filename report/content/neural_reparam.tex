
\section{Reparametrization with neural networks}
\subsection{Neural networks}


\subsection{Approximation error bounds}
universal approximation theorem
based on previous method
Differet way to constrain the network.
\cite[Theomrem 5.1]{ryck2021} Shoed that a neural network with two hidden layers can approximate any funcin 

\subsection{Neural networks as reparametrizations}
Now since the space of neural networks are dense in  \(\text{Diff}(I)^+ \subset C(I)\) we do not need basis for  \(\text{Diff}(I)^+\). 
  
Instead we penalize the network so it stays in  \(\text{Diff}(I)^+\). 
The loss function is then a sum of residuals we want to minimize.
\begin{align}
  {\cal R}_{int,\theta}(t)&= q_1(t) - \sqrt{\dot f_\theta(t)}q_2 \circ f_\theta(t) \\
  {\cal R}_{b,\theta}&= f_\theta(0) + f_\theta(1) - 1\\ 
  {\cal R}_{\text{Diff},\theta}(t)&= \dot f_{\theta}(t).
\end{align}
The derivative as before is computed using automatic differentiation. The resulting loss function is: 
\begin{equation}
  J(\theta) = \sum_{i=n}^{N_i}\big[w_{int}|{\cal R}_{int,\theta}(x_i)| +  w_{\text{Diff}}|{\cal R}_{\text{Diff}, \theta}(x_i)|\big]  +|{\cal R}_{b,\theta}|  + \lambda||\theta||_2,
\end{equation}
where  \(w_{int}, w_{b} \) and  \(w_{\text{Diff}}\) are parameters penalizing each residual, and   \(\lambda\) is a regularization parameter. 
\subsection{Discontinuous transformed curves}
The method proposed will not necessarily work for transformed curves that are discontinuous. If the the transformed curve is discontinuous, then there is no guarantee that the cost function will be continuous. The optimization problem then hard to solve, and can not be solved with the gradient based methods discussed.

A general theory of when discontinuous curves results in a discontinuous cost function will not be attempted. Instead we look at a situation when the the cost function is guaranteed to be disontinious at a point \(\theta_0\). Consider a transformed curve  \(r\) that is discontinuous at  \(a \in I\). We furthermore assume that the reparametrizations  \(f_{\theta}\) is smooth with respect to the parameters  \(\theta\),  \(f_{\theta_0}(t_0)=a\) and   \(\pdv{f}{\theta_1}|_{\theta_0}(t_0)\neq 0\) for some  \(\theta_0\) and direction  \(\theta_1\) Then along the direction  \(\theta_1\),  \(f_{\theta}\) will have a a continuous inverse in some neighborhood  \(N\) of  \(\theta_0\). Thus  since \(\sqrt{f_{\theta}'(t_0)} \geq 0\), \(r \circ f_\theta(t_0)\) and  \(\sqrt{f_{\theta}'(t_0)} r \circ f_\theta(t_0)\) will be discontinuous at \(\theta_0\). If we then assume that
\begin{equation}
  \sqrt{f_{\theta}'(t_0)} r \circ f_\theta(t_0) \geq  q(t_0)\ \text{on} \ N,
\end{equation}
or
\begin{equation}
  \sqrt{f_{\theta}'(t_0)} r \circ f_\theta(t_0) \leq q(t_0) \ \text{on} \ N.
\end{equation}
Then the cost function defined as
\begin{equation}
  E_0(\theta) = \big[q(t_0) - \sqrt{f'_{\theta}} r \circ f_{\theta}(t_0)\big]^2,
\end{equation}
will have a discontinuity at  \(\theta_0\). The cost function as defined in (0) will then be discontinuous if the discontinuities in each part of the sum does not cancel each other.

The assumptions required of  \(f_{\theta}\) are not necessary for  \(E_0\) to be discontinuous. They are only meant as an example for a reasonable situation where  \(E_0\) is guaranteed to be discontinuous. For example, neural networks with a smooth sigmoidal function will be smooth with respect its parameter.

For transformed curves which are not continuous, but piecewise smooth there is a way to find an approximate solution to the optimal reparametrization problem. If the jumps of the discontinuous curve \(r\) are not to large, then we can use a new continuous curve \(\hat r\) that interpolates \(r\). We can find then find a bound for the resulting distance fo the two curves. This is evident by the following proposition.
\begin{proposition}
  Let \(\{t_i\}_{1 \leq i\leq n}\) be a strictly increasing sequence of number in the interval I, and let \(q\),\(r\), \(\hat q\) and  \(\hat r\) curves such that \(q(s_i)=\hat q(s_i)\) and \(r(s_i')=\hat r(s_i')\) for \(s_i, s_i' \in [t_i, t_i+1), 1\leq i < n\). If  \(q\),  \(r\),   \(\hat q\) and  \(\hat r\) are smooth on the intervals  \([t_i, t_{i+1})\) for  \( 1 \leq i <n\), and  \(f_\theta : I \rightarrow I\) is a function with bounded derrivative and parametrized by \( \theta \). Then for the two calculated distances
  \begin{eqnarray}
    \hat d_n^2=\frac{1}{n}\sum_{i=1}^{n}\Big|\hat q(t_i) + \sqrt{f'_{\theta}(t_0)} \hat r \circ f_{\theta}(t_0)\Big|^2, \\
    d_n^2=\frac{1}{n}\sum_{i=1}^{n}\Big|q(t_i) + \sqrt{f'_{\theta}(t_0)} r \circ f_{\theta}(t_0)\Big|^2,
  \end{eqnarray}
  the following bound holds for their difference
  \begin{equation}
    | \hat d_n -d_n | \leq  C h,
  \end{equation}
  where h = \(\max_{1\leq i<n}|t_{i+1} - t_{i} |\), and C is a constant depending on the derivatives of  \(r\),  \(\hat{r}\),  \(q\),  \(\hat q\) and  \(f_{\theta}\).
\end{proposition}
\begin{proof}
  On  \([t_i, t_{i+1})\),  \(\hat q\) and  \(\hat r\) are smooth functions that are equal to  \(q\) and  \(r\) in a point  \(t' \in [t_i, t_{i+1})\). Thus by Taylor's theorem for all  \(t \in [t_i, t_{i+1})\) there exist  \(\xi \in (t_i, t_{i+1})\) such that
  \begin{eqnarray}
    |\hat q(t) - q(t)|  &= |\hat q'(\xi) - q'(\xi)||t' - t| \\
    |\hat r(t) - r(t)|  &= |\hat r'(\xi) - r'(\xi)||t' - t|.
  \end{eqnarray}
  Now define  \(T:= \left \{t_i : 1 \leq i \leq n \right \} \). Then, since all  \(t \in I\) are in some interval of length  \(h\) we have to following bounds
  \begin{eqnarray}
    ||\hat q - q||_{\infty} \leq ||\hat q' - q'||_{\infty, I \setminus T}h = C_q h,
  \end{eqnarray}
  \begin{eqnarray}
    ||\hat r - r||_{\infty} \leq ||\hat r' - r'||_{\infty, I \setminus T}h = C_r h.
  \end{eqnarray}
  Now define constant  \(C_{\theta}:= \sup_{t\in T} | \sqrt{f'_\theta(t)} | \). Then by the reverse triangle inequality for the 2-norm we have the bound
  \begin{align}
    |\hat d_n - d_n | 
      & = \Bigg| \sqrt{\frac{1}{n}\sum_{i=1}^{n}\Big|\hat q(t_i) + \sqrt{f'_{\theta}(t_0)} \hat r \circ f_{\theta}(t_0)\Big|^2}-\sqrt{\frac{1}{n}\sum_{i=1}^{n}\Big|q(t_i) + \sqrt{f'_{\theta}(t_0)} r \circ f_{\theta}(t_0)\Big|^2 } \Bigg|\\
     & \leq \sqrt{\frac{1}{n}\sum_{i=1}^{n}\Big|\hat q(t_i) - q(t_i)  + \sqrt{f'_{\theta}(t_i)}\left[  \hat r \circ f_{\theta}(t_i) - r \circ f_{\theta}(t_i)\right]\Big|^2}\\
     & \leq \left(C_q  + C_{\theta} C_r \right)h.
  \end{align}
\end{proof}

As a consequence if we have two smooth curve with piecewise constant interplants \(r, q\) that results i in the cost function \(E\) defined by [0]. If we then construct a continuous and piecewise linear function \(\hat r\) interpolating \(r\) at \(\{t_i\}, 1 \leq i <n\). Then replacing \(r\) by \(\hat r\) results in cost function  \(\hat{E}\) such that \(|\hat{E} - E |= \mathcal{O}(1/n)\). 

As seen in chapter \ref{subsec:shape-lie} motion capture data can be viewed as a smooth curve in \(SO(3)\). If this motion is record at at discrete times, then the resulting shortest path interpolation results in a SRVT that is piecewise constant. Thus for two motions we have two smooth curves with piecewise constant interplants \(r, q\). The optimal parametrization problem results in the cost function \(E\) defined by [0]. If we then construct a continuous and piecewise linear function \(\hat r\) interpolating \(r\) at \(\{t_i\}, 1 \leq i <n\). Then replacing \(r\) by \(\hat r\) results in cost function  \(\hat{E}\) such that \(|\hat{E} - E |= \mathcal{O}(1/n)\). 