
\section{Experiments}
\FloatBarrier
When dealing with neural networks there are a wide range of network structures and optimization algorithms. An in depth search for the best parameters will not be the focus of this chapter. The main focus of this chapter will be to investigate when reparametrization by neural networks is possible.

We also investigate whether the deviation from the optimal solution is optimization error or approximation error.  Thus by the results in chapter [0] we investigate how increasing the network size impacts the resulting error. This approach will still be somewhat ambiguous; since increasing the networks size simultaneously increases the dimension of the optimization problem. Also with increasing layers we have the vanishing gradient problem.[0]

\subsection{Curves from the same shape}
\FloatBarrier
\input{content/experiments/case_1.tex}

\FloatBarrier
\subsection{Curves from different shapes}
\input{content/experiments/case_2.tex}

\FloatBarrier
\subsection{Piecewise linear and piecewise constant}
\input{content/experiments/case_3.tex}
Error convergence example 

\FloatBarrier
\subsection{Interpolated motion capture data}
\input{content/experiments/case_4.tex}
