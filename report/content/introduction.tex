\section{Introduction}
Kendall was among the first the define the notion of shapes \cite{kendall_1984_fistshape}. He defined them as geometric objects modulo rotation and dilatations. Similarly, pre-shapes were the possible representatives of each shape. Kendall mainly worked with finite collections of points on a boundary called landmarks. However, today's shape analysis also considers continuous objects, such as unparametrized curves defining a boundary. Moreover, as first attempted by \cite{younes_2000_manifold_start}, there has been an effort to study these continuous objects as infinite-dimensional Riemannian manifolds. The Riemannian metric preserves the nonlinearity of the shape space while also providing useful structure. Different Riemannian metrics have been studied further by \cite{michor2003vanishingl2,mio2007} and an overview can be found in \cite{bauer2014overview}.

We will consider a pre-shape space of sufficiently regular curves and the corresponding shape space of unparametrized curves. To define a Riemannian metric and distance on our shape space, we use the popular \emph{square root velocity transform} (SRVT). The SRVT was first introduced by \citeauthor{srivastava2011_srvt} \cite{srivastava2011_srvt}, and has been successfully used in many applications \cite{kurtek2013_a1,joshi2013_a2,laga2014_a3,kurtek2014_a4,laborde2013_a5}. Specifically, the SRVT framework has been applied to motion capture data. Not only producing methods for clustering motions and interpolating between motions \cite{eslitzbichler2014_motion}, but also a method for smoothly closing open curves \cite{celledoni2016}.

To obtain geodesics and distances on the shape space, we first need to find an optimal reparametrization of the curves. There are several algorithms for finding this reparametrization \cite{srivastava2011_srvt,su20017,kima2003_dp,bauer2017dp}. Moreover, the optimization problem has previously been approached using neural-networks\cite{berland2019}, and neural network-like functions \cite{jørgen2021}. We introduce a method for finding an optimal reparametrization using feedforward neural networks that use a different cost function. We then compare our method to other algorithms on model problems and curves generated by motion capture data.

In Section \ref{sec:shape-analysis} we introduce shape analysis and define the shape space we will be working on. Moreover, the structure imposed by the SRVT on our pre-shape space will be explained. We also elaborate on an extension of the SRVT to curves in Lie groups \cite{celledoni2016}. Finally, the resulting optimization problem for finding distances between unparametrized curves will be introduced, and two methods for solving the optimization problem will be given. The first is a dynamic programming approach discussed in \cite{bauer2017dp}, and the second is a gradient-based method by \citeauthor{jørgen2021} \cite{jørgen2021}, which utilizes a structure similar to residual networks.

In Section \ref{sec:nueral-reparam} we introduce a method for finding an optimal reparametrization by using neural networks. We first define neural networks and explain their approximation properties. Then, we propose a cost function and justify why minimizing this cost function yields a valid optimal reparametrization. Moreover, not all curves defined by real-world data will result in a continuous optimization problem. Therefore, we finish the section by giving a simple way to regularize such a discontinuous situation.

In Section \ref{sec:experiments}, the performance of optimization with neural networks is compared to the two other optimization algorithms previously defined. Parametrization by neural networks will first be tested on 3 model problems. Then, we apply the method to curves generated by interpolating motion capture data and compare it to a dynamic programming algorithm \cite{bauer2017dp}. In the first problem, we also compare our results to \citeauthor{jørgen2021}'s algorithm. Based on these experiments, we give some concluding remarks about the convergence of reparametrization by neural networks in Section \ref{sec:conclusion}. Here we also comment on possible future work.
