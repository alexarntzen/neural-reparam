{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Try to learn (1) as a piecewise linear constant curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from neural_reparam.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from neural_reparam.ResNET import ResNET\n",
    "from neural_reparam.interpolation import get_pc_curve, get_pl_curve\n",
    "import experiments.curves as c1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_1_pl/\"\n",
    "SET_NAME = \"exp_2\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "\n",
    "FOLDS = 1\n",
    "N = 128  # training points internal\n",
    "\n",
    "loss_func = get_elastic_metric_loss(r=get_pl_curve(c1.r, N), constrain_cost=1e3, verbose=False)\n",
    "penalty_free_loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=0, verbose=False)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [2],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"model\": [FFNN, ResNET],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [10],\n",
    "    \"learning_rate\": [1],\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_pl = get_pc_curve(c1.q, N)\n",
    "q_train = q_pl(x_train.detach())\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.57012308\n",
      "Training Loss:  1.57012308\n",
      "Training Loss:  1.57012308\n",
      "Training Loss:  1.57012308\n",
      "Training Loss:  1.57011783\n",
      "Training Loss:  1.57011783\n",
      "Training Loss:  1.57011724\n",
      "Training Loss:  1.57011724\n",
      "Training Loss:  1.41135442\n",
      "Training Loss:  1.41135442\n",
      "Final training Loss:  1.41135442\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Training Loss:  0.72286701\n",
      "Final training Loss:  0.72286701\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.33302146\n",
      "Training Loss:  0.33281416\n",
      "Training Loss:  0.33281416\n",
      "Training Loss:  0.33281416\n",
      "Training Loss:  0.33281416\n",
      "Training Loss:  0.33281416\n",
      "Training Loss:  0.33281416\n",
      "Training Loss:  0.33281416\n",
      "Training Loss:  0.33281416\n",
      "Training Loss:  0.33281416\n",
      "Final training Loss:  0.33281416\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23764586\n",
      "Training Loss:  1.23764491\n",
      "Training Loss:  1.23764265\n",
      "Training Loss:  1.23764265\n",
      "Training Loss:  1.23763514\n",
      "Training Loss:  1.23763514\n",
      "Training Loss:  1.23763502\n",
      "Training Loss:  1.23763502\n",
      "Training Loss:  1.23763502\n",
      "Training Loss:  1.23763502\n",
      "Final training Loss:  1.23763502\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71232289\n",
      "Training Loss:  0.71232289\n",
      "Training Loss:  0.71232289\n",
      "Training Loss:  0.71232289\n",
      "Training Loss:  0.71232289\n",
      "Training Loss:  0.71232361\n",
      "Training Loss:  0.71232289\n",
      "Training Loss:  0.59973139\n",
      "Training Loss:  0.57675982\n",
      "Training Loss:  0.57675982\n",
      "Final training Loss:  0.57675982\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.27104938\n",
      "Training Loss:  1.27104521\n",
      "Training Loss:  1.27104521\n",
      "Training Loss:  1.27104521\n",
      "Training Loss:  1.27104521\n",
      "Training Loss:  1.27104521\n",
      "Training Loss:  1.27104521\n",
      "Training Loss:  1.27104521\n",
      "Training Loss:  1.27104521\n",
      "Training Loss:  1.27104521\n",
      "Final training Loss:  1.27104521\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29882014\n",
      "Training Loss:  1.29882014\n",
      "Training Loss:  1.29882014\n",
      "Training Loss:  1.29882014\n",
      "Training Loss:  1.2988193\n",
      "Training Loss:  1.2988193\n",
      "Training Loss:  1.2988193\n",
      "Training Loss:  1.29881549\n",
      "Training Loss:  1.28067756\n",
      "Training Loss:  1.28067756\n",
      "Final training Loss:  1.28067756\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.32208526\n",
      "Training Loss:  1.32208502\n",
      "Training Loss:  1.32208502\n",
      "Training Loss:  1.32208526\n",
      "Training Loss:  1.32208502\n",
      "Training Loss:  1.32208526\n",
      "Training Loss:  1.32208526\n",
      "Training Loss:  1.32208526\n",
      "Training Loss:  1.32208526\n",
      "Training Loss:  1.32201147\n",
      "Final training Loss:  1.32201147\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26518714\n",
      "Training Loss:  1.25298452\n",
      "Training Loss:  1.24309552\n",
      "Training Loss:  1.24309552\n",
      "Training Loss:  1.24309552\n",
      "Training Loss:  1.24309552\n",
      "Training Loss:  1.24269378\n",
      "Training Loss:  1.24269378\n",
      "Training Loss:  1.24268281\n",
      "Training Loss:  1.24268281\n",
      "Final training Loss:  1.24268281\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19951379\n",
      "Training Loss:  1.19951248\n",
      "Training Loss:  1.19951248\n",
      "Training Loss:  1.19951248\n",
      "Training Loss:  1.19950557\n",
      "Training Loss:  1.19950557\n",
      "Training Loss:  1.19950557\n",
      "Training Loss:  1.19950557\n",
      "Training Loss:  1.19950557\n",
      "Training Loss:  1.19950557\n",
      "Final training Loss:  1.19950557\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18761539\n",
      "Training Loss:  1.18761539\n",
      "Training Loss:  1.18760753\n",
      "Training Loss:  1.18759859\n",
      "Training Loss:  1.18756258\n",
      "Training Loss:  1.18756258\n",
      "Training Loss:  1.1872431\n",
      "Training Loss:  1.18705261\n",
      "Training Loss:  1.18705261\n",
      "Training Loss:  1.18704057\n",
      "Final training Loss:  1.18704057\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19959748\n",
      "Training Loss:  1.19848537\n",
      "Training Loss:  1.19848537\n",
      "Training Loss:  1.19848263\n",
      "Training Loss:  1.19848263\n",
      "Training Loss:  1.19845462\n",
      "Training Loss:  1.19843364\n",
      "Training Loss:  1.19843364\n",
      "Training Loss:  1.19843364\n",
      "Training Loss:  1.19843256\n",
      "Final training Loss:  1.19843256\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Training Loss:  599.46655273\n",
      "Final training Loss:  599.46655273\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50360107\n",
      "Training Loss:  599.50360107\n",
      "Training Loss:  599.50360107\n",
      "Training Loss:  599.50360107\n",
      "Training Loss:  599.50360107\n",
      "Final training Loss:  599.50360107\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.21723437\n",
      "Training Loss:  1.21723437\n",
      "Training Loss:  1.21723306\n",
      "Training Loss:  1.21721339\n",
      "Training Loss:  1.21721148\n",
      "Training Loss:  1.17859316\n",
      "Training Loss:  1.17859316\n",
      "Training Loss:  1.17859316\n",
      "Training Loss:  1.17859316\n",
      "Training Loss:  1.17859316\n",
      "Final training Loss:  1.17859316\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.2715379\n",
      "Training Loss:  1.27152705\n",
      "Final training Loss:  1.27152705\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59201407\n",
      "Training Loss:  0.59201407\n",
      "Training Loss:  0.59201407\n",
      "Training Loss:  0.59201407\n",
      "Training Loss:  0.59201407\n",
      "Training Loss:  0.59201407\n",
      "Training Loss:  0.59201407\n",
      "Training Loss:  0.59201407\n",
      "Training Loss:  0.59200424\n",
      "Training Loss:  0.59200424\n",
      "Final training Loss:  0.59200424\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19065976\n",
      "Training Loss:  1.19064808\n",
      "Training Loss:  1.19064808\n",
      "Training Loss:  1.19064808\n",
      "Training Loss:  1.1906352\n",
      "Training Loss:  1.1906352\n",
      "Training Loss:  1.1906352\n",
      "Training Loss:  1.1906352\n",
      "Training Loss:  1.1906352\n",
      "Training Loss:  1.1906352\n",
      "Final training Loss:  1.1906352\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26145732\n",
      "Training Loss:  1.26143157\n",
      "Training Loss:  1.26140416\n",
      "Training Loss:  1.26140416\n",
      "Training Loss:  1.26140416\n",
      "Training Loss:  1.24734175\n",
      "Training Loss:  1.24734175\n",
      "Training Loss:  1.24733937\n",
      "Training Loss:  1.24733937\n",
      "Training Loss:  1.24733937\n",
      "Final training Loss:  1.24733937\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23679984\n",
      "Training Loss:  1.23679984\n",
      "Training Loss:  1.22717166\n",
      "Training Loss:  1.22717166\n",
      "Training Loss:  1.22707629\n",
      "Training Loss:  1.22707629\n",
      "Training Loss:  1.22706878\n",
      "Training Loss:  1.22706819\n",
      "Training Loss:  1.22706819\n",
      "Training Loss:  1.22706819\n",
      "Final training Loss:  1.22706819\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.31450665\n",
      "Training Loss:  1.31450629\n",
      "Training Loss:  1.31447649\n",
      "Training Loss:  1.31447649\n",
      "Training Loss:  1.31447649\n",
      "Training Loss:  1.31447649\n",
      "Training Loss:  1.31447649\n",
      "Training Loss:  1.31447649\n",
      "Training Loss:  1.31447649\n",
      "Training Loss:  1.31447649\n",
      "Final training Loss:  1.31447649\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30612922\n",
      "Training Loss:  1.30612922\n",
      "Training Loss:  1.30612922\n",
      "Training Loss:  1.30612922\n",
      "Training Loss:  1.30612922\n",
      "Training Loss:  1.30612922\n",
      "Training Loss:  1.3061285\n",
      "Training Loss:  1.3061285\n",
      "Training Loss:  1.3061285\n",
      "Training Loss:  1.3061285\n",
      "Final training Loss:  1.3061285\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.54747188\n",
      "Training Loss:  1.29252338\n",
      "Training Loss:  1.29235947\n",
      "Training Loss:  1.29235947\n",
      "Training Loss:  1.29235315\n",
      "Training Loss:  1.29235315\n",
      "Training Loss:  1.29234087\n",
      "Training Loss:  1.29234087\n",
      "Training Loss:  1.29234087\n",
      "Training Loss:  1.29233873\n",
      "Final training Loss:  1.29233873\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64400804\n",
      "Training Loss:  0.64400804\n",
      "Training Loss:  0.64400804\n",
      "Training Loss:  0.64400798\n",
      "Training Loss:  0.62405682\n",
      "Training Loss:  0.62405348\n",
      "Training Loss:  0.62405348\n",
      "Training Loss:  0.62405348\n",
      "Training Loss:  0.62405348\n",
      "Training Loss:  0.62404805\n",
      "Final training Loss:  0.62404805\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18239498\n",
      "Training Loss:  1.18233156\n",
      "Training Loss:  1.17380071\n",
      "Training Loss:  1.17380071\n",
      "Training Loss:  1.17380071\n",
      "Training Loss:  1.17380071\n",
      "Training Loss:  1.17380071\n",
      "Training Loss:  1.17380071\n",
      "Training Loss:  1.17380071\n",
      "Training Loss:  1.17380071\n",
      "Final training Loss:  1.17380071\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.27589774\n",
      "Training Loss:  1.27589774\n",
      "Training Loss:  1.27588367\n",
      "Training Loss:  1.27588367\n",
      "Training Loss:  1.27588367\n",
      "Training Loss:  1.27588367\n",
      "Training Loss:  1.27588367\n",
      "Training Loss:  1.27588367\n",
      "Training Loss:  1.27588367\n",
      "Training Loss:  1.27588367\n",
      "Final training Loss:  1.27588367\n",
      "\n",
      "Running model (trial=0, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.57224947\n",
      "Training Loss:  0.57224947\n",
      "Training Loss:  0.57224947\n",
      "Training Loss:  0.57224137\n",
      "Training Loss:  0.57223713\n",
      "Training Loss:  0.57223713\n",
      "Training Loss:  0.57223713\n",
      "Training Loss:  0.57223713\n",
      "Training Loss:  0.57223713\n",
      "Training Loss:  0.57223713\n",
      "Final training Loss:  0.57223713\n",
      "\n",
      "Running model (trial=0, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Training Loss:  1.24296737\n",
      "Final training Loss:  1.24296737\n",
      "\n",
      "Running model (trial=1, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.38615632\n",
      "Training Loss:  1.42285264\n",
      "Training Loss:  1.42285264\n",
      "Training Loss:  1.42281818\n",
      "Training Loss:  1.42279732\n",
      "Training Loss:  1.42279732\n",
      "Training Loss:  1.422786\n",
      "Training Loss:  1.422786\n",
      "Training Loss:  1.422786\n",
      "Training Loss:  1.422786\n",
      "Final training Loss:  1.422786\n",
      "\n",
      "Running model (trial=1, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71816462\n",
      "Training Loss:  0.63667482\n",
      "Training Loss:  0.63667482\n",
      "Training Loss:  0.63667482\n",
      "Training Loss:  0.63547635\n",
      "Training Loss:  0.63547635\n",
      "Training Loss:  0.63547635\n",
      "Training Loss:  0.63547635\n",
      "Training Loss:  0.63547635\n",
      "Training Loss:  0.63547635\n",
      "Final training Loss:  0.63547635\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.53911406\n",
      "Training Loss:  0.53911406\n",
      "Training Loss:  0.53911406\n",
      "Training Loss:  0.53911406\n",
      "Training Loss:  0.53911406\n",
      "Training Loss:  0.53911406\n",
      "Training Loss:  0.33816579\n",
      "Training Loss:  0.33815563\n",
      "Training Loss:  0.25803688\n",
      "Training Loss:  0.25802749\n",
      "Final training Loss:  0.25802749\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18524551\n",
      "Training Loss:  1.18524551\n",
      "Training Loss:  1.18524551\n",
      "Training Loss:  1.18524265\n",
      "Training Loss:  1.18524265\n",
      "Training Loss:  1.18524265\n",
      "Training Loss:  1.18524265\n",
      "Training Loss:  1.18524265\n",
      "Training Loss:  1.18524265\n",
      "Training Loss:  1.18524265\n",
      "Final training Loss:  1.18524265\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.21339965\n",
      "Training Loss:  1.19274294\n",
      "Training Loss:  1.19274294\n",
      "Training Loss:  1.19273198\n",
      "Training Loss:  1.19273198\n",
      "Training Loss:  1.19273198\n",
      "Training Loss:  1.19273198\n",
      "Training Loss:  1.19273198\n",
      "Training Loss:  1.19273198\n",
      "Training Loss:  1.19273198\n",
      "Final training Loss:  1.19273198\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24947488\n",
      "Training Loss:  1.2494694\n",
      "Training Loss:  1.2494694\n",
      "Training Loss:  1.2494694\n",
      "Training Loss:  1.2494452\n",
      "Training Loss:  1.2494427\n",
      "Training Loss:  1.2494427\n",
      "Training Loss:  1.2494427\n",
      "Training Loss:  1.2494427\n",
      "Training Loss:  1.2494427\n",
      "Final training Loss:  1.2494427\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Training Loss:  0.64495665\n",
      "Final training Loss:  0.64495665\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30842304\n",
      "Training Loss:  1.30842292\n",
      "Training Loss:  1.30382371\n",
      "Training Loss:  1.30382371\n",
      "Training Loss:  1.30382371\n",
      "Training Loss:  1.30382371\n",
      "Training Loss:  1.30382371\n",
      "Training Loss:  1.30382371\n",
      "Training Loss:  1.30382371\n",
      "Training Loss:  1.30382371\n",
      "Final training Loss:  1.30382371\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Training Loss:  0.65706968\n",
      "Final training Loss:  0.65706968\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63542241\n",
      "Training Loss:  0.47493908\n",
      "Training Loss:  0.4749319\n",
      "Training Loss:  0.47269386\n",
      "Training Loss:  0.47269386\n",
      "Training Loss:  0.47269386\n",
      "Training Loss:  0.47269338\n",
      "Training Loss:  0.47269344\n",
      "Training Loss:  0.47269145\n",
      "Training Loss:  0.47269145\n",
      "Final training Loss:  0.47269145\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6362406\n",
      "Training Loss:  0.14342871\n",
      "Training Loss:  0.14342871\n",
      "Training Loss:  0.14342871\n",
      "Training Loss:  0.14342871\n",
      "Training Loss:  0.14321612\n",
      "Training Loss:  0.14321612\n",
      "Training Loss:  0.14321612\n",
      "Training Loss:  0.14321357\n",
      "Training Loss:  0.14321357\n",
      "Final training Loss:  0.14321357\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64055645\n",
      "Training Loss:  0.64054245\n",
      "Training Loss:  0.64054245\n",
      "Training Loss:  0.64053631\n",
      "Training Loss:  0.64053631\n",
      "Training Loss:  0.64053631\n",
      "Training Loss:  0.64053631\n",
      "Training Loss:  0.64053631\n",
      "Training Loss:  0.64053631\n",
      "Training Loss:  0.64053631\n",
      "Final training Loss:  0.64053631\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.4710083\n",
      "Training Loss:  599.47003174\n",
      "Training Loss:  599.47003174\n",
      "Training Loss:  599.47003174\n",
      "Training Loss:  599.47003174\n",
      "Training Loss:  599.47003174\n",
      "Training Loss:  599.47003174\n",
      "Training Loss:  599.47003174\n",
      "Training Loss:  599.47003174\n",
      "Training Loss:  599.47003174\n",
      "Final training Loss:  599.47003174\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Final training Loss:  599.50390625\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.76800156\n",
      "Training Loss:  5.76395321\n",
      "Training Loss:  5.76395226\n",
      "Training Loss:  5.7639513\n",
      "Training Loss:  5.7639513\n",
      "Training Loss:  5.7639513\n",
      "Training Loss:  5.7639513\n",
      "Training Loss:  5.7639513\n",
      "Training Loss:  5.7639513\n",
      "Training Loss:  5.7639513\n",
      "Final training Loss:  5.7639513\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19459617\n",
      "Training Loss:  1.19458997\n",
      "Training Loss:  1.19458997\n",
      "Training Loss:  1.1945858\n",
      "Training Loss:  1.1945858\n",
      "Training Loss:  1.1945858\n",
      "Training Loss:  1.1945858\n",
      "Training Loss:  1.1945858\n",
      "Training Loss:  1.1945858\n",
      "Training Loss:  1.1945858\n",
      "Final training Loss:  1.1945858\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.13865113\n",
      "Training Loss:  1.138641\n",
      "Training Loss:  1.13863277\n",
      "Training Loss:  1.13863277\n",
      "Training Loss:  1.13863277\n",
      "Training Loss:  1.13863277\n",
      "Training Loss:  1.13863277\n",
      "Training Loss:  1.13863277\n",
      "Training Loss:  1.13863254\n",
      "Training Loss:  1.13863254\n",
      "Final training Loss:  1.13863254\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Training Loss:  0.58772117\n",
      "Final training Loss:  0.58772117\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23399854\n",
      "Training Loss:  1.23399854\n",
      "Training Loss:  1.23398137\n",
      "Training Loss:  1.23398137\n",
      "Training Loss:  1.23398137\n",
      "Training Loss:  1.23398137\n",
      "Training Loss:  1.23397207\n",
      "Training Loss:  1.23397207\n",
      "Training Loss:  1.23397207\n",
      "Training Loss:  1.23397207\n",
      "Final training Loss:  1.23397207\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26230323\n",
      "Training Loss:  1.26230323\n",
      "Training Loss:  1.26230323\n",
      "Training Loss:  1.26230323\n",
      "Training Loss:  1.26230323\n",
      "Training Loss:  1.26229811\n",
      "Training Loss:  1.26229811\n",
      "Training Loss:  1.26229811\n",
      "Training Loss:  1.26229811\n",
      "Training Loss:  1.26229811\n",
      "Final training Loss:  1.26229811\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.36029732\n",
      "Training Loss:  1.32442808\n",
      "Training Loss:  1.31745219\n",
      "Training Loss:  1.31745219\n",
      "Training Loss:  1.31745219\n",
      "Training Loss:  1.31745017\n",
      "Training Loss:  1.31745017\n",
      "Training Loss:  1.31742156\n",
      "Training Loss:  1.31742156\n",
      "Training Loss:  1.31741214\n",
      "Final training Loss:  1.31741214\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3584342\n",
      "Training Loss:  1.35843146\n",
      "Training Loss:  1.35842621\n",
      "Training Loss:  1.35842621\n",
      "Training Loss:  1.35842621\n",
      "Training Loss:  1.35842025\n",
      "Training Loss:  1.35842025\n",
      "Training Loss:  1.35842025\n",
      "Training Loss:  1.35842025\n",
      "Training Loss:  1.35842025\n",
      "Final training Loss:  1.35842025\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30755758\n",
      "Training Loss:  1.30755758\n",
      "Training Loss:  1.24553943\n",
      "Training Loss:  1.24553943\n",
      "Training Loss:  1.24553943\n",
      "Training Loss:  1.24553943\n",
      "Training Loss:  1.24553001\n",
      "Training Loss:  1.24553001\n",
      "Training Loss:  1.24552894\n",
      "Training Loss:  1.2452544\n",
      "Final training Loss:  1.2452544\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Training Loss:  1.25445127\n",
      "Final training Loss:  1.25445127\n",
      "\n",
      "Running model (trial=1, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26199746\n",
      "Training Loss:  1.26199746\n",
      "Training Loss:  1.260934\n",
      "Training Loss:  1.26091802\n",
      "Training Loss:  1.26091802\n",
      "Training Loss:  1.26091802\n",
      "Training Loss:  1.26091802\n",
      "Training Loss:  1.26091802\n",
      "Training Loss:  1.26091802\n",
      "Training Loss:  1.26091802\n",
      "Final training Loss:  1.26091802\n",
      "\n",
      "Running model (trial=1, mod=53, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.56117439\n",
      "Training Loss:  0.56117439\n",
      "Training Loss:  0.56116796\n",
      "Training Loss:  0.56116796\n",
      "Training Loss:  0.56116796\n",
      "Training Loss:  0.56116784\n",
      "Training Loss:  0.56116784\n",
      "Training Loss:  0.56116784\n",
      "Training Loss:  0.56116784\n",
      "Training Loss:  0.56116784\n",
      "Final training Loss:  0.56116784\n",
      "\n",
      "Running model (trial=1, mod=54, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.2304399\n",
      "Training Loss:  0.59904462\n",
      "Training Loss:  0.59902775\n",
      "Training Loss:  0.55472863\n",
      "Training Loss:  0.55403519\n",
      "Training Loss:  0.55403519\n",
      "Training Loss:  0.55403018\n",
      "Training Loss:  0.55403018\n",
      "Training Loss:  0.55403018\n",
      "Training Loss:  0.54974306\n",
      "Final training Loss:  0.54974306\n",
      "\n",
      "Running model (trial=1, mod=55, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.58654898\n",
      "Training Loss:  0.58654898\n",
      "Training Loss:  0.58654898\n",
      "Training Loss:  0.56026715\n",
      "Training Loss:  0.56026715\n",
      "Training Loss:  0.56026715\n",
      "Training Loss:  0.56026715\n",
      "Training Loss:  0.56026715\n",
      "Training Loss:  0.56026715\n",
      "Training Loss:  0.56025565\n",
      "Final training Loss:  0.56025565\n",
      "\n",
      "Running model (trial=2, mod=56, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.47044361\n",
      "Training Loss:  1.43212664\n",
      "Training Loss:  1.43093324\n",
      "Training Loss:  1.43022239\n",
      "Training Loss:  1.43022239\n",
      "Training Loss:  1.42858827\n",
      "Training Loss:  1.42858827\n",
      "Training Loss:  1.42858827\n",
      "Training Loss:  1.42858827\n",
      "Training Loss:  1.42858827\n",
      "Final training Loss:  1.42858827\n",
      "\n",
      "Running model (trial=2, mod=57, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.56754315\n",
      "Training Loss:  0.5659349\n",
      "Training Loss:  0.5659349\n",
      "Training Loss:  0.55851138\n",
      "Training Loss:  0.55847514\n",
      "Training Loss:  0.55823249\n",
      "Training Loss:  0.55823249\n",
      "Training Loss:  0.55823249\n",
      "Training Loss:  0.55821633\n",
      "Training Loss:  0.55821633\n",
      "Final training Loss:  0.55821633\n",
      "\n",
      "Running model (trial=2, mod=58, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61799616\n",
      "Training Loss:  0.59292352\n",
      "Training Loss:  0.59048998\n",
      "Training Loss:  0.59048998\n",
      "Training Loss:  0.57093185\n",
      "Training Loss:  0.57093185\n",
      "Training Loss:  0.57093078\n",
      "Training Loss:  0.57092738\n",
      "Training Loss:  0.57092738\n",
      "Training Loss:  0.57092738\n",
      "Final training Loss:  0.57092738\n",
      "\n",
      "Running model (trial=2, mod=59, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19293904\n",
      "Training Loss:  1.19293237\n",
      "Training Loss:  1.19293237\n",
      "Training Loss:  1.19293165\n",
      "Training Loss:  1.19292748\n",
      "Training Loss:  1.19292748\n",
      "Training Loss:  1.19292748\n",
      "Training Loss:  1.19292748\n",
      "Training Loss:  1.19292748\n",
      "Training Loss:  1.19292748\n",
      "Final training Loss:  1.19292748\n",
      "\n",
      "Running model (trial=2, mod=60, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.20845449\n",
      "Training Loss:  1.20845449\n",
      "Training Loss:  1.20845449\n",
      "Training Loss:  1.20845449\n",
      "Training Loss:  1.20845175\n",
      "Training Loss:  1.20845175\n",
      "Training Loss:  1.20845175\n",
      "Training Loss:  1.20845175\n",
      "Training Loss:  1.20845175\n",
      "Training Loss:  1.20845175\n",
      "Final training Loss:  1.20845175\n",
      "\n",
      "Running model (trial=2, mod=61, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18507123\n",
      "Training Loss:  1.17320728\n",
      "Training Loss:  1.17320728\n",
      "Training Loss:  1.17320728\n",
      "Training Loss:  1.17320728\n",
      "Training Loss:  1.17320728\n",
      "Training Loss:  1.17320728\n",
      "Training Loss:  1.17320728\n",
      "Training Loss:  1.17320466\n",
      "Training Loss:  1.17320466\n",
      "Final training Loss:  1.17320466\n",
      "\n",
      "Running model (trial=2, mod=62, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30336118\n",
      "Training Loss:  1.30336118\n",
      "Training Loss:  1.30336118\n",
      "Training Loss:  1.30336118\n",
      "Training Loss:  1.30336118\n",
      "Training Loss:  1.30336118\n",
      "Training Loss:  1.30336118\n",
      "Training Loss:  1.30097163\n",
      "Training Loss:  1.30097163\n",
      "Training Loss:  1.2910856\n",
      "Final training Loss:  1.2910856\n",
      "\n",
      "Running model (trial=2, mod=63, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30763865\n",
      "Training Loss:  1.30763865\n",
      "Training Loss:  1.30761254\n",
      "Training Loss:  1.30761266\n",
      "Training Loss:  1.30761266\n",
      "Training Loss:  1.30760932\n",
      "Training Loss:  1.30760932\n",
      "Training Loss:  1.30760932\n",
      "Training Loss:  1.30760932\n",
      "Training Loss:  1.30758119\n",
      "Final training Loss:  1.30758119\n",
      "\n",
      "Running model (trial=2, mod=64, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.201298\n",
      "Training Loss:  1.201298\n",
      "Training Loss:  1.201298\n",
      "Training Loss:  1.201298\n",
      "Training Loss:  1.201298\n",
      "Training Loss:  1.20129585\n",
      "Training Loss:  1.20129585\n",
      "Training Loss:  1.20129585\n",
      "Training Loss:  1.20129585\n",
      "Training Loss:  1.20129585\n",
      "Final training Loss:  1.20129585\n",
      "\n",
      "Running model (trial=2, mod=65, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70365185\n",
      "Training Loss:  0.7036494\n",
      "Training Loss:  0.70364344\n",
      "Training Loss:  0.70364344\n",
      "Training Loss:  0.70364344\n",
      "Training Loss:  0.70364344\n",
      "Training Loss:  0.70364344\n",
      "Training Loss:  0.70364344\n",
      "Training Loss:  0.70364344\n",
      "Training Loss:  0.70364344\n",
      "Final training Loss:  0.70364344\n",
      "\n",
      "Running model (trial=2, mod=66, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.54418051\n",
      "Training Loss:  0.5441792\n",
      "Training Loss:  0.51614523\n",
      "Training Loss:  0.51607233\n",
      "Training Loss:  0.51607233\n",
      "Training Loss:  0.51607233\n",
      "Training Loss:  0.51607233\n",
      "Training Loss:  0.51607233\n",
      "Training Loss:  0.51607233\n",
      "Training Loss:  0.51607233\n",
      "Final training Loss:  0.51607233\n",
      "\n",
      "Running model (trial=2, mod=67, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.32035708\n",
      "Training Loss:  1.29181993\n",
      "Training Loss:  1.29175198\n",
      "Training Loss:  1.25799084\n",
      "Training Loss:  1.25795841\n",
      "Training Loss:  1.25795841\n",
      "Training Loss:  1.25795841\n",
      "Training Loss:  1.25795841\n",
      "Training Loss:  1.25795841\n",
      "Training Loss:  1.25795841\n",
      "Final training Loss:  1.25795841\n",
      "\n",
      "Running model (trial=2, mod=68, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.43231201\n",
      "Training Loss:  599.43231201\n",
      "Training Loss:  599.43231201\n",
      "Training Loss:  1.85910511\n",
      "Training Loss:  1.85903823\n",
      "Training Loss:  1.85894489\n",
      "Training Loss:  1.85894489\n",
      "Training Loss:  1.85894489\n",
      "Training Loss:  1.85894489\n",
      "Training Loss:  1.85894489\n",
      "Final training Loss:  1.85894489\n",
      "\n",
      "Running model (trial=2, mod=69, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Final training Loss:  599.50390625\n",
      "\n",
      "Running model (trial=2, mod=70, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Training Loss:  2.15144253\n",
      "Final training Loss:  2.15144253\n",
      "\n",
      "Running model (trial=2, mod=71, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.12095749\n",
      "Training Loss:  1.12095749\n",
      "Training Loss:  1.12095749\n",
      "Training Loss:  1.12095749\n",
      "Training Loss:  1.12095666\n",
      "Training Loss:  1.12095666\n",
      "Training Loss:  1.12095571\n",
      "Training Loss:  1.12093377\n",
      "Training Loss:  1.12093258\n",
      "Training Loss:  1.12076354\n",
      "Final training Loss:  1.12076354\n",
      "\n",
      "Running model (trial=2, mod=72, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26965606\n",
      "Training Loss:  1.26965606\n",
      "Training Loss:  1.26963997\n",
      "Training Loss:  1.26963997\n",
      "Training Loss:  1.26963937\n",
      "Training Loss:  1.26963937\n",
      "Training Loss:  1.26963508\n",
      "Training Loss:  1.2409488\n",
      "Training Loss:  1.2409488\n",
      "Training Loss:  1.2409488\n",
      "Final training Loss:  1.2409488\n",
      "\n",
      "Running model (trial=2, mod=73, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24774659\n",
      "Training Loss:  1.24773955\n",
      "Final training Loss:  1.24773955\n",
      "\n",
      "Running model (trial=2, mod=74, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.77021229\n",
      "Training Loss:  1.77021229\n",
      "Training Loss:  1.77021229\n",
      "Training Loss:  1.77020013\n",
      "Training Loss:  1.77020013\n",
      "Training Loss:  1.77019989\n",
      "Training Loss:  1.45333338\n",
      "Training Loss:  1.26638985\n",
      "Training Loss:  1.26638985\n",
      "Training Loss:  1.26638985\n",
      "Final training Loss:  1.26638985\n",
      "\n",
      "Running model (trial=2, mod=75, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3613342\n",
      "Training Loss:  1.36132169\n",
      "Training Loss:  1.36132169\n",
      "Training Loss:  1.36132169\n",
      "Training Loss:  1.36132169\n",
      "Training Loss:  1.36132169\n",
      "Training Loss:  1.36132169\n",
      "Training Loss:  1.36132169\n",
      "Training Loss:  1.36132169\n",
      "Training Loss:  1.36132169\n",
      "Final training Loss:  1.36132169\n",
      "\n",
      "Running model (trial=2, mod=76, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30484092\n",
      "Training Loss:  1.30483294\n",
      "Training Loss:  1.30483294\n",
      "Training Loss:  1.30483294\n",
      "Training Loss:  1.30483294\n",
      "Training Loss:  1.30483294\n",
      "Training Loss:  1.30483294\n",
      "Training Loss:  1.30483294\n",
      "Training Loss:  1.30483294\n",
      "Training Loss:  1.30483294\n",
      "Final training Loss:  1.30483294\n",
      "\n",
      "Running model (trial=2, mod=77, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.40529239\n",
      "Training Loss:  1.40519929\n",
      "Training Loss:  1.40519929\n",
      "Training Loss:  1.40519869\n",
      "Training Loss:  1.40519667\n",
      "Training Loss:  1.40519667\n",
      "Training Loss:  1.40519524\n",
      "Training Loss:  1.40519524\n",
      "Training Loss:  1.40519989\n",
      "Training Loss:  1.40519524\n",
      "Final training Loss:  1.40519524\n",
      "\n",
      "Running model (trial=2, mod=78, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Training Loss:  1.31221187\n",
      "Final training Loss:  1.31221187\n",
      "\n",
      "Running model (trial=2, mod=79, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30601501\n",
      "Training Loss:  1.30601501\n",
      "Training Loss:  1.30601335\n",
      "Training Loss:  1.30601335\n",
      "Training Loss:  1.30601335\n",
      "Training Loss:  1.30601335\n",
      "Training Loss:  1.30601335\n",
      "Training Loss:  1.30601335\n",
      "Training Loss:  1.30601335\n",
      "Training Loss:  1.30601335\n",
      "Final training Loss:  1.30601335\n",
      "\n",
      "Running model (trial=2, mod=80, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.20349538\n",
      "Training Loss:  1.17835355\n",
      "Training Loss:  1.17835355\n",
      "Training Loss:  1.17835355\n",
      "Training Loss:  1.17835164\n",
      "Training Loss:  1.17835164\n",
      "Training Loss:  1.17835164\n",
      "Training Loss:  1.178339\n",
      "Training Loss:  1.178339\n",
      "Training Loss:  1.1751883\n",
      "Final training Loss:  1.1751883\n",
      "\n",
      "Running model (trial=2, mod=81, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22758973\n",
      "Training Loss:  1.22756469\n",
      "Training Loss:  1.22756469\n",
      "Training Loss:  1.22756469\n",
      "Training Loss:  1.22756469\n",
      "Training Loss:  1.22756469\n",
      "Training Loss:  1.22756469\n",
      "Training Loss:  1.22756469\n",
      "Training Loss:  1.22756469\n",
      "Training Loss:  1.22756469\n",
      "Final training Loss:  1.22756469\n",
      "\n",
      "Running model (trial=2, mod=82, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Training Loss:  1.26868761\n",
      "Final training Loss:  1.26868761\n",
      "\n",
      "Running model (trial=2, mod=83, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.39519492\n",
      "Training Loss:  0.39470845\n",
      "Training Loss:  0.39470845\n",
      "Training Loss:  0.39470822\n",
      "Training Loss:  0.39470816\n",
      "Training Loss:  0.39470822\n",
      "Training Loss:  0.39470822\n",
      "Training Loss:  0.39470822\n",
      "Training Loss:  0.39470822\n",
      "Training Loss:  0.39470822\n",
      "Final training Loss:  0.39470822\n",
      "\n",
      "Running model (trial=3, mod=84, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.32852042\n",
      "Training Loss:  1.32852042\n",
      "Training Loss:  1.32852042\n",
      "Training Loss:  1.32852042\n",
      "Training Loss:  1.32852042\n",
      "Training Loss:  1.32850134\n",
      "Training Loss:  1.32850134\n",
      "Training Loss:  1.328426\n",
      "Training Loss:  1.328426\n",
      "Training Loss:  1.328426\n",
      "Final training Loss:  1.328426\n",
      "\n",
      "Running model (trial=3, mod=85, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Training Loss:  1.1264447\n",
      "Final training Loss:  1.1264447\n",
      "\n",
      "Running model (trial=3, mod=86, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.52125323\n",
      "Training Loss:  0.52125311\n",
      "Training Loss:  0.47708377\n",
      "Training Loss:  0.47708377\n",
      "Training Loss:  0.47708377\n",
      "Training Loss:  0.47708377\n",
      "Training Loss:  0.39134037\n",
      "Training Loss:  0.39134037\n",
      "Training Loss:  0.39134037\n",
      "Training Loss:  0.39133799\n",
      "Final training Loss:  0.39133799\n",
      "\n",
      "Running model (trial=3, mod=87, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Training Loss:  1.23713911\n",
      "Final training Loss:  1.23713911\n",
      "\n",
      "Running model (trial=3, mod=88, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19738913\n",
      "Training Loss:  1.16370904\n",
      "Training Loss:  1.16370904\n",
      "Training Loss:  1.16370904\n",
      "Training Loss:  1.16370904\n",
      "Training Loss:  1.16370904\n",
      "Training Loss:  1.16370904\n",
      "Training Loss:  1.16370344\n",
      "Training Loss:  1.16370344\n",
      "Training Loss:  1.16370344\n",
      "Final training Loss:  1.16370344\n",
      "\n",
      "Running model (trial=3, mod=89, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.25752878\n",
      "Training Loss:  1.25752878\n",
      "Training Loss:  1.25751925\n",
      "Training Loss:  1.25662613\n",
      "Training Loss:  1.25662613\n",
      "Training Loss:  1.25662613\n",
      "Training Loss:  1.25662613\n",
      "Training Loss:  1.25662613\n",
      "Training Loss:  1.25662613\n",
      "Training Loss:  1.25662613\n",
      "Final training Loss:  1.25662613\n",
      "\n",
      "Running model (trial=3, mod=90, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23135948\n",
      "Training Loss:  1.23135948\n",
      "Training Loss:  1.23135948\n",
      "Training Loss:  1.23134923\n",
      "Training Loss:  1.23134923\n",
      "Training Loss:  1.23134923\n",
      "Training Loss:  1.23134875\n",
      "Training Loss:  1.23134875\n",
      "Training Loss:  1.23134875\n",
      "Training Loss:  1.23134422\n",
      "Final training Loss:  1.23134422\n",
      "\n",
      "Running model (trial=3, mod=91, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30913222\n",
      "Training Loss:  1.30907655\n",
      "Training Loss:  1.30907369\n",
      "Training Loss:  1.30906737\n",
      "Training Loss:  1.30906737\n",
      "Training Loss:  1.30906737\n",
      "Training Loss:  1.30906725\n",
      "Training Loss:  1.30906725\n",
      "Training Loss:  1.30906725\n",
      "Training Loss:  1.30906725\n",
      "Final training Loss:  1.30906725\n",
      "\n",
      "Running model (trial=3, mod=92, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18661618\n",
      "Training Loss:  1.18661618\n",
      "Training Loss:  1.18661618\n",
      "Training Loss:  1.18661618\n",
      "Training Loss:  1.18661618\n",
      "Training Loss:  1.18661618\n",
      "Training Loss:  1.18661582\n",
      "Training Loss:  1.18661201\n",
      "Training Loss:  1.18661201\n",
      "Training Loss:  1.18661201\n",
      "Final training Loss:  1.18661201\n",
      "\n",
      "Running model (trial=3, mod=93, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.42765909\n",
      "Training Loss:  0.42765909\n",
      "Training Loss:  0.42765909\n",
      "Training Loss:  0.42765909\n",
      "Training Loss:  0.42765909\n",
      "Training Loss:  0.42765909\n",
      "Training Loss:  0.42765909\n",
      "Training Loss:  0.42765597\n",
      "Training Loss:  0.42765597\n",
      "Training Loss:  0.42765597\n",
      "Final training Loss:  0.42765597\n",
      "\n",
      "Running model (trial=3, mod=94, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.25410748\n",
      "Training Loss:  1.25410748\n",
      "Training Loss:  1.19396865\n",
      "Training Loss:  1.19374013\n",
      "Training Loss:  1.19374013\n",
      "Training Loss:  1.18891132\n",
      "Training Loss:  1.18891132\n",
      "Training Loss:  1.18890917\n",
      "Training Loss:  1.18890917\n",
      "Training Loss:  1.18890917\n",
      "Final training Loss:  1.18890917\n",
      "\n",
      "Running model (trial=3, mod=95, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63536823\n",
      "Training Loss:  0.6248396\n",
      "Training Loss:  0.6248073\n",
      "Training Loss:  0.6248073\n",
      "Training Loss:  0.6248073\n",
      "Training Loss:  0.6248073\n",
      "Training Loss:  0.6248073\n",
      "Training Loss:  0.6248073\n",
      "Training Loss:  0.6248073\n",
      "Training Loss:  0.6248073\n",
      "Final training Loss:  0.6248073\n",
      "\n",
      "Running model (trial=3, mod=96, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  195.36186218\n",
      "Training Loss:  195.36186218\n",
      "Training Loss:  195.36100769\n",
      "Training Loss:  194.84596252\n",
      "Training Loss:  194.84596252\n",
      "Training Loss:  194.84596252\n",
      "Training Loss:  193.45123291\n",
      "Training Loss:  193.45123291\n",
      "Training Loss:  193.45123291\n",
      "Training Loss:  193.45123291\n",
      "Final training Loss:  193.45123291\n",
      "\n",
      "Running model (trial=3, mod=97, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Final training Loss:  599.50402832\n",
      "\n",
      "Running model (trial=3, mod=98, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3523519\n",
      "Training Loss:  1.2183919\n",
      "Training Loss:  1.2183919\n",
      "Training Loss:  1.2183919\n",
      "Training Loss:  1.2183919\n",
      "Training Loss:  1.21839118\n",
      "Training Loss:  1.21839118\n",
      "Training Loss:  1.21837878\n",
      "Training Loss:  1.21829522\n",
      "Training Loss:  1.218279\n",
      "Final training Loss:  1.218279\n",
      "\n",
      "Running model (trial=3, mod=99, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59601444\n",
      "Training Loss:  0.59601444\n",
      "Training Loss:  0.59601444\n",
      "Training Loss:  0.5960139\n",
      "Training Loss:  0.59599894\n",
      "Training Loss:  0.59599894\n",
      "Training Loss:  0.59599894\n",
      "Training Loss:  0.59599894\n",
      "Training Loss:  0.59599894\n",
      "Training Loss:  0.59599894\n",
      "Final training Loss:  0.59599894\n",
      "\n",
      "Running model (trial=3, mod=100, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30221009\n",
      "Training Loss:  1.22462249\n",
      "Training Loss:  1.22461677\n",
      "Training Loss:  1.22461677\n",
      "Training Loss:  1.22461677\n",
      "Training Loss:  1.22461283\n",
      "Training Loss:  1.22461283\n",
      "Training Loss:  1.22461283\n",
      "Training Loss:  1.22461283\n",
      "Training Loss:  1.22461283\n",
      "Final training Loss:  1.22461283\n",
      "\n",
      "Running model (trial=3, mod=101, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.33144987\n",
      "Training Loss:  1.33143866\n",
      "Training Loss:  1.33143866\n",
      "Training Loss:  1.33143866\n",
      "Training Loss:  1.33143866\n",
      "Training Loss:  1.33143866\n",
      "Training Loss:  1.33143866\n",
      "Training Loss:  1.33143866\n",
      "Training Loss:  1.33143866\n",
      "Training Loss:  1.33143866\n",
      "Final training Loss:  1.33143866\n",
      "\n",
      "Running model (trial=3, mod=102, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23389566\n",
      "Training Loss:  1.23389566\n",
      "Training Loss:  1.23389566\n",
      "Training Loss:  1.23388064\n",
      "Training Loss:  1.23388064\n",
      "Training Loss:  1.23388064\n",
      "Training Loss:  1.23388064\n",
      "Training Loss:  1.23388064\n",
      "Training Loss:  1.23388064\n",
      "Training Loss:  1.23388064\n",
      "Final training Loss:  1.23388064\n",
      "\n",
      "Running model (trial=3, mod=103, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23936236\n",
      "Training Loss:  1.23936236\n",
      "Training Loss:  1.23936296\n",
      "Training Loss:  1.2393564\n",
      "Training Loss:  1.2393564\n",
      "Training Loss:  1.2393564\n",
      "Training Loss:  1.2393564\n",
      "Training Loss:  1.2393564\n",
      "Training Loss:  1.2393564\n",
      "Training Loss:  1.2393564\n",
      "Final training Loss:  1.2393564\n",
      "\n",
      "Running model (trial=3, mod=104, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.32605565\n",
      "Training Loss:  1.32605565\n",
      "Training Loss:  1.32605541\n",
      "Training Loss:  1.32605565\n",
      "Training Loss:  1.32605565\n",
      "Training Loss:  1.32605445\n",
      "Training Loss:  1.32605445\n",
      "Training Loss:  1.32604933\n",
      "Training Loss:  1.32604933\n",
      "Training Loss:  1.32604933\n",
      "Final training Loss:  1.32604933\n",
      "\n",
      "Running model (trial=3, mod=105, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Training Loss:  1.3357234\n",
      "Final training Loss:  1.3357234\n",
      "\n",
      "Running model (trial=3, mod=106, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.2885288\n",
      "Training Loss:  1.28847682\n",
      "Training Loss:  1.28843796\n",
      "Training Loss:  1.28842854\n",
      "Training Loss:  1.28842854\n",
      "Training Loss:  1.28842854\n",
      "Training Loss:  1.28842854\n",
      "Training Loss:  1.28840375\n",
      "Training Loss:  1.28840375\n",
      "Training Loss:  1.28840375\n",
      "Final training Loss:  1.28840375\n",
      "\n",
      "Running model (trial=3, mod=107, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23720372\n",
      "Training Loss:  1.23720372\n",
      "Training Loss:  1.23720169\n",
      "Training Loss:  1.2096473\n",
      "Training Loss:  1.2096473\n",
      "Training Loss:  1.2096473\n",
      "Training Loss:  1.2096473\n",
      "Training Loss:  1.2096467\n",
      "Training Loss:  1.2096467\n",
      "Training Loss:  1.2096467\n",
      "Final training Loss:  1.2096467\n",
      "\n",
      "Running model (trial=3, mod=108, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.58805776\n",
      "Training Loss:  0.58805776\n",
      "Training Loss:  0.58805776\n",
      "Training Loss:  0.58805776\n",
      "Training Loss:  0.58805776\n",
      "Training Loss:  0.58805686\n",
      "Training Loss:  0.58805311\n",
      "Training Loss:  0.58804095\n",
      "Training Loss:  0.58804095\n",
      "Training Loss:  0.58804095\n",
      "Final training Loss:  0.58804095\n",
      "\n",
      "Running model (trial=3, mod=109, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.25059283\n",
      "Training Loss:  1.25059283\n",
      "Training Loss:  1.25059283\n",
      "Training Loss:  1.25059271\n",
      "Training Loss:  1.24786365\n",
      "Training Loss:  1.24785829\n",
      "Training Loss:  1.24783885\n",
      "Training Loss:  1.24783885\n",
      "Training Loss:  0.44381711\n",
      "Training Loss:  0.44381011\n",
      "Final training Loss:  0.44381011\n",
      "\n",
      "Running model (trial=3, mod=110, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.58009797\n",
      "Training Loss:  0.58007801\n",
      "Training Loss:  0.58007801\n",
      "Training Loss:  0.58007801\n",
      "Training Loss:  0.58007801\n",
      "Training Loss:  0.58007801\n",
      "Training Loss:  0.5800727\n",
      "Training Loss:  0.5800727\n",
      "Training Loss:  0.5800727\n",
      "Training Loss:  0.5800727\n",
      "Final training Loss:  0.5800727\n",
      "\n",
      "Running model (trial=3, mod=111, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.25661087\n",
      "Training Loss:  1.25653028\n",
      "Training Loss:  1.25653028\n",
      "Training Loss:  1.25653028\n",
      "Training Loss:  1.25653028\n",
      "Training Loss:  1.23175526\n",
      "Training Loss:  1.23175526\n",
      "Training Loss:  1.23175526\n",
      "Training Loss:  1.23176646\n",
      "Training Loss:  1.23175526\n",
      "Final training Loss:  1.23175526\n",
      "\n",
      "Running model (trial=4, mod=112, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26875782\n",
      "Training Loss:  1.26873887\n",
      "Training Loss:  1.26873231\n",
      "Training Loss:  1.26873231\n",
      "Training Loss:  1.26873231\n",
      "Training Loss:  1.26873231\n",
      "Training Loss:  1.26873147\n",
      "Training Loss:  1.26762247\n",
      "Training Loss:  1.26762247\n",
      "Training Loss:  1.26762247\n",
      "Final training Loss:  1.26762247\n",
      "\n",
      "Running model (trial=4, mod=113, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.273893\n",
      "Training Loss:  1.20288444\n",
      "Training Loss:  1.20288444\n",
      "Training Loss:  1.19278908\n",
      "Training Loss:  1.19278908\n",
      "Training Loss:  0.62918466\n",
      "Training Loss:  0.62918466\n",
      "Training Loss:  0.62918466\n",
      "Training Loss:  0.62918466\n",
      "Training Loss:  0.62918466\n",
      "Final training Loss:  0.62918466\n",
      "\n",
      "Running model (trial=4, mod=114, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65605223\n",
      "Training Loss:  0.59854168\n",
      "Training Loss:  0.49404642\n",
      "Training Loss:  0.46184695\n",
      "Training Loss:  0.46184695\n",
      "Training Loss:  0.46184513\n",
      "Training Loss:  0.46184075\n",
      "Training Loss:  0.46184075\n",
      "Training Loss:  0.46184075\n",
      "Training Loss:  0.42958486\n",
      "Final training Loss:  0.42958486\n",
      "\n",
      "Running model (trial=4, mod=115, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19619656\n",
      "Training Loss:  1.19619358\n",
      "Training Loss:  1.1961906\n",
      "Training Loss:  1.1961906\n",
      "Training Loss:  1.1961906\n",
      "Training Loss:  1.1961906\n",
      "Training Loss:  1.1961906\n",
      "Training Loss:  1.1961906\n",
      "Training Loss:  1.1961906\n",
      "Training Loss:  1.1961906\n",
      "Final training Loss:  1.1961906\n",
      "\n",
      "Running model (trial=4, mod=116, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.27422166\n",
      "Training Loss:  1.27422142\n",
      "Training Loss:  1.27422011\n",
      "Training Loss:  1.27421176\n",
      "Training Loss:  1.27421176\n",
      "Training Loss:  1.27421176\n",
      "Training Loss:  1.27421176\n",
      "Training Loss:  1.27421176\n",
      "Training Loss:  1.27421176\n",
      "Training Loss:  1.27421176\n",
      "Final training Loss:  1.27421176\n",
      "\n",
      "Running model (trial=4, mod=117, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.2419827\n",
      "Training Loss:  1.24197674\n",
      "Training Loss:  1.2419734\n",
      "Training Loss:  1.2419734\n",
      "Training Loss:  1.2419734\n",
      "Training Loss:  1.2419734\n",
      "Training Loss:  1.2419734\n",
      "Training Loss:  1.2419734\n",
      "Training Loss:  1.2419734\n",
      "Training Loss:  1.2419734\n",
      "Final training Loss:  1.2419734\n",
      "\n",
      "Running model (trial=4, mod=118, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.20826244\n",
      "Training Loss:  1.20824039\n",
      "Training Loss:  1.20824039\n",
      "Training Loss:  1.20824039\n",
      "Training Loss:  1.20823526\n",
      "Training Loss:  1.20822954\n",
      "Training Loss:  1.2082206\n",
      "Training Loss:  1.20821965\n",
      "Training Loss:  1.20821965\n",
      "Training Loss:  1.20821965\n",
      "Final training Loss:  1.20821965\n",
      "\n",
      "Running model (trial=4, mod=119, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29919934\n",
      "Training Loss:  1.29919934\n",
      "Training Loss:  1.29916811\n",
      "Training Loss:  1.29916239\n",
      "Training Loss:  1.29916239\n",
      "Training Loss:  1.29916239\n",
      "Training Loss:  1.2420119\n",
      "Training Loss:  1.24122143\n",
      "Training Loss:  1.24120224\n",
      "Training Loss:  1.24120224\n",
      "Final training Loss:  1.24120224\n",
      "\n",
      "Running model (trial=4, mod=120, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19400263\n",
      "Training Loss:  1.19400263\n",
      "Training Loss:  1.19400263\n",
      "Training Loss:  1.19399977\n",
      "Training Loss:  1.19399977\n",
      "Training Loss:  1.19399977\n",
      "Training Loss:  1.19399977\n",
      "Training Loss:  1.19399977\n",
      "Training Loss:  1.19399977\n",
      "Training Loss:  1.19399977\n",
      "Final training Loss:  1.19399977\n",
      "\n",
      "Running model (trial=4, mod=121, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Training Loss:  1.22241938\n",
      "Final training Loss:  1.22241938\n",
      "\n",
      "Running model (trial=4, mod=122, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.53932416\n",
      "Training Loss:  0.53898311\n",
      "Training Loss:  0.53898311\n",
      "Training Loss:  0.53898311\n",
      "Training Loss:  0.51972592\n",
      "Training Loss:  0.50835931\n",
      "Training Loss:  0.44745886\n",
      "Training Loss:  0.44745886\n",
      "Training Loss:  0.34976786\n",
      "Training Loss:  0.29965523\n",
      "Final training Loss:  0.29965523\n",
      "\n",
      "Running model (trial=4, mod=123, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26332426\n",
      "Training Loss:  1.25567794\n",
      "Training Loss:  1.25567198\n",
      "Training Loss:  1.25564504\n",
      "Training Loss:  1.25564504\n",
      "Training Loss:  1.25564504\n",
      "Training Loss:  1.25564504\n",
      "Training Loss:  1.25564504\n",
      "Training Loss:  1.25564504\n",
      "Training Loss:  1.25564504\n",
      "Final training Loss:  1.25564504\n",
      "\n",
      "Running model (trial=4, mod=124, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.41918945\n",
      "Training Loss:  599.41906738\n",
      "Training Loss:  599.41906738\n",
      "Training Loss:  599.41906738\n",
      "Training Loss:  599.41906738\n",
      "Training Loss:  599.41906738\n",
      "Training Loss:  599.41906738\n",
      "Training Loss:  599.41906738\n",
      "Training Loss:  599.41906738\n",
      "Training Loss:  599.41906738\n",
      "Final training Loss:  599.41906738\n",
      "\n",
      "Running model (trial=4, mod=125, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Final training Loss:  599.50378418\n",
      "\n",
      "Running model (trial=4, mod=126, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26977491\n",
      "Training Loss:  1.26421273\n",
      "Training Loss:  1.26421273\n",
      "Training Loss:  1.26421106\n",
      "Training Loss:  1.26421106\n",
      "Training Loss:  1.26421106\n",
      "Training Loss:  1.26421106\n",
      "Training Loss:  1.26421106\n",
      "Training Loss:  1.26421106\n",
      "Training Loss:  1.26421106\n",
      "Final training Loss:  1.26421106\n",
      "\n",
      "Running model (trial=4, mod=127, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18520844\n",
      "Training Loss:  1.18520844\n",
      "Training Loss:  1.18520844\n",
      "Training Loss:  1.185202\n",
      "Training Loss:  1.185202\n",
      "Training Loss:  1.185202\n",
      "Training Loss:  1.185202\n",
      "Training Loss:  1.185202\n",
      "Training Loss:  1.185202\n",
      "Training Loss:  1.185202\n",
      "Final training Loss:  1.185202\n",
      "\n",
      "Running model (trial=4, mod=128, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3857621\n",
      "Training Loss:  1.3857621\n",
      "Training Loss:  1.3857621\n",
      "Training Loss:  1.38576031\n",
      "Training Loss:  1.38576031\n",
      "Training Loss:  1.38576031\n",
      "Training Loss:  1.38576031\n",
      "Training Loss:  1.38576031\n",
      "Training Loss:  1.38576031\n",
      "Training Loss:  1.38576031\n",
      "Final training Loss:  1.38576031\n",
      "\n",
      "Running model (trial=4, mod=129, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.41309634\n",
      "Training Loss:  0.41309634\n",
      "Training Loss:  0.41309634\n",
      "Training Loss:  0.41309634\n",
      "Training Loss:  0.41309634\n",
      "Training Loss:  0.41309634\n",
      "Training Loss:  0.41308838\n",
      "Training Loss:  0.41308838\n",
      "Training Loss:  0.41308761\n",
      "Training Loss:  0.41308761\n",
      "Final training Loss:  0.41308761\n",
      "\n",
      "Running model (trial=4, mod=130, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29562783\n",
      "Training Loss:  1.29562771\n",
      "Training Loss:  1.29562283\n",
      "Training Loss:  1.29562223\n",
      "Training Loss:  1.29562223\n",
      "Training Loss:  1.295614\n",
      "Training Loss:  1.295614\n",
      "Training Loss:  1.295614\n",
      "Training Loss:  1.29561305\n",
      "Training Loss:  1.29558623\n",
      "Final training Loss:  1.29558623\n",
      "\n",
      "Running model (trial=4, mod=131, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3053776\n",
      "Training Loss:  1.3053776\n",
      "Training Loss:  1.3053776\n",
      "Training Loss:  1.3053776\n",
      "Training Loss:  1.3053776\n",
      "Training Loss:  1.3053776\n",
      "Training Loss:  1.3053776\n",
      "Training Loss:  1.30200231\n",
      "Training Loss:  1.30200231\n",
      "Training Loss:  1.30200231\n",
      "Final training Loss:  1.30200231\n",
      "\n",
      "Running model (trial=4, mod=132, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.31164837\n",
      "Training Loss:  1.31163502\n",
      "Training Loss:  1.31162977\n",
      "Training Loss:  1.31162977\n",
      "Training Loss:  1.31162977\n",
      "Training Loss:  1.31162977\n",
      "Training Loss:  1.31162977\n",
      "Training Loss:  1.31162977\n",
      "Training Loss:  1.31162977\n",
      "Training Loss:  1.31162977\n",
      "Final training Loss:  1.31162977\n",
      "\n",
      "Running model (trial=4, mod=133, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22853577\n",
      "Training Loss:  1.22851884\n",
      "Training Loss:  1.22851861\n",
      "Training Loss:  1.22851861\n",
      "Training Loss:  1.22851861\n",
      "Training Loss:  1.22850513\n",
      "Training Loss:  0.77167892\n",
      "Training Loss:  0.6260131\n",
      "Training Loss:  0.61556923\n",
      "Training Loss:  0.61556923\n",
      "Final training Loss:  0.61556923\n",
      "\n",
      "Running model (trial=4, mod=134, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59066993\n",
      "Training Loss:  0.59066993\n",
      "Training Loss:  0.59066993\n",
      "Training Loss:  0.59066993\n",
      "Training Loss:  0.59066993\n",
      "Training Loss:  0.59066892\n",
      "Training Loss:  0.59066892\n",
      "Training Loss:  0.59066886\n",
      "Training Loss:  0.59066886\n",
      "Training Loss:  0.59066886\n",
      "Final training Loss:  0.59066886\n",
      "\n",
      "Running model (trial=4, mod=135, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26622462\n",
      "Training Loss:  1.26622462\n",
      "Training Loss:  1.26622462\n",
      "Training Loss:  1.26620162\n",
      "Training Loss:  1.26620162\n",
      "Training Loss:  1.26620162\n",
      "Training Loss:  1.26620162\n",
      "Training Loss:  1.26620162\n",
      "Training Loss:  1.26620162\n",
      "Training Loss:  1.26620162\n",
      "Final training Loss:  1.26620162\n",
      "\n",
      "Running model (trial=4, mod=136, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.21689534\n",
      "Training Loss:  1.21689534\n",
      "Training Loss:  1.21689534\n",
      "Training Loss:  1.21689534\n",
      "Training Loss:  1.21689534\n",
      "Training Loss:  1.21687973\n",
      "Training Loss:  1.21687973\n",
      "Training Loss:  1.21687973\n",
      "Training Loss:  1.21687973\n",
      "Training Loss:  1.21687973\n",
      "Final training Loss:  1.21687973\n",
      "\n",
      "Running model (trial=4, mod=137, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24493229\n",
      "Training Loss:  1.24493229\n",
      "Training Loss:  1.24493229\n",
      "Training Loss:  1.24488354\n",
      "Training Loss:  1.24488354\n",
      "Training Loss:  1.24488354\n",
      "Training Loss:  1.24488354\n",
      "Training Loss:  1.24488354\n",
      "Training Loss:  1.24488354\n",
      "Training Loss:  1.24488354\n",
      "Final training Loss:  1.24488354\n",
      "\n",
      "Running model (trial=4, mod=138, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.58500022\n",
      "Training Loss:  0.3921639\n",
      "Training Loss:  0.3921639\n",
      "Training Loss:  0.3921639\n",
      "Training Loss:  0.3921639\n",
      "Training Loss:  0.3921639\n",
      "Training Loss:  0.3921639\n",
      "Training Loss:  0.3921639\n",
      "Training Loss:  0.3921639\n",
      "Training Loss:  0.3921639\n",
      "Final training Loss:  0.3921639\n",
      "\n",
      "Running model (trial=4, mod=139, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.16153097\n",
      "Training Loss:  1.16153097\n",
      "Training Loss:  1.16146183\n",
      "Training Loss:  1.16146183\n",
      "Training Loss:  1.16146183\n",
      "Training Loss:  1.16146183\n",
      "Training Loss:  1.16146183\n",
      "Training Loss:  1.16144454\n",
      "Training Loss:  1.16144443\n",
      "Training Loss:  1.16144454\n",
      "Final training Loss:  1.16144454\n",
      "\n",
      "Running model (trial=5, mod=140, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.96766663\n",
      "Training Loss:  14.967659\n",
      "Final training Loss:  14.967659\n",
      "\n",
      "Running model (trial=5, mod=141, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19707167\n",
      "Training Loss:  1.19705689\n",
      "Training Loss:  1.17962146\n",
      "Training Loss:  1.17962146\n",
      "Training Loss:  1.17962146\n",
      "Training Loss:  1.17962146\n",
      "Training Loss:  1.17962146\n",
      "Training Loss:  1.17962146\n",
      "Training Loss:  1.17962146\n",
      "Training Loss:  1.17962146\n",
      "Final training Loss:  1.17962146\n",
      "\n",
      "Running model (trial=5, mod=142, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.17147934\n",
      "Training Loss:  1.17147601\n",
      "Training Loss:  1.17147601\n",
      "Training Loss:  1.17147601\n",
      "Training Loss:  1.17147601\n",
      "Training Loss:  1.17147601\n",
      "Training Loss:  1.17147601\n",
      "Training Loss:  1.17147601\n",
      "Training Loss:  1.17147601\n",
      "Training Loss:  1.17147601\n",
      "Final training Loss:  1.17147601\n",
      "\n",
      "Running model (trial=5, mod=143, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6417954\n",
      "Training Loss:  0.6417954\n",
      "Training Loss:  0.64176857\n",
      "Training Loss:  0.6417675\n",
      "Training Loss:  0.6417675\n",
      "Training Loss:  0.6417675\n",
      "Training Loss:  0.61562288\n",
      "Training Loss:  0.61560911\n",
      "Training Loss:  0.58711493\n",
      "Training Loss:  0.58711493\n",
      "Final training Loss:  0.58711493\n",
      "\n",
      "Running model (trial=5, mod=144, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23961651\n",
      "Training Loss:  1.23961651\n",
      "Training Loss:  1.23961341\n",
      "Training Loss:  1.23960209\n",
      "Training Loss:  1.23959398\n",
      "Training Loss:  1.23959398\n",
      "Training Loss:  1.23959398\n",
      "Training Loss:  1.23959398\n",
      "Training Loss:  1.23959398\n",
      "Training Loss:  1.23959398\n",
      "Final training Loss:  1.23959398\n",
      "\n",
      "Running model (trial=5, mod=145, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.1706413\n",
      "Training Loss:  1.17060971\n",
      "Training Loss:  1.17039382\n",
      "Training Loss:  0.63540924\n",
      "Training Loss:  0.63540924\n",
      "Training Loss:  0.63540924\n",
      "Training Loss:  0.63540924\n",
      "Training Loss:  0.63537413\n",
      "Training Loss:  0.63537413\n",
      "Training Loss:  0.63537413\n",
      "Final training Loss:  0.63537413\n",
      "\n",
      "Running model (trial=5, mod=146, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.17013919\n",
      "Training Loss:  1.17013919\n",
      "Training Loss:  1.17013919\n",
      "Training Loss:  1.17013907\n",
      "Training Loss:  1.17013609\n",
      "Training Loss:  1.17013609\n",
      "Training Loss:  1.17013609\n",
      "Training Loss:  1.17013609\n",
      "Training Loss:  1.17013609\n",
      "Training Loss:  1.17013609\n",
      "Final training Loss:  1.17013609\n",
      "\n",
      "Running model (trial=5, mod=147, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Training Loss:  1.26807725\n",
      "Final training Loss:  1.26807725\n",
      "\n",
      "Running model (trial=5, mod=148, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19599819\n",
      "Training Loss:  1.19599819\n",
      "Training Loss:  1.1959976\n",
      "Training Loss:  1.1959976\n",
      "Training Loss:  1.1959976\n",
      "Training Loss:  1.1959976\n",
      "Training Loss:  1.1959976\n",
      "Training Loss:  1.1959976\n",
      "Training Loss:  1.1959976\n",
      "Training Loss:  1.1959976\n",
      "Final training Loss:  1.1959976\n",
      "\n",
      "Running model (trial=5, mod=149, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.15206718\n",
      "Training Loss:  1.15206492\n",
      "Training Loss:  1.15206492\n",
      "Training Loss:  1.15206492\n",
      "Training Loss:  1.15206492\n",
      "Training Loss:  1.15206492\n",
      "Training Loss:  1.15206289\n",
      "Training Loss:  1.15206289\n",
      "Training Loss:  1.15206289\n",
      "Training Loss:  1.15206289\n",
      "Final training Loss:  1.15206289\n",
      "\n",
      "Running model (trial=5, mod=150, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5356912\n",
      "Training Loss:  0.5356912\n",
      "Training Loss:  0.5356912\n",
      "Training Loss:  0.5356912\n",
      "Training Loss:  0.5356912\n",
      "Training Loss:  0.5356912\n",
      "Training Loss:  0.25940594\n",
      "Training Loss:  0.25939885\n",
      "Training Loss:  0.25939482\n",
      "Training Loss:  0.25939474\n",
      "Final training Loss:  0.25939474\n",
      "\n",
      "Running model (trial=5, mod=151, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Training Loss:  1.34779048\n",
      "Final training Loss:  1.34779048\n",
      "\n",
      "Running model (trial=5, mod=152, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59560359\n",
      "Training Loss:  0.59559822\n",
      "Training Loss:  0.59559822\n",
      "Training Loss:  0.59559822\n",
      "Training Loss:  0.59559822\n",
      "Training Loss:  0.59559822\n",
      "Training Loss:  0.59559822\n",
      "Training Loss:  0.59559822\n",
      "Training Loss:  0.59559822\n",
      "Training Loss:  0.59559822\n",
      "Final training Loss:  0.59559822\n",
      "\n",
      "Running model (trial=5, mod=153, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Final training Loss:  599.50384521\n",
      "\n",
      "Running model (trial=5, mod=154, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.94718242\n",
      "Training Loss:  2.94694495\n",
      "Training Loss:  1.85332561\n",
      "Training Loss:  1.85332561\n",
      "Training Loss:  1.71716237\n",
      "Training Loss:  1.71716237\n",
      "Training Loss:  1.71716237\n",
      "Training Loss:  1.71714902\n",
      "Training Loss:  1.71714902\n",
      "Training Loss:  1.71714902\n",
      "Final training Loss:  1.71714902\n",
      "\n",
      "Running model (trial=5, mod=155, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66274375\n",
      "Training Loss:  0.66273987\n",
      "Training Loss:  0.62645465\n",
      "Training Loss:  0.62643772\n",
      "Training Loss:  0.62642521\n",
      "Training Loss:  0.62642521\n",
      "Training Loss:  0.62642521\n",
      "Training Loss:  0.62642521\n",
      "Training Loss:  0.62642521\n",
      "Training Loss:  0.62642521\n",
      "Final training Loss:  0.62642521\n",
      "\n",
      "Running model (trial=5, mod=156, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.56803161\n",
      "Training Loss:  0.56803161\n",
      "Training Loss:  0.56232411\n",
      "Training Loss:  0.5621562\n",
      "Training Loss:  0.5621562\n",
      "Training Loss:  0.5621562\n",
      "Training Loss:  0.5621562\n",
      "Training Loss:  0.5621562\n",
      "Training Loss:  0.5621382\n",
      "Training Loss:  0.5621382\n",
      "Final training Loss:  0.5621382\n",
      "\n",
      "Running model (trial=5, mod=157, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  82.39235687\n",
      "Training Loss:  1.2899946\n",
      "Training Loss:  1.2899946\n",
      "Training Loss:  1.2899946\n",
      "Training Loss:  1.2899946\n",
      "Training Loss:  1.2899946\n",
      "Training Loss:  1.2899946\n",
      "Training Loss:  1.2899946\n",
      "Training Loss:  1.2899946\n",
      "Training Loss:  1.2899946\n",
      "Final training Loss:  1.2899946\n",
      "\n",
      "Running model (trial=5, mod=158, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.21543133\n",
      "Training Loss:  1.21543133\n",
      "Training Loss:  1.21490693\n",
      "Training Loss:  1.21487856\n",
      "Training Loss:  1.21486676\n",
      "Training Loss:  1.21486676\n",
      "Training Loss:  1.21484005\n",
      "Training Loss:  1.21484005\n",
      "Training Loss:  1.18811285\n",
      "Training Loss:  1.18811285\n",
      "Final training Loss:  1.18811285\n",
      "\n",
      "Running model (trial=5, mod=159, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29109621\n",
      "Training Loss:  1.29109621\n",
      "Training Loss:  1.29108965\n",
      "Training Loss:  1.29108965\n",
      "Training Loss:  1.29108965\n",
      "Training Loss:  1.29108059\n",
      "Training Loss:  1.29108059\n",
      "Training Loss:  1.29108059\n",
      "Training Loss:  1.29108059\n",
      "Training Loss:  1.29108059\n",
      "Final training Loss:  1.29108059\n",
      "\n",
      "Running model (trial=5, mod=160, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Training Loss:  1.28165042\n",
      "Final training Loss:  1.28165042\n",
      "\n",
      "Running model (trial=5, mod=161, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Training Loss:  1.31508195\n",
      "Final training Loss:  1.31508195\n",
      "\n",
      "Running model (trial=5, mod=162, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.5247916\n",
      "Training Loss:  1.5247916\n",
      "Training Loss:  1.52085209\n",
      "Training Loss:  1.52081072\n",
      "Training Loss:  1.52081072\n",
      "Training Loss:  1.52081072\n",
      "Training Loss:  1.52081072\n",
      "Training Loss:  1.52081072\n",
      "Training Loss:  1.52081072\n",
      "Training Loss:  1.52080011\n",
      "Final training Loss:  1.52080011\n",
      "\n",
      "Running model (trial=5, mod=163, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.2765094\n",
      "Training Loss:  1.2765094\n",
      "Training Loss:  1.2765094\n",
      "Training Loss:  1.2765094\n",
      "Training Loss:  1.2765094\n",
      "Training Loss:  1.2765094\n",
      "Training Loss:  1.22850966\n",
      "Training Loss:  1.22847176\n",
      "Training Loss:  1.22847176\n",
      "Training Loss:  1.22847176\n",
      "Final training Loss:  1.22847176\n",
      "\n",
      "Running model (trial=5, mod=164, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24518108\n",
      "Training Loss:  1.24517417\n",
      "Training Loss:  1.24517417\n",
      "Training Loss:  1.24517417\n",
      "Training Loss:  1.24517417\n",
      "Training Loss:  1.24517417\n",
      "Training Loss:  1.24517417\n",
      "Training Loss:  1.24517417\n",
      "Training Loss:  1.24517417\n",
      "Training Loss:  1.24517417\n",
      "Final training Loss:  1.24517417\n",
      "\n",
      "Running model (trial=5, mod=165, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.220716\n",
      "Training Loss:  1.21788526\n",
      "Training Loss:  1.21788526\n",
      "Training Loss:  0.59911078\n",
      "Training Loss:  0.59911078\n",
      "Training Loss:  0.59911078\n",
      "Training Loss:  0.59911078\n",
      "Training Loss:  0.59910882\n",
      "Training Loss:  0.59904838\n",
      "Training Loss:  0.5512597\n",
      "Final training Loss:  0.5512597\n",
      "\n",
      "Running model (trial=5, mod=166, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Training Loss:  0.59636599\n",
      "Final training Loss:  0.59636599\n",
      "\n",
      "Running model (trial=5, mod=167, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.21543026\n",
      "Training Loss:  1.19301856\n",
      "Training Loss:  1.19301856\n",
      "Training Loss:  1.19301856\n",
      "Training Loss:  1.19301856\n",
      "Training Loss:  1.19300258\n",
      "Training Loss:  1.19300258\n",
      "Training Loss:  1.19300258\n",
      "Training Loss:  1.19300258\n",
      "Training Loss:  1.19300258\n",
      "Final training Loss:  1.19300258\n",
      "\n",
      "Running model (trial=6, mod=168, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.64088488\n",
      "Training Loss:  1.64088488\n",
      "Training Loss:  1.64088118\n",
      "Training Loss:  1.64087629\n",
      "Training Loss:  1.64087629\n",
      "Training Loss:  1.64086807\n",
      "Training Loss:  1.63986397\n",
      "Training Loss:  1.63953972\n",
      "Training Loss:  1.60215664\n",
      "Training Loss:  1.38825715\n",
      "Final training Loss:  1.38825715\n",
      "\n",
      "Running model (trial=6, mod=169, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22032678\n",
      "Training Loss:  1.22032034\n",
      "Training Loss:  1.22032034\n",
      "Training Loss:  1.22031641\n",
      "Training Loss:  1.22031641\n",
      "Training Loss:  1.22031415\n",
      "Training Loss:  1.22031415\n",
      "Training Loss:  1.22031415\n",
      "Training Loss:  1.22031415\n",
      "Training Loss:  1.22031415\n",
      "Final training Loss:  1.22031415\n",
      "\n",
      "Running model (trial=6, mod=170, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23615694\n",
      "Training Loss:  1.23615694\n",
      "Training Loss:  1.23615694\n",
      "Training Loss:  1.23615432\n",
      "Training Loss:  1.23615432\n",
      "Training Loss:  1.23615432\n",
      "Training Loss:  1.23615432\n",
      "Training Loss:  1.23615432\n",
      "Training Loss:  1.23615432\n",
      "Training Loss:  1.23615432\n",
      "Final training Loss:  1.23615432\n",
      "\n",
      "Running model (trial=6, mod=171, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.49411052\n",
      "Training Loss:  0.49409154\n",
      "Training Loss:  0.49406332\n",
      "Training Loss:  0.49406332\n",
      "Training Loss:  0.49406332\n",
      "Training Loss:  0.49406332\n",
      "Training Loss:  0.49406332\n",
      "Training Loss:  0.49406332\n",
      "Training Loss:  0.49406332\n",
      "Training Loss:  0.49406332\n",
      "Final training Loss:  0.49406332\n",
      "\n",
      "Running model (trial=6, mod=172, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7109164\n",
      "Training Loss:  0.7109164\n",
      "Training Loss:  0.7109164\n",
      "Training Loss:  0.7109164\n",
      "Training Loss:  0.71091539\n",
      "Training Loss:  0.71090823\n",
      "Training Loss:  0.71090823\n",
      "Training Loss:  0.7109068\n",
      "Training Loss:  0.71089995\n",
      "Training Loss:  0.71089995\n",
      "Final training Loss:  0.71089995\n",
      "\n",
      "Running model (trial=6, mod=173, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.38131523\n",
      "Training Loss:  1.2766819\n",
      "Final training Loss:  1.2766819\n",
      "\n",
      "Running model (trial=6, mod=174, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29081321\n",
      "Training Loss:  1.29075861\n",
      "Training Loss:  0.67633218\n",
      "Training Loss:  0.67633218\n",
      "Training Loss:  0.67633218\n",
      "Training Loss:  0.67633218\n",
      "Training Loss:  0.62568194\n",
      "Training Loss:  0.62568194\n",
      "Training Loss:  0.62568194\n",
      "Training Loss:  0.62568194\n",
      "Final training Loss:  0.62568194\n",
      "\n",
      "Running model (trial=6, mod=175, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30905044\n",
      "Training Loss:  1.30905044\n",
      "Training Loss:  1.30905044\n",
      "Training Loss:  1.30900705\n",
      "Training Loss:  1.30900407\n",
      "Training Loss:  1.30900407\n",
      "Training Loss:  1.30900407\n",
      "Training Loss:  1.30900407\n",
      "Training Loss:  1.30898941\n",
      "Training Loss:  1.30898941\n",
      "Final training Loss:  1.30898941\n",
      "\n",
      "Running model (trial=6, mod=176, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.27151489\n",
      "Training Loss:  1.27150524\n",
      "Training Loss:  1.27150249\n",
      "Training Loss:  1.27150249\n",
      "Training Loss:  1.27150249\n",
      "Training Loss:  1.27150249\n",
      "Training Loss:  1.27150249\n",
      "Training Loss:  1.27150249\n",
      "Training Loss:  1.27150249\n",
      "Training Loss:  1.27150249\n",
      "Final training Loss:  1.27150249\n",
      "\n",
      "Running model (trial=6, mod=177, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19065952\n",
      "Training Loss:  1.19065952\n",
      "Training Loss:  1.19063985\n",
      "Training Loss:  1.19063985\n",
      "Training Loss:  1.19063985\n",
      "Training Loss:  1.19063985\n",
      "Training Loss:  1.19063985\n",
      "Training Loss:  1.19063985\n",
      "Training Loss:  1.19063985\n",
      "Training Loss:  1.19063985\n",
      "Final training Loss:  1.19063985\n",
      "\n",
      "Running model (trial=6, mod=178, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22151899\n",
      "Training Loss:  1.22151327\n",
      "Training Loss:  1.22151327\n",
      "Training Loss:  1.22151327\n",
      "Training Loss:  1.22151327\n",
      "Training Loss:  1.22151327\n",
      "Training Loss:  1.22151327\n",
      "Training Loss:  1.22151327\n",
      "Training Loss:  1.22151327\n",
      "Training Loss:  1.22151327\n",
      "Final training Loss:  1.22151327\n",
      "\n",
      "Running model (trial=6, mod=179, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.21888566\n",
      "Training Loss:  1.21888566\n",
      "Training Loss:  1.2188797\n",
      "Training Loss:  1.21885538\n",
      "Training Loss:  1.2112453\n",
      "Training Loss:  1.20854604\n",
      "Training Loss:  1.20852828\n",
      "Training Loss:  1.20852828\n",
      "Training Loss:  1.20852828\n",
      "Training Loss:  1.20852828\n",
      "Final training Loss:  1.20852828\n",
      "\n",
      "Running model (trial=6, mod=180, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.00731421\n",
      "Training Loss:  1.61637962\n",
      "Training Loss:  1.61632609\n",
      "Training Loss:  1.61583447\n",
      "Training Loss:  1.61581337\n",
      "Training Loss:  1.61581337\n",
      "Training Loss:  1.61581337\n",
      "Training Loss:  1.61581337\n",
      "Training Loss:  1.61581337\n",
      "Training Loss:  1.61581337\n",
      "Final training Loss:  1.61581337\n",
      "\n",
      "Running model (trial=6, mod=181, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50408936\n",
      "Training Loss:  599.50408936\n",
      "Training Loss:  599.50408936\n",
      "Training Loss:  599.50415039\n",
      "Training Loss:  599.50408936\n",
      "Training Loss:  599.50408936\n",
      "Training Loss:  599.50408936\n",
      "Training Loss:  599.50408936\n",
      "Training Loss:  599.50408936\n",
      "Training Loss:  599.50408936\n",
      "Final training Loss:  599.50408936\n",
      "\n",
      "Running model (trial=6, mod=182, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.04263639\n",
      "Training Loss:  2.04263639\n",
      "Training Loss:  2.04263473\n",
      "Training Loss:  2.04257989\n",
      "Training Loss:  2.04257989\n",
      "Training Loss:  2.04257989\n",
      "Training Loss:  2.04257989\n",
      "Training Loss:  2.04257989\n",
      "Training Loss:  2.04257989\n",
      "Training Loss:  2.04257989\n",
      "Final training Loss:  2.04257989\n",
      "\n",
      "Running model (trial=6, mod=183, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30283296\n",
      "Training Loss:  1.30283296\n",
      "Training Loss:  1.30283153\n",
      "Training Loss:  1.30283153\n",
      "Training Loss:  1.30282724\n",
      "Training Loss:  1.30282724\n",
      "Training Loss:  1.30282724\n",
      "Training Loss:  1.30282724\n",
      "Training Loss:  1.27640784\n",
      "Training Loss:  1.19964683\n",
      "Final training Loss:  1.19964683\n",
      "\n",
      "Running model (trial=6, mod=184, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28498077\n",
      "Training Loss:  1.225618\n",
      "Training Loss:  1.22559524\n",
      "Training Loss:  1.22559524\n",
      "Training Loss:  1.19507229\n",
      "Training Loss:  1.19507122\n",
      "Training Loss:  1.19506705\n",
      "Training Loss:  1.19506609\n",
      "Training Loss:  1.19506609\n",
      "Training Loss:  1.19506609\n",
      "Final training Loss:  1.19506609\n",
      "\n",
      "Running model (trial=6, mod=185, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.20983505\n",
      "Training Loss:  1.19542158\n",
      "Training Loss:  1.19542158\n",
      "Training Loss:  1.19542158\n",
      "Training Loss:  1.19542158\n",
      "Training Loss:  1.19542158\n",
      "Training Loss:  1.19542158\n",
      "Training Loss:  1.19542158\n",
      "Training Loss:  1.19542158\n",
      "Training Loss:  1.19542158\n",
      "Final training Loss:  1.19542158\n",
      "\n",
      "Running model (trial=6, mod=186, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29199815\n",
      "Training Loss:  1.29198623\n",
      "Training Loss:  1.29198623\n",
      "Training Loss:  1.29198623\n",
      "Training Loss:  1.29198623\n",
      "Training Loss:  1.29198623\n",
      "Training Loss:  1.27721131\n",
      "Training Loss:  1.27721131\n",
      "Training Loss:  1.27721131\n",
      "Training Loss:  1.27721131\n",
      "Final training Loss:  1.27721131\n",
      "\n",
      "Running model (trial=6, mod=187, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.32432508\n",
      "Training Loss:  1.32432389\n",
      "Training Loss:  1.32432389\n",
      "Training Loss:  1.32432389\n",
      "Training Loss:  1.32432389\n",
      "Training Loss:  1.32431459\n",
      "Training Loss:  1.32431459\n",
      "Training Loss:  1.32431459\n",
      "Training Loss:  1.32431459\n",
      "Training Loss:  1.32431459\n",
      "Final training Loss:  1.32431459\n",
      "\n",
      "Running model (trial=6, mod=188, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.32793665\n",
      "Training Loss:  1.30922461\n",
      "Training Loss:  1.30922461\n",
      "Training Loss:  1.30922461\n",
      "Training Loss:  1.29498279\n",
      "Training Loss:  1.29498279\n",
      "Training Loss:  1.29498279\n",
      "Training Loss:  1.29498279\n",
      "Training Loss:  1.29498279\n",
      "Training Loss:  1.29498279\n",
      "Final training Loss:  1.29498279\n",
      "\n",
      "Running model (trial=6, mod=189, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.25557613\n",
      "Training Loss:  1.25557613\n",
      "Training Loss:  1.25557613\n",
      "Training Loss:  1.25557613\n",
      "Training Loss:  1.25557613\n",
      "Training Loss:  1.25557613\n",
      "Training Loss:  1.25262105\n",
      "Training Loss:  1.25244558\n",
      "Training Loss:  1.25244558\n",
      "Training Loss:  1.25244558\n",
      "Final training Loss:  1.25244558\n",
      "\n",
      "Running model (trial=6, mod=190, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29664981\n",
      "Training Loss:  1.24245811\n",
      "Training Loss:  1.23813021\n",
      "Training Loss:  1.23813021\n",
      "Training Loss:  1.23810077\n",
      "Training Loss:  1.23806322\n",
      "Training Loss:  1.23793244\n",
      "Training Loss:  1.23793244\n",
      "Training Loss:  1.23793244\n",
      "Training Loss:  1.23793244\n",
      "Final training Loss:  1.23793244\n",
      "\n",
      "Running model (trial=6, mod=191, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.41737401\n",
      "Training Loss:  1.41737401\n",
      "Training Loss:  1.41737401\n",
      "Training Loss:  1.27406156\n",
      "Training Loss:  1.27406156\n",
      "Training Loss:  1.27405572\n",
      "Training Loss:  1.27405572\n",
      "Training Loss:  1.27405572\n",
      "Training Loss:  1.27405572\n",
      "Training Loss:  1.27405572\n",
      "Final training Loss:  1.27405572\n",
      "\n",
      "Running model (trial=6, mod=192, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24717498\n",
      "Training Loss:  1.24711037\n",
      "Training Loss:  1.24711037\n",
      "Training Loss:  1.24711037\n",
      "Training Loss:  1.247105\n",
      "Training Loss:  1.247105\n",
      "Training Loss:  1.247105\n",
      "Training Loss:  1.247105\n",
      "Training Loss:  1.24710488\n",
      "Training Loss:  1.24710488\n",
      "Final training Loss:  1.24710488\n",
      "\n",
      "Running model (trial=6, mod=193, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18099272\n",
      "Training Loss:  1.18099272\n",
      "Training Loss:  1.18099153\n",
      "Training Loss:  1.18096435\n",
      "Training Loss:  1.18096435\n",
      "Training Loss:  1.18096197\n",
      "Training Loss:  0.94254333\n",
      "Training Loss:  0.94254005\n",
      "Training Loss:  0.94252974\n",
      "Training Loss:  0.94252974\n",
      "Final training Loss:  0.94252974\n",
      "\n",
      "Running model (trial=6, mod=194, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18714619\n",
      "Training Loss:  1.18712568\n",
      "Training Loss:  1.18711734\n",
      "Training Loss:  1.18710685\n",
      "Training Loss:  1.18709433\n",
      "Training Loss:  1.18709433\n",
      "Training Loss:  1.18709433\n",
      "Training Loss:  1.18709433\n",
      "Training Loss:  1.18709433\n",
      "Training Loss:  1.18709433\n",
      "Final training Loss:  1.18709433\n",
      "\n",
      "Running model (trial=6, mod=195, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.53672338\n",
      "Training Loss:  0.41422996\n",
      "Training Loss:  0.41422996\n",
      "Training Loss:  0.41422996\n",
      "Training Loss:  0.41422996\n",
      "Training Loss:  0.41422996\n",
      "Training Loss:  0.41422996\n",
      "Training Loss:  0.41422996\n",
      "Training Loss:  0.41422996\n",
      "Training Loss:  0.41422996\n",
      "Final training Loss:  0.41422996\n",
      "\n",
      "Running model (trial=7, mod=196, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.10679483\n",
      "Training Loss:  1.10679483\n",
      "Training Loss:  1.10679364\n",
      "Training Loss:  1.10679364\n",
      "Training Loss:  1.10679364\n",
      "Training Loss:  1.10679364\n",
      "Training Loss:  1.10679364\n",
      "Training Loss:  1.10679364\n",
      "Training Loss:  1.10679364\n",
      "Training Loss:  1.10679364\n",
      "Final training Loss:  1.10679364\n",
      "\n",
      "Running model (trial=7, mod=197, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30389094\n",
      "Training Loss:  1.30388772\n",
      "Training Loss:  1.30388296\n",
      "Training Loss:  1.30388296\n",
      "Training Loss:  1.30388296\n",
      "Training Loss:  1.30388296\n",
      "Training Loss:  1.30388296\n",
      "Training Loss:  1.25559807\n",
      "Training Loss:  1.2548703\n",
      "Training Loss:  1.2548672\n",
      "Final training Loss:  1.2548672\n",
      "\n",
      "Running model (trial=7, mod=198, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26312196\n",
      "Training Loss:  1.17285061\n",
      "Training Loss:  1.1728456\n",
      "Training Loss:  1.1728456\n",
      "Training Loss:  1.17284536\n",
      "Training Loss:  1.17216814\n",
      "Training Loss:  1.1721679\n",
      "Training Loss:  1.17216527\n",
      "Training Loss:  1.17214561\n",
      "Training Loss:  1.17214561\n",
      "Final training Loss:  1.17214561\n",
      "\n",
      "Running model (trial=7, mod=199, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62223017\n",
      "Training Loss:  0.62223017\n",
      "Training Loss:  0.62223017\n",
      "Training Loss:  0.58897471\n",
      "Training Loss:  0.57151139\n",
      "Training Loss:  0.5714727\n",
      "Training Loss:  0.56346524\n",
      "Training Loss:  0.56346524\n",
      "Training Loss:  0.56346524\n",
      "Training Loss:  0.56346524\n",
      "Final training Loss:  0.56346524\n",
      "\n",
      "Running model (trial=7, mod=200, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.21107459\n",
      "Training Loss:  1.21106219\n",
      "Training Loss:  1.21106219\n",
      "Training Loss:  1.1980269\n",
      "Training Loss:  1.19802451\n",
      "Training Loss:  1.19802451\n",
      "Training Loss:  1.19802225\n",
      "Training Loss:  1.19802225\n",
      "Training Loss:  1.19802225\n",
      "Training Loss:  1.19802225\n",
      "Final training Loss:  1.19802225\n",
      "\n",
      "Running model (trial=7, mod=201, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22934937\n",
      "Training Loss:  1.22895455\n",
      "Training Loss:  1.22895455\n",
      "Training Loss:  1.22895455\n",
      "Training Loss:  1.22895455\n",
      "Training Loss:  1.22895455\n",
      "Training Loss:  1.22895455\n",
      "Training Loss:  1.22895455\n",
      "Training Loss:  1.22895455\n",
      "Training Loss:  1.22895455\n",
      "Final training Loss:  1.22895455\n",
      "\n",
      "Running model (trial=7, mod=202, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.40293813\n",
      "Training Loss:  1.40293813\n",
      "Training Loss:  1.40293813\n",
      "Training Loss:  1.40293813\n",
      "Training Loss:  1.40293813\n",
      "Training Loss:  1.40293813\n",
      "Training Loss:  1.40293813\n",
      "Training Loss:  1.2679534\n",
      "Training Loss:  1.2679534\n",
      "Training Loss:  1.2679534\n",
      "Final training Loss:  1.2679534\n",
      "\n",
      "Running model (trial=7, mod=203, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28913307\n",
      "Training Loss:  1.28913307\n",
      "Training Loss:  1.28912783\n",
      "Training Loss:  1.28898931\n",
      "Training Loss:  1.28897953\n",
      "Training Loss:  1.28897953\n",
      "Training Loss:  1.28896546\n",
      "Training Loss:  1.28896546\n",
      "Training Loss:  1.28896546\n",
      "Training Loss:  1.28893888\n",
      "Final training Loss:  1.28893888\n",
      "\n",
      "Running model (trial=7, mod=204, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.25675929\n",
      "Training Loss:  1.25675929\n",
      "Training Loss:  1.2532475\n",
      "Training Loss:  1.25324559\n",
      "Training Loss:  1.25324357\n",
      "Training Loss:  1.25324357\n",
      "Training Loss:  1.25323629\n",
      "Training Loss:  1.25323629\n",
      "Training Loss:  1.2532326\n",
      "Training Loss:  1.2532326\n",
      "Final training Loss:  1.2532326\n",
      "\n",
      "Running model (trial=7, mod=205, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24572504\n",
      "Training Loss:  1.24572504\n",
      "Training Loss:  1.24572504\n",
      "Training Loss:  1.24572504\n",
      "Training Loss:  1.24572504\n",
      "Training Loss:  1.24572504\n",
      "Training Loss:  1.24572504\n",
      "Training Loss:  1.24572504\n",
      "Training Loss:  1.24571943\n",
      "Training Loss:  1.24571657\n",
      "Final training Loss:  1.24571657\n",
      "\n",
      "Running model (trial=7, mod=206, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.57979625\n",
      "Training Loss:  0.57979625\n",
      "Training Loss:  0.56462926\n",
      "Training Loss:  0.56462926\n",
      "Training Loss:  0.56462926\n",
      "Training Loss:  0.56462926\n",
      "Training Loss:  0.56462926\n",
      "Training Loss:  0.56462926\n",
      "Training Loss:  0.56462926\n",
      "Training Loss:  0.56462663\n",
      "Final training Loss:  0.56462663\n",
      "\n",
      "Running model (trial=7, mod=207, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60084856\n",
      "Training Loss:  0.60083908\n",
      "Training Loss:  0.46537957\n",
      "Training Loss:  0.34353188\n",
      "Training Loss:  0.34353188\n",
      "Training Loss:  0.34353188\n",
      "Training Loss:  0.34353188\n",
      "Training Loss:  0.34352258\n",
      "Training Loss:  0.34352258\n",
      "Training Loss:  0.34352258\n",
      "Final training Loss:  0.34352258\n",
      "\n",
      "Running model (trial=7, mod=208, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  104.15194702\n",
      "Training Loss:  104.15194702\n",
      "Training Loss:  104.15194702\n",
      "Training Loss:  104.15184021\n",
      "Training Loss:  104.15184021\n",
      "Training Loss:  104.15184021\n",
      "Training Loss:  104.15184021\n",
      "Training Loss:  104.15184021\n",
      "Training Loss:  104.15184021\n",
      "Training Loss:  104.15171814\n",
      "Final training Loss:  104.15171814\n",
      "\n",
      "Running model (trial=7, mod=209, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Training Loss:  599.50384521\n",
      "Final training Loss:  599.50384521\n",
      "\n",
      "Running model (trial=7, mod=210, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Training Loss:  1.56201184\n",
      "Final training Loss:  1.56201184\n",
      "\n",
      "Running model (trial=7, mod=211, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24670899\n",
      "Training Loss:  1.24664974\n",
      "Training Loss:  1.24664843\n",
      "Training Loss:  1.24664736\n",
      "Training Loss:  1.24664736\n",
      "Training Loss:  1.24664736\n",
      "Training Loss:  1.24664736\n",
      "Training Loss:  1.24664736\n",
      "Training Loss:  1.24664736\n",
      "Training Loss:  1.24664736\n",
      "Final training Loss:  1.24664736\n",
      "\n",
      "Running model (trial=7, mod=212, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67042321\n",
      "Training Loss:  0.67042321\n",
      "Training Loss:  0.67042321\n",
      "Training Loss:  0.67042321\n",
      "Training Loss:  0.66635931\n",
      "Training Loss:  0.66635931\n",
      "Training Loss:  0.66635931\n",
      "Training Loss:  0.66635931\n",
      "Training Loss:  0.66635931\n",
      "Training Loss:  0.66635931\n",
      "Final training Loss:  0.66635931\n",
      "\n",
      "Running model (trial=7, mod=213, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29836977\n",
      "Training Loss:  1.29836977\n",
      "Training Loss:  1.29836977\n",
      "Training Loss:  1.2983526\n",
      "Training Loss:  1.29834819\n",
      "Training Loss:  1.29834819\n",
      "Training Loss:  1.29834104\n",
      "Training Loss:  1.29833877\n",
      "Training Loss:  1.29833877\n",
      "Training Loss:  1.29833877\n",
      "Final training Loss:  1.29833877\n",
      "\n",
      "Running model (trial=7, mod=214, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3021189\n",
      "Training Loss:  1.3020817\n",
      "Training Loss:  1.3020817\n",
      "Training Loss:  1.3020817\n",
      "Training Loss:  1.3020817\n",
      "Training Loss:  1.3020817\n",
      "Training Loss:  1.3020817\n",
      "Training Loss:  1.3020817\n",
      "Training Loss:  1.3020817\n",
      "Training Loss:  1.3020817\n",
      "Final training Loss:  1.3020817\n",
      "\n",
      "Running model (trial=7, mod=215, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.35650873\n",
      "Training Loss:  1.35650086\n",
      "Training Loss:  1.35650086\n",
      "Training Loss:  1.35650086\n",
      "Training Loss:  1.35648847\n",
      "Training Loss:  1.35648847\n",
      "Training Loss:  1.35648847\n",
      "Training Loss:  1.35648847\n",
      "Training Loss:  1.35648847\n",
      "Training Loss:  1.35648847\n",
      "Final training Loss:  1.35648847\n",
      "\n",
      "Running model (trial=7, mod=216, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3092643\n",
      "Training Loss:  1.29381418\n",
      "Training Loss:  1.2937957\n",
      "Training Loss:  1.29378772\n",
      "Training Loss:  1.29378772\n",
      "Training Loss:  1.29378772\n",
      "Training Loss:  1.29378474\n",
      "Training Loss:  1.29378474\n",
      "Training Loss:  1.29378474\n",
      "Training Loss:  1.29378474\n",
      "Final training Loss:  1.29378474\n",
      "\n",
      "Running model (trial=7, mod=217, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.31861758\n",
      "Training Loss:  1.31861687\n",
      "Training Loss:  1.31861687\n",
      "Training Loss:  1.31861675\n",
      "Training Loss:  1.31861675\n",
      "Training Loss:  1.31860125\n",
      "Training Loss:  1.31408739\n",
      "Training Loss:  1.31408739\n",
      "Training Loss:  1.31408739\n",
      "Training Loss:  1.31408739\n",
      "Final training Loss:  1.31408739\n",
      "\n",
      "Running model (trial=7, mod=218, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28821099\n",
      "Training Loss:  1.28820741\n",
      "Training Loss:  1.28820741\n",
      "Training Loss:  1.28820741\n",
      "Training Loss:  1.28820741\n",
      "Training Loss:  1.28820741\n",
      "Training Loss:  1.28820097\n",
      "Training Loss:  1.28820097\n",
      "Training Loss:  1.28820097\n",
      "Training Loss:  1.28820097\n",
      "Final training Loss:  1.28820097\n",
      "\n",
      "Running model (trial=7, mod=219, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30446601\n",
      "Training Loss:  1.30446601\n",
      "Training Loss:  1.30446601\n",
      "Training Loss:  1.30446577\n",
      "Training Loss:  1.30446577\n",
      "Training Loss:  1.30446577\n",
      "Training Loss:  0.62084609\n",
      "Training Loss:  0.62084609\n",
      "Training Loss:  0.62084609\n",
      "Training Loss:  0.62084609\n",
      "Final training Loss:  0.62084609\n",
      "\n",
      "Running model (trial=7, mod=220, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.33254218\n",
      "Training Loss:  1.33254218\n",
      "Training Loss:  1.33254218\n",
      "Training Loss:  1.33254218\n",
      "Training Loss:  1.33254147\n",
      "Training Loss:  1.33254147\n",
      "Training Loss:  1.33254147\n",
      "Training Loss:  1.33253884\n",
      "Training Loss:  1.33253884\n",
      "Training Loss:  1.33253884\n",
      "Final training Loss:  1.33253884\n",
      "\n",
      "Running model (trial=7, mod=221, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61031204\n",
      "Training Loss:  0.60943031\n",
      "Training Loss:  0.6094268\n",
      "Training Loss:  0.60941422\n",
      "Training Loss:  0.60941386\n",
      "Training Loss:  0.58569378\n",
      "Training Loss:  0.58562297\n",
      "Training Loss:  0.58561236\n",
      "Training Loss:  0.58561236\n",
      "Training Loss:  0.58560956\n",
      "Final training Loss:  0.58560956\n",
      "\n",
      "Running model (trial=7, mod=222, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28228331\n",
      "Training Loss:  1.28228331\n",
      "Training Loss:  1.28224051\n",
      "Training Loss:  1.28117645\n",
      "Training Loss:  1.28114128\n",
      "Training Loss:  1.17697644\n",
      "Training Loss:  1.17697644\n",
      "Training Loss:  1.17697644\n",
      "Training Loss:  1.17697644\n",
      "Training Loss:  1.17697644\n",
      "Final training Loss:  1.17697644\n",
      "\n",
      "Running model (trial=7, mod=223, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.20066333\n",
      "Training Loss:  1.18784213\n",
      "Training Loss:  1.18784213\n",
      "Training Loss:  1.18784213\n",
      "Training Loss:  1.18784213\n",
      "Training Loss:  1.18784213\n",
      "Training Loss:  1.18784213\n",
      "Training Loss:  1.18784213\n",
      "Training Loss:  1.18784213\n",
      "Training Loss:  1.18784213\n",
      "Final training Loss:  1.18784213\n",
      "\n",
      "Running model (trial=8, mod=224, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.08919024\n",
      "Training Loss:  3.08918881\n",
      "Training Loss:  3.08918881\n",
      "Training Loss:  3.08918881\n",
      "Training Loss:  3.08918881\n",
      "Training Loss:  3.08918881\n",
      "Training Loss:  3.08918881\n",
      "Training Loss:  3.08917332\n",
      "Training Loss:  3.08912134\n",
      "Training Loss:  1.6265136\n",
      "Final training Loss:  1.6265136\n",
      "\n",
      "Running model (trial=8, mod=225, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.17358732\n",
      "Training Loss:  1.17358088\n",
      "Training Loss:  1.17358088\n",
      "Training Loss:  1.17358088\n",
      "Training Loss:  1.17358088\n",
      "Training Loss:  1.17358088\n",
      "Training Loss:  1.1735779\n",
      "Training Loss:  1.1735779\n",
      "Training Loss:  1.1735779\n",
      "Training Loss:  1.17357433\n",
      "Final training Loss:  1.17357433\n",
      "\n",
      "Running model (trial=8, mod=226, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.25014138\n",
      "Training Loss:  1.07630217\n",
      "Training Loss:  0.61326736\n",
      "Training Loss:  0.61326736\n",
      "Training Loss:  0.61326736\n",
      "Training Loss:  0.61326736\n",
      "Training Loss:  0.61326206\n",
      "Training Loss:  0.61324954\n",
      "Training Loss:  0.61324954\n",
      "Training Loss:  0.61324716\n",
      "Final training Loss:  0.61324716\n",
      "\n",
      "Running model (trial=8, mod=227, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22910237\n",
      "Training Loss:  1.22910237\n",
      "Training Loss:  1.22908866\n",
      "Training Loss:  1.22908866\n",
      "Training Loss:  1.22908282\n",
      "Training Loss:  1.22908282\n",
      "Training Loss:  1.22908282\n",
      "Training Loss:  1.22908282\n",
      "Training Loss:  1.22908282\n",
      "Training Loss:  1.22908282\n",
      "Final training Loss:  1.22908282\n",
      "\n",
      "Running model (trial=8, mod=228, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29131436\n",
      "Training Loss:  1.29131436\n",
      "Training Loss:  1.29131436\n",
      "Training Loss:  1.29131305\n",
      "Training Loss:  1.29131305\n",
      "Training Loss:  1.29131305\n",
      "Training Loss:  1.2913096\n",
      "Training Loss:  1.2913096\n",
      "Training Loss:  1.2913096\n",
      "Training Loss:  1.2913096\n",
      "Final training Loss:  1.2913096\n",
      "\n",
      "Running model (trial=8, mod=229, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22915494\n",
      "Training Loss:  1.22915387\n",
      "Training Loss:  1.22915387\n",
      "Training Loss:  1.22915077\n",
      "Training Loss:  1.22915089\n",
      "Training Loss:  1.22915089\n",
      "Training Loss:  1.22915077\n",
      "Training Loss:  1.22915077\n",
      "Training Loss:  1.22915089\n",
      "Training Loss:  1.22915077\n",
      "Final training Loss:  1.22915077\n",
      "\n",
      "Running model (trial=8, mod=230, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.20296443\n",
      "Training Loss:  1.20294964\n",
      "Training Loss:  1.20294964\n",
      "Training Loss:  1.20294964\n",
      "Training Loss:  1.20294964\n",
      "Training Loss:  1.20294964\n",
      "Training Loss:  1.20294678\n",
      "Training Loss:  1.20294666\n",
      "Training Loss:  1.20294464\n",
      "Training Loss:  1.20294464\n",
      "Final training Loss:  1.20294464\n",
      "\n",
      "Running model (trial=8, mod=231, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Training Loss:  0.72671199\n",
      "Final training Loss:  0.72671199\n",
      "\n",
      "Running model (trial=8, mod=232, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.1924876\n",
      "Training Loss:  1.19248712\n",
      "Training Loss:  1.19246328\n",
      "Training Loss:  1.19245446\n",
      "Training Loss:  1.19245446\n",
      "Training Loss:  1.19245446\n",
      "Training Loss:  1.19245446\n",
      "Training Loss:  1.19245446\n",
      "Training Loss:  1.19244802\n",
      "Training Loss:  1.19244802\n",
      "Final training Loss:  1.19244802\n",
      "\n",
      "Running model (trial=8, mod=233, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Training Loss:  1.24370134\n",
      "Final training Loss:  1.24370134\n",
      "\n",
      "Running model (trial=8, mod=234, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.22947311\n",
      "Training Loss:  1.22947311\n",
      "Training Loss:  1.22947311\n",
      "Training Loss:  1.22947311\n",
      "Training Loss:  1.22947311\n",
      "Training Loss:  0.25392997\n",
      "Training Loss:  0.25392884\n",
      "Training Loss:  0.25392884\n",
      "Training Loss:  0.25392884\n",
      "Training Loss:  0.25392884\n",
      "Final training Loss:  0.25392884\n",
      "\n",
      "Running model (trial=8, mod=235, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.55641145\n",
      "Training Loss:  0.55641145\n",
      "Training Loss:  0.53015798\n",
      "Training Loss:  0.53015798\n",
      "Training Loss:  0.5183472\n",
      "Training Loss:  0.5181163\n",
      "Training Loss:  0.28500885\n",
      "Training Loss:  0.2850014\n",
      "Training Loss:  0.28500062\n",
      "Training Loss:  0.28500062\n",
      "Final training Loss:  0.28500062\n",
      "\n",
      "Running model (trial=8, mod=236, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.64015007\n",
      "Training Loss:  4.64015007\n",
      "Training Loss:  4.64014673\n",
      "Training Loss:  4.63982534\n",
      "Training Loss:  4.63982534\n",
      "Training Loss:  4.63982534\n",
      "Training Loss:  4.63982534\n",
      "Training Loss:  4.63982534\n",
      "Training Loss:  4.63982534\n",
      "Training Loss:  4.63982534\n",
      "Final training Loss:  4.63982534\n",
      "\n",
      "Running model (trial=8, mod=237, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50396729\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Final training Loss:  599.50378418\n",
      "\n",
      "Running model (trial=8, mod=238, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.79309106\n",
      "Training Loss:  0.77838838\n",
      "Training Loss:  0.77834851\n",
      "Training Loss:  0.77834851\n",
      "Training Loss:  0.77834767\n",
      "Training Loss:  0.77834743\n",
      "Training Loss:  0.77834743\n",
      "Training Loss:  0.77834028\n",
      "Training Loss:  0.77834028\n",
      "Training Loss:  0.77834028\n",
      "Final training Loss:  0.77834028\n",
      "\n",
      "Running model (trial=8, mod=239, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.20492196\n",
      "Training Loss:  1.20489025\n",
      "Training Loss:  1.20488238\n",
      "Training Loss:  1.20486975\n",
      "Training Loss:  1.20486975\n",
      "Training Loss:  1.20486975\n",
      "Training Loss:  1.20486975\n",
      "Training Loss:  1.20486975\n",
      "Training Loss:  1.20486975\n",
      "Training Loss:  1.20486975\n",
      "Final training Loss:  1.20486975\n",
      "\n",
      "Running model (trial=8, mod=240, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.29455912\n",
      "Training Loss:  1.29453719\n",
      "Training Loss:  1.16861176\n",
      "Training Loss:  1.16861176\n",
      "Training Loss:  1.16861176\n",
      "Training Loss:  1.16861176\n",
      "Training Loss:  1.16861176\n",
      "Training Loss:  1.16861176\n",
      "Training Loss:  1.16861176\n",
      "Training Loss:  1.16861176\n",
      "Final training Loss:  1.16861176\n",
      "\n",
      "Running model (trial=8, mod=241, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Training Loss:  1.3220582\n",
      "Final training Loss:  1.3220582\n",
      "\n",
      "Running model (trial=8, mod=242, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.2605232\n",
      "Training Loss:  1.2275033\n",
      "Training Loss:  1.22749889\n",
      "Training Loss:  1.22132754\n",
      "Training Loss:  1.20791805\n",
      "Training Loss:  1.20791805\n",
      "Training Loss:  1.20791805\n",
      "Training Loss:  1.20791805\n",
      "Training Loss:  1.20791805\n",
      "Training Loss:  1.20791805\n",
      "Final training Loss:  1.20791805\n",
      "\n",
      "Running model (trial=8, mod=243, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Training Loss:  1.3698051\n",
      "Final training Loss:  1.3698051\n",
      "\n",
      "Running model (trial=8, mod=244, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.42050815\n",
      "Training Loss:  1.39703715\n",
      "Training Loss:  1.39703715\n",
      "Training Loss:  1.39701962\n",
      "Training Loss:  1.39701962\n",
      "Training Loss:  1.39701962\n",
      "Training Loss:  1.39701962\n",
      "Training Loss:  1.39701116\n",
      "Training Loss:  1.39701116\n",
      "Training Loss:  1.39701116\n",
      "Final training Loss:  1.39701116\n",
      "\n",
      "Running model (trial=8, mod=245, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.42297065\n",
      "Training Loss:  1.42297065\n",
      "Training Loss:  1.32651484\n",
      "Training Loss:  1.32651484\n",
      "Training Loss:  1.32650661\n",
      "Training Loss:  1.32650661\n",
      "Training Loss:  1.32650661\n",
      "Training Loss:  1.32650661\n",
      "Training Loss:  1.32650661\n",
      "Training Loss:  1.3264358\n",
      "Final training Loss:  1.3264358\n",
      "\n",
      "Running model (trial=8, mod=246, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.16796279\n",
      "Training Loss:  1.16796279\n",
      "Training Loss:  1.16796279\n",
      "Training Loss:  1.16796279\n",
      "Training Loss:  1.16796279\n",
      "Training Loss:  1.16796279\n",
      "Training Loss:  1.16796279\n",
      "Training Loss:  1.16795528\n",
      "Training Loss:  1.16795349\n",
      "Training Loss:  1.16795349\n",
      "Final training Loss:  1.16795349\n",
      "\n",
      "Running model (trial=8, mod=247, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28000343\n",
      "Training Loss:  0.61507601\n",
      "Training Loss:  0.61507601\n",
      "Training Loss:  0.61507213\n",
      "Training Loss:  0.61506552\n",
      "Training Loss:  0.61506552\n",
      "Training Loss:  0.61506552\n",
      "Training Loss:  0.61506552\n",
      "Training Loss:  0.61506552\n",
      "Training Loss:  0.61506552\n",
      "Final training Loss:  0.61506552\n",
      "\n",
      "Running model (trial=8, mod=248, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28483856\n",
      "Training Loss:  1.28483069\n",
      "Training Loss:  1.28482008\n",
      "Training Loss:  1.1971904\n",
      "Training Loss:  1.19718528\n",
      "Training Loss:  1.1971786\n",
      "Training Loss:  1.1971786\n",
      "Training Loss:  1.19717836\n",
      "Training Loss:  1.19717836\n",
      "Training Loss:  1.19717836\n",
      "Final training Loss:  1.19717836\n",
      "\n",
      "Running model (trial=8, mod=249, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28457105\n",
      "Training Loss:  1.28457105\n",
      "Training Loss:  1.28457105\n",
      "Training Loss:  1.28457105\n",
      "Training Loss:  1.28457105\n",
      "Training Loss:  1.28456473\n",
      "Training Loss:  1.28456473\n",
      "Training Loss:  1.28456473\n",
      "Training Loss:  1.28456473\n",
      "Training Loss:  1.28456473\n",
      "Final training Loss:  1.28456473\n",
      "\n",
      "Running model (trial=8, mod=250, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61180204\n",
      "Training Loss:  0.61180204\n",
      "Training Loss:  0.6117869\n",
      "Training Loss:  0.6117869\n",
      "Training Loss:  0.61178637\n",
      "Training Loss:  0.6117813\n",
      "Training Loss:  0.6117813\n",
      "Training Loss:  0.6117813\n",
      "Training Loss:  0.58475143\n",
      "Training Loss:  0.57294202\n",
      "Final training Loss:  0.57294202\n",
      "\n",
      "Running model (trial=8, mod=251, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.17805576\n",
      "Training Loss:  1.17805576\n",
      "Training Loss:  1.17805576\n",
      "Training Loss:  1.17805576\n",
      "Training Loss:  1.17805576\n",
      "Training Loss:  1.17805576\n",
      "Training Loss:  1.17805576\n",
      "Training Loss:  1.17805576\n",
      "Training Loss:  1.17805314\n",
      "Training Loss:  1.17805314\n",
      "Final training Loss:  1.17805314\n",
      "\n",
      "Running model (trial=9, mod=252, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30600464\n",
      "Training Loss:  1.30600464\n",
      "Training Loss:  1.30600464\n",
      "Training Loss:  1.30600452\n",
      "Training Loss:  1.30600452\n",
      "Training Loss:  1.30600452\n",
      "Training Loss:  1.30600452\n",
      "Training Loss:  1.30600452\n",
      "Training Loss:  1.30600452\n",
      "Training Loss:  1.30600452\n",
      "Final training Loss:  1.30600452\n",
      "\n",
      "Running model (trial=9, mod=253, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62697208\n",
      "Training Loss:  0.62697208\n",
      "Training Loss:  0.62697208\n",
      "Training Loss:  0.62697208\n",
      "Training Loss:  0.62696868\n",
      "Training Loss:  0.62696868\n",
      "Training Loss:  0.62696868\n",
      "Training Loss:  0.62696868\n",
      "Training Loss:  0.62696868\n",
      "Training Loss:  0.62696868\n",
      "Final training Loss:  0.62696868\n",
      "\n",
      "Running model (trial=9, mod=254, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Training Loss:  1.26153517\n",
      "Final training Loss:  1.26153517\n",
      "\n",
      "Running model (trial=9, mod=255, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.17324495\n",
      "Training Loss:  1.17316735\n",
      "Training Loss:  1.17316437\n",
      "Training Loss:  1.17316437\n",
      "Training Loss:  1.17316437\n",
      "Training Loss:  1.17316437\n",
      "Training Loss:  1.17316437\n",
      "Training Loss:  1.17316437\n",
      "Training Loss:  1.17316437\n",
      "Training Loss:  1.17316437\n",
      "Final training Loss:  1.17316437\n",
      "\n",
      "Running model (trial=9, mod=256, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.37078249\n",
      "Training Loss:  1.37078249\n",
      "Training Loss:  1.37077975\n",
      "Training Loss:  1.37077975\n",
      "Training Loss:  1.37076461\n",
      "Training Loss:  1.37076461\n",
      "Training Loss:  1.37076461\n",
      "Training Loss:  1.37074828\n",
      "Training Loss:  1.37074828\n",
      "Training Loss:  1.37074828\n",
      "Final training Loss:  1.37074828\n",
      "\n",
      "Running model (trial=9, mod=257, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.44108558\n",
      "Training Loss:  1.4410826\n",
      "Training Loss:  1.4410826\n",
      "Training Loss:  1.4410826\n",
      "Training Loss:  1.23599124\n",
      "Training Loss:  0.83093524\n",
      "Training Loss:  0.83092725\n",
      "Training Loss:  0.83092517\n",
      "Training Loss:  0.83092517\n",
      "Training Loss:  0.83092517\n",
      "Final training Loss:  0.83092517\n",
      "\n",
      "Running model (trial=9, mod=258, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3303014\n",
      "Training Loss:  1.3303014\n",
      "Training Loss:  1.3303014\n",
      "Training Loss:  1.3303014\n",
      "Training Loss:  1.3303014\n",
      "Training Loss:  1.3303014\n",
      "Training Loss:  1.33029437\n",
      "Training Loss:  1.33029437\n",
      "Training Loss:  1.33029437\n",
      "Training Loss:  1.33029449\n",
      "Final training Loss:  1.33029449\n",
      "\n",
      "Running model (trial=9, mod=259, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.32632267\n",
      "Training Loss:  1.32632267\n",
      "Training Loss:  1.32632267\n",
      "Training Loss:  1.32631409\n",
      "Training Loss:  1.32631409\n",
      "Training Loss:  1.32631409\n",
      "Training Loss:  1.32631409\n",
      "Training Loss:  1.32631409\n",
      "Training Loss:  1.32631409\n",
      "Training Loss:  1.32631409\n",
      "Final training Loss:  1.32631409\n",
      "\n",
      "Running model (trial=9, mod=260, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59252328\n",
      "Training Loss:  0.59252328\n",
      "Training Loss:  0.59252328\n",
      "Training Loss:  0.5925228\n",
      "Training Loss:  0.5925228\n",
      "Training Loss:  0.5925228\n",
      "Training Loss:  0.59251952\n",
      "Training Loss:  0.59251952\n",
      "Training Loss:  0.59251952\n",
      "Training Loss:  0.59251952\n",
      "Final training Loss:  0.59251952\n",
      "\n",
      "Running model (trial=9, mod=261, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24709988\n",
      "Training Loss:  1.24709642\n",
      "Training Loss:  1.24709642\n",
      "Training Loss:  1.24708998\n",
      "Training Loss:  1.24708998\n",
      "Training Loss:  1.24708998\n",
      "Training Loss:  1.24708998\n",
      "Training Loss:  1.24708998\n",
      "Training Loss:  1.24708998\n",
      "Training Loss:  1.24708998\n",
      "Final training Loss:  1.24708998\n",
      "\n",
      "Running model (trial=9, mod=262, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23855639\n",
      "Training Loss:  1.23855639\n",
      "Training Loss:  1.23855639\n",
      "Training Loss:  1.23855639\n",
      "Training Loss:  1.23855639\n",
      "Training Loss:  1.23855579\n",
      "Training Loss:  1.23855579\n",
      "Training Loss:  1.23855579\n",
      "Training Loss:  1.23855579\n",
      "Training Loss:  1.23855579\n",
      "Final training Loss:  1.23855579\n",
      "\n",
      "Running model (trial=9, mod=263, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6053834\n",
      "Training Loss:  0.60467905\n",
      "Training Loss:  0.60467905\n",
      "Training Loss:  0.60467905\n",
      "Training Loss:  0.60467905\n",
      "Training Loss:  0.60467905\n",
      "Training Loss:  0.60467905\n",
      "Training Loss:  0.60467905\n",
      "Training Loss:  0.60467905\n",
      "Training Loss:  0.60467905\n",
      "Final training Loss:  0.60467905\n",
      "\n",
      "Running model (trial=9, mod=264, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.44659424\n",
      "Training Loss:  599.44659424\n",
      "Training Loss:  599.44659424\n",
      "Training Loss:  599.44659424\n",
      "Training Loss:  599.44659424\n",
      "Training Loss:  599.44659424\n",
      "Training Loss:  599.44659424\n",
      "Training Loss:  599.44659424\n",
      "Training Loss:  599.4465332\n",
      "Training Loss:  599.4465332\n",
      "Final training Loss:  599.4465332\n",
      "\n",
      "Running model (trial=9, mod=265, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Final training Loss:  599.50402832\n",
      "\n",
      "Running model (trial=9, mod=266, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.90145063\n",
      "Training Loss:  5.90145063\n",
      "Training Loss:  5.90145063\n",
      "Training Loss:  5.90143538\n",
      "Training Loss:  5.90143538\n",
      "Training Loss:  5.90143538\n",
      "Training Loss:  5.90143538\n",
      "Training Loss:  5.90142393\n",
      "Training Loss:  5.90142393\n",
      "Training Loss:  5.90142393\n",
      "Final training Loss:  5.90142393\n",
      "\n",
      "Running model (trial=9, mod=267, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.27952504\n",
      "Training Loss:  1.27952504\n",
      "Training Loss:  1.2795186\n",
      "Training Loss:  1.2795186\n",
      "Training Loss:  1.2795186\n",
      "Training Loss:  1.2795186\n",
      "Training Loss:  1.2795186\n",
      "Training Loss:  1.2795186\n",
      "Training Loss:  1.2795186\n",
      "Training Loss:  1.2795186\n",
      "Final training Loss:  1.2795186\n",
      "\n",
      "Running model (trial=9, mod=268, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.1167053\n",
      "Training Loss:  1.1167053\n",
      "Training Loss:  1.1167053\n",
      "Training Loss:  1.1167016\n",
      "Training Loss:  1.1167016\n",
      "Training Loss:  1.1167016\n",
      "Training Loss:  1.1167016\n",
      "Training Loss:  1.1167016\n",
      "Training Loss:  1.1167016\n",
      "Training Loss:  1.1167016\n",
      "Final training Loss:  1.1167016\n",
      "\n",
      "Running model (trial=9, mod=269, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.54000145\n",
      "Training Loss:  0.54000145\n",
      "Training Loss:  0.54000145\n",
      "Training Loss:  0.54000145\n",
      "Training Loss:  0.54000145\n",
      "Training Loss:  0.54000098\n",
      "Training Loss:  0.54000098\n",
      "Training Loss:  0.54000098\n",
      "Training Loss:  0.54000098\n",
      "Training Loss:  0.54000098\n",
      "Final training Loss:  0.54000098\n",
      "\n",
      "Running model (trial=9, mod=270, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Training Loss:  1.38414371\n",
      "Final training Loss:  1.38414371\n",
      "\n",
      "Running model (trial=9, mod=271, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.16673267\n",
      "Training Loss:  1.16673267\n",
      "Training Loss:  1.16672516\n",
      "Training Loss:  1.16671002\n",
      "Training Loss:  1.16671002\n",
      "Training Loss:  1.16670978\n",
      "Training Loss:  1.16670978\n",
      "Training Loss:  1.16670978\n",
      "Training Loss:  1.16670978\n",
      "Training Loss:  1.16670954\n",
      "Final training Loss:  1.16670954\n",
      "\n",
      "Running model (trial=9, mod=272, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  87.20365143\n",
      "Training Loss:  87.20362854\n",
      "Training Loss:  87.20362854\n",
      "Training Loss:  87.20362854\n",
      "Training Loss:  87.20362854\n",
      "Training Loss:  87.20362854\n",
      "Training Loss:  87.20362854\n",
      "Training Loss:  87.20362854\n",
      "Training Loss:  87.20362854\n",
      "Training Loss:  87.20362854\n",
      "Final training Loss:  87.20362854\n",
      "\n",
      "Running model (trial=9, mod=273, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.27991533\n",
      "Training Loss:  1.27991533\n",
      "Training Loss:  1.27991068\n",
      "Training Loss:  1.27991068\n",
      "Training Loss:  1.27991068\n",
      "Training Loss:  1.27991068\n",
      "Training Loss:  1.27991068\n",
      "Training Loss:  1.27991068\n",
      "Training Loss:  1.27991068\n",
      "Training Loss:  1.27990806\n",
      "Final training Loss:  1.27990806\n",
      "\n",
      "Running model (trial=9, mod=274, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18168187\n",
      "Training Loss:  1.18168187\n",
      "Training Loss:  1.18168187\n",
      "Training Loss:  1.18168187\n",
      "Training Loss:  1.18168187\n",
      "Training Loss:  1.18168187\n",
      "Training Loss:  1.18167174\n",
      "Training Loss:  1.18167174\n",
      "Training Loss:  1.18167174\n",
      "Training Loss:  1.18167174\n",
      "Final training Loss:  1.18167174\n",
      "\n",
      "Running model (trial=9, mod=275, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24339724\n",
      "Training Loss:  1.24339092\n",
      "Training Loss:  1.24339092\n",
      "Training Loss:  1.24339092\n",
      "Training Loss:  1.2402041\n",
      "Training Loss:  1.2402041\n",
      "Training Loss:  1.2402041\n",
      "Training Loss:  1.2402041\n",
      "Training Loss:  1.23976421\n",
      "Training Loss:  1.23976421\n",
      "Final training Loss:  1.23976421\n",
      "\n",
      "Running model (trial=9, mod=276, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Training Loss:  28.06981277\n",
      "Final training Loss:  28.06981277\n",
      "\n",
      "Running model (trial=9, mod=277, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.21553171\n",
      "Training Loss:  1.20633638\n",
      "Training Loss:  1.20633447\n",
      "Training Loss:  1.20632529\n",
      "Training Loss:  1.2063241\n",
      "Training Loss:  1.2063241\n",
      "Training Loss:  1.2063241\n",
      "Training Loss:  1.2063241\n",
      "Training Loss:  1.2063241\n",
      "Training Loss:  1.20631742\n",
      "Final training Loss:  1.20631742\n",
      "\n",
      "Running model (trial=9, mod=278, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62251991\n",
      "Training Loss:  0.62251997\n",
      "Training Loss:  0.62251997\n",
      "Training Loss:  0.622513\n",
      "Training Loss:  0.622513\n",
      "Training Loss:  0.622513\n",
      "Training Loss:  0.622513\n",
      "Training Loss:  0.62248743\n",
      "Training Loss:  0.62248743\n",
      "Training Loss:  0.62248743\n",
      "Final training Loss:  0.62248743\n",
      "\n",
      "Running model (trial=9, mod=279, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x131bd93a0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x160bd5a60>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.16617548\n",
      "Training Loss:  1.16617548\n",
      "Training Loss:  1.16617548\n",
      "Training Loss:  1.16617548\n",
      "Training Loss:  1.16602457\n",
      "Training Loss:  1.16602457\n",
      "Training Loss:  1.16602457\n",
      "Training Loss:  1.16602457\n",
      "Training Loss:  1.16602457\n",
      "Training Loss:  1.16602457\n",
      "Final training Loss:  1.16602457\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "disc_points = x_train.detach()\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": disc_points,\n",
    "    \"x_train\": disc_points,\n",
    "    \"y_train\": c1.ksi(disc_points),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"analytical solution (smooth)\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: penalty_free_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "\n",
    "# make data frame\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type})\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-03T13:38:19.295350</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"ma42a53f8e4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.654947\" xlink:href=\"#ma42a53f8e4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(152.854947 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"306.094106\" xlink:href=\"#ma42a53f8e4\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(297.294106 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m8ddf2007cb\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"60.696307\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"86.13078\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"104.176826\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"118.174427\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"129.6113\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"139.281038\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"147.657346\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"155.045773\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"205.135466\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"230.569939\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"248.615986\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"262.613586\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"274.050459\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"283.720197\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"292.096505\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"299.484932\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"349.574625\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"375.009099\" xlink:href=\"#m8ddf2007cb\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Neurons -->\n     <g transform=\"translate(192.116406 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"199.707031\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"238.570312\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"299.751953\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"363.130859\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7624b320d5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m7624b320d5\" y=\"197.817489\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(20.878125 201.616708)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m7624b320d5\" y=\"54.202987\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 58.002205)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_23\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m23c063e4c4\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"220.063657\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"211.735172\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"204.388928\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"154.585216\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"129.295957\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"111.352943\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"97.43526\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"86.063684\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"76.449154\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"68.12067\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"60.774426\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m23c063e4c4\" y=\"10.970714\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $E$ -->\n     <g transform=\"translate(14.798438 119.12)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 60.696307 179.608484 \nL 60.696307 113.386682 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 104.176826 211.559539 \nL 104.176826 195.48742 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 147.657346 214.756364 \nL 147.657346 199.13535 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 191.137865 205.727421 \nL 191.137865 189.339111 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 234.618385 196.914674 \nL 234.618385 185.01476 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 278.098904 195.07712 \nL 278.098904 185.399251 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 321.579424 197.187522 \nL 321.579424 185.079616 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 365.059943 187.593541 \nL 365.059943 181.20439 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 60.696307 165.240101 \nL 60.696307 129.182287 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 104.176826 198.929975 \nL 104.176826 186.510363 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 147.657346 200.293771 \nL 147.657346 190.099936 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 191.137865 204.530391 \nL 191.137865 187.127955 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 234.618385 184.122931 \nL 234.618385 182.022285 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 278.098904 183.165749 \nL 278.098904 180.670289 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 321.579424 180.895357 \nL 321.579424 17.083636 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 365.059943 187.742804 \nL 365.059943 180.022757 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 60.696307 138.307402 \nL 104.176826 203.306737 \nL 147.657346 205.767867 \nL 191.137865 197.008015 \nL 234.618385 190.420145 \nL 278.098904 189.507806 \nL 321.579424 190.274447 \nL 365.059943 184.269423 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#pe975fdbb65)\" d=\"M 60.696307 146.346032 \nL 104.176826 192.116517 \nL 147.657346 194.802087 \nL 191.137865 195.111509 \nL 234.618385 183.079477 \nL 278.098904 181.839036 \nL 321.579424 55.96295 \nL 365.059943 183.755355 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\"/>\n   <g id=\"line2d_38\"/>\n   <g id=\"line2d_39\"/>\n   <g id=\"line2d_40\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 52.478125 59.234375 \nL 122.246875 59.234375 \nQ 124.246875 59.234375 124.246875 57.234375 \nL 124.246875 14.2 \nQ 124.246875 12.2 122.246875 12.2 \nL 52.478125 12.2 \nQ 50.478125 12.2 50.478125 14.2 \nL 50.478125 57.234375 \nQ 50.478125 59.234375 52.478125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_7\">\n     <!-- model -->\n     <g transform=\"translate(71.792969 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_41\">\n     <path d=\"M 54.478125 34.976562 \nL 74.478125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_42\"/>\n    <g id=\"text_8\">\n     <!-- FFNN -->\n     <g transform=\"translate(82.478125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_43\">\n     <path d=\"M 54.478125 49.654687 \nL 74.478125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_44\"/>\n    <g id=\"text_9\">\n     <!-- ResNET -->\n     <g transform=\"translate(82.478125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe975fdbb65\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\")\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "6 180\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 2]), len(d_results_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-03T13:38:21.565114</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m0ba65f080d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.696307\" xlink:href=\"#m0ba65f080d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(51.896307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.91113\" xlink:href=\"#m0ba65f080d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(254.11113 239.238437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"mf4f6b446d2\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.443464\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"121.569034\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"157.177297\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"182.441761\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"202.038402\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"218.050024\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"231.587657\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"243.314489\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"253.658287\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"323.783857\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"359.39212\" xlink:href=\"#mf4f6b446d2\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Layers -->\n     <g transform=\"translate(196.332031 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"55.712891\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"116.992188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"176.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"237.695312\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"278.808594\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"ma089defe90\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#ma089defe90\" y=\"189.092866\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(20.878125 192.892085)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#ma089defe90\" y=\"102.995276\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 106.794495)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#ma089defe90\" y=\"16.897685\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 20.696904)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"mc98cc407ba\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"223.354542\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"215.010824\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"208.193509\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"202.429552\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"197.436585\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"193.032476\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"163.174909\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"148.013876\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"137.256952\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"128.913233\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"122.095919\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"116.331961\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"111.338995\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"106.934886\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"77.077319\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"61.916285\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"51.159361\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"42.815643\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"35.998328\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"30.234371\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"25.241404\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc98cc407ba\" y=\"20.837295\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- $E$ -->\n     <g transform=\"translate(14.798438 119.12)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 60.696307 189.428422 \nL 60.696307 181.938737 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 121.569034 199.034803 \nL 121.569034 189.591374 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 182.441761 214.756364 \nL 182.441761 194.221915 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 243.314489 200.72991 \nL 243.314489 187.989099 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 304.187216 49.91588 \nL 304.187216 17.877846 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 365.059943 17.083679 \nL 365.059943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 60.696307 184.73445 \nL 60.696307 179.168388 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 121.569034 190.324022 \nL 121.569034 184.392815 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 182.441761 184.811736 \nL 182.441761 119.896406 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 243.314489 197.342414 \nL 243.314489 186.431014 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 304.187216 206.742367 \nL 304.187216 193.37297 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 365.059943 196.195807 \nL 365.059943 184.918842 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 60.696307 185.452098 \nL 121.569034 193.859157 \nL 182.441761 202.95735 \nL 243.314489 193.882588 \nL 304.187216 30.267013 \nL 365.059943 17.083659 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\">\n    <path clip-path=\"url(#p170c6d5a40)\" d=\"M 60.696307 181.831532 \nL 121.569034 187.285119 \nL 182.441761 139.80695 \nL 243.314489 191.592966 \nL 304.187216 199.610599 \nL 365.059943 189.997377 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_41\"/>\n   <g id=\"line2d_42\"/>\n   <g id=\"line2d_43\"/>\n   <g id=\"line2d_44\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 52.478125 59.234375 \nL 122.246875 59.234375 \nQ 124.246875 59.234375 124.246875 57.234375 \nL 124.246875 14.2 \nQ 124.246875 12.2 122.246875 12.2 \nL 52.478125 12.2 \nQ 50.478125 12.2 50.478125 14.2 \nL 50.478125 57.234375 \nQ 50.478125 59.234375 52.478125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_8\">\n     <!-- model -->\n     <g transform=\"translate(71.792969 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_45\">\n     <path d=\"M 54.478125 34.976562 \nL 74.478125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_46\"/>\n    <g id=\"text_9\">\n     <!-- FFNN -->\n     <g transform=\"translate(82.478125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_47\">\n     <path d=\"M 54.478125 49.654687 \nL 74.478125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_48\"/>\n    <g id=\"text_10\">\n     <!-- ResNET -->\n     <g transform=\"translate(82.478125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p170c6d5a40\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Layers\")\n",
    "fig_layers.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "18 140\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 2]), len(d_results_layer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-31T14:48:21.486548</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 154.906317 \nC 61.491916 154.906317 62.255046 154.590218 62.817627 154.027637 \nC 63.380208 153.465056 63.696307 152.701926 63.696307 151.906317 \nC 63.696307 151.110707 63.380208 150.347577 62.817627 149.784996 \nC 62.255046 149.222416 61.491916 148.906317 60.696307 148.906317 \nC 59.900698 148.906317 59.137567 149.222416 58.574986 149.784996 \nC 58.012406 150.347577 57.696307 151.110707 57.696307 151.906317 \nC 57.696307 152.701926 58.012406 153.465056 58.574986 154.027637 \nC 59.137567 154.590218 59.900698 154.906317 60.696307 154.906317 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 173.215626 \nC 61.570115 173.215626 62.333245 172.899527 62.895826 172.336947 \nC 63.458407 171.774366 63.774506 171.011236 63.774506 170.215626 \nC 63.774506 169.420017 63.458407 168.656887 62.895826 168.094306 \nC 62.333245 167.531725 61.570115 167.215626 60.774506 167.215626 \nC 59.978896 167.215626 59.215766 167.531725 58.653185 168.094306 \nC 58.090605 168.656887 57.774506 169.420017 57.774506 170.215626 \nC 57.774506 171.011236 58.090605 171.774366 58.653185 172.336947 \nC 59.215766 172.899527 59.978896 173.215626 60.774506 173.215626 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 194.394801 \nC 61.836911 194.394801 62.600041 194.078702 63.162622 193.516121 \nC 63.725203 192.953541 64.041302 192.19041 64.041302 191.394801 \nC 64.041302 190.599192 63.725203 189.836061 63.162622 189.273481 \nC 62.600041 188.7109 61.836911 188.394801 61.041302 188.394801 \nC 60.245692 188.394801 59.482562 188.7109 58.919981 189.273481 \nC 58.357401 189.836061 58.041302 190.599192 58.041302 191.394801 \nC 58.041302 192.19041 58.357401 192.953541 58.919981 193.516121 \nC 59.482562 194.078702 60.245692 194.394801 61.041302 194.394801 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 158.467711 \nC 62.812096 158.467711 63.575227 158.151612 64.137807 157.589031 \nC 64.700388 157.02645 65.016487 156.26332 65.016487 155.467711 \nC 65.016487 154.672101 64.700388 153.908971 64.137807 153.34639 \nC 63.575227 152.78381 62.812096 152.467711 62.016487 152.467711 \nC 61.220878 152.467711 60.457747 152.78381 59.895167 153.34639 \nC 59.332586 153.908971 59.016487 154.672101 59.016487 155.467711 \nC 59.016487 156.26332 59.332586 157.02645 59.895167 157.589031 \nC 60.457747 158.151612 61.220878 158.467711 62.016487 158.467711 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 179.451653 \nC 66.528841 179.451653 67.291971 179.135554 67.854552 178.572973 \nC 68.417132 178.010392 68.733231 177.247262 68.733231 176.451653 \nC 68.733231 175.656043 68.417132 174.892913 67.854552 174.330332 \nC 67.291971 173.767752 66.528841 173.451653 65.733231 173.451653 \nC 64.937622 173.451653 64.174492 173.767752 63.611911 174.330332 \nC 63.04933 174.892913 62.733231 175.656043 62.733231 176.451653 \nC 62.733231 177.247262 63.04933 178.010392 63.611911 178.572973 \nC 64.174492 179.135554 64.937622 179.451653 65.733231 179.451653 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 157.764412 \nC 81.027823 157.764412 81.790954 157.448313 82.353534 156.885733 \nC 82.916115 156.323152 83.232214 155.560022 83.232214 154.764412 \nC 83.232214 153.968803 82.916115 153.205673 82.353534 152.643092 \nC 81.790954 152.080511 81.027823 151.764412 80.232214 151.764412 \nC 79.436605 151.764412 78.673474 152.080511 78.110894 152.643092 \nC 77.548313 153.205673 77.232214 153.968803 77.232214 154.764412 \nC 77.232214 155.560022 77.548313 156.323152 78.110894 156.885733 \nC 78.673474 157.448313 79.436605 157.764412 80.232214 157.764412 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 157.557076 \nC 138.287765 157.557076 139.050895 157.240977 139.613476 156.678396 \nC 140.176056 156.115815 140.492155 155.352685 140.492155 154.557076 \nC 140.492155 153.761466 140.176056 152.998336 139.613476 152.435755 \nC 139.050895 151.873175 138.287765 151.557076 137.492155 151.557076 \nC 136.696546 151.557076 135.933416 151.873175 135.370835 152.435755 \nC 134.808254 152.998336 134.492155 153.761466 134.492155 154.557076 \nC 134.492155 155.352685 134.808254 156.115815 135.370835 156.678396 \nC 135.933416 157.240977 136.696546 157.557076 137.492155 157.557076 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 156.703985 \nC 365.855552 156.703985 366.618683 156.387886 367.181264 155.825306 \nC 367.743844 155.262725 368.059943 154.499595 368.059943 153.703985 \nC 368.059943 152.908376 367.743844 152.145246 367.181264 151.582665 \nC 366.618683 151.020084 365.855552 150.703985 365.059943 150.703985 \nC 364.264334 150.703985 363.501204 151.020084 362.938623 151.582665 \nC 362.376042 152.145246 362.059943 152.908376 362.059943 153.703985 \nC 362.059943 154.499595 362.376042 155.262725 362.938623 155.825306 \nC 363.501204 156.387886 364.264334 156.703985 365.059943 156.703985 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 158.365552 \nC 61.671313 158.365552 62.434444 158.049453 62.997024 157.486873 \nC 63.559605 156.924292 63.875704 156.161162 63.875704 155.365552 \nC 63.875704 154.569943 63.559605 153.806813 62.997024 153.244232 \nC 62.434444 152.681651 61.671313 152.365552 60.875704 152.365552 \nC 60.080095 152.365552 59.316965 152.681651 58.754384 153.244232 \nC 58.191803 153.806813 57.875704 154.569943 57.875704 155.365552 \nC 57.875704 156.161162 58.191803 156.924292 58.754384 157.486873 \nC 59.316965 158.049453 60.080095 158.365552 60.875704 158.365552 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 159.321969 \nC 61.836911 159.321969 62.600041 159.00587 63.162622 158.443289 \nC 63.725203 157.880708 64.041302 157.117578 64.041302 156.321969 \nC 64.041302 155.526359 63.725203 154.763229 63.162622 154.200648 \nC 62.600041 153.638068 61.836911 153.321969 61.041302 153.321969 \nC 60.245692 153.321969 59.482562 153.638068 58.919981 154.200648 \nC 58.357401 154.763229 58.041302 155.526359 58.041302 156.321969 \nC 58.041302 157.117578 58.357401 157.880708 58.919981 158.443289 \nC 59.482562 159.00587 60.245692 159.321969 61.041302 159.321969 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 159.613955 \nC 62.168106 159.613955 62.931236 159.297856 63.493817 158.735276 \nC 64.056398 158.172695 64.372497 157.409565 64.372497 156.613955 \nC 64.372497 155.818346 64.056398 155.055216 63.493817 154.492635 \nC 62.931236 153.930054 62.168106 153.613955 61.372497 153.613955 \nC 60.576887 153.613955 59.813757 153.930054 59.251176 154.492635 \nC 58.688596 155.055216 58.372497 155.818346 58.372497 156.613955 \nC 58.372497 157.409565 58.688596 158.172695 59.251176 158.735276 \nC 59.813757 159.297856 60.576887 159.613955 61.372497 159.613955 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 159.347764 \nC 62.830496 159.347764 63.593626 159.031665 64.156207 158.469084 \nC 64.718788 157.906503 65.034887 157.143373 65.034887 156.347764 \nC 65.034887 155.552155 64.718788 154.789024 64.156207 154.226443 \nC 63.593626 153.663863 62.830496 153.347764 62.034887 153.347764 \nC 61.239277 153.347764 60.476147 153.663863 59.913566 154.226443 \nC 59.350986 154.789024 59.034887 155.552155 59.034887 156.347764 \nC 59.034887 157.143373 59.350986 157.906503 59.913566 158.469084 \nC 60.476147 159.031665 61.239277 159.347764 62.034887 159.347764 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 37.879825 \nC 64.155276 37.879825 64.918406 37.563726 65.480987 37.001145 \nC 66.043568 36.438564 66.359667 35.675434 66.359667 34.879825 \nC 66.359667 34.084215 66.043568 33.321085 65.480987 32.758504 \nC 64.918406 32.195923 64.155276 31.879825 63.359667 31.879825 \nC 62.564058 31.879825 61.800927 32.195923 61.238347 32.758504 \nC 60.675766 33.321085 60.359667 34.084215 60.359667 34.879825 \nC 60.359667 35.675434 60.675766 36.438564 61.238347 37.001145 \nC 61.800927 37.563726 62.564058 37.879825 63.359667 37.879825 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869641 \nC 66.804836 37.869641 67.567967 37.553542 68.130547 36.990962 \nC 68.693128 36.428381 69.009227 35.665251 69.009227 34.869641 \nC 69.009227 34.074032 68.693128 33.310902 68.130547 32.748321 \nC 67.567967 32.18574 66.804836 31.869641 66.009227 31.869641 \nC 65.213618 31.869641 64.450488 32.18574 63.887907 32.748321 \nC 63.325326 33.310902 63.009227 34.074032 63.009227 34.869641 \nC 63.009227 35.665251 63.325326 36.428381 63.887907 36.990962 \nC 64.450488 37.553542 65.213618 37.869641 66.009227 37.869641 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 159.844833 \nL 60.696307 158.344833 \nL 62.196307 159.844833 \nL 63.696307 158.344833 \nL 62.196307 156.844833 \nL 63.696307 155.344833 \nL 62.196307 153.844833 \nL 60.696307 155.344833 \nL 59.196307 153.844833 \nL 57.696307 155.344833 \nL 59.196307 156.844833 \nL 57.696307 158.344833 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 157.740463 \nL 60.774506 156.240463 \nL 62.274506 157.740463 \nL 63.774506 156.240463 \nL 62.274506 154.740463 \nL 63.774506 153.240463 \nL 62.274506 151.740463 \nL 60.774506 153.240463 \nL 59.274506 151.740463 \nL 57.774506 153.240463 \nL 59.274506 154.740463 \nL 57.774506 156.240463 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 178.705779 \nL 61.041302 177.205779 \nL 62.541302 178.705779 \nL 64.041302 177.205779 \nL 62.541302 175.705779 \nL 64.041302 174.205779 \nL 62.541302 172.705779 \nL 61.041302 174.205779 \nL 59.541302 172.705779 \nL 58.041302 174.205779 \nL 59.541302 175.705779 \nL 58.041302 177.205779 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 159.537009 \nL 62.016487 158.037009 \nL 63.516487 159.537009 \nL 65.016487 158.037009 \nL 63.516487 156.537009 \nL 65.016487 155.037009 \nL 63.516487 153.537009 \nL 62.016487 155.037009 \nL 60.516487 153.537009 \nL 59.016487 155.037009 \nL 60.516487 156.537009 \nL 59.016487 158.037009 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 158.255825 \nL 65.733231 156.755825 \nL 67.233231 158.255825 \nL 68.733231 156.755825 \nL 67.233231 155.255825 \nL 68.733231 153.755825 \nL 67.233231 152.255825 \nL 65.733231 153.755825 \nL 64.233231 152.255825 \nL 62.733231 153.755825 \nL 64.233231 155.255825 \nL 62.733231 156.755825 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 158.75607 \nL 80.232214 157.25607 \nL 81.732214 158.75607 \nL 83.232214 157.25607 \nL 81.732214 155.75607 \nL 83.232214 154.25607 \nL 81.732214 152.75607 \nL 80.232214 154.25607 \nL 78.732214 152.75607 \nL 77.232214 154.25607 \nL 78.732214 155.75607 \nL 77.232214 157.25607 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 156.841156 \nL 137.492155 155.341156 \nL 138.992155 156.841156 \nL 140.492155 155.341156 \nL 138.992155 153.841156 \nL 140.492155 152.341156 \nL 138.992155 150.841156 \nL 137.492155 152.341156 \nL 135.992155 150.841156 \nL 134.492155 152.341156 \nL 135.992155 153.841156 \nL 134.492155 155.341156 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 157.018682 \nL 365.059943 155.518682 \nL 366.559943 157.018682 \nL 368.059943 155.518682 \nL 366.559943 154.018682 \nL 368.059943 152.518682 \nL 366.559943 151.018682 \nL 365.059943 152.518682 \nL 363.559943 151.018682 \nL 362.059943 152.518682 \nL 363.559943 154.018682 \nL 362.059943 155.518682 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 157.308823 \nL 60.875704 155.808823 \nL 62.375704 157.308823 \nL 63.875704 155.808823 \nL 62.375704 154.308823 \nL 63.875704 152.808823 \nL 62.375704 151.308823 \nL 60.875704 152.808823 \nL 59.375704 151.308823 \nL 57.875704 152.808823 \nL 59.375704 154.308823 \nL 57.875704 155.808823 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 177.270755 \nL 61.041302 175.770755 \nL 62.541302 177.270755 \nL 64.041302 175.770755 \nL 62.541302 174.270755 \nL 64.041302 172.770755 \nL 62.541302 171.270755 \nL 61.041302 172.770755 \nL 59.541302 171.270755 \nL 58.041302 172.770755 \nL 59.541302 174.270755 \nL 58.041302 175.770755 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 159.927261 \nL 61.372497 158.427261 \nL 62.872497 159.927261 \nL 64.372497 158.427261 \nL 62.872497 156.927261 \nL 64.372497 155.427261 \nL 62.872497 153.927261 \nL 61.372497 155.427261 \nL 59.872497 153.927261 \nL 58.372497 155.427261 \nL 59.872497 156.927261 \nL 58.372497 158.427261 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 157.637477 \nL 62.034887 156.137477 \nL 63.534887 157.637477 \nL 65.034887 156.137477 \nL 63.534887 154.637477 \nL 65.034887 153.137477 \nL 63.534887 151.637477 \nL 62.034887 153.137477 \nL 60.534887 151.637477 \nL 59.034887 153.137477 \nL 60.534887 154.637477 \nL 59.034887 156.137477 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 179.651856 \nL 63.359667 178.151856 \nL 64.859667 179.651856 \nL 66.359667 178.151856 \nL 64.859667 176.651856 \nL 66.359667 175.151856 \nL 64.859667 173.651856 \nL 63.359667 175.151856 \nL 61.859667 173.651856 \nL 60.359667 175.151856 \nL 61.859667 176.651856 \nL 60.359667 178.151856 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 158.342823 \nL 66.009227 156.842823 \nL 67.509227 158.342823 \nL 69.009227 156.842823 \nL 67.509227 155.342823 \nL 69.009227 153.842823 \nL 67.509227 152.342823 \nL 66.009227 153.842823 \nL 64.509227 152.342823 \nL 63.009227 153.842823 \nL 64.509227 155.342823 \nL 63.009227 156.842823 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 154.797715 \nC 61.491916 154.797715 62.255046 154.481616 62.817627 153.919036 \nC 63.380208 153.356455 63.696307 152.593325 63.696307 151.797715 \nC 63.696307 151.002106 63.380208 150.238976 62.817627 149.676395 \nC 62.255046 149.113814 61.491916 148.797715 60.696307 148.797715 \nC 59.900698 148.797715 59.137567 149.113814 58.574986 149.676395 \nC 58.012406 150.238976 57.696307 151.002106 57.696307 151.797715 \nC 57.696307 152.593325 58.012406 153.356455 58.574986 153.919036 \nC 59.137567 154.481616 59.900698 154.797715 60.696307 154.797715 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 176.754151 \nC 61.570115 176.754151 62.333245 176.438052 62.895826 175.875472 \nC 63.458407 175.312891 63.774506 174.54976 63.774506 173.754151 \nC 63.774506 172.958542 63.458407 172.195412 62.895826 171.632831 \nC 62.333245 171.07025 61.570115 170.754151 60.774506 170.754151 \nC 59.978896 170.754151 59.215766 171.07025 58.653185 171.632831 \nC 58.090605 172.195412 57.774506 172.958542 57.774506 173.754151 \nC 57.774506 174.54976 58.090605 175.312891 58.653185 175.875472 \nC 59.215766 176.438052 59.978896 176.754151 60.774506 176.754151 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 201.362222 \nC 61.836911 201.362222 62.600041 201.046123 63.162622 200.483543 \nC 63.725203 199.920962 64.041302 199.157832 64.041302 198.362222 \nC 64.041302 197.566613 63.725203 196.803483 63.162622 196.240902 \nC 62.600041 195.678321 61.836911 195.362222 61.041302 195.362222 \nC 60.245692 195.362222 59.482562 195.678321 58.919981 196.240902 \nC 58.357401 196.803483 58.041302 197.566613 58.041302 198.362222 \nC 58.041302 199.157832 58.357401 199.920962 58.919981 200.483543 \nC 59.482562 201.046123 60.245692 201.362222 61.041302 201.362222 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 159.651656 \nC 62.812096 159.651656 63.575227 159.335557 64.137807 158.772976 \nC 64.700388 158.210395 65.016487 157.447265 65.016487 156.651656 \nC 65.016487 155.856046 64.700388 155.092916 64.137807 154.530335 \nC 63.575227 153.967755 62.812096 153.651656 62.016487 153.651656 \nC 61.220878 153.651656 60.457747 153.967755 59.895167 154.530335 \nC 59.332586 155.092916 59.016487 155.856046 59.016487 156.651656 \nC 59.016487 157.447265 59.332586 158.210395 59.895167 158.772976 \nC 60.457747 159.335557 61.220878 159.651656 62.016487 159.651656 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 159.480666 \nC 66.528841 159.480666 67.291971 159.164568 67.854552 158.601987 \nC 68.417132 158.039406 68.733231 157.276276 68.733231 156.480666 \nC 68.733231 155.685057 68.417132 154.921927 67.854552 154.359346 \nC 67.291971 153.796765 66.528841 153.480666 65.733231 153.480666 \nC 64.937622 153.480666 64.174492 153.796765 63.611911 154.359346 \nC 63.04933 154.921927 62.733231 155.685057 62.733231 156.480666 \nC 62.733231 157.276276 63.04933 158.039406 63.611911 158.601987 \nC 64.174492 159.164568 64.937622 159.480666 65.733231 159.480666 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 158.21795 \nC 81.027823 158.21795 81.790954 157.901851 82.353534 157.33927 \nC 82.916115 156.776689 83.232214 156.013559 83.232214 155.21795 \nC 83.232214 154.42234 82.916115 153.65921 82.353534 153.096629 \nC 81.790954 152.534049 81.027823 152.21795 80.232214 152.21795 \nC 79.436605 152.21795 78.673474 152.534049 78.110894 153.096629 \nC 77.548313 153.65921 77.232214 154.42234 77.232214 155.21795 \nC 77.232214 156.013559 77.548313 156.776689 78.110894 157.33927 \nC 78.673474 157.901851 79.436605 158.21795 80.232214 158.21795 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 176.366437 \nC 138.287765 176.366437 139.050895 176.050338 139.613476 175.487758 \nC 140.176056 174.925177 140.492155 174.162047 140.492155 173.366437 \nC 140.492155 172.570828 140.176056 171.807698 139.613476 171.245117 \nC 139.050895 170.682536 138.287765 170.366437 137.492155 170.366437 \nC 136.696546 170.366437 135.933416 170.682536 135.370835 171.245117 \nC 134.808254 171.807698 134.492155 172.570828 134.492155 173.366437 \nC 134.492155 174.162047 134.808254 174.925177 135.370835 175.487758 \nC 135.933416 176.050338 136.696546 176.366437 137.492155 176.366437 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 157.081613 \nC 365.855552 157.081613 366.618683 156.765514 367.181264 156.202934 \nC 367.743844 155.640353 368.059943 154.877223 368.059943 154.081613 \nC 368.059943 153.286004 367.743844 152.522874 367.181264 151.960293 \nC 366.618683 151.397712 365.855552 151.081613 365.059943 151.081613 \nC 364.264334 151.081613 363.501204 151.397712 362.938623 151.960293 \nC 362.376042 152.522874 362.059943 153.286004 362.059943 154.081613 \nC 362.059943 154.877223 362.376042 155.640353 362.938623 156.202934 \nC 363.501204 156.765514 364.264334 157.081613 365.059943 157.081613 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 175.844517 \nC 61.671313 175.844517 62.434444 175.528418 62.997024 174.965838 \nC 63.559605 174.403257 63.875704 173.640127 63.875704 172.844517 \nC 63.875704 172.048908 63.559605 171.285778 62.997024 170.723197 \nC 62.434444 170.160616 61.671313 169.844517 60.875704 169.844517 \nC 60.080095 169.844517 59.316965 170.160616 58.754384 170.723197 \nC 58.191803 171.285778 57.875704 172.048908 57.875704 172.844517 \nC 57.875704 173.640127 58.191803 174.403257 58.754384 174.965838 \nC 59.316965 175.528418 60.080095 175.844517 60.875704 175.844517 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 184.861999 \nC 61.836911 184.861999 62.600041 184.5459 63.162622 183.983319 \nC 63.725203 183.420739 64.041302 182.657608 64.041302 181.861999 \nC 64.041302 181.06639 63.725203 180.303259 63.162622 179.740679 \nC 62.600041 179.178098 61.836911 178.861999 61.041302 178.861999 \nC 60.245692 178.861999 59.482562 179.178098 58.919981 179.740679 \nC 58.357401 180.303259 58.041302 181.06639 58.041302 181.861999 \nC 58.041302 182.657608 58.357401 183.420739 58.919981 183.983319 \nC 59.482562 184.5459 60.245692 184.861999 61.041302 184.861999 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 217.756364 \nC 62.168106 217.756364 62.931236 217.440265 63.493817 216.877684 \nC 64.056398 216.315103 64.372497 215.551973 64.372497 214.756364 \nC 64.372497 213.960754 64.056398 213.197624 63.493817 212.635043 \nC 62.931236 212.072463 62.168106 211.756364 61.372497 211.756364 \nC 60.576887 211.756364 59.813757 212.072463 59.251176 212.635043 \nC 58.688596 213.197624 58.372497 213.960754 58.372497 214.756364 \nC 58.372497 215.551973 58.688596 216.315103 59.251176 216.877684 \nC 59.813757 217.440265 60.576887 217.756364 61.372497 217.756364 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 176.519408 \nC 62.830496 176.519408 63.593626 176.203309 64.156207 175.640728 \nC 64.718788 175.078148 65.034887 174.315017 65.034887 173.519408 \nC 65.034887 172.723799 64.718788 171.960668 64.156207 171.398088 \nC 63.593626 170.835507 62.830496 170.519408 62.034887 170.519408 \nC 61.239277 170.519408 60.476147 170.835507 59.913566 171.398088 \nC 59.350986 171.960668 59.034887 172.723799 59.034887 173.519408 \nC 59.034887 174.315017 59.350986 175.078148 59.913566 175.640728 \nC 60.476147 176.203309 61.239277 176.519408 62.034887 176.519408 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 37.879782 \nC 64.155276 37.879782 64.918406 37.563683 65.480987 37.001103 \nC 66.043568 36.438522 66.359667 35.675392 66.359667 34.879782 \nC 66.359667 34.084173 66.043568 33.321043 65.480987 32.758462 \nC 64.918406 32.195881 64.155276 31.879782 63.359667 31.879782 \nC 62.564058 31.879782 61.800927 32.195881 61.238347 32.758462 \nC 60.675766 33.321043 60.359667 34.084173 60.359667 34.879782 \nC 60.359667 35.675392 60.675766 36.438522 61.238347 37.001103 \nC 61.800927 37.563683 62.564058 37.879782 63.359667 37.879782 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869525 \nC 66.804836 37.869525 67.567967 37.553426 68.130547 36.990846 \nC 68.693128 36.428265 69.009227 35.665135 69.009227 34.869525 \nC 69.009227 34.073916 68.693128 33.310786 68.130547 32.748205 \nC 67.567967 32.185624 66.804836 31.869525 66.009227 31.869525 \nC 65.213618 31.869525 64.450488 32.185624 63.887907 32.748205 \nC 63.325326 33.310786 63.009227 34.073916 63.009227 34.869525 \nC 63.009227 35.665135 63.325326 36.428265 63.887907 36.990846 \nC 64.450488 37.553426 65.213618 37.869525 66.009227 37.869525 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 119.011982 \nL 60.696307 117.511982 \nL 62.196307 119.011982 \nL 63.696307 117.511982 \nL 62.196307 116.011982 \nL 63.696307 114.511982 \nL 62.196307 113.011982 \nL 60.696307 114.511982 \nL 59.196307 113.011982 \nL 57.696307 114.511982 \nL 59.196307 116.011982 \nL 57.696307 117.511982 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 159.436265 \nL 60.774506 157.936265 \nL 62.274506 159.436265 \nL 63.774506 157.936265 \nL 62.274506 156.436265 \nL 63.774506 154.936265 \nL 62.274506 153.436265 \nL 60.774506 154.936265 \nL 59.274506 153.436265 \nL 57.774506 154.936265 \nL 59.274506 156.436265 \nL 57.774506 157.936265 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 160.793851 \nL 61.041302 159.293851 \nL 62.541302 160.793851 \nL 64.041302 159.293851 \nL 62.541302 157.793851 \nL 64.041302 156.293851 \nL 62.541302 154.793851 \nL 61.041302 156.293851 \nL 59.541302 154.793851 \nL 58.041302 156.293851 \nL 59.541302 157.793851 \nL 58.041302 159.293851 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 178.91928 \nL 62.016487 177.41928 \nL 63.516487 178.91928 \nL 65.016487 177.41928 \nL 63.516487 175.91928 \nL 65.016487 174.41928 \nL 63.516487 172.91928 \nL 62.016487 174.41928 \nL 60.516487 172.91928 \nL 59.016487 174.41928 \nL 60.516487 175.91928 \nL 59.016487 177.41928 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 158.571639 \nL 65.733231 157.071639 \nL 67.233231 158.571639 \nL 68.733231 157.071639 \nL 67.233231 155.571639 \nL 68.733231 154.071639 \nL 67.233231 152.571639 \nL 65.733231 154.071639 \nL 64.233231 152.571639 \nL 62.733231 154.071639 \nL 64.233231 155.571639 \nL 62.733231 157.071639 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 157.946141 \nL 80.232214 156.446141 \nL 81.732214 157.946141 \nL 83.232214 156.446141 \nL 81.732214 154.946141 \nL 83.232214 153.446141 \nL 81.732214 151.946141 \nL 80.232214 153.446141 \nL 78.732214 151.946141 \nL 77.232214 153.446141 \nL 78.732214 154.946141 \nL 77.232214 156.446141 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 156.786634 \nL 137.492155 155.286634 \nL 138.992155 156.786634 \nL 140.492155 155.286634 \nL 138.992155 153.786634 \nL 140.492155 152.286634 \nL 138.992155 150.786634 \nL 137.492155 152.286634 \nL 135.992155 150.786634 \nL 134.492155 152.286634 \nL 135.992155 153.786634 \nL 134.492155 155.286634 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 155.950976 \nL 365.059943 154.450976 \nL 366.559943 155.950976 \nL 368.059943 154.450976 \nL 366.559943 152.950976 \nL 368.059943 151.450976 \nL 366.559943 149.950976 \nL 365.059943 151.450976 \nL 363.559943 149.950976 \nL 362.059943 151.450976 \nL 363.559943 152.950976 \nL 362.059943 154.450976 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 158.300461 \nL 60.875704 156.800461 \nL 62.375704 158.300461 \nL 63.875704 156.800461 \nL 62.375704 155.300461 \nL 63.875704 153.800461 \nL 62.375704 152.300461 \nL 60.875704 153.800461 \nL 59.375704 152.300461 \nL 57.875704 153.800461 \nL 59.375704 155.300461 \nL 57.875704 156.800461 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 158.114603 \nL 61.041302 156.614603 \nL 62.541302 158.114603 \nL 64.041302 156.614603 \nL 62.541302 155.114603 \nL 64.041302 153.614603 \nL 62.541302 152.114603 \nL 61.041302 153.614603 \nL 59.541302 152.114603 \nL 58.041302 153.614603 \nL 59.541302 155.114603 \nL 58.041302 156.614603 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 157.956563 \nL 61.372497 156.456563 \nL 62.872497 157.956563 \nL 64.372497 156.456563 \nL 62.872497 154.956563 \nL 64.372497 153.456563 \nL 62.872497 151.956563 \nL 61.372497 153.456563 \nL 59.872497 151.956563 \nL 58.372497 153.456563 \nL 59.872497 154.956563 \nL 58.372497 156.456563 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 180.194213 \nL 62.034887 178.694213 \nL 63.534887 180.194213 \nL 65.034887 178.694213 \nL 63.534887 177.194213 \nL 65.034887 175.694213 \nL 63.534887 174.194213 \nL 62.034887 175.694213 \nL 60.534887 174.194213 \nL 59.034887 175.694213 \nL 60.534887 177.194213 \nL 59.034887 178.694213 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 180.73527 \nL 63.359667 179.23527 \nL 64.859667 180.73527 \nL 66.359667 179.23527 \nL 64.859667 177.73527 \nL 66.359667 176.23527 \nL 64.859667 174.73527 \nL 63.359667 176.23527 \nL 61.859667 174.73527 \nL 60.359667 176.23527 \nL 61.859667 177.73527 \nL 60.359667 179.23527 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 180.244563 \nL 66.009227 178.744563 \nL 67.509227 180.244563 \nL 69.009227 178.744563 \nL 67.509227 177.244563 \nL 69.009227 175.744563 \nL 67.509227 174.244563 \nL 66.009227 175.744563 \nL 64.509227 174.244563 \nL 63.009227 175.744563 \nL 64.509227 177.244563 \nL 63.009227 178.744563 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 154.603271 \nC 61.491916 154.603271 62.255046 154.287172 62.817627 153.724592 \nC 63.380208 153.162011 63.696307 152.398881 63.696307 151.603271 \nC 63.696307 150.807662 63.380208 150.044532 62.817627 149.481951 \nC 62.255046 148.91937 61.491916 148.603271 60.696307 148.603271 \nC 59.900698 148.603271 59.137567 148.91937 58.574986 149.481951 \nC 58.012406 150.044532 57.696307 150.807662 57.696307 151.603271 \nC 57.696307 152.398881 58.012406 153.162011 58.574986 153.724592 \nC 59.137567 154.287172 59.900698 154.603271 60.696307 154.603271 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 180.346546 \nC 61.570115 180.346546 62.333245 180.030447 62.895826 179.467866 \nC 63.458407 178.905285 63.774506 178.142155 63.774506 177.346546 \nC 63.774506 176.550936 63.458407 175.787806 62.895826 175.225225 \nC 62.333245 174.662645 61.570115 174.346546 60.774506 174.346546 \nC 59.978896 174.346546 59.215766 174.662645 58.653185 175.225225 \nC 58.090605 175.787806 57.774506 176.550936 57.774506 177.346546 \nC 57.774506 178.142155 58.090605 178.905285 58.653185 179.467866 \nC 59.215766 180.030447 59.978896 180.346546 60.774506 180.346546 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 179.707434 \nC 61.836911 179.707434 62.600041 179.391335 63.162622 178.828754 \nC 63.725203 178.266173 64.041302 177.503043 64.041302 176.707434 \nC 64.041302 175.911824 63.725203 175.148694 63.162622 174.586113 \nC 62.600041 174.023533 61.836911 173.707434 61.041302 173.707434 \nC 60.245692 173.707434 59.482562 174.023533 58.919981 174.586113 \nC 58.357401 175.148694 58.041302 175.911824 58.041302 176.707434 \nC 58.041302 177.503043 58.357401 178.266173 58.919981 178.828754 \nC 59.482562 179.391335 60.245692 179.707434 61.041302 179.707434 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 159.475542 \nC 62.812096 159.475542 63.575227 159.159443 64.137807 158.596862 \nC 64.700388 158.034281 65.016487 157.271151 65.016487 156.475542 \nC 65.016487 155.679933 64.700388 154.916802 64.137807 154.354222 \nC 63.575227 153.791641 62.812096 153.475542 62.016487 153.475542 \nC 61.220878 153.475542 60.457747 153.791641 59.895167 154.354222 \nC 59.332586 154.916802 59.016487 155.679933 59.016487 156.475542 \nC 59.016487 157.271151 59.332586 158.034281 59.895167 158.596862 \nC 60.457747 159.159443 61.220878 159.475542 62.016487 159.475542 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 159.127248 \nC 66.528841 159.127248 67.291971 158.811149 67.854552 158.248568 \nC 68.417132 157.685988 68.733231 156.922857 68.733231 156.127248 \nC 68.733231 155.331639 68.417132 154.568508 67.854552 154.005928 \nC 67.291971 153.443347 66.528841 153.127248 65.733231 153.127248 \nC 64.937622 153.127248 64.174492 153.443347 63.611911 154.005928 \nC 63.04933 154.568508 62.733231 155.331639 62.733231 156.127248 \nC 62.733231 156.922857 63.04933 157.685988 63.611911 158.248568 \nC 64.174492 158.811149 64.937622 159.127248 65.733231 159.127248 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 159.948397 \nC 81.027823 159.948397 81.790954 159.632298 82.353534 159.069718 \nC 82.916115 158.507137 83.232214 157.744007 83.232214 156.948397 \nC 83.232214 156.152788 82.916115 155.389658 82.353534 154.827077 \nC 81.790954 154.264496 81.027823 153.948397 80.232214 153.948397 \nC 79.436605 153.948397 78.673474 154.264496 78.110894 154.827077 \nC 77.548313 155.389658 77.232214 156.152788 77.232214 156.948397 \nC 77.232214 157.744007 77.548313 158.507137 78.110894 159.069718 \nC 78.673474 159.632298 79.436605 159.948397 80.232214 159.948397 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 157.312344 \nC 138.287765 157.312344 139.050895 156.996245 139.613476 156.433664 \nC 140.176056 155.871084 140.492155 155.107953 140.492155 154.312344 \nC 140.492155 153.516735 140.176056 152.753604 139.613476 152.191024 \nC 139.050895 151.628443 138.287765 151.312344 137.492155 151.312344 \nC 136.696546 151.312344 135.933416 151.628443 135.370835 152.191024 \nC 134.808254 152.753604 134.492155 153.516735 134.492155 154.312344 \nC 134.492155 155.107953 134.808254 155.871084 135.370835 156.433664 \nC 135.933416 156.996245 136.696546 157.312344 137.492155 157.312344 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 157.020597 \nC 365.855552 157.020597 366.618683 156.704498 367.181264 156.141917 \nC 367.743844 155.579336 368.059943 154.816206 368.059943 154.020597 \nC 368.059943 153.224988 367.743844 152.461857 367.181264 151.899276 \nC 366.618683 151.336696 365.855552 151.020597 365.059943 151.020597 \nC 364.264334 151.020597 363.501204 151.336696 362.938623 151.899276 \nC 362.376042 152.461857 362.059943 153.224988 362.059943 154.020597 \nC 362.059943 154.816206 362.376042 155.579336 362.938623 156.141917 \nC 363.501204 156.704498 364.264334 157.020597 365.059943 157.020597 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 159.283468 \nC 61.671313 159.283468 62.434444 158.967369 62.997024 158.404788 \nC 63.559605 157.842208 63.875704 157.079077 63.875704 156.283468 \nC 63.875704 155.487859 63.559605 154.724728 62.997024 154.162148 \nC 62.434444 153.599567 61.671313 153.283468 60.875704 153.283468 \nC 60.080095 153.283468 59.316965 153.599567 58.754384 154.162148 \nC 58.191803 154.724728 57.875704 155.487859 57.875704 156.283468 \nC 57.875704 157.079077 58.191803 157.842208 58.754384 158.404788 \nC 59.316965 158.967369 60.080095 159.283468 60.875704 159.283468 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 173.951629 \nC 61.836911 173.951629 62.600041 173.63553 63.162622 173.072949 \nC 63.725203 172.510369 64.041302 171.747238 64.041302 170.951629 \nC 64.041302 170.15602 63.725203 169.39289 63.162622 168.830309 \nC 62.600041 168.267728 61.836911 167.951629 61.041302 167.951629 \nC 60.245692 167.951629 59.482562 168.267728 58.919981 168.830309 \nC 58.357401 169.39289 58.041302 170.15602 58.041302 170.951629 \nC 58.041302 171.747238 58.357401 172.510369 58.919981 173.072949 \nC 59.482562 173.63553 60.245692 173.951629 61.041302 173.951629 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 182.531233 \nC 62.168106 182.531233 62.931236 182.215134 63.493817 181.652553 \nC 64.056398 181.089973 64.372497 180.326842 64.372497 179.531233 \nC 64.372497 178.735624 64.056398 177.972493 63.493817 177.409913 \nC 62.931236 176.847332 62.168106 176.531233 61.372497 176.531233 \nC 60.576887 176.531233 59.813757 176.847332 59.251176 177.409913 \nC 58.688596 177.972493 58.372497 178.735624 58.372497 179.531233 \nC 58.372497 180.326842 58.688596 181.089973 59.251176 181.652553 \nC 59.813757 182.215134 60.576887 182.531233 61.372497 182.531233 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 158.032553 \nC 62.830496 158.032553 63.593626 157.716454 64.156207 157.153873 \nC 64.718788 156.591293 65.034887 155.828162 65.034887 155.032553 \nC 65.034887 154.236944 64.718788 153.473813 64.156207 152.911233 \nC 63.593626 152.348652 62.830496 152.032553 62.034887 152.032553 \nC 61.239277 152.032553 60.476147 152.348652 59.913566 152.911233 \nC 59.350986 153.473813 59.034887 154.236944 59.034887 155.032553 \nC 59.034887 155.828162 59.350986 156.591293 59.913566 157.153873 \nC 60.476147 157.716454 61.239277 158.032553 62.034887 158.032553 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 147.351577 \nC 64.155276 147.351577 64.918406 147.035478 65.480987 146.472897 \nC 66.043568 145.910317 66.359667 145.147186 66.359667 144.351577 \nC 66.359667 143.555968 66.043568 142.792837 65.480987 142.230257 \nC 64.918406 141.667676 64.155276 141.351577 63.359667 141.351577 \nC 62.564058 141.351577 61.800927 141.667676 61.238347 142.230257 \nC 60.675766 142.792837 60.359667 143.555968 60.359667 144.351577 \nC 60.359667 145.147186 60.675766 145.910317 61.238347 146.472897 \nC 61.800927 147.035478 62.564058 147.351577 63.359667 147.351577 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869544 \nC 66.804836 37.869544 67.567967 37.553445 68.130547 36.990865 \nC 68.693128 36.428284 69.009227 35.665154 69.009227 34.869544 \nC 69.009227 34.073935 68.693128 33.310805 68.130547 32.748224 \nC 67.567967 32.185643 66.804836 31.869544 66.009227 31.869544 \nC 65.213618 31.869544 64.450488 32.185643 63.887907 32.748224 \nC 63.325326 33.310805 63.009227 34.073935 63.009227 34.869544 \nC 63.009227 35.665154 63.325326 36.428284 63.887907 36.990865 \nC 64.450488 37.553445 65.213618 37.869544 66.009227 37.869544 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 144.086063 \nL 60.696307 142.586063 \nL 62.196307 144.086063 \nL 63.696307 142.586063 \nL 62.196307 141.086063 \nL 63.696307 139.586063 \nL 62.196307 138.086063 \nL 60.696307 139.586063 \nL 59.196307 138.086063 \nL 57.696307 139.586063 \nL 59.196307 141.086063 \nL 57.696307 142.586063 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 161.217734 \nL 60.774506 159.717734 \nL 62.274506 161.217734 \nL 63.774506 159.717734 \nL 62.274506 158.217734 \nL 63.774506 156.717734 \nL 62.274506 155.217734 \nL 60.774506 156.717734 \nL 59.274506 155.217734 \nL 57.774506 156.717734 \nL 59.274506 158.217734 \nL 57.774506 159.717734 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 158.39246 \nL 61.041302 156.89246 \nL 62.541302 158.39246 \nL 64.041302 156.89246 \nL 62.541302 155.39246 \nL 64.041302 153.89246 \nL 62.541302 152.39246 \nL 61.041302 153.89246 \nL 59.541302 152.39246 \nL 58.041302 153.89246 \nL 59.541302 155.39246 \nL 58.041302 156.89246 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 158.26373 \nL 62.016487 156.76373 \nL 63.516487 158.26373 \nL 65.016487 156.76373 \nL 63.516487 155.26373 \nL 65.016487 153.76373 \nL 63.516487 152.26373 \nL 62.016487 153.76373 \nL 60.516487 152.26373 \nL 59.016487 153.76373 \nL 60.516487 155.26373 \nL 59.016487 156.76373 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 157.852706 \nL 65.733231 156.352706 \nL 67.233231 157.852706 \nL 68.733231 156.352706 \nL 67.233231 154.852706 \nL 68.733231 153.352706 \nL 67.233231 151.852706 \nL 65.733231 153.352706 \nL 64.233231 151.852706 \nL 62.733231 153.352706 \nL 64.233231 154.852706 \nL 62.733231 156.352706 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 155.919935 \nL 80.232214 154.419935 \nL 81.732214 155.919935 \nL 83.232214 154.419935 \nL 81.732214 152.919935 \nL 83.232214 151.419935 \nL 81.732214 149.919935 \nL 80.232214 151.419935 \nL 78.732214 149.919935 \nL 77.232214 151.419935 \nL 78.732214 152.919935 \nL 77.232214 154.419935 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 157.045686 \nL 137.492155 155.545686 \nL 138.992155 157.045686 \nL 140.492155 155.545686 \nL 138.992155 154.045686 \nL 140.492155 152.545686 \nL 138.992155 151.045686 \nL 137.492155 152.545686 \nL 135.992155 151.045686 \nL 134.492155 152.545686 \nL 135.992155 154.045686 \nL 134.492155 155.545686 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 155.065376 \nL 365.059943 153.565376 \nL 366.559943 155.065376 \nL 368.059943 153.565376 \nL 366.559943 152.065376 \nL 368.059943 150.565376 \nL 366.559943 149.065376 \nL 365.059943 150.565376 \nL 363.559943 149.065376 \nL 362.059943 150.565376 \nL 363.559943 152.065376 \nL 362.059943 153.565376 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 156.883745 \nL 60.875704 155.383745 \nL 62.375704 156.883745 \nL 63.875704 155.383745 \nL 62.375704 153.883745 \nL 63.875704 152.383745 \nL 62.375704 150.883745 \nL 60.875704 152.383745 \nL 59.375704 150.883745 \nL 57.875704 152.383745 \nL 59.375704 153.883745 \nL 57.875704 155.383745 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 157.020191 \nL 61.041302 155.520191 \nL 62.541302 157.020191 \nL 64.041302 155.520191 \nL 62.541302 154.020191 \nL 64.041302 152.520191 \nL 62.541302 151.020191 \nL 61.041302 152.520191 \nL 59.541302 151.020191 \nL 58.041302 152.520191 \nL 59.541302 154.020191 \nL 58.041302 155.520191 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 159.908434 \nL 61.372497 158.408434 \nL 62.872497 159.908434 \nL 64.372497 158.408434 \nL 62.872497 156.908434 \nL 64.372497 155.408434 \nL 62.872497 153.908434 \nL 61.372497 155.408434 \nL 59.872497 153.908434 \nL 58.372497 155.408434 \nL 59.872497 156.908434 \nL 58.372497 158.408434 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 158.687969 \nL 62.034887 157.187969 \nL 63.534887 158.687969 \nL 65.034887 157.187969 \nL 63.534887 155.687969 \nL 65.034887 154.187969 \nL 63.534887 152.687969 \nL 62.034887 154.187969 \nL 60.534887 152.687969 \nL 59.034887 154.187969 \nL 60.534887 155.687969 \nL 59.034887 157.187969 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 157.789434 \nL 63.359667 156.289434 \nL 64.859667 157.789434 \nL 66.359667 156.289434 \nL 64.859667 154.789434 \nL 66.359667 153.289434 \nL 64.859667 151.789434 \nL 63.359667 153.289434 \nL 61.859667 151.789434 \nL 60.359667 153.289434 \nL 61.859667 154.789434 \nL 60.359667 156.289434 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 189.831119 \nL 66.009227 188.331119 \nL 67.509227 189.831119 \nL 69.009227 188.331119 \nL 67.509227 186.831119 \nL 69.009227 185.331119 \nL 67.509227 183.831119 \nL 66.009227 185.331119 \nL 64.509227 183.831119 \nL 63.009227 185.331119 \nL 64.509227 186.831119 \nL 63.009227 188.331119 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 156.538028 \nC 61.491916 156.538028 62.255046 156.221929 62.817627 155.659348 \nC 63.380208 155.096768 63.696307 154.333637 63.696307 153.538028 \nC 63.696307 152.742419 63.380208 151.979289 62.817627 151.416708 \nC 62.255046 150.854127 61.491916 150.538028 60.696307 150.538028 \nC 59.900698 150.538028 59.137567 150.854127 58.574986 151.416708 \nC 58.012406 151.979289 57.696307 152.742419 57.696307 153.538028 \nC 57.696307 154.333637 58.012406 155.096768 58.574986 155.659348 \nC 59.137567 156.221929 59.900698 156.538028 60.696307 156.538028 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 161.088349 \nC 61.570115 161.088349 62.333245 160.77225 62.895826 160.209669 \nC 63.458407 159.647088 63.774506 158.883958 63.774506 158.088349 \nC 63.774506 157.292739 63.458407 156.529609 62.895826 155.967028 \nC 62.333245 155.404448 61.570115 155.088349 60.774506 155.088349 \nC 59.978896 155.088349 59.215766 155.404448 58.653185 155.967028 \nC 58.090605 156.529609 57.774506 157.292739 57.774506 158.088349 \nC 57.774506 158.883958 58.090605 159.647088 58.653185 160.209669 \nC 59.215766 160.77225 59.978896 161.088349 60.774506 161.088349 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 190.084536 \nC 61.836911 190.084536 62.600041 189.768437 63.162622 189.205856 \nC 63.725203 188.643275 64.041302 187.880145 64.041302 187.084536 \nC 64.041302 186.288926 63.725203 185.525796 63.162622 184.963215 \nC 62.600041 184.400634 61.836911 184.084536 61.041302 184.084536 \nC 60.245692 184.084536 59.482562 184.400634 58.919981 184.963215 \nC 58.357401 185.525796 58.041302 186.288926 58.041302 187.084536 \nC 58.041302 187.880145 58.357401 188.643275 58.919981 189.205856 \nC 59.482562 189.768437 60.245692 190.084536 61.041302 190.084536 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 158.471261 \nC 62.812096 158.471261 63.575227 158.155162 64.137807 157.592582 \nC 64.700388 157.030001 65.016487 156.266871 65.016487 155.471261 \nC 65.016487 154.675652 64.700388 153.912522 64.137807 153.349941 \nC 63.575227 152.78736 62.812096 152.471261 62.016487 152.471261 \nC 61.220878 152.471261 60.457747 152.78736 59.895167 153.349941 \nC 59.332586 153.912522 59.016487 154.675652 59.016487 155.471261 \nC 59.016487 156.266871 59.332586 157.030001 59.895167 157.592582 \nC 60.457747 158.155162 61.220878 158.471261 62.016487 158.471261 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 160.160226 \nC 66.528841 160.160226 67.291971 159.844127 67.854552 159.281547 \nC 68.417132 158.718966 68.733231 157.955836 68.733231 157.160226 \nC 68.733231 156.364617 68.417132 155.601487 67.854552 155.038906 \nC 67.291971 154.476325 66.528841 154.160226 65.733231 154.160226 \nC 64.937622 154.160226 64.174492 154.476325 63.611911 155.038906 \nC 63.04933 155.601487 62.733231 156.364617 62.733231 157.160226 \nC 62.733231 157.955836 63.04933 158.718966 63.611911 159.281547 \nC 64.174492 159.844127 64.937622 160.160226 65.733231 160.160226 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 158.059418 \nC 81.027823 158.059418 81.790954 157.743319 82.353534 157.180738 \nC 82.916115 156.618157 83.232214 155.855027 83.232214 155.059418 \nC 83.232214 154.263808 82.916115 153.500678 82.353534 152.938097 \nC 81.790954 152.375517 81.027823 152.059418 80.232214 152.059418 \nC 79.436605 152.059418 78.673474 152.375517 78.110894 152.938097 \nC 77.548313 153.500678 77.232214 154.263808 77.232214 155.059418 \nC 77.232214 155.855027 77.548313 156.618157 78.110894 157.180738 \nC 78.673474 157.743319 79.436605 158.059418 80.232214 158.059418 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 158.616055 \nC 138.287765 158.616055 139.050895 158.299956 139.613476 157.737375 \nC 140.176056 157.174795 140.492155 156.411664 140.492155 155.616055 \nC 140.492155 154.820446 140.176056 154.057315 139.613476 153.494735 \nC 139.050895 152.932154 138.287765 152.616055 137.492155 152.616055 \nC 136.696546 152.616055 135.933416 152.932154 135.370835 153.494735 \nC 134.808254 154.057315 134.492155 154.820446 134.492155 155.616055 \nC 134.492155 156.411664 134.808254 157.174795 135.370835 157.737375 \nC 135.933416 158.299956 136.696546 158.616055 137.492155 158.616055 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 156.948275 \nC 365.855552 156.948275 366.618683 156.632176 367.181264 156.069595 \nC 367.743844 155.507014 368.059943 154.743884 368.059943 153.948275 \nC 368.059943 153.152665 367.743844 152.389535 367.181264 151.826954 \nC 366.618683 151.264374 365.855552 150.948275 365.059943 150.948275 \nC 364.264334 150.948275 363.501204 151.264374 362.938623 151.826954 \nC 362.376042 152.389535 362.059943 153.152665 362.059943 153.948275 \nC 362.059943 154.743884 362.376042 155.507014 362.938623 156.069595 \nC 363.501204 156.632176 364.264334 156.948275 365.059943 156.948275 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 159.623635 \nC 61.671313 159.623635 62.434444 159.307536 62.997024 158.744955 \nC 63.559605 158.182374 63.875704 157.419244 63.875704 156.623635 \nC 63.875704 155.828025 63.559605 155.064895 62.997024 154.502314 \nC 62.434444 153.939734 61.671313 153.623635 60.875704 153.623635 \nC 60.080095 153.623635 59.316965 153.939734 58.754384 154.502314 \nC 58.191803 155.064895 57.875704 155.828025 57.875704 156.623635 \nC 57.875704 157.419244 58.191803 158.182374 58.754384 158.744955 \nC 59.316965 159.307536 60.080095 159.623635 60.875704 159.623635 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 187.663108 \nC 61.836911 187.663108 62.600041 187.347009 63.162622 186.784428 \nC 63.725203 186.221848 64.041302 185.458717 64.041302 184.663108 \nC 64.041302 183.867499 63.725203 183.104368 63.162622 182.541788 \nC 62.600041 181.979207 61.836911 181.663108 61.041302 181.663108 \nC 60.245692 181.663108 59.482562 181.979207 58.919981 182.541788 \nC 58.357401 183.104368 58.041302 183.867499 58.041302 184.663108 \nC 58.041302 185.458717 58.357401 186.221848 58.919981 186.784428 \nC 59.482562 187.347009 60.245692 187.663108 61.041302 187.663108 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 159.576296 \nC 62.168106 159.576296 62.931236 159.260197 63.493817 158.697616 \nC 64.056398 158.135035 64.372497 157.371905 64.372497 156.576296 \nC 64.372497 155.780686 64.056398 155.017556 63.493817 154.454975 \nC 62.931236 153.892395 62.168106 153.576296 61.372497 153.576296 \nC 60.576887 153.576296 59.813757 153.892395 59.251176 154.454975 \nC 58.688596 155.017556 58.372497 155.780686 58.372497 156.576296 \nC 58.372497 157.371905 58.688596 158.135035 59.251176 158.697616 \nC 59.813757 159.260197 60.576887 159.576296 61.372497 159.576296 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 177.251182 \nC 62.830496 177.251182 63.593626 176.935083 64.156207 176.372503 \nC 64.718788 175.809922 65.034887 175.046791 65.034887 174.251182 \nC 65.034887 173.455573 64.718788 172.692443 64.156207 172.129862 \nC 63.593626 171.567281 62.830496 171.251182 62.034887 171.251182 \nC 61.239277 171.251182 60.476147 171.567281 59.913566 172.129862 \nC 59.350986 172.692443 59.034887 173.455573 59.034887 174.251182 \nC 59.034887 175.046791 59.350986 175.809922 59.913566 176.372503 \nC 60.476147 176.935083 61.239277 177.251182 62.034887 177.251182 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 20.083636 \nC 64.155276 20.083636 64.918406 19.767537 65.480987 19.204957 \nC 66.043568 18.642376 66.359667 17.879246 66.359667 17.083636 \nC 66.359667 16.288027 66.043568 15.524897 65.480987 14.962316 \nC 64.918406 14.399735 64.155276 14.083636 63.359667 14.083636 \nC 62.564058 14.083636 61.800927 14.399735 61.238347 14.962316 \nC 60.675766 15.524897 60.359667 16.288027 60.359667 17.083636 \nC 60.359667 17.879246 60.675766 18.642376 61.238347 19.204957 \nC 61.800927 19.767537 62.564058 20.083636 63.359667 20.083636 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869536 \nC 66.804836 37.869536 67.567967 37.553437 68.130547 36.990856 \nC 68.693128 36.428275 69.009227 35.665145 69.009227 34.869536 \nC 69.009227 34.073927 68.693128 33.310796 68.130547 32.748215 \nC 67.567967 32.185635 66.804836 31.869536 66.009227 31.869536 \nC 65.213618 31.869536 64.450488 32.185635 63.887907 32.748215 \nC 63.325326 33.310796 63.009227 34.073927 63.009227 34.869536 \nC 63.009227 35.665145 63.325326 36.428275 63.887907 36.990856 \nC 64.450488 37.553437 65.213618 37.869536 66.009227 37.869536 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 158.916798 \nL 60.696307 157.416798 \nL 62.196307 158.916798 \nL 63.696307 157.416798 \nL 62.196307 155.916798 \nL 63.696307 154.416798 \nL 62.196307 152.916798 \nL 60.696307 154.416798 \nL 59.196307 152.916798 \nL 57.696307 154.416798 \nL 59.196307 155.916798 \nL 57.696307 157.416798 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 178.595881 \nL 60.774506 177.095881 \nL 62.274506 178.595881 \nL 63.774506 177.095881 \nL 62.274506 175.595881 \nL 63.774506 174.095881 \nL 62.274506 172.595881 \nL 60.774506 174.095881 \nL 59.274506 172.595881 \nL 57.774506 174.095881 \nL 59.274506 175.595881 \nL 57.774506 177.095881 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 158.75667 \nL 61.041302 157.25667 \nL 62.541302 158.75667 \nL 64.041302 157.25667 \nL 62.541302 155.75667 \nL 64.041302 154.25667 \nL 62.541302 152.75667 \nL 61.041302 154.25667 \nL 59.541302 152.75667 \nL 58.041302 154.25667 \nL 59.541302 155.75667 \nL 58.041302 157.25667 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 156.494049 \nL 62.016487 154.994049 \nL 63.516487 156.494049 \nL 65.016487 154.994049 \nL 63.516487 153.494049 \nL 65.016487 151.994049 \nL 63.516487 150.494049 \nL 62.016487 151.994049 \nL 60.516487 150.494049 \nL 59.016487 151.994049 \nL 60.516487 153.494049 \nL 59.016487 154.994049 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 158.550363 \nL 65.733231 157.050363 \nL 67.233231 158.550363 \nL 68.733231 157.050363 \nL 67.233231 155.550363 \nL 68.733231 154.050363 \nL 67.233231 152.550363 \nL 65.733231 154.050363 \nL 64.233231 152.550363 \nL 62.733231 154.050363 \nL 64.233231 155.550363 \nL 62.733231 157.050363 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 158.433626 \nL 80.232214 156.933626 \nL 81.732214 158.433626 \nL 83.232214 156.933626 \nL 81.732214 155.433626 \nL 83.232214 153.933626 \nL 81.732214 152.433626 \nL 80.232214 153.933626 \nL 78.732214 152.433626 \nL 77.232214 153.933626 \nL 78.732214 155.433626 \nL 77.232214 156.933626 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 156.599345 \nL 137.492155 155.099345 \nL 138.992155 156.599345 \nL 140.492155 155.099345 \nL 138.992155 153.599345 \nL 140.492155 152.099345 \nL 138.992155 150.599345 \nL 137.492155 152.099345 \nL 135.992155 150.599345 \nL 134.492155 152.099345 \nL 135.992155 153.599345 \nL 134.492155 155.099345 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 156.431521 \nL 365.059943 154.931521 \nL 366.559943 156.431521 \nL 368.059943 154.931521 \nL 366.559943 153.431521 \nL 368.059943 151.931521 \nL 366.559943 150.431521 \nL 365.059943 151.931521 \nL 363.559943 150.431521 \nL 362.059943 151.931521 \nL 363.559943 153.431521 \nL 362.059943 154.931521 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 157.388041 \nL 60.875704 155.888041 \nL 62.375704 157.388041 \nL 63.875704 155.888041 \nL 62.375704 154.388041 \nL 63.875704 152.888041 \nL 62.375704 151.388041 \nL 60.875704 152.888041 \nL 59.375704 151.388041 \nL 57.875704 152.888041 \nL 59.375704 154.388041 \nL 57.875704 155.888041 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 159.090408 \nL 61.041302 157.590408 \nL 62.541302 159.090408 \nL 64.041302 157.590408 \nL 62.541302 156.090408 \nL 64.041302 154.590408 \nL 62.541302 153.090408 \nL 61.041302 154.590408 \nL 59.541302 153.090408 \nL 58.041302 154.590408 \nL 59.541302 156.090408 \nL 58.041302 157.590408 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 178.899747 \nL 61.372497 177.399747 \nL 62.872497 178.899747 \nL 64.372497 177.399747 \nL 62.872497 175.899747 \nL 64.372497 174.399747 \nL 62.872497 172.899747 \nL 61.372497 174.399747 \nL 59.872497 172.899747 \nL 58.372497 174.399747 \nL 59.872497 175.899747 \nL 58.372497 177.399747 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 186.659689 \nL 62.034887 185.159689 \nL 63.534887 186.659689 \nL 65.034887 185.159689 \nL 63.534887 183.659689 \nL 65.034887 182.159689 \nL 63.534887 180.659689 \nL 62.034887 182.159689 \nL 60.534887 180.659689 \nL 59.034887 182.159689 \nL 60.534887 183.659689 \nL 59.034887 185.159689 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 179.273007 \nL 63.359667 177.773007 \nL 64.859667 179.273007 \nL 66.359667 177.773007 \nL 64.859667 176.273007 \nL 66.359667 174.773007 \nL 64.859667 173.273007 \nL 63.359667 174.773007 \nL 61.859667 173.273007 \nL 60.359667 174.773007 \nL 61.859667 176.273007 \nL 60.359667 177.773007 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 158.596228 \nL 66.009227 157.096228 \nL 67.509227 158.596228 \nL 69.009227 157.096228 \nL 67.509227 155.596228 \nL 69.009227 154.096228 \nL 67.509227 152.596228 \nL 66.009227 154.096228 \nL 64.509227 152.596228 \nL 63.009227 154.096228 \nL 64.509227 155.596228 \nL 63.009227 157.096228 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 157.837436 \nC 61.491916 157.837436 62.255046 157.521337 62.817627 156.958756 \nC 63.380208 156.396175 63.696307 155.633045 63.696307 154.837436 \nC 63.696307 154.041826 63.380208 153.278696 62.817627 152.716115 \nC 62.255046 152.153535 61.491916 151.837436 60.696307 151.837436 \nC 59.900698 151.837436 59.137567 152.153535 58.574986 152.716115 \nC 58.012406 153.278696 57.696307 154.041826 57.696307 154.837436 \nC 57.696307 155.633045 58.012406 156.396175 58.574986 156.958756 \nC 59.137567 157.521337 59.900698 157.837436 60.696307 157.837436 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 177.054919 \nC 61.570115 177.054919 62.333245 176.73882 62.895826 176.176239 \nC 63.458407 175.613658 63.774506 174.850528 63.774506 174.054919 \nC 63.774506 173.259309 63.458407 172.496179 62.895826 171.933598 \nC 62.333245 171.371017 61.570115 171.054919 60.774506 171.054919 \nC 59.978896 171.054919 59.215766 171.371017 58.653185 171.933598 \nC 58.090605 172.496179 57.774506 173.259309 57.774506 174.054919 \nC 57.774506 174.850528 58.090605 175.613658 58.653185 176.176239 \nC 59.215766 176.73882 59.978896 177.054919 60.774506 177.054919 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 187.532967 \nC 61.836911 187.532967 62.600041 187.216868 63.162622 186.654288 \nC 63.725203 186.091707 64.041302 185.328576 64.041302 184.532967 \nC 64.041302 183.737358 63.725203 182.974228 63.162622 182.411647 \nC 62.600041 181.849066 61.836911 181.532967 61.041302 181.532967 \nC 60.245692 181.532967 59.482562 181.849066 58.919981 182.411647 \nC 58.357401 182.974228 58.041302 183.737358 58.041302 184.532967 \nC 58.041302 185.328576 58.357401 186.091707 58.919981 186.654288 \nC 59.482562 187.216868 60.245692 187.532967 61.041302 187.532967 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 159.404731 \nC 62.812096 159.404731 63.575227 159.088632 64.137807 158.526052 \nC 64.700388 157.963471 65.016487 157.200341 65.016487 156.404731 \nC 65.016487 155.609122 64.700388 154.845992 64.137807 154.283411 \nC 63.575227 153.72083 62.812096 153.404731 62.016487 153.404731 \nC 61.220878 153.404731 60.457747 153.72083 59.895167 154.283411 \nC 59.332586 154.845992 59.016487 155.609122 59.016487 156.404731 \nC 59.016487 157.200341 59.332586 157.963471 59.895167 158.526052 \nC 60.457747 159.088632 61.220878 159.404731 62.016487 159.404731 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 157.692612 \nC 66.528841 157.692612 67.291971 157.376513 67.854552 156.813932 \nC 68.417132 156.251352 68.733231 155.488221 68.733231 154.692612 \nC 68.733231 153.897003 68.417132 153.133872 67.854552 152.571292 \nC 67.291971 152.008711 66.528841 151.692612 65.733231 151.692612 \nC 64.937622 151.692612 64.174492 152.008711 63.611911 152.571292 \nC 63.04933 153.133872 62.733231 153.897003 62.733231 154.692612 \nC 62.733231 155.488221 63.04933 156.251352 63.611911 156.813932 \nC 64.174492 157.376513 64.937622 157.692612 65.733231 157.692612 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 158.386522 \nC 81.027823 158.386522 81.790954 158.070424 82.353534 157.507843 \nC 82.916115 156.945262 83.232214 156.182132 83.232214 155.386522 \nC 83.232214 154.590913 82.916115 153.827783 82.353534 153.265202 \nC 81.790954 152.702621 81.027823 152.386522 80.232214 152.386522 \nC 79.436605 152.386522 78.673474 152.702621 78.110894 153.265202 \nC 77.548313 153.827783 77.232214 154.590913 77.232214 155.386522 \nC 77.232214 156.182132 77.548313 156.945262 78.110894 157.507843 \nC 78.673474 158.070424 79.436605 158.386522 80.232214 158.386522 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 159.131293 \nC 138.287765 159.131293 139.050895 158.815194 139.613476 158.252614 \nC 140.176056 157.690033 140.492155 156.926903 140.492155 156.131293 \nC 140.492155 155.335684 140.176056 154.572554 139.613476 154.009973 \nC 139.050895 153.447392 138.287765 153.131293 137.492155 153.131293 \nC 136.696546 153.131293 135.933416 153.447392 135.370835 154.009973 \nC 134.808254 154.572554 134.492155 155.335684 134.492155 156.131293 \nC 134.492155 156.926903 134.808254 157.690033 135.370835 158.252614 \nC 135.933416 158.815194 136.696546 159.131293 137.492155 159.131293 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 158.397481 \nC 365.855552 158.397481 366.618683 158.081382 367.181264 157.518802 \nC 367.743844 156.956221 368.059943 156.193091 368.059943 155.397481 \nC 368.059943 154.601872 367.743844 153.838742 367.181264 153.276161 \nC 366.618683 152.71358 365.855552 152.397481 365.059943 152.397481 \nC 364.264334 152.397481 363.501204 152.71358 362.938623 153.276161 \nC 362.376042 153.838742 362.059943 154.601872 362.059943 155.397481 \nC 362.059943 156.193091 362.376042 156.956221 362.938623 157.518802 \nC 363.501204 158.081382 364.264334 158.397481 365.059943 158.397481 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 159.449528 \nC 61.671313 159.449528 62.434444 159.133429 62.997024 158.570848 \nC 63.559605 158.008268 63.875704 157.245137 63.875704 156.449528 \nC 63.875704 155.653919 63.559605 154.890788 62.997024 154.328208 \nC 62.434444 153.765627 61.671313 153.449528 60.875704 153.449528 \nC 60.080095 153.449528 59.316965 153.765627 58.754384 154.328208 \nC 58.191803 154.890788 57.875704 155.653919 57.875704 156.449528 \nC 57.875704 157.245137 58.191803 158.008268 58.754384 158.570848 \nC 59.316965 159.133429 60.080095 159.449528 60.875704 159.449528 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 158.80663 \nC 61.836911 158.80663 62.600041 158.490531 63.162622 157.92795 \nC 63.725203 157.365369 64.041302 156.602239 64.041302 155.80663 \nC 64.041302 155.011021 63.725203 154.24789 63.162622 153.68531 \nC 62.600041 153.122729 61.836911 152.80663 61.041302 152.80663 \nC 60.245692 152.80663 59.482562 153.122729 58.919981 153.68531 \nC 58.357401 154.24789 58.041302 155.011021 58.041302 155.80663 \nC 58.041302 156.602239 58.357401 157.365369 58.919981 157.92795 \nC 59.482562 158.490531 60.245692 158.80663 61.041302 158.80663 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 197.280125 \nC 62.168106 197.280125 62.931236 196.964026 63.493817 196.401445 \nC 64.056398 195.838864 64.372497 195.075734 64.372497 194.280125 \nC 64.372497 193.484516 64.056398 192.721385 63.493817 192.158805 \nC 62.931236 191.596224 62.168106 191.280125 61.372497 191.280125 \nC 60.576887 191.280125 59.813757 191.596224 59.251176 192.158805 \nC 58.688596 192.721385 58.372497 193.484516 58.372497 194.280125 \nC 58.372497 195.075734 58.688596 195.838864 59.251176 196.401445 \nC 59.813757 196.964026 60.576887 197.280125 61.372497 197.280125 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 158.079757 \nC 62.830496 158.079757 63.593626 157.763658 64.156207 157.201078 \nC 64.718788 156.638497 65.034887 155.875367 65.034887 155.079757 \nC 65.034887 154.284148 64.718788 153.521018 64.156207 152.958437 \nC 63.593626 152.395856 62.830496 152.079757 62.034887 152.079757 \nC 61.239277 152.079757 60.476147 152.395856 59.913566 152.958437 \nC 59.350986 153.521018 59.034887 154.284148 59.034887 155.079757 \nC 59.034887 155.875367 59.350986 156.638497 59.913566 157.201078 \nC 60.476147 157.763658 61.239277 158.079757 62.034887 158.079757 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 37.895832 \nC 64.155276 37.895832 64.918406 37.579733 65.480987 37.017152 \nC 66.043568 36.454571 66.359667 35.691441 66.359667 34.895832 \nC 66.359667 34.100222 66.043568 33.337092 65.480987 32.774511 \nC 64.918406 32.211931 64.155276 31.895832 63.359667 31.895832 \nC 62.564058 31.895832 61.800927 32.211931 61.238347 32.774511 \nC 60.675766 33.337092 60.359667 34.100222 60.359667 34.895832 \nC 60.359667 35.691441 60.675766 36.454571 61.238347 37.017152 \nC 61.800927 37.579733 62.564058 37.895832 63.359667 37.895832 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869559 \nC 66.804836 37.869559 67.567967 37.55346 68.130547 36.990879 \nC 68.693128 36.428299 69.009227 35.665168 69.009227 34.869559 \nC 69.009227 34.07395 68.693128 33.310819 68.130547 32.748239 \nC 67.567967 32.185658 66.804836 31.869559 66.009227 31.869559 \nC 65.213618 31.869559 64.450488 32.185658 63.887907 32.748239 \nC 63.325326 33.310819 63.009227 34.07395 63.009227 34.869559 \nC 63.009227 35.665168 63.325326 36.428299 63.887907 36.990879 \nC 64.450488 37.55346 65.213618 37.869559 66.009227 37.869559 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 157.884505 \nL 60.696307 156.384505 \nL 62.196307 157.884505 \nL 63.696307 156.384505 \nL 62.196307 154.884505 \nL 63.696307 153.384505 \nL 62.196307 151.884505 \nL 60.696307 153.384505 \nL 59.196307 151.884505 \nL 57.696307 153.384505 \nL 59.196307 154.884505 \nL 57.696307 156.384505 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 159.656546 \nL 60.774506 158.156546 \nL 62.274506 159.656546 \nL 63.774506 158.156546 \nL 62.274506 156.656546 \nL 63.774506 155.156546 \nL 62.274506 153.656546 \nL 60.774506 155.156546 \nL 59.274506 153.656546 \nL 57.774506 155.156546 \nL 59.274506 156.656546 \nL 57.774506 158.156546 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 155.418386 \nL 61.041302 153.918386 \nL 62.541302 155.418386 \nL 64.041302 153.918386 \nL 62.541302 152.418386 \nL 64.041302 150.918386 \nL 62.541302 149.418386 \nL 61.041302 150.918386 \nL 59.541302 149.418386 \nL 58.041302 150.918386 \nL 59.541302 152.418386 \nL 58.041302 153.918386 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 188.613421 \nL 62.016487 187.113421 \nL 63.516487 188.613421 \nL 65.016487 187.113421 \nL 63.516487 185.613421 \nL 65.016487 184.113421 \nL 63.516487 182.613421 \nL 62.016487 184.113421 \nL 60.516487 182.613421 \nL 59.016487 184.113421 \nL 60.516487 185.613421 \nL 59.016487 187.113421 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 157.247385 \nL 65.733231 155.747385 \nL 67.233231 157.247385 \nL 68.733231 155.747385 \nL 67.233231 154.247385 \nL 68.733231 152.747385 \nL 67.233231 151.247385 \nL 65.733231 152.747385 \nL 64.233231 151.247385 \nL 62.733231 152.747385 \nL 64.233231 154.247385 \nL 62.733231 155.747385 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 157.10132 \nL 80.232214 155.60132 \nL 81.732214 157.10132 \nL 83.232214 155.60132 \nL 81.732214 154.10132 \nL 83.232214 152.60132 \nL 81.732214 151.10132 \nL 80.232214 152.60132 \nL 78.732214 151.10132 \nL 77.232214 152.60132 \nL 78.732214 154.10132 \nL 77.232214 155.60132 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 156.910234 \nL 137.492155 155.410234 \nL 138.992155 156.910234 \nL 140.492155 155.410234 \nL 138.992155 153.910234 \nL 140.492155 152.410234 \nL 138.992155 150.910234 \nL 137.492155 152.410234 \nL 135.992155 150.910234 \nL 134.492155 152.410234 \nL 135.992155 153.910234 \nL 134.492155 155.410234 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 177.602634 \nL 365.059943 176.102634 \nL 366.559943 177.602634 \nL 368.059943 176.102634 \nL 366.559943 174.602634 \nL 368.059943 173.102634 \nL 366.559943 171.602634 \nL 365.059943 173.102634 \nL 363.559943 171.602634 \nL 362.059943 173.102634 \nL 363.559943 174.602634 \nL 362.059943 176.102634 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 178.767229 \nL 60.875704 177.267229 \nL 62.375704 178.767229 \nL 63.875704 177.267229 \nL 62.375704 175.767229 \nL 63.875704 174.267229 \nL 62.375704 172.767229 \nL 60.875704 174.267229 \nL 59.375704 172.767229 \nL 57.875704 174.267229 \nL 59.375704 175.767229 \nL 57.875704 177.267229 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 157.84236 \nL 61.041302 156.34236 \nL 62.541302 157.84236 \nL 64.041302 156.34236 \nL 62.541302 154.84236 \nL 64.041302 153.34236 \nL 62.541302 151.84236 \nL 61.041302 153.34236 \nL 59.541302 151.84236 \nL 58.041302 153.34236 \nL 59.541302 154.84236 \nL 58.041302 156.34236 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 158.923884 \nL 61.372497 157.423884 \nL 62.872497 158.923884 \nL 64.372497 157.423884 \nL 62.872497 155.923884 \nL 64.372497 154.423884 \nL 62.872497 152.923884 \nL 61.372497 154.423884 \nL 59.872497 152.923884 \nL 58.372497 154.423884 \nL 59.872497 155.923884 \nL 58.372497 157.423884 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 158.314635 \nL 62.034887 156.814635 \nL 63.534887 158.314635 \nL 65.034887 156.814635 \nL 63.534887 155.314635 \nL 65.034887 153.814635 \nL 63.534887 152.314635 \nL 62.034887 153.814635 \nL 60.534887 152.314635 \nL 59.034887 153.814635 \nL 60.534887 155.314635 \nL 59.034887 156.814635 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 190.031201 \nL 63.359667 188.531201 \nL 64.859667 190.031201 \nL 66.359667 188.531201 \nL 64.859667 187.031201 \nL 66.359667 185.531201 \nL 64.859667 184.031201 \nL 63.359667 185.531201 \nL 61.859667 184.031201 \nL 60.359667 185.531201 \nL 61.859667 187.031201 \nL 60.359667 188.531201 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 160.233898 \nL 66.009227 158.733898 \nL 67.509227 160.233898 \nL 69.009227 158.733898 \nL 67.509227 157.233898 \nL 69.009227 155.733898 \nL 67.509227 154.233898 \nL 66.009227 155.733898 \nL 64.509227 154.233898 \nL 63.009227 155.733898 \nL 64.509227 157.233898 \nL 63.009227 158.733898 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 92.45358 \nC 61.491916 92.45358 62.255046 92.137482 62.817627 91.574901 \nC 63.380208 91.01232 63.696307 90.24919 63.696307 89.45358 \nC 63.696307 88.657971 63.380208 87.894841 62.817627 87.33226 \nC 62.255046 86.769679 61.491916 86.45358 60.696307 86.45358 \nC 59.900698 86.45358 59.137567 86.769679 58.574986 87.33226 \nC 58.012406 87.894841 57.696307 88.657971 57.696307 89.45358 \nC 57.696307 90.24919 58.012406 91.01232 58.574986 91.574901 \nC 59.137567 92.137482 59.900698 92.45358 60.696307 92.45358 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 159.797331 \nC 61.570115 159.797331 62.333245 159.481232 62.895826 158.918651 \nC 63.458407 158.35607 63.774506 157.59294 63.774506 156.797331 \nC 63.774506 156.001721 63.458407 155.238591 62.895826 154.67601 \nC 62.333245 154.11343 61.570115 153.797331 60.774506 153.797331 \nC 59.978896 153.797331 59.215766 154.11343 58.653185 154.67601 \nC 58.090605 155.238591 57.774506 156.001721 57.774506 156.797331 \nC 57.774506 157.59294 58.090605 158.35607 58.653185 158.918651 \nC 59.215766 159.481232 59.978896 159.797331 60.774506 159.797331 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 159.985487 \nC 61.836911 159.985487 62.600041 159.669388 63.162622 159.106808 \nC 63.725203 158.544227 64.041302 157.781096 64.041302 156.985487 \nC 64.041302 156.189878 63.725203 155.426748 63.162622 154.864167 \nC 62.600041 154.301586 61.836911 153.985487 61.041302 153.985487 \nC 60.245692 153.985487 59.482562 154.301586 58.919981 154.864167 \nC 58.357401 155.426748 58.041302 156.189878 58.041302 156.985487 \nC 58.041302 157.781096 58.357401 158.544227 58.919981 159.106808 \nC 59.482562 159.669388 60.245692 159.985487 61.041302 159.985487 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 178.921857 \nC 62.812096 178.921857 63.575227 178.605759 64.137807 178.043178 \nC 64.700388 177.480597 65.016487 176.717467 65.016487 175.921857 \nC 65.016487 175.126248 64.700388 174.363118 64.137807 173.800537 \nC 63.575227 173.237956 62.812096 172.921857 62.016487 172.921857 \nC 61.220878 172.921857 60.457747 173.237956 59.895167 173.800537 \nC 59.332586 174.363118 59.016487 175.126248 59.016487 175.921857 \nC 59.016487 176.717467 59.332586 177.480597 59.895167 178.043178 \nC 60.457747 178.605759 61.220878 178.921857 62.016487 178.921857 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 158.418399 \nC 66.528841 158.418399 67.291971 158.102301 67.854552 157.53972 \nC 68.417132 156.977139 68.733231 156.214009 68.733231 155.418399 \nC 68.733231 154.62279 68.417132 153.85966 67.854552 153.297079 \nC 67.291971 152.734498 66.528841 152.418399 65.733231 152.418399 \nC 64.937622 152.418399 64.174492 152.734498 63.611911 153.297079 \nC 63.04933 153.85966 62.733231 154.62279 62.733231 155.418399 \nC 62.733231 156.214009 63.04933 156.977139 63.611911 157.53972 \nC 64.174492 158.102301 64.937622 158.418399 65.733231 158.418399 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 176.75554 \nC 81.027823 176.75554 81.790954 176.439441 82.353534 175.876861 \nC 82.916115 175.31428 83.232214 174.55115 83.232214 173.75554 \nC 83.232214 172.959931 82.916115 172.196801 82.353534 171.63422 \nC 81.790954 171.071639 81.027823 170.75554 80.232214 170.75554 \nC 79.436605 170.75554 78.673474 171.071639 78.110894 171.63422 \nC 77.548313 172.196801 77.232214 172.959931 77.232214 173.75554 \nC 77.232214 174.55115 77.548313 175.31428 78.110894 175.876861 \nC 78.673474 176.439441 79.436605 176.75554 80.232214 176.75554 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 160.011678 \nC 138.287765 160.011678 139.050895 159.695579 139.613476 159.132998 \nC 140.176056 158.570417 140.492155 157.807287 140.492155 157.011678 \nC 140.492155 156.216068 140.176056 155.452938 139.613476 154.890357 \nC 139.050895 154.327777 138.287765 154.011678 137.492155 154.011678 \nC 136.696546 154.011678 135.933416 154.327777 135.370835 154.890357 \nC 134.808254 155.452938 134.492155 156.216068 134.492155 157.011678 \nC 134.492155 157.807287 134.808254 158.570417 135.370835 159.132998 \nC 135.933416 159.695579 136.696546 160.011678 137.492155 160.011678 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 157.815435 \nC 365.855552 157.815435 366.618683 157.499336 367.181264 156.936755 \nC 367.743844 156.374175 368.059943 155.611044 368.059943 154.815435 \nC 368.059943 154.019826 367.743844 153.256695 367.181264 152.694115 \nC 366.618683 152.131534 365.855552 151.815435 365.059943 151.815435 \nC 364.264334 151.815435 363.501204 152.131534 362.938623 152.694115 \nC 362.376042 153.256695 362.059943 154.019826 362.059943 154.815435 \nC 362.059943 155.611044 362.376042 156.374175 362.938623 156.936755 \nC 363.501204 157.499336 364.264334 157.815435 365.059943 157.815435 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 159.406308 \nC 61.671313 159.406308 62.434444 159.090209 62.997024 158.527629 \nC 63.559605 157.965048 63.875704 157.201918 63.875704 156.406308 \nC 63.875704 155.610699 63.559605 154.847569 62.997024 154.284988 \nC 62.434444 153.722407 61.671313 153.406308 60.875704 153.406308 \nC 60.080095 153.406308 59.316965 153.722407 58.754384 154.284988 \nC 58.191803 154.847569 57.875704 155.610699 57.875704 156.406308 \nC 57.875704 157.201918 58.191803 157.965048 58.754384 158.527629 \nC 59.316965 159.090209 60.080095 159.406308 60.875704 159.406308 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 160.453548 \nC 61.836911 160.453548 62.600041 160.137449 63.162622 159.574868 \nC 63.725203 159.012287 64.041302 158.249157 64.041302 157.453548 \nC 64.041302 156.657938 63.725203 155.894808 63.162622 155.332227 \nC 62.600041 154.769646 61.836911 154.453548 61.041302 154.453548 \nC 60.245692 154.453548 59.482562 154.769646 58.919981 155.332227 \nC 58.357401 155.894808 58.041302 156.657938 58.041302 157.453548 \nC 58.041302 158.249157 58.357401 159.012287 58.919981 159.574868 \nC 59.482562 160.137449 60.245692 160.453548 61.041302 160.453548 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 201.190236 \nC 62.168106 201.190236 62.931236 200.874137 63.493817 200.311557 \nC 64.056398 199.748976 64.372497 198.985846 64.372497 198.190236 \nC 64.372497 197.394627 64.056398 196.631497 63.493817 196.068916 \nC 62.931236 195.506335 62.168106 195.190236 61.372497 195.190236 \nC 60.576887 195.190236 59.813757 195.506335 59.251176 196.068916 \nC 58.688596 196.631497 58.372497 197.394627 58.372497 198.190236 \nC 58.372497 198.985846 58.688596 199.748976 59.251176 200.311557 \nC 59.813757 200.874137 60.576887 201.190236 61.372497 201.190236 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 156.196106 \nC 62.830496 156.196106 63.593626 155.880007 64.156207 155.317427 \nC 64.718788 154.754846 65.034887 153.991716 65.034887 153.196106 \nC 65.034887 152.400497 64.718788 151.637367 64.156207 151.074786 \nC 63.593626 150.512205 62.830496 150.196106 62.034887 150.196106 \nC 61.239277 150.196106 60.476147 150.512205 59.913566 151.074786 \nC 59.350986 151.637367 59.034887 152.400497 59.034887 153.196106 \nC 59.034887 153.991716 59.350986 154.754846 59.913566 155.317427 \nC 60.476147 155.880007 61.239277 156.196106 62.034887 156.196106 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 178.526822 \nC 64.155276 178.526822 64.918406 178.210723 65.480987 177.648142 \nC 66.043568 177.085562 66.359667 176.322431 66.359667 175.526822 \nC 66.359667 174.731213 66.043568 173.968082 65.480987 173.405502 \nC 64.918406 172.842921 64.155276 172.526822 63.359667 172.526822 \nC 62.564058 172.526822 61.800927 172.842921 61.238347 173.405502 \nC 60.675766 173.968082 60.359667 174.731213 60.359667 175.526822 \nC 60.359667 176.322431 60.675766 177.085562 61.238347 177.648142 \nC 61.800927 178.210723 62.564058 178.526822 63.359667 178.526822 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869576 \nC 66.804836 37.869576 67.567967 37.553477 68.130547 36.990896 \nC 68.693128 36.428316 69.009227 35.665185 69.009227 34.869576 \nC 69.009227 34.073967 68.693128 33.310836 68.130547 32.748256 \nC 67.567967 32.185675 66.804836 31.869576 66.009227 31.869576 \nC 65.213618 31.869576 64.450488 32.185675 63.887907 32.748256 \nC 63.325326 33.310836 63.009227 34.073967 63.009227 34.869576 \nC 63.009227 35.665185 63.325326 36.428316 63.887907 36.990896 \nC 64.450488 37.553477 65.213618 37.869576 66.009227 37.869576 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 150.382354 \nL 60.696307 148.882354 \nL 62.196307 150.382354 \nL 63.696307 148.882354 \nL 62.196307 147.382354 \nL 63.696307 145.882354 \nL 62.196307 144.382354 \nL 60.696307 145.882354 \nL 59.196307 144.382354 \nL 57.696307 145.882354 \nL 59.196307 147.382354 \nL 57.696307 148.882354 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 177.150823 \nL 60.774506 175.650823 \nL 62.274506 177.150823 \nL 63.774506 175.650823 \nL 62.274506 174.150823 \nL 63.774506 172.650823 \nL 62.274506 171.150823 \nL 60.774506 172.650823 \nL 59.274506 171.150823 \nL 57.774506 172.650823 \nL 59.274506 174.150823 \nL 57.774506 175.650823 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 180.139191 \nL 61.041302 178.639191 \nL 62.541302 180.139191 \nL 64.041302 178.639191 \nL 62.541302 177.139191 \nL 64.041302 175.639191 \nL 62.541302 174.139191 \nL 61.041302 175.639191 \nL 59.541302 174.139191 \nL 58.041302 175.639191 \nL 59.541302 177.139191 \nL 58.041302 178.639191 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 157.359048 \nL 62.016487 155.859048 \nL 63.516487 157.359048 \nL 65.016487 155.859048 \nL 63.516487 154.359048 \nL 65.016487 152.859048 \nL 63.516487 151.359048 \nL 62.016487 152.859048 \nL 60.516487 151.359048 \nL 59.016487 152.859048 \nL 60.516487 154.359048 \nL 59.016487 155.859048 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 159.603421 \nL 65.733231 158.103421 \nL 67.233231 159.603421 \nL 68.733231 158.103421 \nL 67.233231 156.603421 \nL 68.733231 155.103421 \nL 67.233231 153.603421 \nL 65.733231 155.103421 \nL 64.233231 153.603421 \nL 62.733231 155.103421 \nL 64.233231 156.603421 \nL 62.733231 158.103421 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 157.336139 \nL 80.232214 155.836139 \nL 81.732214 157.336139 \nL 83.232214 155.836139 \nL 81.732214 154.336139 \nL 83.232214 152.836139 \nL 81.732214 151.336139 \nL 80.232214 152.836139 \nL 78.732214 151.336139 \nL 77.232214 152.836139 \nL 78.732214 154.336139 \nL 77.232214 155.836139 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 157.52494 \nL 137.492155 156.02494 \nL 138.992155 157.52494 \nL 140.492155 156.02494 \nL 138.992155 154.52494 \nL 140.492155 153.02494 \nL 138.992155 151.52494 \nL 137.492155 153.02494 \nL 135.992155 151.52494 \nL 134.492155 153.02494 \nL 135.992155 154.52494 \nL 134.492155 156.02494 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 156.857699 \nL 365.059943 155.357699 \nL 366.559943 156.857699 \nL 368.059943 155.357699 \nL 366.559943 153.857699 \nL 368.059943 152.357699 \nL 366.559943 150.857699 \nL 365.059943 152.357699 \nL 363.559943 150.857699 \nL 362.059943 152.357699 \nL 363.559943 153.857699 \nL 362.059943 155.357699 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 153.012737 \nL 60.875704 151.512737 \nL 62.375704 153.012737 \nL 63.875704 151.512737 \nL 62.375704 150.012737 \nL 63.875704 148.512737 \nL 62.375704 147.012737 \nL 60.875704 148.512737 \nL 59.375704 147.012737 \nL 57.875704 148.512737 \nL 59.375704 150.012737 \nL 57.875704 151.512737 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 158.677314 \nL 61.041302 157.177314 \nL 62.541302 158.677314 \nL 64.041302 157.177314 \nL 62.541302 155.677314 \nL 64.041302 154.177314 \nL 62.541302 152.677314 \nL 61.041302 154.177314 \nL 59.541302 152.677314 \nL 58.041302 154.177314 \nL 59.541302 155.677314 \nL 58.041302 157.177314 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 158.317188 \nL 61.372497 156.817188 \nL 62.872497 158.317188 \nL 64.372497 156.817188 \nL 62.872497 155.317188 \nL 64.372497 153.817188 \nL 62.872497 152.317188 \nL 61.372497 153.817188 \nL 59.872497 152.317188 \nL 58.372497 153.817188 \nL 59.872497 155.317188 \nL 58.372497 156.817188 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 180.697645 \nL 62.034887 179.197645 \nL 63.534887 180.697645 \nL 65.034887 179.197645 \nL 63.534887 177.697645 \nL 65.034887 176.197645 \nL 63.534887 174.697645 \nL 62.034887 176.197645 \nL 60.534887 174.697645 \nL 59.034887 176.197645 \nL 60.534887 177.697645 \nL 59.034887 179.197645 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 178.504832 \nL 63.359667 177.004832 \nL 64.859667 178.504832 \nL 66.359667 177.004832 \nL 64.859667 175.504832 \nL 66.359667 174.004832 \nL 64.859667 172.504832 \nL 63.359667 174.004832 \nL 61.859667 172.504832 \nL 60.359667 174.004832 \nL 61.859667 175.504832 \nL 60.359667 177.004832 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 159.481379 \nL 66.009227 157.981379 \nL 67.509227 159.481379 \nL 69.009227 157.981379 \nL 67.509227 156.481379 \nL 69.009227 154.981379 \nL 67.509227 153.481379 \nL 66.009227 154.981379 \nL 64.509227 153.481379 \nL 63.009227 154.981379 \nL 64.509227 156.481379 \nL 63.009227 157.981379 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 155.352609 \nC 61.491916 155.352609 62.255046 155.03651 62.817627 154.473929 \nC 63.380208 153.911348 63.696307 153.148218 63.696307 152.352609 \nC 63.696307 151.556999 63.380208 150.793869 62.817627 150.231288 \nC 62.255046 149.668708 61.491916 149.352609 60.696307 149.352609 \nC 59.900698 149.352609 59.137567 149.668708 58.574986 150.231288 \nC 58.012406 150.793869 57.696307 151.556999 57.696307 152.352609 \nC 57.696307 153.148218 58.012406 153.911348 58.574986 154.473929 \nC 59.137567 155.03651 59.900698 155.352609 60.696307 155.352609 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 158.852115 \nC 61.570115 158.852115 62.333245 158.536016 62.895826 157.973435 \nC 63.458407 157.410855 63.774506 156.647724 63.774506 155.852115 \nC 63.774506 155.056506 63.458407 154.293376 62.895826 153.730795 \nC 62.333245 153.168214 61.570115 152.852115 60.774506 152.852115 \nC 59.978896 152.852115 59.215766 153.168214 58.653185 153.730795 \nC 58.090605 154.293376 57.774506 155.056506 57.774506 155.852115 \nC 57.774506 156.647724 58.090605 157.410855 58.653185 157.973435 \nC 59.215766 158.536016 59.978896 158.852115 60.774506 158.852115 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 158.494147 \nC 61.836911 158.494147 62.600041 158.178048 63.162622 157.615468 \nC 63.725203 157.052887 64.041302 156.289757 64.041302 155.494147 \nC 64.041302 154.698538 63.725203 153.935408 63.162622 153.372827 \nC 62.600041 152.810246 61.836911 152.494147 61.041302 152.494147 \nC 60.245692 152.494147 59.482562 152.810246 58.919981 153.372827 \nC 58.357401 153.935408 58.041302 154.698538 58.041302 155.494147 \nC 58.041302 156.289757 58.357401 157.052887 58.919981 157.615468 \nC 59.482562 158.178048 60.245692 158.494147 61.041302 158.494147 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 183.696419 \nC 62.812096 183.696419 63.575227 183.38032 64.137807 182.81774 \nC 64.700388 182.255159 65.016487 181.492029 65.016487 180.696419 \nC 65.016487 179.90081 64.700388 179.13768 64.137807 178.575099 \nC 63.575227 178.012518 62.812096 177.696419 62.016487 177.696419 \nC 61.220878 177.696419 60.457747 178.012518 59.895167 178.575099 \nC 59.332586 179.13768 59.016487 179.90081 59.016487 180.696419 \nC 59.016487 181.492029 59.332586 182.255159 59.895167 182.81774 \nC 60.457747 183.38032 61.220878 183.696419 62.016487 183.696419 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 173.680587 \nC 66.528841 173.680587 67.291971 173.364488 67.854552 172.801907 \nC 68.417132 172.239326 68.733231 171.476196 68.733231 170.680587 \nC 68.733231 169.884977 68.417132 169.121847 67.854552 168.559266 \nC 67.291971 167.996686 66.528841 167.680587 65.733231 167.680587 \nC 64.937622 167.680587 64.174492 167.996686 63.611911 168.559266 \nC 63.04933 169.121847 62.733231 169.884977 62.733231 170.680587 \nC 62.733231 171.476196 63.04933 172.239326 63.611911 172.801907 \nC 64.174492 173.364488 64.937622 173.680587 65.733231 173.680587 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 157.633407 \nC 81.027823 157.633407 81.790954 157.317308 82.353534 156.754727 \nC 82.916115 156.192146 83.232214 155.429016 83.232214 154.633407 \nC 83.232214 153.837797 82.916115 153.074667 82.353534 152.512086 \nC 81.790954 151.949506 81.027823 151.633407 80.232214 151.633407 \nC 79.436605 151.633407 78.673474 151.949506 78.110894 152.512086 \nC 77.548313 153.074667 77.232214 153.837797 77.232214 154.633407 \nC 77.232214 155.429016 77.548313 156.192146 78.110894 156.754727 \nC 78.673474 157.317308 79.436605 157.633407 80.232214 157.633407 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 177.163376 \nC 138.287765 177.163376 139.050895 176.847277 139.613476 176.284696 \nC 140.176056 175.722116 140.492155 174.958985 140.492155 174.163376 \nC 140.492155 173.367767 140.176056 172.604636 139.613476 172.042056 \nC 139.050895 171.479475 138.287765 171.163376 137.492155 171.163376 \nC 136.696546 171.163376 135.933416 171.479475 135.370835 172.042056 \nC 134.808254 172.604636 134.492155 173.367767 134.492155 174.163376 \nC 134.492155 174.958985 134.808254 175.722116 135.370835 176.284696 \nC 135.933416 176.847277 136.696546 177.163376 137.492155 177.163376 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 156.968952 \nC 365.855552 156.968952 366.618683 156.652853 367.181264 156.090272 \nC 367.743844 155.527692 368.059943 154.764561 368.059943 153.968952 \nC 368.059943 153.173343 367.743844 152.410212 367.181264 151.847632 \nC 366.618683 151.285051 365.855552 150.968952 365.059943 150.968952 \nC 364.264334 150.968952 363.501204 151.285051 362.938623 151.847632 \nC 362.376042 152.410212 362.059943 153.173343 362.059943 153.968952 \nC 362.059943 154.764561 362.376042 155.527692 362.938623 156.090272 \nC 363.501204 156.652853 364.264334 156.968952 365.059943 156.968952 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 157.72334 \nC 61.671313 157.72334 62.434444 157.407241 62.997024 156.84466 \nC 63.559605 156.282079 63.875704 155.518949 63.875704 154.72334 \nC 63.875704 153.92773 63.559605 153.1646 62.997024 152.602019 \nC 62.434444 152.039439 61.671313 151.72334 60.875704 151.72334 \nC 60.080095 151.72334 59.316965 152.039439 58.754384 152.602019 \nC 58.191803 153.1646 57.875704 153.92773 57.875704 154.72334 \nC 57.875704 155.518949 58.191803 156.282079 58.754384 156.84466 \nC 59.316965 157.407241 60.080095 157.72334 60.875704 157.72334 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 159.54011 \nC 61.836911 159.54011 62.600041 159.224011 63.162622 158.66143 \nC 63.725203 158.09885 64.041302 157.335719 64.041302 156.54011 \nC 64.041302 155.744501 63.725203 154.98137 63.162622 154.41879 \nC 62.600041 153.856209 61.836911 153.54011 61.041302 153.54011 \nC 60.245692 153.54011 59.482562 153.856209 58.919981 154.41879 \nC 58.357401 154.98137 58.041302 155.744501 58.041302 156.54011 \nC 58.041302 157.335719 58.357401 158.09885 58.919981 158.66143 \nC 59.482562 159.224011 60.245692 159.54011 61.041302 159.54011 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 158.826633 \nC 62.168106 158.826633 62.931236 158.510534 63.493817 157.947953 \nC 64.056398 157.385373 64.372497 156.622242 64.372497 155.826633 \nC 64.372497 155.031024 64.056398 154.267894 63.493817 153.705313 \nC 62.931236 153.142732 62.168106 152.826633 61.372497 152.826633 \nC 60.576887 152.826633 59.813757 153.142732 59.251176 153.705313 \nC 58.688596 154.267894 58.372497 155.031024 58.372497 155.826633 \nC 58.372497 156.622242 58.688596 157.385373 59.251176 157.947953 \nC 59.813757 158.510534 60.576887 158.826633 61.372497 158.826633 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 159.118945 \nC 62.830496 159.118945 63.593626 158.802847 64.156207 158.240266 \nC 64.718788 157.677685 65.034887 156.914555 65.034887 156.118945 \nC 65.034887 155.323336 64.718788 154.560206 64.156207 153.997625 \nC 63.593626 153.435044 62.830496 153.118945 62.034887 153.118945 \nC 61.239277 153.118945 60.476147 153.435044 59.913566 153.997625 \nC 59.350986 154.560206 59.034887 155.323336 59.034887 156.118945 \nC 59.034887 156.914555 59.350986 157.677685 59.913566 158.240266 \nC 60.476147 158.802847 61.239277 159.118945 62.034887 159.118945 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 151.484464 \nC 64.155276 151.484464 64.918406 151.168366 65.480987 150.605785 \nC 66.043568 150.043204 66.359667 149.280074 66.359667 148.484464 \nC 66.359667 147.688855 66.043568 146.925725 65.480987 146.363144 \nC 64.918406 145.800563 64.155276 145.484464 63.359667 145.484464 \nC 62.564058 145.484464 61.800927 145.800563 61.238347 146.363144 \nC 60.675766 146.925725 60.359667 147.688855 60.359667 148.484464 \nC 60.359667 149.280074 60.675766 150.043204 61.238347 150.605785 \nC 61.800927 151.168366 62.564058 151.484464 63.359667 151.484464 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869492 \nC 66.804836 37.869492 67.567967 37.553393 68.130547 36.990812 \nC 68.693128 36.428231 69.009227 35.665101 69.009227 34.869492 \nC 69.009227 34.073882 68.693128 33.310752 68.130547 32.748171 \nC 67.567967 32.18559 66.804836 31.869492 66.009227 31.869492 \nC 65.213618 31.869492 64.450488 32.18559 63.887907 32.748171 \nC 63.325326 33.310752 63.009227 34.073882 63.009227 34.869492 \nC 63.009227 35.665101 63.325326 36.428231 63.887907 36.990812 \nC 64.450488 37.553393 65.213618 37.869492 66.009227 37.869492 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 145.226194 \nL 60.696307 143.726194 \nL 62.196307 145.226194 \nL 63.696307 143.726194 \nL 62.196307 142.226194 \nL 63.696307 140.726194 \nL 62.196307 139.226194 \nL 60.696307 140.726194 \nL 59.196307 139.226194 \nL 57.696307 140.726194 \nL 59.196307 142.226194 \nL 57.696307 143.726194 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 159.329324 \nL 60.774506 157.829324 \nL 62.274506 159.329324 \nL 63.774506 157.829324 \nL 62.274506 156.329324 \nL 63.774506 154.829324 \nL 62.274506 153.329324 \nL 60.774506 154.829324 \nL 59.274506 153.329324 \nL 57.774506 154.829324 \nL 59.274506 156.329324 \nL 57.774506 157.829324 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 159.424597 \nL 61.041302 157.924597 \nL 62.541302 159.424597 \nL 64.041302 157.924597 \nL 62.541302 156.424597 \nL 64.041302 154.924597 \nL 62.541302 153.424597 \nL 61.041302 154.924597 \nL 59.541302 153.424597 \nL 58.041302 154.924597 \nL 59.541302 156.424597 \nL 58.041302 157.924597 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 159.434082 \nL 62.016487 157.934082 \nL 63.516487 159.434082 \nL 65.016487 157.934082 \nL 63.516487 156.434082 \nL 65.016487 154.934082 \nL 63.516487 153.434082 \nL 62.016487 154.934082 \nL 60.516487 153.434082 \nL 59.016487 154.934082 \nL 60.516487 156.434082 \nL 59.016487 157.934082 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 157.670106 \nL 65.733231 156.170106 \nL 67.233231 157.670106 \nL 68.733231 156.170106 \nL 67.233231 154.670106 \nL 68.733231 153.170106 \nL 67.233231 151.670106 \nL 65.733231 153.170106 \nL 64.233231 151.670106 \nL 62.733231 153.170106 \nL 64.233231 154.670106 \nL 62.733231 156.170106 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 156.636935 \nL 80.232214 155.136935 \nL 81.732214 156.636935 \nL 83.232214 155.136935 \nL 81.732214 153.636935 \nL 83.232214 152.136935 \nL 81.732214 150.636935 \nL 80.232214 152.136935 \nL 78.732214 150.636935 \nL 77.232214 152.136935 \nL 78.732214 153.636935 \nL 77.232214 155.136935 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 157.254724 \nL 137.492155 155.754724 \nL 138.992155 157.254724 \nL 140.492155 155.754724 \nL 138.992155 154.254724 \nL 140.492155 152.754724 \nL 138.992155 151.254724 \nL 137.492155 152.754724 \nL 135.992155 151.254724 \nL 134.492155 152.754724 \nL 135.992155 154.254724 \nL 134.492155 155.754724 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 158.148349 \nL 365.059943 156.648349 \nL 366.559943 158.148349 \nL 368.059943 156.648349 \nL 366.559943 155.148349 \nL 368.059943 153.648349 \nL 366.559943 152.148349 \nL 365.059943 153.648349 \nL 363.559943 152.148349 \nL 362.059943 153.648349 \nL 363.559943 155.148349 \nL 362.059943 156.648349 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 158.460648 \nL 60.875704 156.960648 \nL 62.375704 158.460648 \nL 63.875704 156.960648 \nL 62.375704 155.460648 \nL 63.875704 153.960648 \nL 62.375704 152.460648 \nL 60.875704 153.960648 \nL 59.375704 152.460648 \nL 57.875704 153.960648 \nL 59.375704 155.460648 \nL 57.875704 156.960648 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 157.682048 \nL 61.041302 156.182048 \nL 62.541302 157.682048 \nL 64.041302 156.182048 \nL 62.541302 154.682048 \nL 64.041302 153.182048 \nL 62.541302 151.682048 \nL 61.041302 153.182048 \nL 59.541302 151.682048 \nL 58.041302 153.182048 \nL 59.541302 154.682048 \nL 58.041302 156.182048 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 158.256363 \nL 61.372497 156.756363 \nL 62.872497 158.256363 \nL 64.372497 156.756363 \nL 62.872497 155.256363 \nL 64.372497 153.756363 \nL 62.872497 152.256363 \nL 61.372497 153.756363 \nL 59.872497 152.256363 \nL 58.372497 153.756363 \nL 59.872497 155.256363 \nL 58.372497 156.756363 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 165.982083 \nL 62.034887 164.482083 \nL 63.534887 165.982083 \nL 65.034887 164.482083 \nL 63.534887 162.982083 \nL 65.034887 161.482083 \nL 63.534887 159.982083 \nL 62.034887 161.482083 \nL 60.534887 159.982083 \nL 59.034887 161.482083 \nL 60.534887 162.982083 \nL 59.034887 164.482083 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 159.614308 \nL 63.359667 158.114308 \nL 64.859667 159.614308 \nL 66.359667 158.114308 \nL 64.859667 156.614308 \nL 66.359667 155.114308 \nL 64.859667 153.614308 \nL 63.359667 155.114308 \nL 61.859667 153.614308 \nL 60.359667 155.114308 \nL 61.859667 156.614308 \nL 60.359667 158.114308 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 188.512072 \nL 66.009227 187.012072 \nL 67.509227 188.512072 \nL 69.009227 187.012072 \nL 67.509227 185.512072 \nL 69.009227 184.012072 \nL 67.509227 182.512072 \nL 66.009227 184.012072 \nL 64.509227 182.512072 \nL 63.009227 184.012072 \nL 64.509227 185.512072 \nL 63.009227 187.012072 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 161.601791 \nC 61.491916 161.601791 62.255046 161.285692 62.817627 160.723111 \nC 63.380208 160.160531 63.696307 159.3974 63.696307 158.601791 \nC 63.696307 157.806182 63.380208 157.043051 62.817627 156.480471 \nC 62.255046 155.91789 61.491916 155.601791 60.696307 155.601791 \nC 59.900698 155.601791 59.137567 155.91789 58.574986 156.480471 \nC 58.012406 157.043051 57.696307 157.806182 57.696307 158.601791 \nC 57.696307 159.3974 58.012406 160.160531 58.574986 160.723111 \nC 59.137567 161.285692 59.900698 161.601791 60.696307 161.601791 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 158.091236 \nC 61.570115 158.091236 62.333245 157.775137 62.895826 157.212556 \nC 63.458407 156.649975 63.774506 155.886845 63.774506 155.091236 \nC 63.774506 154.295626 63.458407 153.532496 62.895826 152.969915 \nC 62.333245 152.407335 61.570115 152.091236 60.774506 152.091236 \nC 59.978896 152.091236 59.215766 152.407335 58.653185 152.969915 \nC 58.090605 153.532496 57.774506 154.295626 57.774506 155.091236 \nC 57.774506 155.886845 58.090605 156.649975 58.653185 157.212556 \nC 59.215766 157.775137 59.978896 158.091236 60.774506 158.091236 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 159.963003 \nC 61.836911 159.963003 62.600041 159.646904 63.162622 159.084323 \nC 63.725203 158.521743 64.041302 157.758612 64.041302 156.963003 \nC 64.041302 156.167394 63.725203 155.404263 63.162622 154.841683 \nC 62.600041 154.279102 61.836911 153.963003 61.041302 153.963003 \nC 60.245692 153.963003 59.482562 154.279102 58.919981 154.841683 \nC 58.357401 155.404263 58.041302 156.167394 58.041302 156.963003 \nC 58.041302 157.758612 58.357401 158.521743 58.919981 159.084323 \nC 59.482562 159.646904 60.245692 159.963003 61.041302 159.963003 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 180.077893 \nC 62.812096 180.077893 63.575227 179.761794 64.137807 179.199214 \nC 64.700388 178.636633 65.016487 177.873503 65.016487 177.077893 \nC 65.016487 176.282284 64.700388 175.519154 64.137807 174.956573 \nC 63.575227 174.393992 62.812096 174.077893 62.016487 174.077893 \nC 61.220878 174.077893 60.457747 174.393992 59.895167 174.956573 \nC 59.332586 175.519154 59.016487 176.282284 59.016487 177.077893 \nC 59.016487 177.873503 59.332586 178.636633 59.895167 179.199214 \nC 60.457747 179.761794 61.220878 180.077893 62.016487 180.077893 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 159.357181 \nC 66.528841 159.357181 67.291971 159.041082 67.854552 158.478501 \nC 68.417132 157.91592 68.733231 157.15279 68.733231 156.357181 \nC 68.733231 155.561571 68.417132 154.798441 67.854552 154.23586 \nC 67.291971 153.67328 66.528841 153.357181 65.733231 153.357181 \nC 64.937622 153.357181 64.174492 153.67328 63.611911 154.23586 \nC 63.04933 154.798441 62.733231 155.561571 62.733231 156.357181 \nC 62.733231 157.15279 63.04933 157.91592 63.611911 158.478501 \nC 64.174492 159.041082 64.937622 159.357181 65.733231 159.357181 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 158.657457 \nC 81.027823 158.657457 81.790954 158.341358 82.353534 157.778777 \nC 82.916115 157.216196 83.232214 156.453066 83.232214 155.657457 \nC 83.232214 154.861847 82.916115 154.098717 82.353534 153.536136 \nC 81.790954 152.973556 81.027823 152.657457 80.232214 152.657457 \nC 79.436605 152.657457 78.673474 152.973556 78.110894 153.536136 \nC 77.548313 154.098717 77.232214 154.861847 77.232214 155.657457 \nC 77.232214 156.453066 77.548313 157.216196 78.110894 157.778777 \nC 78.673474 158.341358 79.436605 158.657457 80.232214 158.657457 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 157.822731 \nC 138.287765 157.822731 139.050895 157.506633 139.613476 156.944052 \nC 140.176056 156.381471 140.492155 155.618341 140.492155 154.822731 \nC 140.492155 154.027122 140.176056 153.263992 139.613476 152.701411 \nC 139.050895 152.13883 138.287765 151.822731 137.492155 151.822731 \nC 136.696546 151.822731 135.933416 152.13883 135.370835 152.701411 \nC 134.808254 153.263992 134.492155 154.027122 134.492155 154.822731 \nC 134.492155 155.618341 134.808254 156.381471 135.370835 156.944052 \nC 135.933416 157.506633 136.696546 157.822731 137.492155 157.822731 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 157.386276 \nC 365.855552 157.386276 366.618683 157.070177 367.181264 156.507596 \nC 367.743844 155.945016 368.059943 155.181885 368.059943 154.386276 \nC 368.059943 153.590667 367.743844 152.827536 367.181264 152.264956 \nC 366.618683 151.702375 365.855552 151.386276 365.059943 151.386276 \nC 364.264334 151.386276 363.501204 151.702375 362.938623 152.264956 \nC 362.376042 152.827536 362.059943 153.590667 362.059943 154.386276 \nC 362.059943 155.181885 362.376042 155.945016 362.938623 156.507596 \nC 363.501204 157.070177 364.264334 157.386276 365.059943 157.386276 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 158.141107 \nC 61.671313 158.141107 62.434444 157.825008 62.997024 157.262427 \nC 63.559605 156.699846 63.875704 155.936716 63.875704 155.141107 \nC 63.875704 154.345497 63.559605 153.582367 62.997024 153.019786 \nC 62.434444 152.457206 61.671313 152.141107 60.875704 152.141107 \nC 60.080095 152.141107 59.316965 152.457206 58.754384 153.019786 \nC 58.191803 153.582367 57.875704 154.345497 57.875704 155.141107 \nC 57.875704 155.936716 58.191803 156.699846 58.754384 157.262427 \nC 59.316965 157.825008 60.080095 158.141107 60.875704 158.141107 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 158.291947 \nC 61.836911 158.291947 62.600041 157.975848 63.162622 157.413268 \nC 63.725203 156.850687 64.041302 156.087557 64.041302 155.291947 \nC 64.041302 154.496338 63.725203 153.733208 63.162622 153.170627 \nC 62.600041 152.608046 61.836911 152.291947 61.041302 152.291947 \nC 60.245692 152.291947 59.482562 152.608046 58.919981 153.170627 \nC 58.357401 153.733208 58.041302 154.496338 58.041302 155.291947 \nC 58.041302 156.087557 58.357401 156.850687 58.919981 157.413268 \nC 59.482562 157.975848 60.245692 158.291947 61.041302 158.291947 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 180.040161 \nC 62.168106 180.040161 62.931236 179.724062 63.493817 179.161481 \nC 64.056398 178.5989 64.372497 177.83577 64.372497 177.040161 \nC 64.372497 176.244551 64.056398 175.481421 63.493817 174.91884 \nC 62.931236 174.35626 62.168106 174.040161 61.372497 174.040161 \nC 60.576887 174.040161 59.813757 174.35626 59.251176 174.91884 \nC 58.688596 175.481421 58.372497 176.244551 58.372497 177.040161 \nC 58.372497 177.83577 58.688596 178.5989 59.251176 179.161481 \nC 59.813757 179.724062 60.576887 180.040161 61.372497 180.040161 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 193.556386 \nC 62.830496 193.556386 63.593626 193.240287 64.156207 192.677706 \nC 64.718788 192.115126 65.034887 191.351995 65.034887 190.556386 \nC 65.034887 189.760777 64.718788 188.997646 64.156207 188.435066 \nC 63.593626 187.872485 62.830496 187.556386 62.034887 187.556386 \nC 61.239277 187.556386 60.476147 187.872485 59.913566 188.435066 \nC 59.350986 188.997646 59.034887 189.760777 59.034887 190.556386 \nC 59.034887 191.351995 59.350986 192.115126 59.913566 192.677706 \nC 60.476147 193.240287 61.239277 193.556386 62.034887 193.556386 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 36.905205 \nC 64.155276 36.905205 64.918406 36.589106 65.480987 36.026525 \nC 66.043568 35.463945 66.359667 34.700814 66.359667 33.905205 \nC 66.359667 33.109596 66.043568 32.346465 65.480987 31.783885 \nC 64.918406 31.221304 64.155276 30.905205 63.359667 30.905205 \nC 62.564058 30.905205 61.800927 31.221304 61.238347 31.783885 \nC 60.675766 32.346465 60.359667 33.109596 60.359667 33.905205 \nC 60.359667 34.700814 60.675766 35.463945 61.238347 36.026525 \nC 61.800927 36.589106 62.564058 36.905205 63.359667 36.905205 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869557 \nC 66.804836 37.869557 67.567967 37.553458 68.130547 36.990877 \nC 68.693128 36.428297 69.009227 35.665166 69.009227 34.869557 \nC 69.009227 34.073948 68.693128 33.310817 68.130547 32.748237 \nC 67.567967 32.185656 66.804836 31.869557 66.009227 31.869557 \nC 65.213618 31.869557 64.450488 32.185656 63.887907 32.748237 \nC 63.325326 33.310817 63.009227 34.073948 63.009227 34.869557 \nC 63.009227 35.665166 63.325326 36.428297 63.887907 36.990877 \nC 64.450488 37.553458 65.213618 37.869557 66.009227 37.869557 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 152.130785 \nL 60.696307 150.630785 \nL 62.196307 152.130785 \nL 63.696307 150.630785 \nL 62.196307 149.130785 \nL 63.696307 147.630785 \nL 62.196307 146.130785 \nL 60.696307 147.630785 \nL 59.196307 146.130785 \nL 57.696307 147.630785 \nL 59.196307 149.130785 \nL 57.696307 150.630785 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 158.269667 \nL 60.774506 156.769667 \nL 62.274506 158.269667 \nL 63.774506 156.769667 \nL 62.274506 155.269667 \nL 63.774506 153.769667 \nL 62.274506 152.269667 \nL 60.774506 153.769667 \nL 59.274506 152.269667 \nL 57.774506 153.769667 \nL 59.274506 155.269667 \nL 57.774506 156.769667 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 175.466047 \nL 61.041302 173.966047 \nL 62.541302 175.466047 \nL 64.041302 173.966047 \nL 62.541302 172.466047 \nL 64.041302 170.966047 \nL 62.541302 169.466047 \nL 61.041302 170.966047 \nL 59.541302 169.466047 \nL 58.041302 170.966047 \nL 59.541302 172.466047 \nL 58.041302 173.966047 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 157.159851 \nL 62.016487 155.659851 \nL 63.516487 157.159851 \nL 65.016487 155.659851 \nL 63.516487 154.159851 \nL 65.016487 152.659851 \nL 63.516487 151.159851 \nL 62.016487 152.659851 \nL 60.516487 151.159851 \nL 59.016487 152.659851 \nL 60.516487 154.159851 \nL 59.016487 155.659851 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 157.081334 \nL 65.733231 155.581334 \nL 67.233231 157.081334 \nL 68.733231 155.581334 \nL 67.233231 154.081334 \nL 68.733231 152.581334 \nL 67.233231 151.081334 \nL 65.733231 152.581334 \nL 64.233231 151.081334 \nL 62.733231 152.581334 \nL 64.233231 154.081334 \nL 62.733231 155.581334 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 156.002448 \nL 80.232214 154.502448 \nL 81.732214 156.002448 \nL 83.232214 154.502448 \nL 81.732214 153.002448 \nL 83.232214 151.502448 \nL 81.732214 150.002448 \nL 80.232214 151.502448 \nL 78.732214 150.002448 \nL 77.232214 151.502448 \nL 78.732214 153.002448 \nL 77.232214 154.502448 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 157.286545 \nL 137.492155 155.786545 \nL 138.992155 157.286545 \nL 140.492155 155.786545 \nL 138.992155 154.286545 \nL 140.492155 152.786545 \nL 138.992155 151.286545 \nL 137.492155 152.786545 \nL 135.992155 151.286545 \nL 134.492155 152.786545 \nL 135.992155 154.286545 \nL 134.492155 155.786545 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 156.852468 \nL 365.059943 155.352468 \nL 366.559943 156.852468 \nL 368.059943 155.352468 \nL 366.559943 153.852468 \nL 368.059943 152.352468 \nL 366.559943 150.852468 \nL 365.059943 152.352468 \nL 363.559943 150.852468 \nL 362.059943 152.352468 \nL 363.559943 153.852468 \nL 362.059943 155.352468 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 157.434299 \nL 60.875704 155.934299 \nL 62.375704 157.434299 \nL 63.875704 155.934299 \nL 62.375704 154.434299 \nL 63.875704 152.934299 \nL 62.375704 151.434299 \nL 60.875704 152.934299 \nL 59.375704 151.434299 \nL 57.875704 152.934299 \nL 59.375704 154.434299 \nL 57.875704 155.934299 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 177.408226 \nL 61.041302 175.908226 \nL 62.541302 177.408226 \nL 64.041302 175.908226 \nL 62.541302 174.408226 \nL 64.041302 172.908226 \nL 62.541302 171.408226 \nL 61.041302 172.908226 \nL 59.541302 171.408226 \nL 58.041302 172.908226 \nL 59.541302 174.408226 \nL 58.041302 175.908226 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 156.474391 \nL 61.372497 154.974391 \nL 62.872497 156.474391 \nL 64.372497 154.974391 \nL 62.872497 153.474391 \nL 64.372497 151.974391 \nL 62.872497 150.474391 \nL 61.372497 151.974391 \nL 59.872497 150.474391 \nL 58.372497 151.974391 \nL 59.872497 153.474391 \nL 58.372497 154.974391 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 179.005425 \nL 62.034887 177.505425 \nL 63.534887 179.005425 \nL 65.034887 177.505425 \nL 63.534887 176.005425 \nL 65.034887 174.505425 \nL 63.534887 173.005425 \nL 62.034887 174.505425 \nL 60.534887 173.005425 \nL 59.034887 174.505425 \nL 60.534887 176.005425 \nL 59.034887 177.505425 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 159.846002 \nL 63.359667 158.346002 \nL 64.859667 159.846002 \nL 66.359667 158.346002 \nL 64.859667 156.846002 \nL 66.359667 155.346002 \nL 64.859667 153.846002 \nL 63.359667 155.346002 \nL 61.859667 153.846002 \nL 60.359667 155.346002 \nL 61.859667 156.846002 \nL 60.359667 158.346002 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 159.591068 \nL 66.009227 158.091068 \nL 67.509227 159.591068 \nL 69.009227 158.091068 \nL 67.509227 156.591068 \nL 69.009227 155.091068 \nL 67.509227 153.591068 \nL 66.009227 155.091068 \nL 64.509227 153.591068 \nL 63.009227 155.091068 \nL 64.509227 156.591068 \nL 63.009227 158.091068 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 151.226775 \nC 61.491916 151.226775 62.255046 150.910676 62.817627 150.348095 \nC 63.380208 149.785515 63.696307 149.022384 63.696307 148.226775 \nC 63.696307 147.431166 63.380208 146.668035 62.817627 146.105455 \nC 62.255046 145.542874 61.491916 145.226775 60.696307 145.226775 \nC 59.900698 145.226775 59.137567 145.542874 58.574986 146.105455 \nC 58.012406 146.668035 57.696307 147.431166 57.696307 148.226775 \nC 57.696307 149.022384 58.012406 149.785515 58.574986 150.348095 \nC 59.137567 150.910676 59.900698 151.226775 60.696307 151.226775 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 159.928924 \nC 61.570115 159.928924 62.333245 159.612825 62.895826 159.050244 \nC 63.458407 158.487664 63.774506 157.724533 63.774506 156.928924 \nC 63.774506 156.133315 63.458407 155.370184 62.895826 154.807604 \nC 62.333245 154.245023 61.570115 153.928924 60.774506 153.928924 \nC 59.978896 153.928924 59.215766 154.245023 58.653185 154.807604 \nC 58.090605 155.370184 57.774506 156.133315 57.774506 156.928924 \nC 57.774506 157.724533 58.090605 158.487664 58.653185 159.050244 \nC 59.215766 159.612825 59.978896 159.928924 60.774506 159.928924 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 177.74154 \nC 61.836911 177.74154 62.600041 177.425441 63.162622 176.86286 \nC 63.725203 176.30028 64.041302 175.537149 64.041302 174.74154 \nC 64.041302 173.945931 63.725203 173.182801 63.162622 172.62022 \nC 62.600041 172.057639 61.836911 171.74154 61.041302 171.74154 \nC 60.245692 171.74154 59.482562 172.057639 58.919981 172.62022 \nC 58.357401 173.182801 58.041302 173.945931 58.041302 174.74154 \nC 58.041302 175.537149 58.357401 176.30028 58.919981 176.86286 \nC 59.482562 177.425441 60.245692 177.74154 61.041302 177.74154 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 158.655773 \nC 62.812096 158.655773 63.575227 158.339674 64.137807 157.777093 \nC 64.700388 157.214513 65.016487 156.451382 65.016487 155.655773 \nC 65.016487 154.860164 64.700388 154.097033 64.137807 153.534453 \nC 63.575227 152.971872 62.812096 152.655773 62.016487 152.655773 \nC 61.220878 152.655773 60.457747 152.971872 59.895167 153.534453 \nC 59.332586 154.097033 59.016487 154.860164 59.016487 155.655773 \nC 59.016487 156.451382 59.332586 157.214513 59.895167 157.777093 \nC 60.457747 158.339674 61.220878 158.655773 62.016487 158.655773 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 157.309961 \nC 66.528841 157.309961 67.291971 156.993862 67.854552 156.431281 \nC 68.417132 155.8687 68.733231 155.10557 68.733231 154.309961 \nC 68.733231 153.514351 68.417132 152.751221 67.854552 152.18864 \nC 67.291971 151.626059 66.528841 151.309961 65.733231 151.309961 \nC 64.937622 151.309961 64.174492 151.626059 63.611911 152.18864 \nC 63.04933 152.751221 62.733231 153.514351 62.733231 154.309961 \nC 62.733231 155.10557 63.04933 155.8687 63.611911 156.431281 \nC 64.174492 156.993862 64.937622 157.309961 65.733231 157.309961 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 158.665983 \nC 81.027823 158.665983 81.790954 158.349884 82.353534 157.787303 \nC 82.916115 157.224722 83.232214 156.461592 83.232214 155.665983 \nC 83.232214 154.870374 82.916115 154.107243 82.353534 153.544663 \nC 81.790954 152.982082 81.027823 152.665983 80.232214 152.665983 \nC 79.436605 152.665983 78.673474 152.982082 78.110894 153.544663 \nC 77.548313 154.107243 77.232214 154.870374 77.232214 155.665983 \nC 77.232214 156.461592 77.548313 157.224722 78.110894 157.787303 \nC 78.673474 158.349884 79.436605 158.665983 80.232214 158.665983 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 159.244771 \nC 138.287765 159.244771 139.050895 158.928672 139.613476 158.366092 \nC 140.176056 157.803511 140.492155 157.040381 140.492155 156.244771 \nC 140.492155 155.449162 140.176056 154.686032 139.613476 154.123451 \nC 139.050895 153.56087 138.287765 153.244771 137.492155 153.244771 \nC 136.696546 153.244771 135.933416 153.56087 135.370835 154.123451 \nC 134.808254 154.686032 134.492155 155.449162 134.492155 156.244771 \nC 134.492155 157.040381 134.808254 157.803511 135.370835 158.366092 \nC 135.933416 158.928672 136.696546 159.244771 137.492155 159.244771 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 173.072444 \nC 365.855552 173.072444 366.618683 172.756345 367.181264 172.193764 \nC 367.743844 171.631184 368.059943 170.868053 368.059943 170.072444 \nC 368.059943 169.276835 367.743844 168.513704 367.181264 167.951124 \nC 366.618683 167.388543 365.855552 167.072444 365.059943 167.072444 \nC 364.264334 167.072444 363.501204 167.388543 362.938623 167.951124 \nC 362.376042 168.513704 362.059943 169.276835 362.059943 170.072444 \nC 362.059943 170.868053 362.376042 171.631184 362.938623 172.193764 \nC 363.501204 172.756345 364.264334 173.072444 365.059943 173.072444 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 159.485164 \nC 61.671313 159.485164 62.434444 159.169065 62.997024 158.606485 \nC 63.559605 158.043904 63.875704 157.280774 63.875704 156.485164 \nC 63.875704 155.689555 63.559605 154.926425 62.997024 154.363844 \nC 62.434444 153.801263 61.671313 153.485164 60.875704 153.485164 \nC 60.080095 153.485164 59.316965 153.801263 58.754384 154.363844 \nC 58.191803 154.926425 57.875704 155.689555 57.875704 156.485164 \nC 57.875704 157.280774 58.191803 158.043904 58.754384 158.606485 \nC 59.316965 159.169065 60.080095 159.485164 60.875704 159.485164 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 158.334236 \nC 61.836911 158.334236 62.600041 158.018137 63.162622 157.455556 \nC 63.725203 156.892975 64.041302 156.129845 64.041302 155.334236 \nC 64.041302 154.538627 63.725203 153.775496 63.162622 153.212915 \nC 62.600041 152.650335 61.836911 152.334236 61.041302 152.334236 \nC 60.245692 152.334236 59.482562 152.650335 58.919981 153.212915 \nC 58.357401 153.775496 58.041302 154.538627 58.041302 155.334236 \nC 58.041302 156.129845 58.357401 156.892975 58.919981 157.455556 \nC 59.482562 158.018137 60.245692 158.334236 61.041302 158.334236 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 201.776705 \nC 62.168106 201.776705 62.931236 201.460606 63.493817 200.898025 \nC 64.056398 200.335445 64.372497 199.572314 64.372497 198.776705 \nC 64.372497 197.981096 64.056398 197.217965 63.493817 196.655385 \nC 62.931236 196.092804 62.168106 195.776705 61.372497 195.776705 \nC 60.576887 195.776705 59.813757 196.092804 59.251176 196.655385 \nC 58.688596 197.217965 58.372497 197.981096 58.372497 198.776705 \nC 58.372497 199.572314 58.688596 200.335445 59.251176 200.898025 \nC 59.813757 201.460606 60.576887 201.776705 61.372497 201.776705 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 198.602638 \nC 62.830496 198.602638 63.593626 198.286539 64.156207 197.723959 \nC 64.718788 197.161378 65.034887 196.398248 65.034887 195.602638 \nC 65.034887 194.807029 64.718788 194.043899 64.156207 193.481318 \nC 63.593626 192.918737 62.830496 192.602638 62.034887 192.602638 \nC 61.239277 192.602638 60.476147 192.918737 59.913566 193.481318 \nC 59.350986 194.043899 59.034887 194.807029 59.034887 195.602638 \nC 59.034887 196.398248 59.350986 197.161378 59.913566 197.723959 \nC 60.476147 198.286539 61.239277 198.602638 62.034887 198.602638 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 122.776994 \nC 64.155276 122.776994 64.918406 122.460895 65.480987 121.898314 \nC 66.043568 121.335733 66.359667 120.572603 66.359667 119.776994 \nC 66.359667 118.981385 66.043568 118.218254 65.480987 117.655673 \nC 64.918406 117.093093 64.155276 116.776994 63.359667 116.776994 \nC 62.564058 116.776994 61.800927 117.093093 61.238347 117.655673 \nC 60.675766 118.218254 60.359667 118.981385 60.359667 119.776994 \nC 60.359667 120.572603 60.675766 121.335733 61.238347 121.898314 \nC 61.800927 122.460895 62.564058 122.776994 63.359667 122.776994 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869563 \nC 66.804836 37.869563 67.567967 37.553464 68.130547 36.990884 \nC 68.693128 36.428303 69.009227 35.665173 69.009227 34.869563 \nC 69.009227 34.073954 68.693128 33.310824 68.130547 32.748243 \nC 67.567967 32.185662 66.804836 31.869563 66.009227 31.869563 \nC 65.213618 31.869563 64.450488 32.185662 63.887907 32.748243 \nC 63.325326 33.310824 63.009227 34.073954 63.009227 34.869563 \nC 63.009227 35.665173 63.325326 36.428303 63.887907 36.990884 \nC 64.450488 37.553464 65.213618 37.869563 66.009227 37.869563 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 171.217987 \nL 60.696307 169.717987 \nL 62.196307 171.217987 \nL 63.696307 169.717987 \nL 62.196307 168.217987 \nL 63.696307 166.717987 \nL 62.196307 165.217987 \nL 60.696307 166.717987 \nL 59.196307 165.217987 \nL 57.696307 166.717987 \nL 59.196307 168.217987 \nL 57.696307 169.717987 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 159.206088 \nL 60.774506 157.706088 \nL 62.274506 159.206088 \nL 63.774506 157.706088 \nL 62.274506 156.206088 \nL 63.774506 154.706088 \nL 62.274506 153.206088 \nL 60.774506 154.706088 \nL 59.274506 153.206088 \nL 57.774506 154.706088 \nL 59.274506 156.206088 \nL 57.774506 157.706088 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 160.041585 \nL 61.041302 158.541585 \nL 62.541302 160.041585 \nL 64.041302 158.541585 \nL 62.541302 157.041585 \nL 64.041302 155.541585 \nL 62.541302 154.041585 \nL 61.041302 155.541585 \nL 59.541302 154.041585 \nL 58.041302 155.541585 \nL 59.541302 157.041585 \nL 58.041302 158.541585 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 156.676439 \nL 62.016487 155.176439 \nL 63.516487 156.676439 \nL 65.016487 155.176439 \nL 63.516487 153.676439 \nL 65.016487 152.176439 \nL 63.516487 150.676439 \nL 62.016487 152.176439 \nL 60.516487 150.676439 \nL 59.016487 152.176439 \nL 60.516487 153.676439 \nL 59.016487 155.176439 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 159.132316 \nL 65.733231 157.632316 \nL 67.233231 159.132316 \nL 68.733231 157.632316 \nL 67.233231 156.132316 \nL 68.733231 154.632316 \nL 67.233231 153.132316 \nL 65.733231 154.632316 \nL 64.233231 153.132316 \nL 62.733231 154.632316 \nL 64.233231 156.132316 \nL 62.733231 157.632316 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 155.783209 \nL 80.232214 154.283209 \nL 81.732214 155.783209 \nL 83.232214 154.283209 \nL 81.732214 152.783209 \nL 83.232214 151.283209 \nL 81.732214 149.783209 \nL 80.232214 151.283209 \nL 78.732214 149.783209 \nL 77.232214 151.283209 \nL 78.732214 152.783209 \nL 77.232214 154.283209 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 155.187699 \nL 137.492155 153.687699 \nL 138.992155 155.187699 \nL 140.492155 153.687699 \nL 138.992155 152.187699 \nL 140.492155 150.687699 \nL 138.992155 149.187699 \nL 137.492155 150.687699 \nL 135.992155 149.187699 \nL 134.492155 150.687699 \nL 135.992155 152.187699 \nL 134.492155 153.687699 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 156.60814 \nL 365.059943 155.10814 \nL 366.559943 156.60814 \nL 368.059943 155.10814 \nL 366.559943 153.60814 \nL 368.059943 152.10814 \nL 366.559943 150.60814 \nL 365.059943 152.10814 \nL 363.559943 150.60814 \nL 362.059943 152.10814 \nL 363.559943 153.60814 \nL 362.059943 155.10814 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 160.093524 \nL 60.875704 158.593524 \nL 62.375704 160.093524 \nL 63.875704 158.593524 \nL 62.375704 157.093524 \nL 63.875704 155.593524 \nL 62.375704 154.093524 \nL 60.875704 155.593524 \nL 59.375704 154.093524 \nL 57.875704 155.593524 \nL 59.375704 157.093524 \nL 57.875704 158.593524 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 177.68542 \nL 61.041302 176.18542 \nL 62.541302 177.68542 \nL 64.041302 176.18542 \nL 62.541302 174.68542 \nL 64.041302 173.18542 \nL 62.541302 171.68542 \nL 61.041302 173.18542 \nL 59.541302 171.68542 \nL 58.041302 173.18542 \nL 59.541302 174.68542 \nL 58.041302 176.18542 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 159.375958 \nL 61.372497 157.875958 \nL 62.872497 159.375958 \nL 64.372497 157.875958 \nL 62.872497 156.375958 \nL 64.372497 154.875958 \nL 62.872497 153.375958 \nL 61.372497 154.875958 \nL 59.872497 153.375958 \nL 58.372497 154.875958 \nL 59.872497 156.375958 \nL 58.372497 157.875958 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 157.45145 \nL 62.034887 155.95145 \nL 63.534887 157.45145 \nL 65.034887 155.95145 \nL 63.534887 154.45145 \nL 65.034887 152.95145 \nL 63.534887 151.45145 \nL 62.034887 152.95145 \nL 60.534887 151.45145 \nL 59.034887 152.95145 \nL 60.534887 154.45145 \nL 59.034887 155.95145 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 179.616794 \nL 63.359667 178.116794 \nL 64.859667 179.616794 \nL 66.359667 178.116794 \nL 64.859667 176.616794 \nL 66.359667 175.116794 \nL 64.859667 173.616794 \nL 63.359667 175.116794 \nL 61.859667 173.616794 \nL 60.359667 175.116794 \nL 61.859667 176.616794 \nL 60.359667 178.116794 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 159.833268 \nL 66.009227 158.333268 \nL 67.509227 159.833268 \nL 69.009227 158.333268 \nL 67.509227 156.833268 \nL 69.009227 155.333268 \nL 67.509227 153.833268 \nL 66.009227 155.333268 \nL 64.509227 153.833268 \nL 63.009227 155.333268 \nL 64.509227 156.833268 \nL 63.009227 158.333268 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.696307 157.027645 \nC 61.491916 157.027645 62.255046 156.711546 62.817627 156.148966 \nC 63.380208 155.586385 63.696307 154.823255 63.696307 154.027645 \nC 63.696307 153.232036 63.380208 152.468906 62.817627 151.906325 \nC 62.255046 151.343744 61.491916 151.027645 60.696307 151.027645 \nC 59.900698 151.027645 59.137567 151.343744 58.574986 151.906325 \nC 58.012406 152.468906 57.696307 153.232036 57.696307 154.027645 \nC 57.696307 154.823255 58.012406 155.586385 58.574986 156.148966 \nC 59.137567 156.711546 59.900698 157.027645 60.696307 157.027645 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.774506 177.162862 \nC 61.570115 177.162862 62.333245 176.846764 62.895826 176.284183 \nC 63.458407 175.721602 63.774506 174.958472 63.774506 174.162862 \nC 63.774506 173.367253 63.458407 172.604123 62.895826 172.041542 \nC 62.333245 171.478961 61.570115 171.162862 60.774506 171.162862 \nC 59.978896 171.162862 59.215766 171.478961 58.653185 172.041542 \nC 58.090605 172.604123 57.774506 173.367253 57.774506 174.162862 \nC 57.774506 174.958472 58.090605 175.721602 58.653185 176.284183 \nC 59.215766 176.846764 59.978896 177.162862 60.774506 177.162862 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 157.943041 \nC 61.836911 157.943041 62.600041 157.626942 63.162622 157.064361 \nC 63.725203 156.501781 64.041302 155.73865 64.041302 154.943041 \nC 64.041302 154.147432 63.725203 153.384301 63.162622 152.821721 \nC 62.600041 152.25914 61.836911 151.943041 61.041302 151.943041 \nC 60.245692 151.943041 59.482562 152.25914 58.919981 152.821721 \nC 58.357401 153.384301 58.041302 154.147432 58.041302 154.943041 \nC 58.041302 155.73865 58.357401 156.501781 58.919981 157.064361 \nC 59.482562 157.626942 60.245692 157.943041 61.041302 157.943041 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.016487 159.935583 \nC 62.812096 159.935583 63.575227 159.619484 64.137807 159.056903 \nC 64.700388 158.494322 65.016487 157.731192 65.016487 156.935583 \nC 65.016487 156.139973 64.700388 155.376843 64.137807 154.814262 \nC 63.575227 154.251681 62.812096 153.935583 62.016487 153.935583 \nC 61.220878 153.935583 60.457747 154.251681 59.895167 154.814262 \nC 59.332586 155.376843 59.016487 156.139973 59.016487 156.935583 \nC 59.016487 157.731192 59.332586 158.494322 59.895167 159.056903 \nC 60.457747 159.619484 61.220878 159.935583 62.016487 159.935583 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 65.733231 155.696541 \nC 66.528841 155.696541 67.291971 155.380442 67.854552 154.817861 \nC 68.417132 154.255281 68.733231 153.49215 68.733231 152.696541 \nC 68.733231 151.900932 68.417132 151.137801 67.854552 150.575221 \nC 67.291971 150.01264 66.528841 149.696541 65.733231 149.696541 \nC 64.937622 149.696541 64.174492 150.01264 63.611911 150.575221 \nC 63.04933 151.137801 62.733231 151.900932 62.733231 152.696541 \nC 62.733231 153.49215 63.04933 154.255281 63.611911 154.817861 \nC 64.174492 155.380442 64.937622 155.696541 65.733231 155.696541 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 80.232214 169.419531 \nC 81.027823 169.419531 81.790954 169.103432 82.353534 168.540852 \nC 82.916115 167.978271 83.232214 167.215141 83.232214 166.419531 \nC 83.232214 165.623922 82.916115 164.860792 82.353534 164.298211 \nC 81.790954 163.73563 81.027823 163.419531 80.232214 163.419531 \nC 79.436605 163.419531 78.673474 163.73563 78.110894 164.298211 \nC 77.548313 164.860792 77.232214 165.623922 77.232214 166.419531 \nC 77.232214 167.215141 77.548313 167.978271 78.110894 168.540852 \nC 78.673474 169.103432 79.436605 169.419531 80.232214 169.419531 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 137.492155 156.512928 \nC 138.287765 156.512928 139.050895 156.196829 139.613476 155.634248 \nC 140.176056 155.071668 140.492155 154.308537 140.492155 153.512928 \nC 140.492155 152.717319 140.176056 151.954188 139.613476 151.391608 \nC 139.050895 150.829027 138.287765 150.512928 137.492155 150.512928 \nC 136.696546 150.512928 135.933416 150.829027 135.370835 151.391608 \nC 134.808254 151.954188 134.492155 152.717319 134.492155 153.512928 \nC 134.492155 154.308537 134.808254 155.071668 135.370835 155.634248 \nC 135.933416 156.196829 136.696546 156.512928 137.492155 156.512928 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 365.059943 156.602294 \nC 365.855552 156.602294 366.618683 156.286195 367.181264 155.723615 \nC 367.743844 155.161034 368.059943 154.397904 368.059943 153.602294 \nC 368.059943 152.806685 367.743844 152.043555 367.181264 151.480974 \nC 366.618683 150.918393 365.855552 150.602294 365.059943 150.602294 \nC 364.264334 150.602294 363.501204 150.918393 362.938623 151.480974 \nC 362.376042 152.043555 362.059943 152.806685 362.059943 153.602294 \nC 362.059943 154.397904 362.376042 155.161034 362.938623 155.723615 \nC 363.501204 156.286195 364.264334 156.602294 365.059943 156.602294 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.875704 178.686053 \nC 61.671313 178.686053 62.434444 178.369954 62.997024 177.807373 \nC 63.559605 177.244792 63.875704 176.481662 63.875704 175.686053 \nC 63.875704 174.890443 63.559605 174.127313 62.997024 173.564732 \nC 62.434444 173.002152 61.671313 172.686053 60.875704 172.686053 \nC 60.080095 172.686053 59.316965 173.002152 58.754384 173.564732 \nC 58.191803 174.127313 57.875704 174.890443 57.875704 175.686053 \nC 57.875704 176.481662 58.191803 177.244792 58.754384 177.807373 \nC 59.316965 178.369954 60.080095 178.686053 60.875704 178.686053 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.041302 158.263651 \nC 61.836911 158.263651 62.600041 157.947552 63.162622 157.384971 \nC 63.725203 156.822391 64.041302 156.05926 64.041302 155.263651 \nC 64.041302 154.468042 63.725203 153.704911 63.162622 153.142331 \nC 62.600041 152.57975 61.836911 152.263651 61.041302 152.263651 \nC 60.245692 152.263651 59.482562 152.57975 58.919981 153.142331 \nC 58.357401 153.704911 58.041302 154.468042 58.041302 155.263651 \nC 58.041302 156.05926 58.357401 156.822391 58.919981 157.384971 \nC 59.482562 157.947552 60.245692 158.263651 61.041302 158.263651 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.372497 158.44269 \nC 62.168106 158.44269 62.931236 158.126591 63.493817 157.564011 \nC 64.056398 157.00143 64.372497 156.2383 64.372497 155.44269 \nC 64.372497 154.647081 64.056398 153.883951 63.493817 153.32137 \nC 62.931236 152.758789 62.168106 152.44269 61.372497 152.44269 \nC 60.576887 152.44269 59.813757 152.758789 59.251176 153.32137 \nC 58.688596 153.883951 58.372497 154.647081 58.372497 155.44269 \nC 58.372497 156.2383 58.688596 157.00143 59.251176 157.564011 \nC 59.813757 158.126591 60.576887 158.44269 61.372497 158.44269 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 62.034887 178.111463 \nC 62.830496 178.111463 63.593626 177.795364 64.156207 177.232784 \nC 64.718788 176.670203 65.034887 175.907072 65.034887 175.111463 \nC 65.034887 174.315854 64.718788 173.552724 64.156207 172.990143 \nC 63.593626 172.427562 62.830496 172.111463 62.034887 172.111463 \nC 61.239277 172.111463 60.476147 172.427562 59.913566 172.990143 \nC 59.350986 173.552724 59.034887 174.315854 59.034887 175.111463 \nC 59.034887 175.907072 59.350986 176.670203 59.913566 177.232784 \nC 60.476147 177.795364 61.239277 178.111463 62.034887 178.111463 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 63.359667 37.886016 \nC 64.155276 37.886016 64.918406 37.569917 65.480987 37.007336 \nC 66.043568 36.444756 66.359667 35.681625 66.359667 34.886016 \nC 66.359667 34.090407 66.043568 33.327276 65.480987 32.764696 \nC 64.918406 32.202115 64.155276 31.886016 63.359667 31.886016 \nC 62.564058 31.886016 61.800927 32.202115 61.238347 32.764696 \nC 60.675766 33.327276 60.359667 34.090407 60.359667 34.886016 \nC 60.359667 35.681625 60.675766 36.444756 61.238347 37.007336 \nC 61.800927 37.569917 62.564058 37.886016 63.359667 37.886016 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 66.009227 37.869525 \nC 66.804836 37.869525 67.567967 37.553426 68.130547 36.990846 \nC 68.693128 36.428265 69.009227 35.665135 69.009227 34.869525 \nC 69.009227 34.073916 68.693128 33.310786 68.130547 32.748205 \nC 67.567967 32.185624 66.804836 31.869525 66.009227 31.869525 \nC 65.213618 31.869525 64.450488 32.185624 63.887907 32.748205 \nC 63.325326 33.310786 63.009227 34.073916 63.009227 34.869525 \nC 63.009227 35.665135 63.325326 36.428265 63.887907 36.990846 \nC 64.450488 37.553426 65.213618 37.869525 66.009227 37.869525 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.196307 115.908314 \nL 60.696307 114.408314 \nL 62.196307 115.908314 \nL 63.696307 114.408314 \nL 62.196307 112.908314 \nL 63.696307 111.408314 \nL 62.196307 109.908314 \nL 60.696307 111.408314 \nL 59.196307 109.908314 \nL 57.696307 111.408314 \nL 59.196307 112.908314 \nL 57.696307 114.408314 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.274506 157.563243 \nL 60.774506 156.063243 \nL 62.274506 157.563243 \nL 63.774506 156.063243 \nL 62.274506 154.563243 \nL 63.774506 153.063243 \nL 62.274506 151.563243 \nL 60.774506 153.063243 \nL 59.274506 151.563243 \nL 57.774506 153.063243 \nL 59.274506 154.563243 \nL 57.774506 156.063243 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 161.316705 \nL 61.041302 159.816705 \nL 62.541302 161.316705 \nL 64.041302 159.816705 \nL 62.541302 158.316705 \nL 64.041302 156.816705 \nL 62.541302 155.316705 \nL 61.041302 156.816705 \nL 59.541302 155.316705 \nL 58.041302 156.816705 \nL 59.541302 158.316705 \nL 58.041302 159.816705 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.516487 181.277351 \nL 62.016487 179.777351 \nL 63.516487 181.277351 \nL 65.016487 179.777351 \nL 63.516487 178.277351 \nL 65.016487 176.777351 \nL 63.516487 175.277351 \nL 62.016487 176.777351 \nL 60.516487 175.277351 \nL 59.016487 176.777351 \nL 60.516487 178.277351 \nL 59.016487 179.777351 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.233231 155.4326 \nL 65.733231 153.9326 \nL 67.233231 155.4326 \nL 68.733231 153.9326 \nL 67.233231 152.4326 \nL 68.733231 150.9326 \nL 67.233231 149.4326 \nL 65.733231 150.9326 \nL 64.233231 149.4326 \nL 62.733231 150.9326 \nL 64.233231 152.4326 \nL 62.733231 153.9326 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 78.732214 160.102047 \nL 80.232214 158.602047 \nL 81.732214 160.102047 \nL 83.232214 158.602047 \nL 81.732214 157.102047 \nL 83.232214 155.602047 \nL 81.732214 154.102047 \nL 80.232214 155.602047 \nL 78.732214 154.102047 \nL 77.232214 155.602047 \nL 78.732214 157.102047 \nL 77.232214 158.602047 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 135.992155 42.088943 \nL 137.492155 40.588943 \nL 138.992155 42.088943 \nL 140.492155 40.588943 \nL 138.992155 39.088943 \nL 140.492155 37.588943 \nL 138.992155 36.088943 \nL 137.492155 37.588943 \nL 135.992155 36.088943 \nL 134.492155 37.588943 \nL 135.992155 39.088943 \nL 134.492155 40.588943 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 363.559943 157.577349 \nL 365.059943 156.077349 \nL 366.559943 157.577349 \nL 368.059943 156.077349 \nL 366.559943 154.577349 \nL 368.059943 153.077349 \nL 366.559943 151.577349 \nL 365.059943 153.077349 \nL 363.559943 151.577349 \nL 362.059943 153.077349 \nL 363.559943 154.577349 \nL 362.059943 156.077349 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.375704 159.741106 \nL 60.875704 158.241106 \nL 62.375704 159.741106 \nL 63.875704 158.241106 \nL 62.375704 156.741106 \nL 63.875704 155.241106 \nL 62.375704 153.741106 \nL 60.875704 155.241106 \nL 59.375704 153.741106 \nL 57.875704 155.241106 \nL 59.375704 156.741106 \nL 57.875704 158.241106 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.541302 158.418552 \nL 61.041302 156.918552 \nL 62.541302 158.418552 \nL 64.041302 156.918552 \nL 62.541302 155.418552 \nL 64.041302 153.918552 \nL 62.541302 152.418552 \nL 61.041302 153.918552 \nL 59.541302 152.418552 \nL 58.041302 153.918552 \nL 59.541302 155.418552 \nL 58.041302 156.918552 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 59.872497 73.857252 \nL 61.372497 72.357252 \nL 62.872497 73.857252 \nL 64.372497 72.357252 \nL 62.872497 70.857252 \nL 64.372497 69.357252 \nL 62.872497 67.857252 \nL 61.372497 69.357252 \nL 59.872497 67.857252 \nL 58.372497 69.357252 \nL 59.872497 70.857252 \nL 58.372497 72.357252 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 60.534887 159.166281 \nL 62.034887 157.666281 \nL 63.534887 159.166281 \nL 65.034887 157.666281 \nL 63.534887 156.166281 \nL 65.034887 154.666281 \nL 63.534887 153.166281 \nL 62.034887 154.666281 \nL 60.534887 153.166281 \nL 59.034887 154.666281 \nL 60.534887 156.166281 \nL 59.034887 157.666281 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 61.859667 177.330498 \nL 63.359667 175.830498 \nL 64.859667 177.330498 \nL 66.359667 175.830498 \nL 64.859667 174.330498 \nL 66.359667 172.830498 \nL 64.859667 171.330498 \nL 63.359667 172.830498 \nL 61.859667 171.330498 \nL 60.359667 172.830498 \nL 61.859667 174.330498 \nL 60.359667 175.830498 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pd669eca831)\" d=\"M 64.509227 160.132917 \nL 66.009227 158.632917 \nL 67.509227 160.132917 \nL 69.009227 158.632917 \nL 67.509227 157.132917 \nL 69.009227 155.632917 \nL 67.509227 154.132917 \nL 66.009227 155.632917 \nL 64.509227 154.132917 \nL 63.009227 155.632917 \nL 64.509227 157.132917 \nL 63.009227 158.632917 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n   </g>\n   <g id=\"PathCollection_2\"/>\n   <g id=\"PathCollection_3\"/>\n   <g id=\"PathCollection_4\"/>\n   <g id=\"PathCollection_5\"/>\n   <g id=\"PathCollection_6\"/>\n   <g id=\"PathCollection_7\"/>\n   <g id=\"PathCollection_8\"/>\n   <g id=\"PathCollection_9\"/>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m140e507355\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.652607\" xlink:href=\"#m140e507355\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(57.471357 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.651918\" xlink:href=\"#m140e507355\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20000 -->\n      <g transform=\"translate(90.745668 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"152.651228\" xlink:href=\"#m140e507355\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40000 -->\n      <g transform=\"translate(136.744978 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.650538\" xlink:href=\"#m140e507355\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60000 -->\n      <g transform=\"translate(182.744288 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"244.649849\" xlink:href=\"#m140e507355\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80000 -->\n      <g transform=\"translate(228.743599 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"290.649159\" xlink:href=\"#m140e507355\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100000 -->\n      <g transform=\"translate(271.561659 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"336.648469\" xlink:href=\"#m140e507355\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120000 -->\n      <g transform=\"translate(317.560969 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- parameters -->\n     <g transform=\"translate(183.876563 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"165.869141\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"324.560547\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"386.083984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"425.292969\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"486.816406\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"527.929688\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m6cda6c54e3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m6cda6c54e3\" y=\"161.416861\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(20.878125 165.21608)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m6cda6c54e3\" y=\"98.074796\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 101.874015)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m6cda6c54e3\" y=\"34.732732\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 38.53195)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m04ee50a165\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"205.691064\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"194.53708\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"186.623203\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"180.484722\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"175.469219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"171.228671\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"167.555341\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"164.315235\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"142.348999\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"131.195015\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"123.281138\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"117.142658\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"112.127154\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"107.886606\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"104.213276\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"100.97317\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"79.006935\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"67.852951\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"59.939073\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"53.800593\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"48.785089\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"44.544542\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"40.871212\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"37.631105\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_28\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m04ee50a165\" y=\"15.66487\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 125.577812)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 177.99375 219.64 \nL 247.7625 219.64 \nQ 249.7625 219.64 249.7625 217.64 \nL 249.7625 71.85875 \nQ 249.7625 69.85875 247.7625 69.85875 \nL 177.99375 69.85875 \nQ 175.99375 69.85875 175.99375 71.85875 \nL 175.99375 217.64 \nQ 175.99375 219.64 177.99375 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- layers -->\n     <g transform=\"translate(187.99375 81.457187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"209.765625\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"250.878906\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_10\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mbb9e6d0cb9\" style=\"stroke:#edd1cb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#edd1cb;stroke:#edd1cb;\" x=\"189.99375\" xlink:href=\"#mbb9e6d0cb9\" y=\"93.510312\"/>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- 1 -->\n     <g transform=\"translate(207.99375 96.135312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_11\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m6f0d1a5f11\" style=\"stroke:#ebcac5;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ebcac5;stroke:#ebcac5;\" x=\"189.99375\" xlink:href=\"#m6f0d1a5f11\" y=\"108.188437\"/>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- 2 -->\n     <g transform=\"translate(207.99375 110.813437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_12\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m8a8dc3aabc\" style=\"stroke:#e5bcbb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#e5bcbb;stroke:#e5bcbb;\" x=\"189.99375\" xlink:href=\"#m8a8dc3aabc\" y=\"122.866562\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- 4 -->\n     <g transform=\"translate(207.99375 125.491562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_13\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"md3aea2b1bd\" style=\"stroke:#d89faa;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#d89faa;stroke:#d89faa;\" x=\"189.99375\" xlink:href=\"#md3aea2b1bd\" y=\"137.544688\"/>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- 8 -->\n     <g transform=\"translate(207.99375 140.169688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-38\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_14\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m71d105d149\" style=\"stroke:#ae6b91;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ae6b91;stroke:#ae6b91;\" x=\"189.99375\" xlink:href=\"#m71d105d149\" y=\"152.222812\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- 16 -->\n     <g transform=\"translate(207.99375 154.847812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_15\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"ma08b9d60e1\" style=\"stroke:#2d1e3e;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#2d1e3e;stroke:#2d1e3e;\" x=\"189.99375\" xlink:href=\"#ma08b9d60e1\" y=\"166.900937\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- 32 -->\n     <g transform=\"translate(207.99375 169.525937)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- model -->\n     <g transform=\"translate(187.99375 184.204062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_16\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"md9d77263ef\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"189.99375\" xlink:href=\"#md9d77263ef\" y=\"196.257187\"/>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- FFNN -->\n     <g transform=\"translate(207.99375 198.882187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_17\">\n     <defs>\n      <path d=\"M -1.5 3 \nL 0 1.5 \nL 1.5 3 \nL 3 1.5 \nL 1.5 0 \nL 3 -1.5 \nL 1.5 -3 \nL 0 -1.5 \nL -1.5 -3 \nL -3 -1.5 \nL -1.5 0 \nL -3 1.5 \nz\n\" id=\"m15f759bb64\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"189.99375\" xlink:href=\"#m15f759bb64\" y=\"210.935312\"/>\n     </g>\n    </g>\n    <g id=\"text_22\">\n     <!-- ResNET -->\n     <g transform=\"translate(207.99375 213.560312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pd669eca831\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 280\n"
     ]
    }
   ],
   "source": [
    "print(len(d_results[d_results.loss < 2]), len(d_results))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}