{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Try to learn (1) as a piecewise linear constant curve"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "import os\n",
    "from itertools import chain\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from neural_reparam.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from neural_reparam.ResNet import ResNet\n",
    "from neural_reparam.interpolation import get_pc_curve, get_pl_curve\n",
    "import experiments.curves as c1\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_1_pl/\"\n",
    "SET_NAME = \"exp_2\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "\n",
    "FOLDS = 1\n",
    "N = 128  # training points internal\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=5,\n",
    "                                                                        verbose=True)\n",
    "loss_func = get_elastic_metric_loss(r=get_pl_curve(c1.r, N), constrain_cost=1e3, verbose=False)\n",
    "penalty_free_loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=0, verbose=False)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [2],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"model\": [FFNN, ResNet],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N // 2],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [100],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"lr_scheduler\": [lr_scheduler]\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_pl = get_pc_curve(c1.q, N)\n",
    "q_train = q_pl(x_train.detach())\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  36.93458939\n",
      "Training Loss:  1.63318217\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.01471972\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.66208291\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.6444453\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.6444453\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.6444453\n",
      "Training Loss:  1.6444453\n",
      "Training Loss:  1.6444453\n",
      "Training Loss:  1.6444453\n",
      "Final training Loss:  1.6444453\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  394.67324829\n",
      "Training Loss:  1.09218907\n",
      "Training Loss:  0.85255891\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.98291504\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.85563749\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.84106779\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.84100854\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.84100854\n",
      "Training Loss:  0.84100854\n",
      "Training Loss:  0.84100854\n",
      "Final training Loss:  0.84100854\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.20785809\n",
      "Training Loss:  1.2521503\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.40954316\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.47467637\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.26416099\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.26414132\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.26414239\n",
      "Training Loss:  1.26414239\n",
      "Training Loss:  1.26414239\n",
      "Training Loss:  1.26414239\n",
      "Final training Loss:  1.26414239\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.7253356\n",
      "Training Loss:  0.76455134\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  44.23255539\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.49078715\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.49078715\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.49079281\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.49079627\n",
      "Training Loss:  0.49079627\n",
      "Training Loss:  0.49079627\n",
      "Training Loss:  0.49079627\n",
      "Final training Loss:  0.49079627\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.6195668\n",
      "Training Loss:  1.50412619\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.35862744\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.71338153\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.33852303\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.33854079\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.33854055\n",
      "Training Loss:  1.33854055\n",
      "Training Loss:  1.33854055\n",
      "Training Loss:  1.33854055\n",
      "Final training Loss:  1.33854055\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.4594574\n",
      "Training Loss:  104.55848694\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.73622131\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.02136993\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.32880032\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.29927588\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.29926431\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.29926443\n",
      "Training Loss:  1.29926443\n",
      "Training Loss:  1.29926443\n",
      "Training Loss:  1.29926443\n",
      "Training Loss:  1.29926443\n",
      "Final training Loss:  1.29926443\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.00541234\n",
      "Training Loss:  1.66289032\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.90602899\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.36460376\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.38038325\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.38038325\n",
      "Training Loss:  1.38038325\n",
      "Training Loss:  1.38038325\n",
      "Training Loss:  1.38038325\n",
      "Training Loss:  1.38038325\n",
      "Final training Loss:  1.38038325\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.67821336\n",
      "Training Loss:  1.44605076\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.80920017\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.39327681\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.39135933\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.39134383\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.39134431\n",
      "Training Loss:  1.39134431\n",
      "Training Loss:  1.39134431\n",
      "Training Loss:  1.39134431\n",
      "Final training Loss:  1.39134431\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.28820074\n",
      "Training Loss:  1.54038632\n",
      "Training Loss:  4.59762859\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.22308087\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.17354214\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.17353511\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.17353511\n",
      "Training Loss:  1.17353511\n",
      "Training Loss:  1.17353511\n",
      "Training Loss:  1.17353511\n",
      "Final training Loss:  1.17353511\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.56177282\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  5.41567898\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.78619438\n",
      "Training Loss:  2.18830895\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.64592457\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.64998293\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.64998162\n",
      "Training Loss:  0.64998162\n",
      "Training Loss:  0.64998162\n",
      "Training Loss:  0.64998162\n",
      "Final training Loss:  0.64998162\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.68390656\n",
      "Training Loss:  146.75102234\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.53932083\n",
      "Training Loss:  1.4121263\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.37823021\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.4205029\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.45028436\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.45024562\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.4502455\n",
      "Training Loss:  1.4502455\n",
      "Final training Loss:  1.4502455\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50408936\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50396729\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50396729\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50396729\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50396729\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50396729\n",
      "Training Loss:  599.50396729\n",
      "Training Loss:  599.50396729\n",
      "Training Loss:  599.50396729\n",
      "Training Loss:  599.50396729\n",
      "Final training Loss:  599.50396729\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  38.46779251\n",
      "Training Loss:  4.23431492\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.9553709\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  2.55691385\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.55702019\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  2.55702019\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  2.55702019\n",
      "Training Loss:  2.55702019\n",
      "Training Loss:  2.55702019\n",
      "Training Loss:  2.55702019\n",
      "Final training Loss:  2.55702019\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.75329936\n",
      "Training Loss:  1.40724134\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.73577613\n",
      "Training Loss:  0.69947779\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.62067997\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.62064898\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.62068856\n",
      "Training Loss:  0.62068856\n",
      "Training Loss:  0.62068856\n",
      "Training Loss:  0.62068856\n",
      "Final training Loss:  0.62068856\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10.48639584\n",
      "Training Loss:  0.92213416\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.85692167\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.64863789\n",
      "Training Loss:  0.61810499\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.69206595\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.92837358\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.92837542\n",
      "Training Loss:  0.92837542\n",
      "Training Loss:  0.92837542\n",
      "Final training Loss:  0.92837542\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  87.85916138\n",
      "Training Loss:  1.75651658\n",
      "Training Loss:  0.73012358\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.91209203\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.68411589\n",
      "Training Loss:  0.74414426\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.6010468\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.61978787\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.61976618\n",
      "Training Loss:  0.61976618\n",
      "Final training Loss:  0.61976618\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.81638002\n",
      "Training Loss:  3.24618196\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.72870374\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.56706285\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.31922495\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.31925881\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.31925881\n",
      "Training Loss:  1.31925881\n",
      "Training Loss:  1.31925881\n",
      "Training Loss:  1.31925881\n",
      "Final training Loss:  1.31925881\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.55627728\n",
      "Training Loss:  1.39639151\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.29220927\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.46019244\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.32657492\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.32657254\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.32657254\n",
      "Training Loss:  1.32657254\n",
      "Training Loss:  1.32657254\n",
      "Training Loss:  1.32657254\n",
      "Final training Loss:  1.32657254\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.26552868\n",
      "Training Loss:  1.54556322\n",
      "Training Loss:  1.53254771\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.37830091\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.4572984\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.6643362\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.66434479\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.66423905\n",
      "Training Loss:  1.66423905\n",
      "Training Loss:  1.66423905\n",
      "Final training Loss:  1.66423905\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  16.18518829\n",
      "Training Loss:  0.85487634\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.25386822\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.72920632\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.68658841\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.68610358\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.68588513\n",
      "Training Loss:  0.68588513\n",
      "Training Loss:  0.68588513\n",
      "Training Loss:  0.68588513\n",
      "Final training Loss:  0.68588513\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.96783733\n",
      "Training Loss:  1.22217643\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.78701878\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.08696771\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.65113264\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.88324773\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.88324976\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.88324976\n",
      "Training Loss:  0.88324976\n",
      "Training Loss:  0.88324976\n",
      "Final training Loss:  0.88324976\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.69478512\n",
      "Training Loss:  0.71371692\n",
      "Training Loss:  2.30285192\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.94635153\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.36132205\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.77333146\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.77331501\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.77332026\n",
      "Training Loss:  0.77332026\n",
      "Training Loss:  0.77332026\n",
      "Final training Loss:  0.77332026\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.52718687\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.30645168\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.91245377\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.35502768\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.35501254\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.35501254\n",
      "Training Loss:  1.35501254\n",
      "Training Loss:  1.35501254\n",
      "Training Loss:  1.35501254\n",
      "Training Loss:  1.35501254\n",
      "Final training Loss:  1.35501254\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.7816937\n",
      "Training Loss:  0.62246889\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.28449261\n",
      "Training Loss:  0.55046302\n",
      "Training Loss:  220.43089294\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.7623812\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.7231077\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.72310686\n",
      "Training Loss:  0.72310686\n",
      "Training Loss:  0.72310686\n",
      "Final training Loss:  0.72310686\n",
      "\n",
      "Running model (trial=1, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.52932119\n",
      "Training Loss:  1.52853978\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.31719661\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.52185774\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.53163075\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.53163075\n",
      "Training Loss:  1.53163075\n",
      "Training Loss:  1.53163075\n",
      "Training Loss:  1.53163075\n",
      "Training Loss:  1.53163075\n",
      "Final training Loss:  1.53163075\n",
      "\n",
      "Running model (trial=1, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.82278991\n",
      "Training Loss:  3.01518965\n",
      "Training Loss:  1.18021119\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.90328413\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.01938558\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.01904845\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.01904845\n",
      "Training Loss:  1.01904845\n",
      "Training Loss:  1.01904845\n",
      "Training Loss:  1.01904845\n",
      "Final training Loss:  1.01904845\n",
      "\n",
      "Running model (trial=1, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.83102894\n",
      "Training Loss:  1.46970165\n",
      "Training Loss:  1.03578579\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.24606073\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.7943446\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.77272379\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.77277571\n",
      "Training Loss:  0.77277571\n",
      "Training Loss:  0.77277571\n",
      "Training Loss:  0.77277571\n",
      "Final training Loss:  0.77277571\n",
      "\n",
      "Running model (trial=1, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.46029687\n",
      "Training Loss:  3.09674668\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.019032\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.48411548\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.28049684\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.28050637\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.28050649\n",
      "Training Loss:  1.28050649\n",
      "Training Loss:  1.28050649\n",
      "Training Loss:  1.28050649\n",
      "Final training Loss:  1.28050649\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.74165106\n",
      "Training Loss:  2.36960673\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.41324413\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.40868306\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.30925441\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.30923676\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.30923676\n",
      "Training Loss:  1.30923676\n",
      "Training Loss:  1.30923676\n",
      "Training Loss:  1.30923676\n",
      "Final training Loss:  1.30923676\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.87042832\n",
      "Training Loss:  4.42803001\n",
      "Training Loss:  1.14273429\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.97484493\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.76857215\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.61835808\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.61834425\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.61834645\n",
      "Training Loss:  0.61834645\n",
      "Training Loss:  0.61834645\n",
      "Final training Loss:  0.61834645\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  31.33049583\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.74831343\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.28259039\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.11449146\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.52204669\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.52205086\n",
      "Training Loss:  1.52205086\n",
      "Training Loss:  1.52205086\n",
      "Training Loss:  1.52205086\n",
      "Training Loss:  1.52205086\n",
      "Final training Loss:  1.52205086\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  74.3895874\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.93824792\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.39529717\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.3133769\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.44152701\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.44151413\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.44151413\n",
      "Training Loss:  1.44151413\n",
      "Training Loss:  1.44151413\n",
      "Training Loss:  1.44151413\n",
      "Final training Loss:  1.44151413\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.44901097\n",
      "Training Loss:  1.47918618\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.60482645\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.11859167\n",
      "Training Loss:  0.91443169\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.62800592\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.63447696\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.63448191\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.63448191\n",
      "Training Loss:  0.63448191\n",
      "Final training Loss:  0.63448191\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.24146271\n",
      "Training Loss:  0.88346374\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.40641356\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.00753939\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.89185226\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.77822232\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.6804294\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.68043447\n",
      "Training Loss:  0.68043447\n",
      "Training Loss:  0.68043447\n",
      "Final training Loss:  0.68043447\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.65347183\n",
      "Training Loss:  2.06039929\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.97010738\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.95402092\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.85283422\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.70567602\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.7056911\n",
      "Training Loss:  0.7056911\n",
      "Training Loss:  0.7056911\n",
      "Training Loss:  0.7056911\n",
      "Final training Loss:  0.7056911\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.40100098\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.40112305\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.40112305\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.40112305\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.40112305\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.40112305\n",
      "Training Loss:  599.40112305\n",
      "Training Loss:  599.40112305\n",
      "Training Loss:  599.40112305\n",
      "Training Loss:  599.40112305\n",
      "Final training Loss:  599.40112305\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50402832\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Final training Loss:  599.50390625\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  61.99573135\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.29374313\n",
      "Training Loss:  2.87738681\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.72632241\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.72482681\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  2.72500181\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  2.72499061\n",
      "Training Loss:  2.72499061\n",
      "Training Loss:  2.72499061\n",
      "Training Loss:  2.72499061\n",
      "Final training Loss:  2.72499061\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.61604095\n",
      "Training Loss:  2.48607326\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.32389486\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.54008603\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.27248192\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.27209568\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.27209568\n",
      "Training Loss:  1.27209568\n",
      "Training Loss:  1.27209568\n",
      "Training Loss:  1.27209568\n",
      "Final training Loss:  1.27209568\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.00687075\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.52635181\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.26866949\n",
      "Training Loss:  1.35451579\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.3566432\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.33146214\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.33146083\n",
      "Training Loss:  1.33146083\n",
      "Training Loss:  1.33146083\n",
      "Training Loss:  1.33146083\n",
      "Final training Loss:  1.33146083\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  32.46063232\n",
      "Training Loss:  1.27111936\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.6513319\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.15263689\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.24495685\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.45938802\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.4593879\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.4593879\n",
      "Training Loss:  1.4593879\n",
      "Training Loss:  1.4593879\n",
      "Final training Loss:  1.4593879\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  76.16396332\n",
      "Training Loss:  3.20607257\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.8801105\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.26119733\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.31148982\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.31145263\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.31144857\n",
      "Training Loss:  1.31144857\n",
      "Training Loss:  1.31144857\n",
      "Training Loss:  1.31144857\n",
      "Final training Loss:  1.31144857\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.5927844\n",
      "Training Loss:  1.40563571\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.47372723\n",
      "Training Loss:  1.40884078\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.56235325\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.33031476\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.33031464\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.33031464\n",
      "Training Loss:  1.33031464\n",
      "Training Loss:  1.33031464\n",
      "Final training Loss:  1.33031464\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  64.6023407\n",
      "Training Loss:  1.33654201\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.67981899\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.41809165\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.44214404\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.4181664\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.4181664\n",
      "Training Loss:  1.4181664\n",
      "Training Loss:  1.4181664\n",
      "Training Loss:  1.4181664\n",
      "Final training Loss:  1.4181664\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  24.20686722\n",
      "Training Loss:  2.74291062\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.24009109\n",
      "Training Loss:  1.43528938\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  2.60567379\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.37517858\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.37520111\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.37520111\n",
      "Training Loss:  1.37520111\n",
      "Training Loss:  1.37520111\n",
      "Final training Loss:  1.37520111\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.78824449\n",
      "Training Loss:  1.98245585\n",
      "Training Loss:  1.53176248\n",
      "Training Loss:  1.41636872\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.4553057\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.23113072\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.17625833\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.17621744\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.17621744\n",
      "Training Loss:  1.17621744\n",
      "Final training Loss:  1.17621744\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.64839077\n",
      "Training Loss:  3.64304662\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.41640198\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.42456186\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.49065948\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.41767681\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.41768122\n",
      "Training Loss:  1.41768122\n",
      "Training Loss:  1.41768122\n",
      "Training Loss:  1.41768122\n",
      "Final training Loss:  1.41768122\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.92345309\n",
      "Training Loss:  4.22083282\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.58814132\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.26440728\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.3481847\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.32181489\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.32181501\n",
      "Training Loss:  1.32181501\n",
      "Training Loss:  1.32181501\n",
      "Training Loss:  1.32181501\n",
      "Final training Loss:  1.32181501\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.60716772\n",
      "Training Loss:  3580.17749023\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.28284287\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.81879544\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.95429349\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.85702032\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.8570202\n",
      "Training Loss:  0.8570202\n",
      "Training Loss:  0.8570202\n",
      "Training Loss:  0.8570202\n",
      "Final training Loss:  0.8570202\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.1886096\n",
      "Training Loss:  4.50659037\n",
      "Training Loss:  3.89159894\n",
      "Training Loss:  4.18755484\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.92784119\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  3.98168898\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  3.83010125\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  3.83009481\n",
      "Training Loss:  3.83009481\n",
      "Training Loss:  3.83009481\n",
      "Final training Loss:  3.83009481\n",
      "\n",
      "Running model (trial=2, mod=53, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  122.82437134\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.37428832\n",
      "Training Loss:  1.33202982\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.31739187\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.45123863\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.45120907\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.45121622\n",
      "Training Loss:  1.45121622\n",
      "Training Loss:  1.45121622\n",
      "Training Loss:  1.45121622\n",
      "Final training Loss:  1.45121622\n",
      "\n",
      "Running model (trial=2, mod=54, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.9848094\n",
      "Training Loss:  1450.9453125\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.15598106\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.60659879\n",
      "Training Loss:  0.58584619\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58577281\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.5857479\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.5857479\n",
      "Training Loss:  0.5857479\n",
      "Training Loss:  0.5857479\n",
      "Final training Loss:  0.5857479\n",
      "\n",
      "Running model (trial=2, mod=55, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.21756458\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.10606253\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.69444019\n",
      "Training Loss:  0.59088939\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.63463241\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.69466871\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.69466877\n",
      "Training Loss:  0.69466877\n",
      "Training Loss:  0.69466877\n",
      "Training Loss:  0.69466877\n",
      "Final training Loss:  0.69466877\n",
      "\n",
      "Running model (trial=2, mod=56, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=57, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.47355604\n",
      "Training Loss:  0.53494877\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.4097687\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.62560868\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.55183613\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.55185956\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.55185956\n",
      "Training Loss:  0.55185974\n",
      "Training Loss:  0.55185974\n",
      "Training Loss:  0.55186307\n",
      "Final training Loss:  0.55185235\n",
      "\n",
      "Running model (trial=2, mod=58, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.60318279\n",
      "Training Loss:  1.54537225\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.52228594\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.24280298\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.26312828\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.26312506\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.263116\n",
      "Training Loss:  1.2631191\n",
      "Training Loss:  1.26312602\n",
      "Training Loss:  1.26312602\n",
      "Final training Loss:  1.26312602\n",
      "\n",
      "Running model (trial=2, mod=59, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.9361527\n",
      "Training Loss:  3.923105\n",
      "Training Loss:  1.34042954\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.63706636\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  2.02087188\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.84517515\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.84517503\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.84518313\n",
      "Training Loss:  1.84518313\n",
      "Training Loss:  1.84518313\n",
      "Final training Loss:  1.84518313\n",
      "\n",
      "Running model (trial=2, mod=60, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.98389661\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.34147453\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.37725019\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.32733762\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.32733679\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.32733679\n",
      "Training Loss:  1.32733679\n",
      "Training Loss:  1.32733679\n",
      "Training Loss:  1.32733679\n",
      "Training Loss:  1.32733679\n",
      "Final training Loss:  1.32733679\n",
      "\n",
      "Running model (trial=2, mod=61, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.39493001\n",
      "Training Loss:  2.90457773\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  666.40313721\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.58952445\n",
      "Training Loss:  0.57185996\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.62829757\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.62829667\n",
      "Training Loss:  0.62829667\n",
      "Training Loss:  0.62829667\n",
      "Training Loss:  0.62829667\n",
      "Final training Loss:  0.62829667\n",
      "\n",
      "Running model (trial=2, mod=62, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.24991035\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.41841006\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.40930176\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  599.40930176\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.40930176\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.40930176\n",
      "Training Loss:  599.40930176\n",
      "Training Loss:  599.40930176\n",
      "Training Loss:  599.40930176\n",
      "Training Loss:  599.40930176\n",
      "Final training Loss:  599.40930176\n",
      "\n",
      "Running model (trial=2, mod=63, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  189.15193176\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.49981689\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  599.49981689\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  599.49981689\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.49981689\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.49981689\n",
      "Training Loss:  599.49981689\n",
      "Training Loss:  599.49981689\n",
      "Training Loss:  599.49981689\n",
      "Training Loss:  599.49981689\n",
      "Final training Loss:  599.49981689\n",
      "\n",
      "Running model (trial=2, mod=64, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50402832\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50402832\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50402832\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50402832\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50402832\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Training Loss:  599.50402832\n",
      "Final training Loss:  599.50402832\n",
      "\n",
      "Running model (trial=2, mod=65, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.8874321\n",
      "Training Loss:  2.08104014\n",
      "Training Loss:  1.80395675\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.01725721\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.85010147\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.85331643\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.85329843\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.85329843\n",
      "Training Loss:  1.85329843\n",
      "Training Loss:  1.85329843\n",
      "Final training Loss:  1.85329843\n",
      "\n",
      "Running model (trial=2, mod=66, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.70754385\n",
      "Training Loss:  0.65747011\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.30806959\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.64202982\n",
      "Training Loss:  0.6253221\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.65226173\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.6522482\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.65225101\n",
      "Training Loss:  0.65225101\n",
      "Training Loss:  0.65225101\n",
      "Final training Loss:  0.65225101\n",
      "\n",
      "Running model (trial=2, mod=67, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.54770124\n",
      "Training Loss:  2.06413579\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.9681865\n",
      "Training Loss:  0.79321027\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.94790381\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.68227869\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.67205971\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.67204785\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.67204785\n",
      "Training Loss:  0.67204785\n",
      "Final training Loss:  0.67204785\n",
      "\n",
      "Running model (trial=2, mod=68, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  38.08003235\n",
      "Training Loss:  1.10893905\n",
      "Training Loss:  1.20053434\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.73970693\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.65372884\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58679295\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58679008\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58679456\n",
      "Training Loss:  0.58679456\n",
      "Training Loss:  0.58679456\n",
      "Final training Loss:  0.58679456\n",
      "\n",
      "Running model (trial=2, mod=69, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  93.30511475\n",
      "Training Loss:  1.56353247\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.12520385\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.46645284\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.90592682\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.76638114\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.76638603\n",
      "Training Loss:  1.76638603\n",
      "Training Loss:  1.76638603\n",
      "Training Loss:  1.76638603\n",
      "Final training Loss:  1.76638603\n",
      "\n",
      "Running model (trial=2, mod=70, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  325.47238159\n",
      "Training Loss:  1.33065724\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.86017507\n",
      "Training Loss:  0.89957905\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.54446197\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.76095665\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.76093376\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.76092064\n",
      "Training Loss:  0.76092064\n",
      "Training Loss:  0.76092064\n",
      "Final training Loss:  0.76092064\n",
      "\n",
      "Running model (trial=2, mod=71, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  368.58328247\n",
      "Training Loss:  2.12245798\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.41740811\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.31170917\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.29829264\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.29823112\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.2982322\n",
      "Training Loss:  1.2982322\n",
      "Training Loss:  1.2982322\n",
      "Training Loss:  1.2982322\n",
      "Final training Loss:  1.2982322\n",
      "\n",
      "Running model (trial=2, mod=72, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.51157606\n",
      "Training Loss:  1.33293939\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.56949854\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.51430655\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.35794592\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.35685265\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.35685289\n",
      "Training Loss:  1.35685289\n",
      "Training Loss:  1.35685289\n",
      "Training Loss:  1.35685289\n",
      "Final training Loss:  1.35685289\n",
      "\n",
      "Running model (trial=2, mod=73, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.43337655\n",
      "Training Loss:  1.58587253\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.74200296\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.04830801\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.72805059\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.61803085\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.61801022\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.61801344\n",
      "Training Loss:  0.61801344\n",
      "Training Loss:  0.61801344\n",
      "Final training Loss:  0.61801344\n",
      "\n",
      "Running model (trial=2, mod=74, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.22164106\n",
      "Training Loss:  1.30855262\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.34606802\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.35330844\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.30738986\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.30739737\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.30739582\n",
      "Training Loss:  1.30739689\n",
      "Training Loss:  1.30739689\n",
      "Training Loss:  1.30739689\n",
      "Final training Loss:  1.30739689\n",
      "\n",
      "Running model (trial=2, mod=75, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.18872929\n",
      "Training Loss:  1.53295386\n",
      "Training Loss:  0.75543642\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.9341718\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.60340673\n",
      "Training Loss:  0.60982621\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58275616\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58275479\n",
      "Training Loss:  0.58275479\n",
      "Training Loss:  0.58275479\n",
      "Final training Loss:  0.58275479\n",
      "\n",
      "Running model (trial=2, mod=76, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.76435089\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.29821169\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.14090753\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.51965541\n",
      "Training Loss:  0.44943365\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.51440626\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.5144065\n",
      "Training Loss:  0.5144065\n",
      "Training Loss:  0.5144065\n",
      "Training Loss:  0.5144065\n",
      "Final training Loss:  0.5144065\n",
      "\n",
      "Running model (trial=2, mod=77, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=78, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12.92875099\n",
      "Training Loss:  2.68089962\n",
      "Training Loss:  3.99746752\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.68255234\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.68198133\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  2.68198133\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  2.68198133\n",
      "Training Loss:  2.68198133\n",
      "Training Loss:  2.68198133\n",
      "Training Loss:  2.68198133\n",
      "Final training Loss:  2.68198133\n",
      "\n",
      "Running model (trial=3, mod=79, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.4938128\n",
      "Training Loss:  1.51314425\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.56965458\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.52973819\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.31497228\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.31496656\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.31496656\n",
      "Training Loss:  1.31496656\n",
      "Training Loss:  1.31496656\n",
      "Training Loss:  1.31496656\n",
      "Final training Loss:  1.31496656\n",
      "\n",
      "Running model (trial=3, mod=80, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.5011282\n",
      "Training Loss:  1.72391152\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.34073961\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.31875157\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.44816911\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.44816649\n",
      "Training Loss:  1.44816649\n",
      "Training Loss:  1.44816649\n",
      "Training Loss:  1.44816649\n",
      "Training Loss:  1.44816649\n",
      "Final training Loss:  1.44816649\n",
      "\n",
      "Running model (trial=3, mod=81, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.13953853\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.36030042\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.35820234\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.46686471\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.33970749\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.33970535\n",
      "Training Loss:  1.33970535\n",
      "Training Loss:  1.33970535\n",
      "Training Loss:  1.33970535\n",
      "Training Loss:  1.33970535\n",
      "Final training Loss:  1.33970535\n",
      "\n",
      "Running model (trial=3, mod=82, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.55246449\n",
      "Training Loss:  1.89942837\n",
      "Training Loss:  1.29240954\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.26241505\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.28188288\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.24367881\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.24368238\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.24368203\n",
      "Training Loss:  1.24368203\n",
      "Training Loss:  1.24368203\n",
      "Final training Loss:  1.24368203\n",
      "\n",
      "Running model (trial=3, mod=83, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  17.37578392\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.76106179\n",
      "Training Loss:  1.2301904\n",
      "Training Loss:  5.22588253\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.69446254\n",
      "Training Loss:  0.69269431\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.66398704\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.65450156\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.65450156\n",
      "Training Loss:  0.65450156\n",
      "Final training Loss:  0.65450156\n",
      "\n",
      "Running model (trial=3, mod=84, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.70454025\n",
      "Training Loss:  1.99065483\n",
      "Training Loss:  1.5115751\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.51203191\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.35799968\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.3255595\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.32556307\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.32556307\n",
      "Training Loss:  1.32556307\n",
      "Training Loss:  1.32556307\n",
      "Final training Loss:  1.32556307\n",
      "\n",
      "Running model (trial=3, mod=85, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.81079817\n",
      "Training Loss:  1.04811323\n",
      "Training Loss:  1.31362069\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.87956387\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.89568144\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.89581108\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.89571506\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.89571506\n",
      "Training Loss:  0.89571506\n",
      "Training Loss:  0.89571506\n",
      "Final training Loss:  0.89571506\n",
      "\n",
      "Running model (trial=3, mod=86, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.92455387\n",
      "Training Loss:  1.00598657\n",
      "Training Loss:  0.78947979\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.00639689\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.66488904\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.66485947\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.66487169\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.66487169\n",
      "Training Loss:  0.66487169\n",
      "Training Loss:  0.66487169\n",
      "Final training Loss:  0.66487169\n",
      "\n",
      "Running model (trial=3, mod=87, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.75763726\n",
      "Training Loss:  1.20220554\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.64947009\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.92637968\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.78738546\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.7873913\n",
      "Training Loss:  0.7873913\n",
      "Training Loss:  0.7873913\n",
      "Training Loss:  0.7873913\n",
      "Training Loss:  0.7873913\n",
      "Final training Loss:  0.7873913\n",
      "\n",
      "Running model (trial=3, mod=88, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.56147385\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  162.90701294\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.90574789\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.79554874\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.79547352\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.79545635\n",
      "Training Loss:  0.79545891\n",
      "Training Loss:  0.79545707\n",
      "Training Loss:  0.79545736\n",
      "Training Loss:  0.79545742\n",
      "Final training Loss:  0.79545742\n",
      "\n",
      "Running model (trial=3, mod=89, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  86.96653748\n",
      "Training Loss:  1.82606852\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.55187678\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.50799692\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.64071345\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.64067554\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.64067686\n",
      "Training Loss:  1.64067686\n",
      "Training Loss:  1.64067686\n",
      "Training Loss:  1.64067686\n",
      "Final training Loss:  1.64067686\n",
      "\n",
      "Running model (trial=3, mod=90, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50421143\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50415039\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50415039\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50415039\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50415039\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50415039\n",
      "Training Loss:  599.50415039\n",
      "Training Loss:  599.50415039\n",
      "Training Loss:  599.50415039\n",
      "Training Loss:  599.50415039\n",
      "Final training Loss:  599.50415039\n",
      "\n",
      "Running model (trial=3, mod=91, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.67499208\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.62885499\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.77777648\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.68081391\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.68059945\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.68059945\n",
      "Training Loss:  1.68059945\n",
      "Training Loss:  1.68059945\n",
      "Training Loss:  1.68059945\n",
      "Training Loss:  1.68059945\n",
      "Final training Loss:  1.68059945\n",
      "\n",
      "Running model (trial=3, mod=92, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.4201746\n",
      "Training Loss:  1.85936952\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=93, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  23.90423203\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.56280351\n",
      "Training Loss:  1.34836435\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.38901734\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.40810597\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.40806091\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.40806091\n",
      "Training Loss:  1.40806091\n",
      "Training Loss:  1.40806091\n",
      "Training Loss:  1.40806091\n",
      "Final training Loss:  1.40806091\n",
      "\n",
      "Running model (trial=3, mod=94, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.43012667\n",
      "Training Loss:  1.35002351\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.40556324\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.52601171\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.60368383\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.60360825\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.60365403\n",
      "Training Loss:  1.60365403\n",
      "Training Loss:  1.60365403\n",
      "Training Loss:  1.60365403\n",
      "Final training Loss:  1.60365403\n",
      "\n",
      "Running model (trial=3, mod=95, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.98751044\n",
      "Training Loss:  1.31317782\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.47213793\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.36036038\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.34223568\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.3422395\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.3422395\n",
      "Training Loss:  1.3422395\n",
      "Training Loss:  1.3422395\n",
      "Training Loss:  1.3422395\n",
      "Final training Loss:  1.3422395\n",
      "\n",
      "Running model (trial=3, mod=96, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  87.27362061\n",
      "Training Loss:  0.84473169\n",
      "Training Loss:  2.14192772\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.10088396\n",
      "Training Loss:  0.69459486\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.62858456\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.61739922\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.61739928\n",
      "Training Loss:  0.61739928\n",
      "Training Loss:  0.61739928\n",
      "Final training Loss:  0.61739928\n",
      "\n",
      "Running model (trial=3, mod=97, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  86.60720825\n",
      "Training Loss:  1.39120173\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.96158385\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.38029778\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.32857525\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.36238515\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.36238945\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.36238945\n",
      "Training Loss:  1.36238945\n",
      "Training Loss:  1.36238945\n",
      "Final training Loss:  1.36238945\n",
      "\n",
      "Running model (trial=3, mod=98, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.99779773\n",
      "Training Loss:  1.58250809\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.79894245\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.41147947\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.41562676\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.4153986\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.4153986\n",
      "Training Loss:  1.4153986\n",
      "Training Loss:  1.4153986\n",
      "Training Loss:  1.4153986\n",
      "Final training Loss:  1.4153986\n",
      "\n",
      "Running model (trial=3, mod=99, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  14.95442581\n",
      "Training Loss:  4.10819197\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.21146631\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.36367369\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.39980781\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.39955235\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.39955235\n",
      "Training Loss:  1.39955235\n",
      "Training Loss:  1.39955235\n",
      "Training Loss:  1.39955235\n",
      "Final training Loss:  1.39955235\n",
      "\n",
      "Running model (trial=3, mod=100, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.5026772\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.61610663\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.91111541\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.75272179\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.71774787\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.71774948\n",
      "Training Loss:  0.71774948\n",
      "Training Loss:  0.71774948\n",
      "Training Loss:  0.71774948\n",
      "Training Loss:  0.71774948\n",
      "Final training Loss:  0.71774948\n",
      "\n",
      "Running model (trial=3, mod=101, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.98753166\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.94415367\n",
      "Training Loss:  0.73710608\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.62744421\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.53984541\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.53984725\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.53984469\n",
      "Training Loss:  0.53984469\n",
      "Training Loss:  0.53984469\n",
      "Training Loss:  0.53984469\n",
      "Final training Loss:  0.53984469\n",
      "\n",
      "Running model (trial=3, mod=102, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.60688245\n",
      "Training Loss:  0.88079703\n",
      "Training Loss:  447.19009399\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.41490841\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.53652984\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.55782723\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.55781448\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.55781448\n",
      "Training Loss:  0.55781448\n",
      "Training Loss:  0.55781448\n",
      "Final training Loss:  0.55781448\n",
      "\n",
      "Running model (trial=3, mod=103, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.67269194\n",
      "Training Loss:  1.59465206\n",
      "Training Loss:  2.71429253\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  5.52628136\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.72013801\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.49551392\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.49550426\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.4955031\n",
      "Training Loss:  0.49550313\n",
      "Training Loss:  0.49550313\n",
      "Final training Loss:  0.49550313\n",
      "\n",
      "Running model (trial=4, mod=104, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.79873466\n",
      "Training Loss:  2.05814862\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.53723848\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.4320662\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.45488012\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.45466578\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.4546268\n",
      "Training Loss:  1.4546268\n",
      "Training Loss:  1.4546268\n",
      "Training Loss:  1.4546268\n",
      "Final training Loss:  1.4546268\n",
      "\n",
      "Running model (trial=4, mod=105, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.76789951\n",
      "Training Loss:  0.87089843\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.72216463\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.77885562\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.72619635\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.72619468\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.72619468\n",
      "Training Loss:  0.72619468\n",
      "Training Loss:  0.72619468\n",
      "Training Loss:  0.72619468\n",
      "Final training Loss:  0.72619468\n",
      "\n",
      "Running model (trial=4, mod=106, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.90583634\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.61800659\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.2664634\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.4180187\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.41801882\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.41801929\n",
      "Training Loss:  1.41801929\n",
      "Training Loss:  1.41801929\n",
      "Training Loss:  1.41801929\n",
      "Training Loss:  1.41801929\n",
      "Final training Loss:  1.41801929\n",
      "\n",
      "Running model (trial=4, mod=107, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.53174222\n",
      "Training Loss:  0.94628394\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.98050869\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.66430616\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.50856113\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.45860493\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.4585714\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.45857072\n",
      "Training Loss:  0.45857072\n",
      "Training Loss:  0.45857072\n",
      "Final training Loss:  0.45857072\n",
      "\n",
      "Running model (trial=4, mod=108, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.00040221\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.69934654\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.42168212\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.96083015\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.96082091\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.96082115\n",
      "Training Loss:  0.96082115\n",
      "Training Loss:  0.96082115\n",
      "Training Loss:  0.96082115\n",
      "Training Loss:  0.96082115\n",
      "Final training Loss:  0.96082115\n",
      "\n",
      "Running model (trial=4, mod=109, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.15192246\n",
      "Training Loss:  1.34331024\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.53609347\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.37644446\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.37643862\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.37642956\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.37642956\n",
      "Training Loss:  1.37642956\n",
      "Training Loss:  1.37642956\n",
      "Training Loss:  1.37642956\n",
      "Final training Loss:  1.37642956\n",
      "\n",
      "Running model (trial=4, mod=110, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.74055099\n",
      "Training Loss:  1.36994874\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.33620358\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.3132658\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.35478163\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.44816232\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.44814193\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.44814217\n",
      "Training Loss:  1.44814217\n",
      "Training Loss:  1.44814217\n",
      "Final training Loss:  1.44814217\n",
      "\n",
      "Running model (trial=4, mod=111, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  35.37377548\n",
      "Training Loss:  1.38714337\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.3487711\n",
      "Training Loss:  1.32858324\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.32599592\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.39528179\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.3951472\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.3951472\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.3951472\n",
      "Training Loss:  1.3951472\n",
      "Final training Loss:  1.3951472\n",
      "\n",
      "Running model (trial=4, mod=112, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.53857911\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.46604693\n",
      "Training Loss:  1.49496996\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.96043783\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.04700339\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.86836368\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.83182293\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.8318274\n",
      "Training Loss:  0.8318274\n",
      "Training Loss:  0.8318274\n",
      "Final training Loss:  0.8318274\n",
      "\n",
      "Running model (trial=4, mod=113, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.92353654\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.32876039\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.4470706\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.63587654\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.68491256\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.6849131\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.68491328\n",
      "Training Loss:  0.68491328\n",
      "Training Loss:  0.68491328\n",
      "Training Loss:  0.68491328\n",
      "Final training Loss:  0.68491328\n",
      "\n",
      "Running model (trial=4, mod=114, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  115.31487274\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  90.06185913\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  95.26107025\n",
      "Training Loss:  88.90985107\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  88.90982056\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  88.90997314\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  88.90997314\n",
      "Training Loss:  88.90997314\n",
      "Training Loss:  88.90997314\n",
      "Training Loss:  88.90997314\n",
      "Final training Loss:  88.90997314\n",
      "\n",
      "Running model (trial=4, mod=115, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.51818848\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.45837402\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.45843506\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.45843506\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.45843506\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.45843506\n",
      "Training Loss:  599.45843506\n",
      "Training Loss:  599.45843506\n",
      "Training Loss:  599.45843506\n",
      "Training Loss:  599.45843506\n",
      "Final training Loss:  599.45843506\n",
      "\n",
      "Running model (trial=4, mod=116, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50366211\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50354004\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50354004\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50354004\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50354004\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50354004\n",
      "Training Loss:  599.50354004\n",
      "Training Loss:  599.50354004\n",
      "Training Loss:  599.50354004\n",
      "Training Loss:  599.50354004\n",
      "Final training Loss:  599.50354004\n",
      "\n",
      "Running model (trial=4, mod=117, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.96739197\n",
      "Training Loss:  2.33256316\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.34865618\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  2.17176461\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.18080997\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  2.18082666\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  2.18082666\n",
      "Training Loss:  2.18082666\n",
      "Training Loss:  2.18082666\n",
      "Training Loss:  2.18082666\n",
      "Final training Loss:  2.18082666\n",
      "\n",
      "Running model (trial=4, mod=118, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.77528572\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.42387199\n",
      "Training Loss:  0.88224232\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.7699638\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.90718561\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.90714842\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.90714884\n",
      "Training Loss:  0.90714884\n",
      "Training Loss:  0.90714884\n",
      "Training Loss:  0.90714884\n",
      "Final training Loss:  0.90714884\n",
      "\n",
      "Running model (trial=4, mod=119, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.14996147\n",
      "Training Loss:  1.21645999\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.42668951\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.26869297\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.38462877\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.38463259\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.38463306\n",
      "Training Loss:  1.38463306\n",
      "Training Loss:  1.38463306\n",
      "Training Loss:  1.38463306\n",
      "Final training Loss:  1.38463306\n",
      "\n",
      "Running model (trial=4, mod=120, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  19.52252197\n",
      "Training Loss:  1.31862092\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.30465591\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.76344228\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.29798424\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.29799592\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.29799402\n",
      "Training Loss:  1.29799402\n",
      "Training Loss:  1.29799402\n",
      "Training Loss:  1.29799402\n",
      "Final training Loss:  1.29799402\n",
      "\n",
      "Running model (trial=4, mod=121, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  24.37960243\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.84937334\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.31897831\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.73350191\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.7333082\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.73330343\n",
      "Training Loss:  1.73330343\n",
      "Training Loss:  1.73330343\n",
      "Training Loss:  1.73330343\n",
      "Training Loss:  1.73330343\n",
      "Final training Loss:  1.73330343\n",
      "\n",
      "Running model (trial=4, mod=122, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  11.74946499\n",
      "Training Loss:  0.90589303\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.73105514\n",
      "Training Loss:  0.70479929\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.69605803\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.63822383\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.6382153\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.6382153\n",
      "Training Loss:  0.6382153\n",
      "Training Loss:  0.6382153\n",
      "Final training Loss:  0.6382153\n",
      "\n",
      "Running model (trial=4, mod=123, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  113.96411896\n",
      "Training Loss:  1.50814307\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.1834054\n",
      "Training Loss:  1.51439977\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.32821786\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.36221838\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.3620913\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.36209083\n",
      "Training Loss:  1.36209083\n",
      "Training Loss:  1.36209083\n",
      "Final training Loss:  1.36209083\n",
      "\n",
      "Running model (trial=4, mod=124, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.81699657\n",
      "Training Loss:  1.44941974\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.9906671\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.93611205\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.93024367\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.93034768\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.93034524\n",
      "Training Loss:  0.93034524\n",
      "Training Loss:  0.93034524\n",
      "Training Loss:  0.93034524\n",
      "Final training Loss:  0.93034524\n",
      "\n",
      "Running model (trial=4, mod=125, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.2022295\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.23031592\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.28490686\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.33046973\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.33041632\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.33041346\n",
      "Training Loss:  1.33041346\n",
      "Training Loss:  1.33041346\n",
      "Training Loss:  1.33041346\n",
      "Training Loss:  1.33041346\n",
      "Final training Loss:  1.33041346\n",
      "\n",
      "Running model (trial=4, mod=126, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  26.81327629\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.64174283\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.7624101\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.798711\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.53784025\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.53784072\n",
      "Training Loss:  0.53784072\n",
      "Training Loss:  0.53784072\n",
      "Training Loss:  0.53784072\n",
      "Training Loss:  0.53784096\n",
      "Final training Loss:  0.53784192\n",
      "\n",
      "Running model (trial=4, mod=127, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  179.33280945\n",
      "Training Loss:  1.20378983\n",
      "Training Loss:  1.04624414\n",
      "Training Loss:  1.35602725\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3657.62451172\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.25960669\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.26389036\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.26389486\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.26389495\n",
      "Training Loss:  0.26389495\n",
      "Final training Loss:  0.26389495\n",
      "\n",
      "Running model (trial=4, mod=128, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.43927312\n",
      "Training Loss:  1.24303746\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.24335825\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.57696295\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.24683297\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.24684894\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.24686301\n",
      "Training Loss:  1.24686301\n",
      "Training Loss:  1.24686301\n",
      "Training Loss:  1.24686301\n",
      "Final training Loss:  1.24686301\n",
      "\n",
      "Running model (trial=4, mod=129, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  14.62308693\n",
      "Training Loss:  0.79657942\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.75987524\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.98413682\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.6878168\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.68782127\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.68782145\n",
      "Training Loss:  0.68782216\n",
      "Training Loss:  0.68782216\n",
      "Training Loss:  0.68782216\n",
      "Final training Loss:  0.68782216\n",
      "\n",
      "Running model (trial=5, mod=130, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=131, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.78518677\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.12218809\n",
      "Training Loss:  0.31406486\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=132, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.20561028\n",
      "Training Loss:  3.3179853\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.39832175\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.42331791\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.39673758\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.44717371\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.44716191\n",
      "Training Loss:  1.44716191\n",
      "Training Loss:  1.44716191\n",
      "Training Loss:  1.44716191\n",
      "Final training Loss:  1.44716191\n",
      "\n",
      "Running model (trial=5, mod=133, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  36.61135864\n",
      "Training Loss:  0.91280246\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.23807824\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.65441573\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58448797\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.58449215\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58449215\n",
      "Training Loss:  0.58449215\n",
      "Training Loss:  0.58449215\n",
      "Training Loss:  0.58449215\n",
      "Final training Loss:  0.58449215\n",
      "\n",
      "Running model (trial=5, mod=134, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.97302961\n",
      "Training Loss:  0.95697904\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.83368921\n",
      "Training Loss:  0.70885074\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.69622254\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.69620496\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.69619298\n",
      "Training Loss:  0.69619298\n",
      "Training Loss:  0.69619298\n",
      "Training Loss:  0.69619298\n",
      "Final training Loss:  0.69619298\n",
      "\n",
      "Running model (trial=5, mod=135, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.50478768\n",
      "Training Loss:  1.32461095\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.35963976\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.34827852\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.45397758\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.45398068\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.45398068\n",
      "Training Loss:  1.45398068\n",
      "Training Loss:  1.45398068\n",
      "Training Loss:  1.45398068\n",
      "Final training Loss:  1.45398068\n",
      "\n",
      "Running model (trial=5, mod=136, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.96724749\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.49167609\n",
      "Training Loss:  1.42422318\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.92884064\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.38281059\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.38280582\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.38280594\n",
      "Training Loss:  1.38280594\n",
      "Training Loss:  1.38280594\n",
      "Training Loss:  1.38280594\n",
      "Final training Loss:  1.38280594\n",
      "\n",
      "Running model (trial=5, mod=137, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  24.13643074\n",
      "Training Loss:  1.48511541\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.43134928\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.4466691\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.44674563\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.44674563\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.44674563\n",
      "Training Loss:  1.44674563\n",
      "Training Loss:  1.44674563\n",
      "Training Loss:  1.44674563\n",
      "Final training Loss:  1.44674563\n",
      "\n",
      "Running model (trial=5, mod=138, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.90800548\n",
      "Training Loss:  1.29579937\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.46699536\n",
      "Training Loss:  0.71268094\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.88486499\n",
      "Training Loss:  0.65994614\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.68039435\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.68039864\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.68040097\n",
      "Training Loss:  0.68040097\n",
      "Final training Loss:  0.68040097\n",
      "\n",
      "Running model (trial=5, mod=139, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.24473977\n",
      "Training Loss:  2.30875087\n",
      "Training Loss:  0.89695406\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.63403296\n",
      "Training Loss:  0.45320255\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.36472094\n",
      "Training Loss:  0.31557596\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.3249779\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.32496393\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.32496309\n",
      "Final training Loss:  0.32496417\n",
      "\n",
      "Running model (trial=5, mod=140, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.46198297\n",
      "Training Loss:  1.0758965\n",
      "Training Loss:  0.7807526\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.10083723\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  903.30615234\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  103.95563507\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  103.95431519\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  103.95431519\n",
      "Training Loss:  103.95431519\n",
      "Training Loss:  103.95431519\n",
      "Final training Loss:  103.95431519\n",
      "\n",
      "Running model (trial=5, mod=141, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  62.79530334\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.49287605\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  4.3241353\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  4.34418106\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  4.32711887\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  4.32700682\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  4.32700682\n",
      "Training Loss:  4.32700682\n",
      "Training Loss:  4.32700682\n",
      "Training Loss:  4.32700682\n",
      "Final training Loss:  4.32700682\n",
      "\n",
      "Running model (trial=5, mod=142, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50354004\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50335693\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50341797\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50341797\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50341797\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50341797\n",
      "Training Loss:  599.50341797\n",
      "Training Loss:  599.50341797\n",
      "Training Loss:  599.50341797\n",
      "Training Loss:  599.50341797\n",
      "Final training Loss:  599.50341797\n",
      "\n",
      "Running model (trial=5, mod=143, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  112.12740326\n",
      "Training Loss:  4.73686361\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.78310347\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  4.75914097\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  4.75906944\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  4.75906944\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  4.75906944\n",
      "Training Loss:  4.75906944\n",
      "Training Loss:  4.75906944\n",
      "Training Loss:  4.75906944\n",
      "Final training Loss:  4.75906944\n",
      "\n",
      "Running model (trial=5, mod=144, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.83007765\n",
      "Training Loss:  1.47733963\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  17.32261467\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.22513127\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.91791791\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.9179076\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.9179076\n",
      "Training Loss:  0.9179076\n",
      "Training Loss:  0.9179076\n",
      "Training Loss:  0.9179076\n",
      "Final training Loss:  0.9179076\n",
      "\n",
      "Running model (trial=5, mod=145, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.59444046\n",
      "Training Loss:  0.95865822\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.87611938\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.75312668\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.74936128\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.74941164\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.74941164\n",
      "Training Loss:  0.74941164\n",
      "Training Loss:  0.74941164\n",
      "Training Loss:  0.74941164\n",
      "Final training Loss:  0.74941164\n",
      "\n",
      "Running model (trial=5, mod=146, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  56.44415283\n",
      "Training Loss:  4.07182741\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.35162103\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  3.01968384\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.78187239\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.78185236\n",
      "Training Loss:  1.78185236\n",
      "Training Loss:  1.78185236\n",
      "Training Loss:  1.78185236\n",
      "Training Loss:  1.78185236\n",
      "Final training Loss:  1.78185236\n",
      "\n",
      "Running model (trial=5, mod=147, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  79.39077759\n",
      "Training Loss:  0.95037085\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.94716811\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.75413609\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.79820842\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.79824126\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.7982474\n",
      "Training Loss:  0.7982474\n",
      "Training Loss:  0.7982474\n",
      "Training Loss:  0.7982474\n",
      "Final training Loss:  0.7982474\n",
      "\n",
      "Running model (trial=5, mod=148, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=149, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  127.00559235\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.70134223\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.49024808\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.50833416\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.51272607\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.51271915\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.51271832\n",
      "Training Loss:  1.51271832\n",
      "Training Loss:  1.51271832\n",
      "Training Loss:  1.51271832\n",
      "Final training Loss:  1.51271832\n",
      "\n",
      "Running model (trial=5, mod=150, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  60.49598694\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.36581683\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.33769083\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.34959304\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.36457992\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.36447573\n",
      "Training Loss:  1.36447573\n",
      "Training Loss:  1.36447573\n",
      "Training Loss:  1.36447573\n",
      "Training Loss:  1.36447573\n",
      "Final training Loss:  1.36447573\n",
      "\n",
      "Running model (trial=5, mod=151, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.59232759\n",
      "Training Loss:  1.08543551\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.65985262\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.01158488\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.91818452\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.64555466\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.64563179\n",
      "Training Loss:  0.64563179\n",
      "Training Loss:  0.64563179\n",
      "Training Loss:  0.64563179\n",
      "Final training Loss:  0.64563179\n",
      "\n",
      "Running model (trial=5, mod=152, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.58503652\n",
      "Training Loss:  1.5818212\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.75772297\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.23113739\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.06498599\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.0649122\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.06491458\n",
      "Training Loss:  1.06491458\n",
      "Training Loss:  1.06491458\n",
      "Training Loss:  1.06491458\n",
      "Final training Loss:  1.06491458\n",
      "\n",
      "Running model (trial=5, mod=153, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  11.68963146\n",
      "Training Loss:  1.67111051\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.59600675\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.84035259\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.88521028\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.88520688\n",
      "Training Loss:  0.88520688\n",
      "Training Loss:  0.88520688\n",
      "Training Loss:  0.88520688\n",
      "Training Loss:  0.88520688\n",
      "Final training Loss:  0.88520688\n",
      "\n",
      "Running model (trial=5, mod=154, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  152.36854553\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.91957831\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.34913659\n",
      "Training Loss:  0.82647246\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.54223824\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.5422532\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.5422532\n",
      "Training Loss:  0.5422532\n",
      "Training Loss:  0.5422532\n",
      "Training Loss:  0.5422532\n",
      "Final training Loss:  0.5422532\n",
      "\n",
      "Running model (trial=5, mod=155, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  171.7230072\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=156, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.42527485\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  6.12608862\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  5.64332294\n",
      "Training Loss:  5.78895235\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  5.93110943\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  5.93111086\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  5.93111086\n",
      "Training Loss:  5.93111086\n",
      "Training Loss:  5.93111086\n",
      "Training Loss:  5.93111086\n",
      "Final training Loss:  5.93111086\n",
      "\n",
      "Running model (trial=6, mod=157, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.14264679\n",
      "Training Loss:  1.94699156\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.29749\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.2684586\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.73540545\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.73468697\n",
      "Training Loss:  1.73468697\n",
      "Training Loss:  1.73468697\n",
      "Training Loss:  1.73468697\n",
      "Training Loss:  1.73468697\n",
      "Final training Loss:  1.73468697\n",
      "\n",
      "Running model (trial=6, mod=158, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.49157822\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.26755273\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.0806824\n",
      "Training Loss:  0.6008535\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.61708248\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.60630959\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.60630959\n",
      "Training Loss:  0.60630959\n",
      "Training Loss:  0.60630959\n",
      "Training Loss:  0.60630959\n",
      "Final training Loss:  0.60630959\n",
      "\n",
      "Running model (trial=6, mod=159, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  23.36970901\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.21994448\n",
      "Training Loss:  2.34570742\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.53937042\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.21048081\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.21048474\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.21048486\n",
      "Training Loss:  1.21048486\n",
      "Training Loss:  1.21048486\n",
      "Training Loss:  1.21048486\n",
      "Final training Loss:  1.21048486\n",
      "\n",
      "Running model (trial=6, mod=160, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.64066744\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.89181209\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.28625381\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59907311\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.6259461\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.62593257\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.62593353\n",
      "Training Loss:  0.62593353\n",
      "Training Loss:  0.62593353\n",
      "Training Loss:  0.62593353\n",
      "Final training Loss:  0.62593353\n",
      "\n",
      "Running model (trial=6, mod=161, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.06383705\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.67545354\n",
      "Training Loss:  3.15150309\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.32228112\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.48589742\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.48575783\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.48576236\n",
      "Training Loss:  1.48576236\n",
      "Training Loss:  1.48576236\n",
      "Training Loss:  1.48576236\n",
      "Final training Loss:  1.48576236\n",
      "\n",
      "Running model (trial=6, mod=162, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.97978115\n",
      "Training Loss:  1.32828593\n",
      "Training Loss:  1.47411835\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.32411158\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.29131699\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.24867868\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.24868166\n",
      "Training Loss:  1.24868166\n",
      "Training Loss:  1.24868166\n",
      "Training Loss:  1.24868166\n",
      "Final training Loss:  1.24868166\n",
      "\n",
      "Running model (trial=6, mod=163, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.10009384\n",
      "Training Loss:  2.9954288\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.52102017\n",
      "Training Loss:  1.43336344\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.39282787\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.39236474\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.39236474\n",
      "Training Loss:  1.39236474\n",
      "Training Loss:  1.39236474\n",
      "Training Loss:  1.39236474\n",
      "Final training Loss:  1.39236474\n",
      "\n",
      "Running model (trial=6, mod=164, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.69049525\n",
      "Training Loss:  1.02751684\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.63033152\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.79930001\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.70742941\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.70741814\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.70741838\n",
      "Training Loss:  0.70741838\n",
      "Training Loss:  0.70741838\n",
      "Training Loss:  0.70741838\n",
      "Final training Loss:  0.70741838\n",
      "\n",
      "Running model (trial=6, mod=165, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.72456419\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.4021275\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.81884247\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.73853648\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.73851979\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.73851979\n",
      "Training Loss:  0.73851979\n",
      "Training Loss:  0.73851979\n",
      "Training Loss:  0.73851979\n",
      "Training Loss:  0.73851979\n",
      "Final training Loss:  0.73851979\n",
      "\n",
      "Running model (trial=6, mod=166, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.11007023\n",
      "Training Loss:  1.36091137\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.26351106\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.4038136\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.2259382\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.22593558\n",
      "Training Loss:  1.22593558\n",
      "Training Loss:  1.22593558\n",
      "Training Loss:  1.22593558\n",
      "Training Loss:  1.22593558\n",
      "Final training Loss:  1.22593558\n",
      "\n",
      "Running model (trial=6, mod=167, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.49395752\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.49365234\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.49365234\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.49365234\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.49365234\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.49365234\n",
      "Training Loss:  599.49365234\n",
      "Training Loss:  599.49365234\n",
      "Training Loss:  599.49365234\n",
      "Training Loss:  599.49365234\n",
      "Final training Loss:  599.49365234\n",
      "\n",
      "Running model (trial=6, mod=168, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50372314\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50372314\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50378418\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50378418\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50378418\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Training Loss:  599.50378418\n",
      "Final training Loss:  599.50378418\n",
      "\n",
      "Running model (trial=6, mod=169, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  63.24034119\n",
      "Training Loss:  2.27965927\n",
      "Training Loss:  0.88419431\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.96747327\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.34067166\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.3310957\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.33109927\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.33109927\n",
      "Training Loss:  1.33109927\n",
      "Training Loss:  1.33109927\n",
      "Final training Loss:  1.33109927\n",
      "\n",
      "Running model (trial=6, mod=170, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.59640932\n",
      "Training Loss:  0.77676946\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.59071648\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.66609865\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58436942\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.58435458\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58435458\n",
      "Training Loss:  0.58435458\n",
      "Training Loss:  0.58435458\n",
      "Training Loss:  0.58435458\n",
      "Final training Loss:  0.58435458\n",
      "\n",
      "Running model (trial=6, mod=171, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.05865002\n",
      "Training Loss:  1.72115147\n",
      "Training Loss:  1.16987503\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.67825031\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.64536029\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.61301786\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.61302149\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.61304003\n",
      "Training Loss:  0.61304003\n",
      "Training Loss:  0.61304003\n",
      "Final training Loss:  0.61304003\n",
      "\n",
      "Running model (trial=6, mod=172, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.37555552\n",
      "Training Loss:  1.48525393\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.61443228\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.08380723\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.56928027\n",
      "Training Loss:  0.58218497\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58993477\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.58988911\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58988941\n",
      "Training Loss:  0.58988941\n",
      "Final training Loss:  0.58988941\n",
      "\n",
      "Running model (trial=6, mod=173, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  24.15382385\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.43064737\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.42623436\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.18509245\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.39345288\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.39345574\n",
      "Training Loss:  1.39345574\n",
      "Training Loss:  1.39345574\n",
      "Training Loss:  1.39345574\n",
      "Training Loss:  1.39345574\n",
      "Final training Loss:  1.39345574\n",
      "\n",
      "Running model (trial=6, mod=174, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.6171385\n",
      "Training Loss:  1.74720597\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.45596647\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.38107276\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.76793218\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.40697479\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.40696824\n",
      "Training Loss:  1.40698195\n",
      "Training Loss:  1.4069823\n",
      "Training Loss:  1.4069823\n",
      "Final training Loss:  1.40697038\n",
      "\n",
      "Running model (trial=6, mod=175, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  270.94454956\n",
      "Training Loss:  2.65820527\n",
      "Training Loss:  1.33357346\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.3336122\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.45291972\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.33891988\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.33273745\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.33274102\n",
      "Training Loss:  1.33274102\n",
      "Training Loss:  1.33274102\n",
      "Final training Loss:  1.33274102\n",
      "\n",
      "Running model (trial=6, mod=176, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.13124394\n",
      "Training Loss:  3.09610009\n",
      "Training Loss:  1.46519613\n",
      "Training Loss:  2.87317896\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.55630255\n",
      "Training Loss:  1.5080452\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.35176051\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.35959303\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.35959196\n",
      "Training Loss:  1.35959196\n",
      "Final training Loss:  1.35959196\n",
      "\n",
      "Running model (trial=6, mod=177, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.25563455\n",
      "Training Loss:  1.23154724\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.35598552\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.40396094\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.27274334\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.27274442\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.27274442\n",
      "Training Loss:  1.27274442\n",
      "Training Loss:  1.27274442\n",
      "Training Loss:  1.27274442\n",
      "Final training Loss:  1.27274442\n",
      "\n",
      "Running model (trial=6, mod=178, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.01610804\n",
      "Training Loss:  1.37402523\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.28076375\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.34579706\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.29755747\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.24924266\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.24924278\n",
      "Training Loss:  1.24924278\n",
      "Training Loss:  1.24924278\n",
      "Training Loss:  1.24924278\n",
      "Final training Loss:  1.24924278\n",
      "\n",
      "Running model (trial=6, mod=179, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.24908137\n",
      "Training Loss:  1.76168811\n",
      "Training Loss:  182.6078186\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.62443268\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.82164353\n",
      "Training Loss:  0.65225953\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.47906128\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.47906351\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.47906402\n",
      "Training Loss:  0.47906402\n",
      "Final training Loss:  0.47906402\n",
      "\n",
      "Running model (trial=6, mod=180, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.82340574\n",
      "Training Loss:  1.53158736\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.58843589\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.2919451\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.29194522\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.29193091\n",
      "Training Loss:  1.29193091\n",
      "Training Loss:  1.29193091\n",
      "Training Loss:  1.29193091\n",
      "Training Loss:  1.29193091\n",
      "Final training Loss:  1.29193091\n",
      "\n",
      "Running model (trial=6, mod=181, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.4570303\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.12478232\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.65386766\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.68976933\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.89152676\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.89153594\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.89153582\n",
      "Training Loss:  0.89153582\n",
      "Training Loss:  0.89153582\n",
      "Training Loss:  0.89153582\n",
      "Final training Loss:  0.89153582\n",
      "\n",
      "Running model (trial=7, mod=182, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  11.02572441\n",
      "Training Loss:  1.46102071\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.33721709\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.39388442\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.44024956\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.55451655\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.55451655\n",
      "Training Loss:  1.55451655\n",
      "Training Loss:  1.55451655\n",
      "Training Loss:  1.55451655\n",
      "Final training Loss:  1.55451655\n",
      "\n",
      "Running model (trial=7, mod=183, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  24.08895493\n",
      "Training Loss:  2.10937142\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.34238195\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.25931096\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.48143542\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.48110998\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.48110783\n",
      "Training Loss:  1.48110783\n",
      "Training Loss:  1.48110783\n",
      "Training Loss:  1.48110783\n",
      "Final training Loss:  1.48110783\n",
      "\n",
      "Running model (trial=7, mod=184, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.72983515\n",
      "Training Loss:  1.97020471\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.00535178\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.70721173\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.42382097\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.42382348\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.42382252\n",
      "Training Loss:  1.42382431\n",
      "Training Loss:  1.42382431\n",
      "Training Loss:  1.42382431\n",
      "Final training Loss:  1.42382431\n",
      "\n",
      "Running model (trial=7, mod=185, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.65927076\n",
      "Training Loss:  1.95827782\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.39571011\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.20092142\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.48803222\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.48802066\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.48801482\n",
      "Training Loss:  1.4880302\n",
      "Training Loss:  1.48804927\n",
      "Training Loss:  1.48811865\n",
      "Final training Loss:  1.48814118\n",
      "\n",
      "Running model (trial=7, mod=186, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.5288651\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.52155292\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.32498908\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.3011837\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.30119121\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.30119121\n",
      "Training Loss:  1.30119121\n",
      "Training Loss:  1.30119121\n",
      "Training Loss:  1.30119121\n",
      "Training Loss:  1.30119121\n",
      "Final training Loss:  1.30119121\n",
      "\n",
      "Running model (trial=7, mod=187, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.71727157\n",
      "Training Loss:  1.59640849\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.4407841\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.26415503\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.37001789\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.37001824\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.37001872\n",
      "Training Loss:  1.37001872\n",
      "Training Loss:  1.37001872\n",
      "Training Loss:  1.37001872\n",
      "Final training Loss:  1.37001872\n",
      "\n",
      "Running model (trial=7, mod=188, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10.51078224\n",
      "Training Loss:  1.48426902\n",
      "Training Loss:  0.85343486\n",
      "Training Loss:  0.80584264\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.13868845\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.77559501\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.77552271\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.77552253\n",
      "Training Loss:  0.77552253\n",
      "Training Loss:  0.77552253\n",
      "Final training Loss:  0.77552253\n",
      "\n",
      "Running model (trial=7, mod=189, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  41.30312729\n",
      "Training Loss:  1.80757034\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.5989598\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.44701588\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.38744402\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.38745904\n",
      "Training Loss:  1.38745904\n",
      "Training Loss:  1.38745904\n",
      "Training Loss:  1.38745904\n",
      "Training Loss:  1.38745904\n",
      "Final training Loss:  1.38745904\n",
      "\n",
      "Running model (trial=7, mod=190, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.16129136\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.62519467\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.47511268\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.24611723\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.38423979\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.38426054\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.38426018\n",
      "Training Loss:  1.38426018\n",
      "Training Loss:  1.38426018\n",
      "Training Loss:  1.38426018\n",
      "Final training Loss:  1.38426018\n",
      "\n",
      "Running model (trial=7, mod=191, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.96582675\n",
      "Training Loss:  1.19378531\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.00411808\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.28368449\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.26290226\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.26290846\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.26291299\n",
      "Training Loss:  1.26291299\n",
      "Training Loss:  1.26291299\n",
      "Training Loss:  1.26291299\n",
      "Final training Loss:  1.26291299\n",
      "\n",
      "Running model (trial=7, mod=192, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.4941349\n",
      "Training Loss:  0.99105376\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  98.86210632\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.46461105\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.46595383\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  38.46571732\n",
      "Training Loss:  38.46571732\n",
      "Training Loss:  38.46571732\n",
      "Training Loss:  38.46571732\n",
      "Training Loss:  38.46571732\n",
      "Final training Loss:  38.46571732\n",
      "\n",
      "Running model (trial=7, mod=193, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.47613525\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.47583008\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.47583008\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.47583008\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.47583008\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.47583008\n",
      "Training Loss:  599.47583008\n",
      "Training Loss:  599.47583008\n",
      "Training Loss:  599.47583008\n",
      "Training Loss:  599.47583008\n",
      "Final training Loss:  599.47583008\n",
      "\n",
      "Running model (trial=7, mod=194, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50366211\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.503479\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50354004\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50360107\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50360107\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50360107\n",
      "Training Loss:  599.50360107\n",
      "Training Loss:  599.50360107\n",
      "Training Loss:  599.50360107\n",
      "Training Loss:  599.50360107\n",
      "Final training Loss:  599.50360107\n",
      "\n",
      "Running model (trial=7, mod=195, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  62.99993134\n",
      "Training Loss:  2.0268178\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.37335491\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.774472\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.77945733\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.77955127\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.77954447\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.77954447\n",
      "Training Loss:  1.77954447\n",
      "Training Loss:  1.77954447\n",
      "Final training Loss:  1.77954447\n",
      "\n",
      "Running model (trial=7, mod=196, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  63.56452179\n",
      "Training Loss:  0.71299499\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.84660947\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.6553784\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.72212702\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.72198099\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.72198099\n",
      "Training Loss:  0.72198099\n",
      "Training Loss:  0.72198099\n",
      "Training Loss:  0.72198099\n",
      "Final training Loss:  0.72198099\n",
      "\n",
      "Running model (trial=7, mod=197, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  29.33466721\n",
      "Training Loss:  1.2807641\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.00541961\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.95597363\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.70558584\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.70557672\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.70557672\n",
      "Training Loss:  0.70557672\n",
      "Training Loss:  0.70557672\n",
      "Training Loss:  0.70557672\n",
      "Final training Loss:  0.70557672\n",
      "\n",
      "Running model (trial=7, mod=198, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  96.34958649\n",
      "Training Loss:  0.78297156\n",
      "Training Loss:  1.68108463\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.88598168\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.68619359\n",
      "Training Loss:  0.66463965\n",
      "Training Loss:  0.66048861\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.66047555\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.66049129\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.66049129\n",
      "Final training Loss:  0.66049129\n",
      "\n",
      "Running model (trial=7, mod=199, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  37.61922073\n",
      "Training Loss:  1.84510362\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.0760715\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.00381172\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.66465706\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.6626026\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.66261017\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.66261017\n",
      "Training Loss:  0.66261017\n",
      "Training Loss:  0.66261017\n",
      "Final training Loss:  0.66261017\n",
      "\n",
      "Running model (trial=7, mod=200, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  311.20233154\n",
      "Training Loss:  2.43981719\n",
      "Training Loss:  1.65929449\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.08398008\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.51915419\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.56121242\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.56121671\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.56119883\n",
      "Training Loss:  1.56119883\n",
      "Training Loss:  1.56119883\n",
      "Final training Loss:  1.56119883\n",
      "\n",
      "Running model (trial=7, mod=201, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  104.50605011\n",
      "Training Loss:  1.6554985\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.60979009\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.45972931\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.36221611\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.36221445\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.36221576\n",
      "Training Loss:  1.36221433\n",
      "Training Loss:  1.36220002\n",
      "Training Loss:  1.36220002\n",
      "Final training Loss:  1.36220825\n",
      "\n",
      "Running model (trial=7, mod=202, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.63451886\n",
      "Training Loss:  1.49744248\n",
      "Training Loss:  4.31167078\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.36241508\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.51537085\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.52165639\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.5216502\n",
      "Training Loss:  1.5216502\n",
      "Training Loss:  1.5216502\n",
      "Training Loss:  1.5216502\n",
      "Final training Loss:  1.5216502\n",
      "\n",
      "Running model (trial=7, mod=203, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.88812399\n",
      "Training Loss:  2.09494185\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.98563272\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.93094891\n",
      "Training Loss:  1.32064927\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.58009589\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58010197\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58009595\n",
      "Training Loss:  0.58009595\n",
      "Training Loss:  0.58009595\n",
      "Final training Loss:  0.58009595\n",
      "\n",
      "Running model (trial=7, mod=204, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.34610176\n",
      "Training Loss:  1.31520951\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.92986202\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  3.42734861\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.24542165\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.24542272\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.24542272\n",
      "Training Loss:  1.24542272\n",
      "Training Loss:  1.24542272\n",
      "Training Loss:  1.24542272\n",
      "Final training Loss:  1.24542272\n",
      "\n",
      "Running model (trial=7, mod=205, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.72309458\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.46347487\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  2.33003139\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.24436736\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.24437308\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.24437737\n",
      "Training Loss:  1.24437737\n",
      "Training Loss:  1.24437737\n",
      "Training Loss:  1.24437737\n",
      "Training Loss:  1.24437737\n",
      "Final training Loss:  1.24437737\n",
      "\n",
      "Running model (trial=7, mod=206, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.04290891\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.29625297\n",
      "Training Loss:  0.7233147\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.91791189\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.67797834\n",
      "Training Loss:  0.58408082\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.49945399\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.49945268\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.49945301\n",
      "Training Loss:  0.49945301\n",
      "Final training Loss:  0.49945301\n",
      "\n",
      "Running model (trial=7, mod=207, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.2225461\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.64090395\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59397888\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59494299\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.79056078\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.74166441\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.74166417\n",
      "Training Loss:  0.74166417\n",
      "Training Loss:  0.74166417\n",
      "Training Loss:  0.74166417\n",
      "Final training Loss:  0.74166417\n",
      "\n",
      "Running model (trial=8, mod=208, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  56.46029282\n",
      "Training Loss:  1.40271914\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.54550409\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.8682704\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.43593967\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.43593931\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.43593931\n",
      "Training Loss:  1.43593931\n",
      "Training Loss:  1.43593931\n",
      "Training Loss:  1.43593931\n",
      "Final training Loss:  1.43593931\n",
      "\n",
      "Running model (trial=8, mod=209, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.90162754\n",
      "Training Loss:  1.29460001\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.43039584\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.45962262\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.42219579\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.42156971\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.42156947\n",
      "Training Loss:  1.42156947\n",
      "Training Loss:  1.42156947\n",
      "Training Loss:  1.42156947\n",
      "Final training Loss:  1.42156947\n",
      "\n",
      "Running model (trial=8, mod=210, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.71784556\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.79044557\n",
      "Training Loss:  263.76293945\n",
      "Training Loss:  0.95767438\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.71550292\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.72708917\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.72707814\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.72707814\n",
      "Training Loss:  0.72707814\n",
      "Training Loss:  0.72707814\n",
      "Final training Loss:  0.72707814\n",
      "\n",
      "Running model (trial=8, mod=211, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.30672908\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.6532135\n",
      "Training Loss:  1.69147944\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.28951418\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.53241813\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.53239882\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.53239882\n",
      "Training Loss:  1.53239882\n",
      "Training Loss:  1.53239882\n",
      "Training Loss:  1.53239882\n",
      "Final training Loss:  1.53239882\n",
      "\n",
      "Running model (trial=8, mod=212, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.23238134\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.35735071\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.21560526\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.5181911\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.47638774\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.47639048\n",
      "Training Loss:  1.47639048\n",
      "Training Loss:  1.47639048\n",
      "Training Loss:  1.47639048\n",
      "Training Loss:  1.47639048\n",
      "Final training Loss:  1.47639048\n",
      "\n",
      "Running model (trial=8, mod=213, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.92197132\n",
      "Training Loss:  1.44827211\n",
      "Training Loss:  1.16100872\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.73360664\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.56262606\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.5643571\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.56436563\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.564367\n",
      "Training Loss:  0.564367\n",
      "Training Loss:  0.564367\n",
      "Final training Loss:  0.564367\n",
      "\n",
      "Running model (trial=8, mod=214, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.93267226\n",
      "Training Loss:  1.36760318\n",
      "Training Loss:  1.35862935\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.48390007\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.60982478\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.41635191\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.41636574\n",
      "Training Loss:  1.41636574\n",
      "Training Loss:  1.41636574\n",
      "Training Loss:  1.41636574\n",
      "Final training Loss:  1.41636574\n",
      "\n",
      "Running model (trial=8, mod=215, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  47.35300827\n",
      "Training Loss:  1.05935884\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.98942804\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.95621198\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.95612144\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.95612139\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.95612139\n",
      "Training Loss:  0.95612139\n",
      "Training Loss:  0.95612139\n",
      "Training Loss:  0.95612139\n",
      "Final training Loss:  0.95612139\n",
      "\n",
      "Running model (trial=8, mod=216, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.60940933\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.89985013\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.55318141\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.30387485\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.30386734\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.30386186\n",
      "Training Loss:  1.30386186\n",
      "Training Loss:  1.30386186\n",
      "Training Loss:  1.30386186\n",
      "Training Loss:  1.30386186\n",
      "Final training Loss:  1.30386186\n",
      "\n",
      "Running model (trial=8, mod=217, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.65831041\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.78762078\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.14779878\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  3.46342683\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  3.4630661\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  3.46306586\n",
      "Training Loss:  3.46306586\n",
      "Training Loss:  3.46306586\n",
      "Training Loss:  3.46306586\n",
      "Training Loss:  3.46306586\n",
      "Final training Loss:  3.46306586\n",
      "\n",
      "Running model (trial=8, mod=218, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.80251837\n",
      "Training Loss:  2.00745153\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.09776628\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.67939138\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.60082388\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.60081893\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.60081857\n",
      "Training Loss:  0.60081857\n",
      "Training Loss:  0.60081857\n",
      "Training Loss:  0.60081857\n",
      "Final training Loss:  0.60081857\n",
      "\n",
      "Running model (trial=8, mod=219, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50793457\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50805664\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50811768\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50811768\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50811768\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50811768\n",
      "Training Loss:  599.50811768\n",
      "Training Loss:  599.50811768\n",
      "Training Loss:  599.50811768\n",
      "Training Loss:  599.50811768\n",
      "Final training Loss:  599.50811768\n",
      "\n",
      "Running model (trial=8, mod=220, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50415039\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Final training Loss:  599.50390625\n",
      "\n",
      "Running model (trial=8, mod=221, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.96086121\n",
      "Training Loss:  1.23671269\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.40177381\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.36336541\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.56985772\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.57695866\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.57695866\n",
      "Training Loss:  1.57695866\n",
      "Training Loss:  1.57695866\n",
      "Training Loss:  1.57695866\n",
      "Final training Loss:  1.57695866\n",
      "\n",
      "Running model (trial=8, mod=222, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.02109718\n",
      "Training Loss:  1.3378073\n",
      "Training Loss:  0.84742701\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.75398558\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.55709398\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.5212664\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.78453034\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.78457707\n",
      "Training Loss:  0.78457707\n",
      "Training Loss:  0.78457707\n",
      "Final training Loss:  0.78457707\n",
      "\n",
      "Running model (trial=8, mod=223, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.16431713\n",
      "Training Loss:  1.55138373\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.22880459\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.65418971\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.65060967\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.65059489\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.65059251\n",
      "Training Loss:  0.65059251\n",
      "Training Loss:  0.65059251\n",
      "Training Loss:  0.65059251\n",
      "Final training Loss:  0.65059251\n",
      "\n",
      "Running model (trial=8, mod=224, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  84.04579926\n",
      "Training Loss:  1.35678673\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.44167686\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.64122319\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.64109325\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.64109576\n",
      "Training Loss:  1.64109886\n",
      "Training Loss:  1.64109886\n",
      "Training Loss:  1.64109886\n",
      "Training Loss:  1.64109886\n",
      "Final training Loss:  1.64109886\n",
      "\n",
      "Running model (trial=8, mod=225, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  278.33688354\n",
      "Training Loss:  1.73025858\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.31051242\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.69021869\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.37796175\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.37792909\n",
      "Training Loss:  1.37792909\n",
      "Training Loss:  1.37792909\n",
      "Training Loss:  1.37792909\n",
      "Training Loss:  1.37792909\n",
      "Final training Loss:  1.37792909\n",
      "\n",
      "Running model (trial=8, mod=226, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  150.21958923\n",
      "Training Loss:  1.32585478\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.49223721\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.54825675\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.41175663\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.4117806\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.4117806\n",
      "Training Loss:  1.4117806\n",
      "Training Loss:  1.4117806\n",
      "Training Loss:  1.4117806\n",
      "Final training Loss:  1.4117806\n",
      "\n",
      "Running model (trial=8, mod=227, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  95.17163849\n",
      "Training Loss:  1.27475834\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.39181805\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.45257032\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.83579397\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.32689452\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.32689381\n",
      "Training Loss:  1.32689381\n",
      "Training Loss:  1.32689381\n",
      "Training Loss:  1.32689381\n",
      "Final training Loss:  1.32689381\n",
      "\n",
      "Running model (trial=8, mod=228, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.08186793\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.60982919\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.97174335\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.38934577\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.38933635\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.38933277\n",
      "Training Loss:  1.38933277\n",
      "Training Loss:  1.38933277\n",
      "Training Loss:  1.38933277\n",
      "Training Loss:  1.38933277\n",
      "Final training Loss:  1.38933277\n",
      "\n",
      "Running model (trial=8, mod=229, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  35.12071991\n",
      "Training Loss:  1.53316391\n",
      "Training Loss:  0.8665781\n",
      "Training Loss:  2.05305696\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.461303\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.76179183\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.7065419\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.70645255\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.70645255\n",
      "Training Loss:  0.70645255\n",
      "Final training Loss:  0.70645255\n",
      "\n",
      "Running model (trial=8, mod=230, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  24.03459358\n",
      "Training Loss:  1.34336483\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.32292831\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.30329895\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.25668132\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.25668454\n",
      "Training Loss:  1.25668454\n",
      "Training Loss:  1.25668454\n",
      "Training Loss:  1.25668454\n",
      "Training Loss:  1.25668454\n",
      "Final training Loss:  1.25668454\n",
      "\n",
      "Running model (trial=8, mod=231, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  15.42290592\n",
      "Training Loss:  1.16116714\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.03549194\n",
      "Training Loss:  0.99356645\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.78246772\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.75268722\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.75268805\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.75268954\n",
      "Training Loss:  0.75268954\n",
      "Training Loss:  0.75268954\n",
      "Final training Loss:  0.75268954\n",
      "\n",
      "Running model (trial=8, mod=232, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.51732779\n",
      "Training Loss:  4.14606571\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.82215428\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.64203501\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.86451173\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.86451119\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.86451119\n",
      "Training Loss:  0.86451119\n",
      "Training Loss:  0.86451596\n",
      "Training Loss:  0.86451596\n",
      "Final training Loss:  0.86451596\n",
      "\n",
      "Running model (trial=8, mod=233, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  27.00581169\n",
      "Training Loss:  0.70237255\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.43607187\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.92058641\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.49278235\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.49279332\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.49278742\n",
      "Training Loss:  0.49278742\n",
      "Training Loss:  0.49278742\n",
      "Training Loss:  0.49278742\n",
      "Final training Loss:  0.49278742\n",
      "\n",
      "Running model (trial=9, mod=234, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.40956259\n",
      "Training Loss:  1.3962388\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.35309482\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.57890999\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.62997007\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.62993598\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.62993598\n",
      "Training Loss:  1.62993598\n",
      "Training Loss:  1.62993598\n",
      "Training Loss:  1.62993598\n",
      "Final training Loss:  1.62993598\n",
      "\n",
      "Running model (trial=9, mod=235, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.80396223\n",
      "Training Loss:  1.3150841\n",
      "Training Loss:  1.17422462\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.21802127\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.22937846\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.31335044\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.31332791\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.31332397\n",
      "Training Loss:  1.31332469\n",
      "Training Loss:  1.31332469\n",
      "Final training Loss:  1.31332469\n",
      "\n",
      "Running model (trial=9, mod=236, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.75413108\n",
      "Training Loss:  1.22581279\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.64634305\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.72948843\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.75051516\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.75049245\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.75049186\n",
      "Training Loss:  0.75049186\n",
      "Training Loss:  0.75049186\n",
      "Training Loss:  0.75049186\n",
      "Final training Loss:  0.75049186\n",
      "\n",
      "Running model (trial=9, mod=237, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  146.50491333\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=238, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.69890952\n",
      "Training Loss:  1.40189826\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.61552763\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.31921709\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.69112003\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.69112241\n",
      "Training Loss:  1.69112241\n",
      "Training Loss:  1.69112241\n",
      "Training Loss:  1.69112241\n",
      "Training Loss:  1.69112241\n",
      "Final training Loss:  1.69112241\n",
      "\n",
      "Running model (trial=9, mod=239, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.87203562\n",
      "Training Loss:  1.42894518\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.50710726\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.6635952\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  2.23597169\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.33415866\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.33416188\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.33416188\n",
      "Training Loss:  1.33416188\n",
      "Training Loss:  1.33416188\n",
      "Final training Loss:  1.33416188\n",
      "\n",
      "Running model (trial=9, mod=240, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  108.03118896\n",
      "Training Loss:  1.1765691\n",
      "Training Loss:  1.55248225\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  4.64600515\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.6855312\n",
      "Training Loss:  0.77514815\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.77515101\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.77515173\n",
      "Training Loss:  0.77515322\n",
      "Training Loss:  0.77515304\n",
      "Final training Loss:  0.77515304\n",
      "\n",
      "Running model (trial=9, mod=241, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  17.18693352\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.26524007\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.03599072\n",
      "Training Loss:  0.8875463\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.90164214\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.90164214\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.90164214\n",
      "Training Loss:  0.90164214\n",
      "Training Loss:  0.90164214\n",
      "Training Loss:  0.90164214\n",
      "Final training Loss:  0.90164214\n",
      "\n",
      "Running model (trial=9, mod=242, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.6560365\n",
      "Training Loss:  1.73343039\n",
      "Training Loss:  1.41795504\n",
      "Training Loss:  1.18240833\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.72194922\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.88563031\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.84145164\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.8414557\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.84145749\n",
      "Training Loss:  0.84146011\n",
      "Final training Loss:  0.84146035\n",
      "\n",
      "Running model (trial=9, mod=243, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.35895884\n",
      "Training Loss:  0.78505886\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.97023195\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.90147173\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.63700229\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.70220017\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.63765323\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.63765329\n",
      "Training Loss:  0.63765329\n",
      "Training Loss:  0.63765329\n",
      "Final training Loss:  0.63765329\n",
      "\n",
      "Running model (trial=9, mod=244, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.62634718\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.83059621\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=245, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.54333496\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50958252\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50958252\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50958252\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50958252\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50958252\n",
      "Training Loss:  599.50958252\n",
      "Training Loss:  599.50958252\n",
      "Training Loss:  599.50958252\n",
      "Training Loss:  599.50958252\n",
      "Final training Loss:  599.50958252\n",
      "\n",
      "Running model (trial=9, mod=246, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.50390625\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  599.50390625\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Training Loss:  599.50390625\n",
      "Final training Loss:  599.50390625\n",
      "\n",
      "Running model (trial=9, mod=247, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.71993136\n",
      "Training Loss:  1.3448627\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.45462143\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.45356083\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.45353794\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.45348382\n",
      "Training Loss:  1.45348382\n",
      "Training Loss:  1.45348382\n",
      "Training Loss:  1.45348382\n",
      "Training Loss:  1.45348382\n",
      "Final training Loss:  1.45348382\n",
      "\n",
      "Running model (trial=9, mod=248, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.5589323\n",
      "Training Loss:  1.59239244\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.16522563\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.92256147\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.11903608\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.84771043\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.84769988\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.84755641\n",
      "Training Loss:  0.84755641\n",
      "Training Loss:  0.84755641\n",
      "Final training Loss:  0.84755641\n",
      "\n",
      "Running model (trial=9, mod=249, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.02836871\n",
      "Training Loss:  2.36037993\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.25675631\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.29166067\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.49091089\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.49091685\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.49091733\n",
      "Training Loss:  1.49091733\n",
      "Training Loss:  1.49091733\n",
      "Training Loss:  1.49091733\n",
      "Final training Loss:  1.49091733\n",
      "\n",
      "Running model (trial=9, mod=250, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  23.29676247\n",
      "Training Loss:  3.76381683\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.41767597\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.37578642\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.37988365\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.37988305\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.37988436\n",
      "Training Loss:  1.37988794\n",
      "Training Loss:  1.37988794\n",
      "Training Loss:  1.37988794\n",
      "Final training Loss:  1.37988794\n",
      "\n",
      "Running model (trial=9, mod=251, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  79.29086304\n",
      "Training Loss:  1.39612031\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.08987689\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.93971068\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.68059063\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.71672386\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.7167089\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.7167089\n",
      "Training Loss:  0.7167089\n",
      "Training Loss:  0.7167089\n",
      "Final training Loss:  0.7167089\n",
      "\n",
      "Running model (trial=9, mod=252, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.4848659\n",
      "Training Loss:  1.65935302\n",
      "Training Loss:  1.73055029\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.3729738\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.332389\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.38887429\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.38880575\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.38881469\n",
      "Training Loss:  1.38881469\n",
      "Training Loss:  1.38881469\n",
      "Final training Loss:  1.38881469\n",
      "\n",
      "Running model (trial=9, mod=253, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  26.54129028\n",
      "Training Loss:  1.61990607\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.41862273\n",
      "Training Loss:  1.42168534\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.34942889\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.3494072\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.34941566\n",
      "Training Loss:  1.34941566\n",
      "Training Loss:  1.34941566\n",
      "Training Loss:  1.34941566\n",
      "Final training Loss:  1.34941566\n",
      "\n",
      "Running model (trial=9, mod=254, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.54732037\n",
      "Training Loss:  3.20850444\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.40618718\n",
      "Training Loss:  1.60268199\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.35064399\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.36930358\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  1.36930358\n",
      "Training Loss:  1.36930358\n",
      "Training Loss:  1.36930358\n",
      "Training Loss:  1.36930358\n",
      "Final training Loss:  1.36930358\n",
      "\n",
      "Running model (trial=9, mod=255, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  55.41952515\n",
      "Training Loss:  2.32749987\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.71657091\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.29949713\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59834939\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59830683\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59830683\n",
      "Training Loss:  0.59830683\n",
      "Training Loss:  0.59830683\n",
      "Training Loss:  0.59830683\n",
      "Final training Loss:  0.59830683\n",
      "\n",
      "Running model (trial=9, mod=256, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  72.4753418\n",
      "Training Loss:  0.80624622\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.49124217\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.13447165\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.55852807\n",
      "Training Loss:  0.51042247\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.51678538\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.5167833\n",
      "Training Loss:  0.51678419\n",
      "Training Loss:  0.51678419\n",
      "Final training Loss:  0.51678419\n",
      "\n",
      "Running model (trial=9, mod=257, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.16764688\n",
      "Training Loss:  2.2539928\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.72161728\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.8578856\n",
      "Training Loss:  3413.20703125\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.50708854\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.38661385\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.38661346\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.38661346\n",
      "Training Loss:  0.38661346\n",
      "Final training Loss:  0.38661346\n",
      "\n",
      "Running model (trial=9, mod=258, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.16476512\n",
      "Training Loss:  935.2432251\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2196.27075195\n",
      "Training Loss:  1.58059752\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  4948.49755859\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.08235598\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.92959869\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.9296\n",
      "Training Loss:  0.9296\n",
      "Training Loss:  0.9296\n",
      "Final training Loss:  0.9296\n",
      "\n",
      "Running model (trial=9, mod=259, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a906670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x13ff19d30>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12d3488b0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.16046262\n",
      "Training Loss:  7365.22460938\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.50073266\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  1.70421481\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.47422099\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.49424979\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.49424931\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.49424937\n",
      "Training Loss:  0.49424937\n",
      "Training Loss:  0.49424937\n",
      "Final training Loss:  0.49424937\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "disc_points = x_train.detach()\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": disc_points,\n",
    "    \"x_train\": disc_points,\n",
    "    \"y_train\": c1.ksi(disc_points),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"analytical solution (smooth)\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: divide by zero encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: overflow encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: invalid value encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: penalty_free_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "\n",
    "# make data frame\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type})\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 404.478125 262.19625\" width=\"404.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T01:57:07.667945</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 404.478125 262.19625 \nL 404.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 62.478125 224.64 \nL 397.278125 224.64 \nL 397.278125 7.2 \nL 62.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mc2c11db09d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"195.481387\" xlink:href=\"#mc2c11db09d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(186.681387 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"363.993739\" xlink:href=\"#mc2c11db09d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(355.193739 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m78e8aa2f15\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"77.696307\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"107.369859\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"128.42358\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"144.754114\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"158.097132\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"169.378493\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"179.150852\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"187.770684\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"246.208659\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"275.882212\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"296.935932\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"313.266466\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"326.609484\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"337.890845\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"347.663205\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"356.283037\" xlink:href=\"#m78e8aa2f15\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Neurons -->\n     <g transform=\"translate(209.116406 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"199.707031\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"238.570312\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"299.751953\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"363.130859\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_19\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m533ee28325\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"62.478125\" xlink:href=\"#m533ee28325\" y=\"167.720667\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(37.878125 171.519886)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m6e8269198b\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"62.478125\" xlink:href=\"#m6e8269198b\" y=\"220.722605\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"62.478125\" xlink:href=\"#m6e8269198b\" y=\"200.879825\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"62.478125\" xlink:href=\"#m6e8269198b\" y=\"183.377252\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"62.478125\" xlink:href=\"#m6e8269198b\" y=\"64.718914\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{2\\times10^{0}}$ -->\n      <g transform=\"translate(20.878125 68.518133)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 4488 3438 \nL 3059 2003 \nL 4488 575 \nL 4116 197 \nL 2681 1631 \nL 1247 197 \nL 878 575 \nL 2303 2003 \nL 878 3438 \nL 1247 3816 \nL 2681 2381 \nL 4116 3816 \nL 4488 3438 \nz\n\" id=\"DejaVuSans-d7\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-32\"/>\n       <use transform=\"translate(83.105469 0.765625)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $E$ -->\n     <g transform=\"translate(14.798438 119.12)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 77.696307 81.504531 \nL 77.696307 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 128.42358 150.431528 \nL 128.42358 120.04145 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 179.150852 182.183204 \nL 179.150852 153.046586 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 229.878125 197.415546 \nL 229.878125 141.762767 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 280.605398 162.24572 \nL 280.605398 126.547295 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 331.33267 189.161952 \nL 331.33267 137.409955 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 382.059943 148.846877 \nL 382.059943 124.666498 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 77.696307 79.884728 \nL 77.696307 32.658709 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 128.42358 214.756364 \nL 128.42358 184.513716 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 179.150852 191.622724 \nL 179.150852 161.791768 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 229.878125 170.704647 \nL 229.878125 122.924211 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 280.605398 153.552551 \nL 280.605398 118.800293 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 331.33267 173.527433 \nL 331.33267 128.29374 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 382.059943 124.053023 \nL 382.059943 119.153006 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_24\">\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 77.696307 47.283168 \nL 128.42358 134.705775 \nL 179.150852 166.830262 \nL 229.878125 166.419422 \nL 280.605398 142.882178 \nL 331.33267 161.141809 \nL 382.059943 135.251338 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_25\">\n    <path clip-path=\"url(#pc896cba88f)\" d=\"M 77.696307 55.903649 \nL 128.42358 198.787514 \nL 179.150852 176.13015 \nL 229.878125 145.594294 \nL 280.605398 135.733276 \nL 331.33267 149.531431 \nL 382.059943 121.535312 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_26\"/>\n   <g id=\"line2d_27\"/>\n   <g id=\"line2d_28\"/>\n   <g id=\"line2d_29\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 62.478125 224.64 \nL 62.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 397.278125 224.64 \nL 397.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 62.478125 224.64 \nL 397.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 62.478125 7.2 \nL 397.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 322.8625 59.234375 \nL 390.278125 59.234375 \nQ 392.278125 59.234375 392.278125 57.234375 \nL 392.278125 14.2 \nQ 392.278125 12.2 390.278125 12.2 \nL 322.8625 12.2 \nQ 320.8625 12.2 320.8625 14.2 \nL 320.8625 57.234375 \nQ 320.8625 59.234375 322.8625 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_7\">\n     <!-- model -->\n     <g transform=\"translate(341.000781 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_30\">\n     <path d=\"M 324.8625 34.976562 \nL 344.8625 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_31\"/>\n    <g id=\"text_8\">\n     <!-- FFNN -->\n     <g transform=\"translate(352.8625 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_32\">\n     <path d=\"M 324.8625 49.654687 \nL 344.8625 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_33\"/>\n    <g id=\"text_9\">\n     <!-- ResNet -->\n     <g transform=\"translate(352.8625 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pc896cba88f\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"62.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\")\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "7 160\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 2]), len(d_results_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T01:57:09.075375</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md8990b4111\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.696307\" xlink:href=\"#md8990b4111\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(51.896307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.91113\" xlink:href=\"#md8990b4111\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(254.11113 239.238437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m19ce4d581b\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.443464\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"121.569034\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"157.177297\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"182.441761\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"202.038402\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"218.050024\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"231.587657\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"243.314489\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"253.658287\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"323.783857\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"359.39212\" xlink:href=\"#m19ce4d581b\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Layers -->\n     <g transform=\"translate(196.332031 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"55.712891\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"116.992188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"176.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"237.695312\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"278.808594\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5c2018024a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m5c2018024a\" y=\"192.58809\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(20.878125 196.387309)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m5c2018024a\" y=\"104.740991\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 108.54021)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m5c2018024a\" y=\"16.893892\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 20.693111)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"mbfcd00c533\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"219.032702\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"212.076859\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"206.195778\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"201.101354\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"196.607753\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"166.143478\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"150.674372\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"139.698867\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"131.185603\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"124.22976\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"118.348679\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"113.254255\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"108.760654\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"78.296379\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"62.827273\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"51.851768\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"43.338504\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"36.382661\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"30.50158\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"25.407156\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mbfcd00c533\" y=\"20.913555\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- $E$ -->\n     <g transform=\"translate(14.798438 119.12)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 60.696307 186.613596 \nL 60.696307 179.741678 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 121.569034 195.887313 \nL 121.569034 188.992861 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 182.441761 205.538726 \nL 182.441761 180.46026 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 243.314489 80.856031 \nL 243.314489 38.891286 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 304.187216 42.563344 \nL 304.187216 21.050709 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 365.059943 17.083707 \nL 365.059943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 60.696307 182.74793 \nL 60.696307 178.958557 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 121.569034 198.830031 \nL 121.569034 190.791676 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 182.441761 197.068776 \nL 182.441761 187.302225 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 243.314489 212.806994 \nL 243.314489 198.307232 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 304.187216 204.315188 \nL 304.187216 192.391288 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 365.059943 214.756364 \nL 365.059943 205.862918 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\">\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 60.696307 182.560507 \nL 121.569034 192.359488 \nL 182.441761 191.214577 \nL 243.314489 54.86161 \nL 304.187216 30.305457 \nL 365.059943 17.083671 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_39\">\n    <path clip-path=\"url(#p9404ce6ff7)\" d=\"M 60.696307 180.707747 \nL 121.569034 194.747136 \nL 182.441761 192.004679 \nL 243.314489 204.985677 \nL 304.187216 198.201719 \nL 365.059943 209.539841 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_40\"/>\n   <g id=\"line2d_41\"/>\n   <g id=\"line2d_42\"/>\n   <g id=\"line2d_43\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 52.478125 59.234375 \nL 119.89375 59.234375 \nQ 121.89375 59.234375 121.89375 57.234375 \nL 121.89375 14.2 \nQ 121.89375 12.2 119.89375 12.2 \nL 52.478125 12.2 \nQ 50.478125 12.2 50.478125 14.2 \nL 50.478125 57.234375 \nQ 50.478125 59.234375 52.478125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_8\">\n     <!-- model -->\n     <g transform=\"translate(70.616406 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_44\">\n     <path d=\"M 54.478125 34.976562 \nL 74.478125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_45\"/>\n    <g id=\"text_9\">\n     <!-- FFNN -->\n     <g transform=\"translate(82.478125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_46\">\n     <path d=\"M 54.478125 49.654687 \nL 74.478125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_47\"/>\n    <g id=\"text_10\">\n     <!-- ResNet -->\n     <g transform=\"translate(82.478125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9404ce6ff7\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Layers\")\n",
    "fig_layers.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\")\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "23 140\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 2]), len(d_results_layer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 399.036323 262.19625\" width=\"399.036323pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T01:57:10.028975</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 399.036323 262.19625 \nL 399.036323 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 157.13729 \nC 61.491916 157.13729 62.255046 156.821191 62.817627 156.25861 \nC 63.380208 155.696029 63.696307 154.932899 63.696307 154.13729 \nC 63.696307 153.34168 63.380208 152.57855 62.817627 152.015969 \nC 62.255046 151.453389 61.491916 151.13729 60.696307 151.13729 \nC 59.900698 151.13729 59.137567 151.453389 58.574986 152.015969 \nC 58.012406 152.57855 57.696307 153.34168 57.696307 154.13729 \nC 57.696307 154.932899 58.012406 155.696029 58.574986 156.25861 \nC 59.137567 156.821191 59.900698 157.13729 60.696307 157.13729 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 179.740341 \nC 61.852451 179.740341 62.615581 179.424242 63.178162 178.861661 \nC 63.740743 178.29908 64.056842 177.53595 64.056842 176.740341 \nC 64.056842 175.944731 63.740743 175.181601 63.178162 174.61902 \nC 62.615581 174.056439 61.852451 173.740341 61.056842 173.740341 \nC 60.261233 173.740341 59.498102 174.056439 58.935521 174.61902 \nC 58.372941 175.181601 58.056842 175.944731 58.056842 176.740341 \nC 58.056842 177.53595 58.372941 178.29908 58.935521 178.861661 \nC 59.498102 179.424242 60.261233 179.740341 61.056842 179.740341 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 165.790034 \nC 63.006163 165.790034 63.769293 165.473935 64.331874 164.911355 \nC 64.894455 164.348774 65.210554 163.585643 65.210554 162.790034 \nC 65.210554 161.994425 64.894455 161.231295 64.331874 160.668714 \nC 63.769293 160.106133 63.006163 159.790034 62.210554 159.790034 \nC 61.414944 159.790034 60.651814 160.106133 60.089233 160.668714 \nC 59.526653 161.231295 59.210554 161.994425 59.210554 162.790034 \nC 59.210554 163.585643 59.526653 164.348774 60.089233 164.911355 \nC 60.651814 165.473935 61.414944 165.790034 62.210554 165.790034 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 197.044512 \nC 67.044155 197.044512 67.807285 196.728413 68.369866 196.165833 \nC 68.932447 195.603252 69.248546 194.840122 69.248546 194.044512 \nC 69.248546 193.248903 68.932447 192.485773 68.369866 191.923192 \nC 67.807285 191.360611 67.044155 191.044512 66.248546 191.044512 \nC 65.452936 191.044512 64.689806 191.360611 64.127225 191.923192 \nC 63.564645 192.485773 63.248546 193.248903 63.248546 194.044512 \nC 63.248546 194.840122 63.564645 195.603252 64.127225 196.165833 \nC 64.689806 196.728413 65.452936 197.044512 66.248546 197.044512 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 164.066564 \nC 82.04241 164.066564 82.805541 163.750465 83.368121 163.187884 \nC 83.930702 162.625304 84.246801 161.862173 84.246801 161.066564 \nC 84.246801 160.270955 83.930702 159.507824 83.368121 158.945244 \nC 82.805541 158.382663 82.04241 158.066564 81.246801 158.066564 \nC 80.451192 158.066564 79.688061 158.382663 79.125481 158.945244 \nC 78.5629 159.507824 78.246801 160.270955 78.246801 161.066564 \nC 78.246801 161.862173 78.5629 162.625304 79.125481 163.187884 \nC 79.688061 163.750465 80.451192 164.066564 81.246801 164.066564 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 165.059665 \nC 365.855552 165.059665 366.618683 164.743566 367.181264 164.180985 \nC 367.743844 163.618404 368.059943 162.855274 368.059943 162.059665 \nC 368.059943 161.264055 367.743844 160.500925 367.181264 159.938344 \nC 366.618683 159.375764 365.855552 159.059665 365.059943 159.059665 \nC 364.264334 159.059665 363.501204 159.375764 362.938623 159.938344 \nC 362.376042 160.500925 362.059943 161.264055 362.059943 162.059665 \nC 362.059943 162.855274 362.376042 163.618404 362.938623 164.180985 \nC 363.501204 164.743566 364.264334 165.059665 365.059943 165.059665 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 162.889988 \nC 61.708237 162.889988 62.471367 162.573889 63.033948 162.011308 \nC 63.596529 161.448727 63.912628 160.685597 63.912628 159.889988 \nC 63.912628 159.094378 63.596529 158.331248 63.033948 157.768667 \nC 62.471367 157.206087 61.708237 156.889988 60.912628 156.889988 \nC 60.117019 156.889988 59.353888 157.206087 58.791307 157.768667 \nC 58.228727 158.331248 57.912628 159.094378 57.912628 159.889988 \nC 57.912628 160.685597 58.228727 161.448727 58.791307 162.011308 \nC 59.353888 162.573889 60.117019 162.889988 60.912628 162.889988 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 162.67413 \nC 63.006163 162.67413 63.769293 162.358031 64.331874 161.79545 \nC 64.894455 161.232869 65.210554 160.469739 65.210554 159.67413 \nC 65.210554 158.878521 64.894455 158.11539 64.331874 157.552809 \nC 63.769293 156.990229 63.006163 156.67413 62.210554 156.67413 \nC 61.414944 156.67413 60.651814 156.990229 60.089233 157.552809 \nC 59.526653 158.11539 59.210554 158.878521 59.210554 159.67413 \nC 59.210554 160.469739 59.526653 161.232869 60.089233 161.79545 \nC 60.651814 162.358031 61.414944 162.67413 62.210554 162.67413 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 168.337407 \nC 65.602015 168.337407 66.365145 168.021308 66.927726 167.458727 \nC 67.490307 166.896147 67.806406 166.133016 67.806406 165.337407 \nC 67.806406 164.541798 67.490307 163.778667 66.927726 163.216087 \nC 66.365145 162.653506 65.602015 162.337407 64.806406 162.337407 \nC 64.010796 162.337407 63.247666 162.653506 62.685085 163.216087 \nC 62.122505 163.778667 61.806406 164.541798 61.806406 165.337407 \nC 61.806406 166.133016 62.122505 166.896147 62.685085 167.458727 \nC 63.247666 168.021308 64.010796 168.337407 64.806406 168.337407 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 187.892798 \nC 70.793719 187.892798 71.556849 187.576699 72.11943 187.014118 \nC 72.682011 186.451537 72.998109 185.688407 72.998109 184.892798 \nC 72.998109 184.097188 72.682011 183.334058 72.11943 182.771477 \nC 71.556849 182.208897 70.793719 181.892798 69.998109 181.892798 \nC 69.2025 181.892798 68.43937 182.208897 67.876789 182.771477 \nC 67.314208 183.334058 66.998109 184.097188 66.998109 184.892798 \nC 66.998109 185.688407 67.314208 186.451537 67.876789 187.014118 \nC 68.43937 187.576699 69.2025 187.892798 69.998109 187.892798 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 161.277564 \nC 81.177126 161.277564 81.940257 160.961465 82.502837 160.398885 \nC 83.065418 159.836304 83.381517 159.073174 83.381517 158.277564 \nC 83.381517 157.481955 83.065418 156.718825 82.502837 156.156244 \nC 81.940257 155.593663 81.177126 155.277564 80.381517 155.277564 \nC 79.585908 155.277564 78.822778 155.593663 78.260197 156.156244 \nC 77.697616 156.718825 77.381517 157.481955 77.381517 158.277564 \nC 77.381517 159.073174 77.697616 159.836304 78.260197 160.398885 \nC 78.822778 160.961465 79.585908 161.277564 80.381517 161.277564 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.517906 \nC 101.943942 21.517906 102.707072 21.201807 103.269653 20.639226 \nC 103.832233 20.076645 104.148332 19.313515 104.148332 18.517906 \nC 104.148332 17.722296 103.832233 16.959166 103.269653 16.396585 \nC 102.707072 15.834005 101.943942 15.517906 101.148332 15.517906 \nC 100.352723 15.517906 99.589593 15.834005 99.027012 16.396585 \nC 98.464431 16.959166 98.148332 17.722296 98.148332 18.517906 \nC 98.148332 19.313515 98.464431 20.076645 99.027012 20.639226 \nC 99.589593 21.201807 100.352723 21.517906 101.148332 21.517906 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 144.064606 \nL 60.696307 142.564606 \nL 62.196307 144.064606 \nL 63.696307 142.564606 \nL 62.196307 141.064606 \nL 63.696307 139.564606 \nL 62.196307 138.064606 \nL 60.696307 139.564606 \nL 59.196307 138.064606 \nL 57.696307 139.564606 \nL 59.196307 141.064606 \nL 57.696307 142.564606 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 189.25437 \nL 61.056842 187.75437 \nL 62.556842 189.25437 \nL 64.056842 187.75437 \nL 62.556842 186.25437 \nL 64.056842 184.75437 \nL 62.556842 183.25437 \nL 61.056842 184.75437 \nL 59.556842 183.25437 \nL 58.056842 184.75437 \nL 59.556842 186.25437 \nL 58.056842 187.75437 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 176.107715 \nL 62.210554 174.607715 \nL 63.710554 176.107715 \nL 65.210554 174.607715 \nL 63.710554 173.107715 \nL 65.210554 171.607715 \nL 63.710554 170.107715 \nL 62.210554 171.607715 \nL 60.710554 170.107715 \nL 59.210554 171.607715 \nL 60.710554 173.107715 \nL 59.210554 174.607715 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 189.494787 \nL 66.248546 187.994787 \nL 67.748546 189.494787 \nL 69.248546 187.994787 \nL 67.748546 186.494787 \nL 69.248546 184.994787 \nL 67.748546 183.494787 \nL 66.248546 184.994787 \nL 64.748546 183.494787 \nL 63.248546 184.994787 \nL 64.748546 186.494787 \nL 63.248546 187.994787 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 164.512639 \nL 81.246801 163.012639 \nL 82.746801 164.512639 \nL 84.246801 163.012639 \nL 82.746801 161.512639 \nL 84.246801 160.012639 \nL 82.746801 158.512639 \nL 81.246801 160.012639 \nL 79.746801 158.512639 \nL 78.246801 160.012639 \nL 79.746801 161.512639 \nL 78.246801 163.012639 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 164.192341 \nL 365.059943 162.692341 \nL 366.559943 164.192341 \nL 368.059943 162.692341 \nL 366.559943 161.192341 \nL 368.059943 159.692341 \nL 366.559943 158.192341 \nL 365.059943 159.692341 \nL 363.559943 158.192341 \nL 362.059943 159.692341 \nL 363.559943 161.192341 \nL 362.059943 162.692341 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 157.956351 \nL 60.912628 156.456351 \nL 62.412628 157.956351 \nL 63.912628 156.456351 \nL 62.412628 154.956351 \nL 63.912628 153.456351 \nL 62.412628 151.956351 \nL 60.912628 153.456351 \nL 59.412628 151.956351 \nL 57.912628 153.456351 \nL 59.412628 154.956351 \nL 57.912628 156.456351 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 186.268944 \nL 62.210554 184.768944 \nL 63.710554 186.268944 \nL 65.210554 184.768944 \nL 63.710554 183.268944 \nL 65.210554 181.768944 \nL 63.710554 180.268944 \nL 62.210554 181.768944 \nL 60.710554 180.268944 \nL 59.210554 181.768944 \nL 60.710554 183.268944 \nL 59.210554 184.768944 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 177.825306 \nL 64.806406 176.325306 \nL 66.306406 177.825306 \nL 67.806406 176.325306 \nL 66.306406 174.825306 \nL 67.806406 173.325306 \nL 66.306406 171.825306 \nL 64.806406 173.325306 \nL 63.306406 171.825306 \nL 61.806406 173.325306 \nL 63.306406 174.825306 \nL 61.806406 176.325306 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 182.055555 \nL 69.998109 180.555555 \nL 71.498109 182.055555 \nL 72.998109 180.555555 \nL 71.498109 179.055555 \nL 72.998109 177.555555 \nL 71.498109 176.055555 \nL 69.998109 177.555555 \nL 68.498109 176.055555 \nL 66.998109 177.555555 \nL 68.498109 179.055555 \nL 66.998109 180.555555 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 163.641303 \nL 80.381517 162.141303 \nL 81.881517 163.641303 \nL 83.381517 162.141303 \nL 81.881517 160.641303 \nL 83.381517 159.141303 \nL 81.881517 157.641303 \nL 80.381517 159.141303 \nL 78.881517 157.641303 \nL 77.381517 159.141303 \nL 78.881517 160.641303 \nL 77.381517 162.141303 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 99.648332 184.436901 \nL 101.148332 182.936901 \nL 102.648332 184.436901 \nL 104.148332 182.936901 \nL 102.648332 181.436901 \nL 104.148332 179.936901 \nL 102.648332 178.436901 \nL 101.148332 179.936901 \nL 99.648332 178.436901 \nL 98.148332 179.936901 \nL 99.648332 181.436901 \nL 98.148332 182.936901 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 159.719992 \nC 61.491916 159.719992 62.255046 159.403893 62.817627 158.841313 \nC 63.380208 158.278732 63.696307 157.515602 63.696307 156.719992 \nC 63.696307 155.924383 63.380208 155.161253 62.817627 154.598672 \nC 62.255046 154.036091 61.491916 153.719992 60.696307 153.719992 \nC 59.900698 153.719992 59.137567 154.036091 58.574986 154.598672 \nC 58.012406 155.161253 57.696307 155.924383 57.696307 156.719992 \nC 57.696307 157.515602 58.012406 158.278732 58.574986 158.841313 \nC 59.137567 159.403893 59.900698 159.719992 60.696307 159.719992 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 173.790306 \nC 61.852451 173.790306 62.615581 173.474207 63.178162 172.911626 \nC 63.740743 172.349045 64.056842 171.585915 64.056842 170.790306 \nC 64.056842 169.994696 63.740743 169.231566 63.178162 168.668985 \nC 62.615581 168.106405 61.852451 167.790306 61.056842 167.790306 \nC 60.261233 167.790306 59.498102 168.106405 58.935521 168.668985 \nC 58.372941 169.231566 58.056842 169.994696 58.056842 170.790306 \nC 58.056842 171.585915 58.372941 172.349045 58.935521 172.911626 \nC 59.498102 173.474207 60.261233 173.790306 61.056842 173.790306 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 182.365026 \nC 63.006163 182.365026 63.769293 182.048927 64.331874 181.486346 \nC 64.894455 180.923765 65.210554 180.160635 65.210554 179.365026 \nC 65.210554 178.569416 64.894455 177.806286 64.331874 177.243705 \nC 63.769293 176.681125 63.006163 176.365026 62.210554 176.365026 \nC 61.414944 176.365026 60.651814 176.681125 60.089233 177.243705 \nC 59.526653 177.806286 59.210554 178.569416 59.210554 179.365026 \nC 59.210554 180.160635 59.526653 180.923765 60.089233 181.486346 \nC 60.651814 182.048927 61.414944 182.365026 62.210554 182.365026 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 165.363622 \nC 67.044155 165.363622 67.807285 165.047523 68.369866 164.484942 \nC 68.932447 163.922362 69.248546 163.159231 69.248546 162.363622 \nC 69.248546 161.568013 68.932447 160.804882 68.369866 160.242302 \nC 67.807285 159.679721 67.044155 159.363622 66.248546 159.363622 \nC 65.452936 159.363622 64.689806 159.679721 64.127225 160.242302 \nC 63.564645 160.804882 63.248546 161.568013 63.248546 162.363622 \nC 63.248546 163.159231 63.564645 163.922362 64.127225 164.484942 \nC 64.689806 165.047523 65.452936 165.363622 66.248546 165.363622 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 164.641033 \nC 82.04241 164.641033 82.805541 164.324934 83.368121 163.762353 \nC 83.930702 163.199772 84.246801 162.436642 84.246801 161.641033 \nC 84.246801 160.845423 83.930702 160.082293 83.368121 159.519712 \nC 82.805541 158.957132 82.04241 158.641033 81.246801 158.641033 \nC 80.451192 158.641033 79.688061 158.957132 79.125481 159.519712 \nC 78.5629 160.082293 78.246801 160.845423 78.246801 161.641033 \nC 78.246801 162.436642 78.5629 163.199772 79.125481 163.762353 \nC 79.688061 164.324934 80.451192 164.641033 81.246801 164.641033 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 189.460663 \nC 139.728008 189.460663 140.491139 189.144564 141.053719 188.581983 \nC 141.6163 188.019403 141.932399 187.256272 141.932399 186.460663 \nC 141.932399 185.665054 141.6163 184.901923 141.053719 184.339343 \nC 140.491139 183.776762 139.728008 183.460663 138.932399 183.460663 \nC 138.13679 183.460663 137.373659 183.776762 136.811079 184.339343 \nC 136.248498 184.901923 135.932399 185.665054 135.932399 186.460663 \nC 135.932399 187.256272 136.248498 188.019403 136.811079 188.581983 \nC 137.373659 189.144564 138.13679 189.460663 138.932399 189.460663 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 159.667194 \nC 365.855552 159.667194 366.618683 159.351095 367.181264 158.788515 \nC 367.743844 158.225934 368.059943 157.462804 368.059943 156.667194 \nC 368.059943 155.871585 367.743844 155.108455 367.181264 154.545874 \nC 366.618683 153.983293 365.855552 153.667194 365.059943 153.667194 \nC 364.264334 153.667194 363.501204 153.983293 362.938623 154.545874 \nC 362.376042 155.108455 362.059943 155.871585 362.059943 156.667194 \nC 362.059943 157.462804 362.376042 158.225934 362.938623 158.788515 \nC 363.501204 159.351095 364.264334 159.667194 365.059943 159.667194 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 161.535854 \nC 61.708237 161.535854 62.471367 161.219755 63.033948 160.657174 \nC 63.596529 160.094594 63.912628 159.331463 63.912628 158.535854 \nC 63.912628 157.740245 63.596529 156.977114 63.033948 156.414534 \nC 62.471367 155.851953 61.708237 155.535854 60.912628 155.535854 \nC 60.117019 155.535854 59.353888 155.851953 58.791307 156.414534 \nC 58.228727 156.977114 57.912628 157.740245 57.912628 158.535854 \nC 57.912628 159.331463 58.228727 160.094594 58.791307 160.657174 \nC 59.353888 161.219755 60.117019 161.535854 60.912628 161.535854 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 188.582595 \nC 63.006163 188.582595 63.769293 188.266496 64.331874 187.703916 \nC 64.894455 187.141335 65.210554 186.378204 65.210554 185.582595 \nC 65.210554 184.786986 64.894455 184.023856 64.331874 183.461275 \nC 63.769293 182.898694 63.006163 182.582595 62.210554 182.582595 \nC 61.414944 182.582595 60.651814 182.898694 60.089233 183.461275 \nC 59.526653 184.023856 59.210554 184.786986 59.210554 185.582595 \nC 59.210554 186.378204 59.526653 187.141335 60.089233 187.703916 \nC 60.651814 188.266496 61.414944 188.582595 62.210554 188.582595 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 186.556682 \nC 65.602015 186.556682 66.365145 186.240583 66.927726 185.678002 \nC 67.490307 185.115421 67.806406 184.352291 67.806406 183.556682 \nC 67.806406 182.761072 67.490307 181.997942 66.927726 181.435361 \nC 66.365145 180.872781 65.602015 180.556682 64.806406 180.556682 \nC 64.010796 180.556682 63.247666 180.872781 62.685085 181.435361 \nC 62.122505 181.997942 61.806406 182.761072 61.806406 183.556682 \nC 61.806406 184.352291 62.122505 185.115421 62.685085 185.678002 \nC 63.247666 186.240583 64.010796 186.556682 64.806406 186.556682 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 185.401922 \nC 70.793719 185.401922 71.556849 185.085824 72.11943 184.523243 \nC 72.682011 183.960662 72.998109 183.197532 72.998109 182.401922 \nC 72.998109 181.606313 72.682011 180.843183 72.11943 180.280602 \nC 71.556849 179.718021 70.793719 179.401922 69.998109 179.401922 \nC 69.2025 179.401922 68.43937 179.718021 67.876789 180.280602 \nC 67.314208 180.843183 66.998109 181.606313 66.998109 182.401922 \nC 66.998109 183.197532 67.314208 183.960662 67.876789 184.523243 \nC 68.43937 185.085824 69.2025 185.401922 69.998109 185.401922 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 21.551124 \nC 81.177126 21.551124 81.940257 21.235025 82.502837 20.672445 \nC 83.065418 20.109864 83.381517 19.346734 83.381517 18.551124 \nC 83.381517 17.755515 83.065418 16.992385 82.502837 16.429804 \nC 81.940257 15.867223 81.177126 15.551124 80.381517 15.551124 \nC 79.585908 15.551124 78.822778 15.867223 78.260197 16.429804 \nC 77.697616 16.992385 77.381517 17.755515 77.381517 18.551124 \nC 77.381517 19.346734 77.697616 20.109864 78.260197 20.672445 \nC 78.822778 21.235025 79.585908 21.551124 80.381517 21.551124 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.517918 \nC 101.943942 21.517918 102.707072 21.201819 103.269653 20.639239 \nC 103.832233 20.076658 104.148332 19.313528 104.148332 18.517918 \nC 104.148332 17.722309 103.832233 16.959179 103.269653 16.396598 \nC 102.707072 15.834017 101.943942 15.517918 101.148332 15.517918 \nC 100.352723 15.517918 99.589593 15.834017 99.027012 16.396598 \nC 98.464431 16.959179 98.148332 17.722309 98.148332 18.517918 \nC 98.148332 19.313528 98.464431 20.076658 99.027012 20.639239 \nC 99.589593 21.201819 100.352723 21.517918 101.148332 21.517918 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 140.389693 \nL 60.696307 138.889693 \nL 62.196307 140.389693 \nL 63.696307 138.889693 \nL 62.196307 137.389693 \nL 63.696307 135.889693 \nL 62.196307 134.389693 \nL 60.696307 135.889693 \nL 59.196307 134.389693 \nL 57.696307 135.889693 \nL 59.196307 137.389693 \nL 57.696307 138.889693 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 165.557001 \nL 61.056842 164.057001 \nL 62.556842 165.557001 \nL 64.056842 164.057001 \nL 62.556842 162.557001 \nL 64.056842 161.057001 \nL 62.556842 159.557001 \nL 61.056842 161.057001 \nL 59.556842 159.557001 \nL 58.056842 161.057001 \nL 59.556842 162.557001 \nL 58.056842 164.057001 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 164.076199 \nL 62.210554 162.576199 \nL 63.710554 164.076199 \nL 65.210554 162.576199 \nL 63.710554 161.076199 \nL 65.210554 159.576199 \nL 63.710554 158.076199 \nL 62.210554 159.576199 \nL 60.710554 158.076199 \nL 59.210554 159.576199 \nL 60.710554 161.076199 \nL 59.210554 162.576199 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 161.287569 \nL 66.248546 159.787569 \nL 67.748546 161.287569 \nL 69.248546 159.787569 \nL 67.748546 158.287569 \nL 69.248546 156.787569 \nL 67.748546 155.287569 \nL 66.248546 156.787569 \nL 64.748546 155.287569 \nL 63.248546 156.787569 \nL 64.748546 158.287569 \nL 63.248546 159.787569 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 164.654563 \nL 81.246801 163.154563 \nL 82.746801 164.654563 \nL 84.246801 163.154563 \nL 82.746801 161.654563 \nL 84.246801 160.154563 \nL 82.746801 158.654563 \nL 81.246801 160.154563 \nL 79.746801 158.654563 \nL 78.246801 160.154563 \nL 79.746801 161.654563 \nL 78.246801 163.154563 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 137.432399 164.20493 \nL 138.932399 162.70493 \nL 140.432399 164.20493 \nL 141.932399 162.70493 \nL 140.432399 161.20493 \nL 141.932399 159.70493 \nL 140.432399 158.20493 \nL 138.932399 159.70493 \nL 137.432399 158.20493 \nL 135.932399 159.70493 \nL 137.432399 161.20493 \nL 135.932399 162.70493 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 161.9686 \nL 365.059943 160.4686 \nL 366.559943 161.9686 \nL 368.059943 160.4686 \nL 366.559943 158.9686 \nL 368.059943 157.4686 \nL 366.559943 155.9686 \nL 365.059943 157.4686 \nL 363.559943 155.9686 \nL 362.059943 157.4686 \nL 363.559943 158.9686 \nL 362.059943 160.4686 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 163.099273 \nL 60.912628 161.599273 \nL 62.412628 163.099273 \nL 63.912628 161.599273 \nL 62.412628 160.099273 \nL 63.912628 158.599273 \nL 62.412628 157.099273 \nL 60.912628 158.599273 \nL 59.412628 157.099273 \nL 57.912628 158.599273 \nL 59.412628 160.099273 \nL 57.912628 161.599273 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 168.335424 \nL 62.210554 166.835424 \nL 63.710554 168.335424 \nL 65.210554 166.835424 \nL 63.710554 165.335424 \nL 65.210554 163.835424 \nL 63.710554 162.335424 \nL 62.210554 163.835424 \nL 60.710554 162.335424 \nL 59.210554 163.835424 \nL 60.710554 165.335424 \nL 59.210554 166.835424 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 162.214122 \nL 64.806406 160.714122 \nL 66.306406 162.214122 \nL 67.806406 160.714122 \nL 66.306406 159.214122 \nL 67.806406 157.714122 \nL 66.306406 156.214122 \nL 64.806406 157.714122 \nL 63.306406 156.214122 \nL 61.806406 157.714122 \nL 63.306406 159.214122 \nL 61.806406 160.714122 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 164.357849 \nL 69.998109 162.857849 \nL 71.498109 164.357849 \nL 72.998109 162.857849 \nL 71.498109 161.357849 \nL 72.998109 159.857849 \nL 71.498109 158.357849 \nL 69.998109 159.857849 \nL 68.498109 158.357849 \nL 66.998109 159.857849 \nL 68.498109 161.357849 \nL 66.998109 162.857849 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 178.752038 \nL 80.381517 177.252038 \nL 81.881517 178.752038 \nL 83.381517 177.252038 \nL 81.881517 175.752038 \nL 83.381517 174.252038 \nL 81.881517 172.752038 \nL 80.381517 174.252038 \nL 78.881517 172.752038 \nL 77.381517 174.252038 \nL 78.881517 175.752038 \nL 77.381517 177.252038 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 129.416643 \nC 61.491916 129.416643 62.255046 129.100544 62.817627 128.537964 \nC 63.380208 127.975383 63.696307 127.212253 63.696307 126.416643 \nC 63.696307 125.621034 63.380208 124.857904 62.817627 124.295323 \nC 62.255046 123.732742 61.491916 123.416643 60.696307 123.416643 \nC 59.900698 123.416643 59.137567 123.732742 58.574986 124.295323 \nC 58.012406 124.857904 57.696307 125.621034 57.696307 126.416643 \nC 57.696307 127.212253 58.012406 127.975383 58.574986 128.537964 \nC 59.137567 129.100544 59.900698 129.416643 60.696307 129.416643 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 161.291499 \nC 61.852451 161.291499 62.615581 160.975401 63.178162 160.41282 \nC 63.740743 159.850239 64.056842 159.087109 64.056842 158.291499 \nC 64.056842 157.49589 63.740743 156.73276 63.178162 156.170179 \nC 62.615581 155.607598 61.852451 155.291499 61.056842 155.291499 \nC 60.261233 155.291499 59.498102 155.607598 58.935521 156.170179 \nC 58.372941 156.73276 58.056842 157.49589 58.056842 158.291499 \nC 58.056842 159.087109 58.372941 159.850239 58.935521 160.41282 \nC 59.498102 160.975401 60.261233 161.291499 61.056842 161.291499 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 191.075451 \nC 63.006163 191.075451 63.769293 190.759352 64.331874 190.196771 \nC 64.894455 189.63419 65.210554 188.87106 65.210554 188.075451 \nC 65.210554 187.279841 64.894455 186.516711 64.331874 185.95413 \nC 63.769293 185.39155 63.006163 185.075451 62.210554 185.075451 \nC 61.414944 185.075451 60.651814 185.39155 60.089233 185.95413 \nC 59.526653 186.516711 59.210554 187.279841 59.210554 188.075451 \nC 59.210554 188.87106 59.526653 189.63419 60.089233 190.196771 \nC 60.651814 190.759352 61.414944 191.075451 62.210554 191.075451 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 185.691681 \nC 67.044155 185.691681 67.807285 185.375582 68.369866 184.813001 \nC 68.932447 184.25042 69.248546 183.48729 69.248546 182.691681 \nC 69.248546 181.896071 68.932447 181.132941 68.369866 180.57036 \nC 67.807285 180.007779 67.044155 179.691681 66.248546 179.691681 \nC 65.452936 179.691681 64.689806 180.007779 64.127225 180.57036 \nC 63.564645 181.132941 63.248546 181.896071 63.248546 182.691681 \nC 63.248546 183.48729 63.564645 184.25042 64.127225 184.813001 \nC 64.689806 185.375582 65.452936 185.691681 66.248546 185.691681 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 193.241959 \nC 139.728008 193.241959 140.491139 192.92586 141.053719 192.36328 \nC 141.6163 191.800699 141.932399 191.037569 141.932399 190.241959 \nC 141.932399 189.44635 141.6163 188.68322 141.053719 188.120639 \nC 140.491139 187.558058 139.728008 187.241959 138.932399 187.241959 \nC 138.13679 187.241959 137.373659 187.558058 136.811079 188.120639 \nC 136.248498 188.68322 135.932399 189.44635 135.932399 190.241959 \nC 135.932399 191.037569 136.248498 191.800699 136.811079 192.36328 \nC 137.373659 192.92586 138.13679 193.241959 138.932399 193.241959 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 165.845183 \nC 365.855552 165.845183 366.618683 165.529084 367.181264 164.966503 \nC 367.743844 164.403922 368.059943 163.640792 368.059943 162.845183 \nC 368.059943 162.049573 367.743844 161.286443 367.181264 160.723862 \nC 366.618683 160.161282 365.855552 159.845183 365.059943 159.845183 \nC 364.264334 159.845183 363.501204 160.161282 362.938623 160.723862 \nC 362.376042 161.286443 362.059943 162.049573 362.059943 162.845183 \nC 362.059943 163.640792 362.376042 164.403922 362.938623 164.966503 \nC 363.501204 165.529084 364.264334 165.845183 365.059943 165.845183 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 153.921401 \nC 61.708237 153.921401 62.471367 153.605302 63.033948 153.042722 \nC 63.596529 152.480141 63.912628 151.717011 63.912628 150.921401 \nC 63.912628 150.125792 63.596529 149.362662 63.033948 148.800081 \nC 62.471367 148.2375 61.708237 147.921401 60.912628 147.921401 \nC 60.117019 147.921401 59.353888 148.2375 58.791307 148.800081 \nC 58.228727 149.362662 57.912628 150.125792 57.912628 150.921401 \nC 57.912628 151.717011 58.228727 152.480141 58.791307 153.042722 \nC 59.353888 153.605302 60.117019 153.921401 60.912628 153.921401 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 164.183956 \nC 63.006163 164.183956 63.769293 163.867857 64.331874 163.305276 \nC 64.894455 162.742695 65.210554 161.979565 65.210554 161.183956 \nC 65.210554 160.388346 64.894455 159.625216 64.331874 159.062635 \nC 63.769293 158.500055 63.006163 158.183956 62.210554 158.183956 \nC 61.414944 158.183956 60.651814 158.500055 60.089233 159.062635 \nC 59.526653 159.625216 59.210554 160.388346 59.210554 161.183956 \nC 59.210554 161.979565 59.526653 162.742695 60.089233 163.305276 \nC 60.651814 163.867857 61.414944 164.183956 62.210554 164.183956 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 188.922256 \nC 65.602015 188.922256 66.365145 188.606157 66.927726 188.043576 \nC 67.490307 187.480995 67.806406 186.717865 67.806406 185.922256 \nC 67.806406 185.126646 67.490307 184.363516 66.927726 183.800935 \nC 66.365145 183.238355 65.602015 182.922256 64.806406 182.922256 \nC 64.010796 182.922256 63.247666 183.238355 62.685085 183.800935 \nC 62.122505 184.363516 61.806406 185.126646 61.806406 185.922256 \nC 61.806406 186.717865 62.122505 187.480995 62.685085 188.043576 \nC 63.247666 188.606157 64.010796 188.922256 64.806406 188.922256 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 21.563859 \nC 70.793719 21.563859 71.556849 21.24776 72.11943 20.685179 \nC 72.682011 20.122598 72.998109 19.359468 72.998109 18.563859 \nC 72.998109 17.768249 72.682011 17.005119 72.11943 16.442538 \nC 71.556849 15.879958 70.793719 15.563859 69.998109 15.563859 \nC 69.2025 15.563859 68.43937 15.879958 67.876789 16.442538 \nC 67.314208 17.005119 66.998109 17.768249 66.998109 18.563859 \nC 66.998109 19.359468 67.314208 20.122598 67.876789 20.685179 \nC 68.43937 21.24776 69.2025 21.563859 69.998109 21.563859 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 21.520101 \nC 81.177126 21.520101 81.940257 21.204002 82.502837 20.641421 \nC 83.065418 20.07884 83.381517 19.31571 83.381517 18.520101 \nC 83.381517 17.724491 83.065418 16.961361 82.502837 16.39878 \nC 81.940257 15.8362 81.177126 15.520101 80.381517 15.520101 \nC 79.585908 15.520101 78.822778 15.8362 78.260197 16.39878 \nC 77.697616 16.961361 77.381517 17.724491 77.381517 18.520101 \nC 77.381517 19.31571 77.697616 20.07884 78.260197 20.641421 \nC 78.822778 21.204002 79.585908 21.520101 80.381517 21.520101 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.517885 \nC 101.943942 21.517885 102.707072 21.201786 103.269653 20.639206 \nC 103.832233 20.076625 104.148332 19.313495 104.148332 18.517885 \nC 104.148332 17.722276 103.832233 16.959146 103.269653 16.396565 \nC 102.707072 15.833984 101.943942 15.517885 101.148332 15.517885 \nC 100.352723 15.517885 99.589593 15.833984 99.027012 16.396565 \nC 98.464431 16.959146 98.148332 17.722276 98.148332 18.517885 \nC 98.148332 19.313495 98.464431 20.076625 99.027012 20.639206 \nC 99.589593 21.201786 100.352723 21.517885 101.148332 21.517885 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 153.458358 \nL 60.696307 151.958358 \nL 62.196307 153.458358 \nL 63.696307 151.958358 \nL 62.196307 150.458358 \nL 63.696307 148.958358 \nL 62.196307 147.458358 \nL 60.696307 148.958358 \nL 59.196307 147.458358 \nL 57.696307 148.958358 \nL 59.196307 150.458358 \nL 57.696307 151.958358 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 187.753016 \nL 61.056842 186.253016 \nL 62.556842 187.753016 \nL 64.056842 186.253016 \nL 62.556842 184.753016 \nL 64.056842 183.253016 \nL 62.556842 181.753016 \nL 61.056842 183.253016 \nL 59.556842 181.753016 \nL 58.056842 183.253016 \nL 59.556842 184.753016 \nL 58.056842 186.253016 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 186.556967 \nL 62.210554 185.056967 \nL 63.710554 186.556967 \nL 65.210554 185.056967 \nL 63.710554 183.556967 \nL 65.210554 182.056967 \nL 63.710554 180.556967 \nL 62.210554 182.056967 \nL 60.710554 180.556967 \nL 59.210554 182.056967 \nL 60.710554 183.556967 \nL 59.210554 185.056967 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 191.282749 \nL 66.248546 189.782749 \nL 67.748546 191.282749 \nL 69.248546 189.782749 \nL 67.748546 188.282749 \nL 69.248546 186.782749 \nL 67.748546 185.282749 \nL 66.248546 186.782749 \nL 64.748546 185.282749 \nL 63.248546 186.782749 \nL 64.748546 188.282749 \nL 63.248546 189.782749 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 154.742883 \nL 81.246801 153.242883 \nL 82.746801 154.742883 \nL 84.246801 153.242883 \nL 82.746801 151.742883 \nL 84.246801 150.242883 \nL 82.746801 148.742883 \nL 81.246801 150.242883 \nL 79.746801 148.742883 \nL 78.246801 150.242883 \nL 79.746801 151.742883 \nL 78.246801 153.242883 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 137.432399 182.520084 \nL 138.932399 181.020084 \nL 140.432399 182.520084 \nL 141.932399 181.020084 \nL 140.432399 179.520084 \nL 141.932399 178.020084 \nL 140.432399 176.520084 \nL 138.932399 178.020084 \nL 137.432399 176.520084 \nL 135.932399 178.020084 \nL 137.432399 179.520084 \nL 135.932399 181.020084 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 164.971533 \nL 365.059943 163.471533 \nL 366.559943 164.971533 \nL 368.059943 163.471533 \nL 366.559943 161.971533 \nL 368.059943 160.471533 \nL 366.559943 158.971533 \nL 365.059943 160.471533 \nL 363.559943 158.971533 \nL 362.059943 160.471533 \nL 363.559943 161.971533 \nL 362.059943 163.471533 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 163.481654 \nL 60.912628 161.981654 \nL 62.412628 163.481654 \nL 63.912628 161.981654 \nL 62.412628 160.481654 \nL 63.912628 158.981654 \nL 62.412628 157.481654 \nL 60.912628 158.981654 \nL 59.412628 157.481654 \nL 57.912628 158.981654 \nL 59.412628 160.481654 \nL 57.912628 161.981654 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 189.40323 \nL 62.210554 187.90323 \nL 63.710554 189.40323 \nL 65.210554 187.90323 \nL 63.710554 186.40323 \nL 65.210554 184.90323 \nL 63.710554 183.40323 \nL 62.210554 184.90323 \nL 60.710554 183.40323 \nL 59.210554 184.90323 \nL 60.710554 186.40323 \nL 59.210554 187.90323 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 164.771995 \nL 64.806406 163.271995 \nL 66.306406 164.771995 \nL 67.806406 163.271995 \nL 66.306406 161.771995 \nL 67.806406 160.271995 \nL 66.306406 158.771995 \nL 64.806406 160.271995 \nL 63.306406 158.771995 \nL 61.806406 160.271995 \nL 63.306406 161.771995 \nL 61.806406 163.271995 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 191.431322 \nL 69.998109 189.931322 \nL 71.498109 191.431322 \nL 72.998109 189.931322 \nL 71.498109 188.431322 \nL 72.998109 186.931322 \nL 71.498109 185.431322 \nL 69.998109 186.931322 \nL 68.498109 185.431322 \nL 66.998109 186.931322 \nL 68.498109 188.431322 \nL 66.998109 189.931322 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 195.44481 \nL 80.381517 193.94481 \nL 81.881517 195.44481 \nL 83.381517 193.94481 \nL 81.881517 192.44481 \nL 83.381517 190.94481 \nL 81.881517 189.44481 \nL 80.381517 190.94481 \nL 78.881517 189.44481 \nL 77.381517 190.94481 \nL 78.881517 192.44481 \nL 77.381517 193.94481 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 143.437245 \nC 61.491916 143.437245 62.255046 143.121146 62.817627 142.558565 \nC 63.380208 141.995985 63.696307 141.232854 63.696307 140.437245 \nC 63.696307 139.641636 63.380208 138.878505 62.817627 138.315925 \nC 62.255046 137.753344 61.491916 137.437245 60.696307 137.437245 \nC 59.900698 137.437245 59.137567 137.753344 58.574986 138.315925 \nC 58.012406 138.878505 57.696307 139.641636 57.696307 140.437245 \nC 57.696307 141.232854 58.012406 141.995985 58.574986 142.558565 \nC 59.137567 143.121146 59.900698 143.437245 60.696307 143.437245 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 164.541776 \nC 61.852451 164.541776 62.615581 164.225677 63.178162 163.663096 \nC 63.740743 163.100515 64.056842 162.337385 64.056842 161.541776 \nC 64.056842 160.746166 63.740743 159.983036 63.178162 159.420455 \nC 62.615581 158.857874 61.852451 158.541776 61.056842 158.541776 \nC 60.261233 158.541776 59.498102 158.857874 58.935521 159.420455 \nC 58.372941 159.983036 58.056842 160.746166 58.056842 161.541776 \nC 58.056842 162.337385 58.372941 163.100515 58.935521 163.663096 \nC 59.498102 164.225677 60.261233 164.541776 61.056842 164.541776 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 161.505566 \nC 63.006163 161.505566 63.769293 161.189467 64.331874 160.626886 \nC 64.894455 160.064305 65.210554 159.301175 65.210554 158.505566 \nC 65.210554 157.709956 64.894455 156.946826 64.331874 156.384245 \nC 63.769293 155.821664 63.006163 155.505566 62.210554 155.505566 \nC 61.414944 155.505566 60.651814 155.821664 60.089233 156.384245 \nC 59.526653 156.946826 59.210554 157.709956 59.210554 158.505566 \nC 59.210554 159.301175 59.526653 160.064305 60.089233 160.626886 \nC 60.651814 161.189467 61.414944 161.505566 62.210554 161.505566 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 163.890245 \nC 67.044155 163.890245 67.807285 163.574146 68.369866 163.011565 \nC 68.932447 162.448985 69.248546 161.685854 69.248546 160.890245 \nC 69.248546 160.094636 68.932447 159.331505 68.369866 158.768925 \nC 67.807285 158.206344 67.044155 157.890245 66.248546 157.890245 \nC 65.452936 157.890245 64.689806 158.206344 64.127225 158.768925 \nC 63.564645 159.331505 63.248546 160.094636 63.248546 160.890245 \nC 63.248546 161.685854 63.564645 162.448985 64.127225 163.011565 \nC 64.689806 163.574146 65.452936 163.890245 66.248546 163.890245 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 166.320949 \nC 82.04241 166.320949 82.805541 166.00485 83.368121 165.442269 \nC 83.930702 164.879689 84.246801 164.116558 84.246801 163.320949 \nC 84.246801 162.52534 83.930702 161.762209 83.368121 161.199629 \nC 82.805541 160.637048 82.04241 160.320949 81.246801 160.320949 \nC 80.451192 160.320949 79.688061 160.637048 79.125481 161.199629 \nC 78.5629 161.762209 78.246801 162.52534 78.246801 163.320949 \nC 78.246801 164.116558 78.5629 164.879689 79.125481 165.442269 \nC 79.688061 166.00485 80.451192 166.320949 81.246801 166.320949 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 187.557129 \nC 139.728008 187.557129 140.491139 187.24103 141.053719 186.678449 \nC 141.6163 186.115869 141.932399 185.352738 141.932399 184.557129 \nC 141.932399 183.76152 141.6163 182.998389 141.053719 182.435809 \nC 140.491139 181.873228 139.728008 181.557129 138.932399 181.557129 \nC 138.13679 181.557129 137.373659 181.873228 136.811079 182.435809 \nC 136.248498 182.998389 135.932399 183.76152 135.932399 184.557129 \nC 135.932399 185.352738 136.248498 186.115869 136.811079 186.678449 \nC 137.373659 187.24103 138.13679 187.557129 138.932399 187.557129 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 164.273597 \nC 365.855552 164.273597 366.618683 163.957498 367.181264 163.394918 \nC 367.743844 162.832337 368.059943 162.069207 368.059943 161.273597 \nC 368.059943 160.477988 367.743844 159.714858 367.181264 159.152277 \nC 366.618683 158.589696 365.855552 158.273597 365.059943 158.273597 \nC 364.264334 158.273597 363.501204 158.589696 362.938623 159.152277 \nC 362.376042 159.714858 362.059943 160.477988 362.059943 161.273597 \nC 362.059943 162.069207 362.376042 162.832337 362.938623 163.394918 \nC 363.501204 163.957498 364.264334 164.273597 365.059943 164.273597 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 177.318234 \nC 61.708237 177.318234 62.471367 177.002135 63.033948 176.439555 \nC 63.596529 175.876974 63.912628 175.113844 63.912628 174.318234 \nC 63.912628 173.522625 63.596529 172.759495 63.033948 172.196914 \nC 62.471367 171.634333 61.708237 171.318234 60.912628 171.318234 \nC 60.117019 171.318234 59.353888 171.634333 58.791307 172.196914 \nC 58.228727 172.759495 57.912628 173.522625 57.912628 174.318234 \nC 57.912628 175.113844 58.228727 175.876974 58.791307 176.439555 \nC 59.353888 177.002135 60.117019 177.318234 60.912628 177.318234 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 186.997942 \nC 63.006163 186.997942 63.769293 186.681843 64.331874 186.119262 \nC 64.894455 185.556681 65.210554 184.793551 65.210554 183.997942 \nC 65.210554 183.202332 64.894455 182.439202 64.331874 181.876621 \nC 63.769293 181.314041 63.006163 180.997942 62.210554 180.997942 \nC 61.414944 180.997942 60.651814 181.314041 60.089233 181.876621 \nC 59.526653 182.439202 59.210554 183.202332 59.210554 183.997942 \nC 59.210554 184.793551 59.526653 185.556681 60.089233 186.119262 \nC 60.651814 186.681843 61.414944 186.997942 62.210554 186.997942 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 181.787349 \nC 65.602015 181.787349 66.365145 181.471251 66.927726 180.90867 \nC 67.490307 180.346089 67.806406 179.582959 67.806406 178.787349 \nC 67.806406 177.99174 67.490307 177.22861 66.927726 176.666029 \nC 66.365145 176.103448 65.602015 175.787349 64.806406 175.787349 \nC 64.010796 175.787349 63.247666 176.103448 62.685085 176.666029 \nC 62.122505 177.22861 61.806406 177.99174 61.806406 178.787349 \nC 61.806406 179.582959 62.122505 180.346089 62.685085 180.90867 \nC 63.247666 181.471251 64.010796 181.787349 64.806406 181.787349 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 181.189821 \nC 70.793719 181.189821 71.556849 180.873722 72.11943 180.311142 \nC 72.682011 179.748561 72.998109 178.985431 72.998109 178.189821 \nC 72.998109 177.394212 72.682011 176.631082 72.11943 176.068501 \nC 71.556849 175.50592 70.793719 175.189821 69.998109 175.189821 \nC 69.2025 175.189821 68.43937 175.50592 67.876789 176.068501 \nC 67.314208 176.631082 66.998109 177.394212 66.998109 178.189821 \nC 66.998109 178.985431 67.314208 179.748561 67.876789 180.311142 \nC 68.43937 180.873722 69.2025 181.189821 69.998109 181.189821 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 157.468213 \nC 81.177126 157.468213 81.940257 157.152114 82.502837 156.589533 \nC 83.065418 156.026953 83.381517 155.263822 83.381517 154.468213 \nC 83.381517 153.672604 83.065418 152.909473 82.502837 152.346893 \nC 81.940257 151.784312 81.177126 151.468213 80.381517 151.468213 \nC 79.585908 151.468213 78.822778 151.784312 78.260197 152.346893 \nC 77.697616 152.909473 77.381517 153.672604 77.381517 154.468213 \nC 77.381517 155.263822 77.697616 156.026953 78.260197 156.589533 \nC 78.822778 157.152114 79.585908 157.468213 80.381517 157.468213 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.517842 \nC 101.943942 21.517842 102.707072 21.201743 103.269653 20.639163 \nC 103.832233 20.076582 104.148332 19.313452 104.148332 18.517842 \nC 104.148332 17.722233 103.832233 16.959103 103.269653 16.396522 \nC 102.707072 15.833941 101.943942 15.517842 101.148332 15.517842 \nC 100.352723 15.517842 99.589593 15.833941 99.027012 16.396522 \nC 98.464431 16.959103 98.148332 17.722233 98.148332 18.517842 \nC 98.148332 19.313452 98.464431 20.076582 99.027012 20.639163 \nC 99.589593 21.201743 100.352723 21.517842 101.148332 21.517842 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 156.537186 \nL 60.696307 155.037186 \nL 62.196307 156.537186 \nL 63.696307 155.037186 \nL 62.196307 153.537186 \nL 63.696307 152.037186 \nL 62.196307 150.537186 \nL 60.696307 152.037186 \nL 59.196307 150.537186 \nL 57.696307 152.037186 \nL 59.196307 153.537186 \nL 57.696307 155.037186 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 162.501145 \nL 62.210554 161.001145 \nL 63.710554 162.501145 \nL 65.210554 161.001145 \nL 63.710554 159.501145 \nL 65.210554 158.001145 \nL 63.710554 156.501145 \nL 62.210554 158.001145 \nL 60.710554 156.501145 \nL 59.210554 158.001145 \nL 60.710554 159.501145 \nL 59.210554 161.001145 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 157.978556 \nL 66.248546 156.478556 \nL 67.748546 157.978556 \nL 69.248546 156.478556 \nL 67.748546 154.978556 \nL 69.248546 153.478556 \nL 67.748546 151.978556 \nL 66.248546 153.478556 \nL 64.748546 151.978556 \nL 63.248546 153.478556 \nL 64.748546 154.978556 \nL 63.248546 156.478556 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 163.871907 \nL 81.246801 162.371907 \nL 82.746801 163.871907 \nL 84.246801 162.371907 \nL 82.746801 160.871907 \nL 84.246801 159.371907 \nL 82.746801 157.871907 \nL 81.246801 159.371907 \nL 79.746801 157.871907 \nL 78.246801 159.371907 \nL 79.746801 160.871907 \nL 78.246801 162.371907 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 137.432399 189.772641 \nL 138.932399 188.272641 \nL 140.432399 189.772641 \nL 141.932399 188.272641 \nL 140.432399 186.772641 \nL 141.932399 185.272641 \nL 140.432399 183.772641 \nL 138.932399 185.272641 \nL 137.432399 183.772641 \nL 135.932399 185.272641 \nL 137.432399 186.772641 \nL 135.932399 188.272641 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 163.317019 \nL 365.059943 161.817019 \nL 366.559943 163.317019 \nL 368.059943 161.817019 \nL 366.559943 160.317019 \nL 368.059943 158.817019 \nL 366.559943 157.317019 \nL 365.059943 158.817019 \nL 363.559943 157.317019 \nL 362.059943 158.817019 \nL 363.559943 160.317019 \nL 362.059943 161.817019 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 162.1099 \nL 60.912628 160.6099 \nL 62.412628 162.1099 \nL 63.912628 160.6099 \nL 62.412628 159.1099 \nL 63.912628 157.6099 \nL 62.412628 156.1099 \nL 60.912628 157.6099 \nL 59.412628 156.1099 \nL 57.912628 157.6099 \nL 59.412628 159.1099 \nL 57.912628 160.6099 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 162.650091 \nL 62.210554 161.150091 \nL 63.710554 162.650091 \nL 65.210554 161.150091 \nL 63.710554 159.650091 \nL 65.210554 158.150091 \nL 63.710554 156.650091 \nL 62.210554 158.150091 \nL 60.710554 156.650091 \nL 59.210554 158.150091 \nL 60.710554 159.650091 \nL 59.210554 161.150091 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 184.555087 \nL 64.806406 183.055087 \nL 66.306406 184.555087 \nL 67.806406 183.055087 \nL 66.306406 181.555087 \nL 67.806406 180.055087 \nL 66.306406 178.555087 \nL 64.806406 180.055087 \nL 63.306406 178.555087 \nL 61.806406 180.055087 \nL 63.306406 181.555087 \nL 61.806406 183.055087 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 194.059813 \nL 69.998109 192.559813 \nL 71.498109 194.059813 \nL 72.998109 192.559813 \nL 71.498109 191.059813 \nL 72.998109 189.559813 \nL 71.498109 188.059813 \nL 69.998109 189.559813 \nL 68.498109 188.059813 \nL 66.998109 189.559813 \nL 68.498109 191.059813 \nL 66.998109 192.559813 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 192.810393 \nL 80.381517 191.310393 \nL 81.881517 192.810393 \nL 83.381517 191.310393 \nL 81.881517 189.810393 \nL 83.381517 188.310393 \nL 81.881517 186.810393 \nL 80.381517 188.310393 \nL 78.881517 186.810393 \nL 77.381517 188.310393 \nL 78.881517 189.810393 \nL 77.381517 191.310393 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 99.648332 196.80634 \nL 101.148332 195.30634 \nL 102.648332 196.80634 \nL 104.148332 195.30634 \nL 102.648332 193.80634 \nL 104.148332 192.30634 \nL 102.648332 190.80634 \nL 101.148332 192.30634 \nL 99.648332 190.80634 \nL 98.148332 192.30634 \nL 99.648332 193.80634 \nL 98.148332 195.30634 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 161.291719 \nC 61.491916 161.291719 62.255046 160.975621 62.817627 160.41304 \nC 63.380208 159.850459 63.696307 159.087329 63.696307 158.291719 \nC 63.696307 157.49611 63.380208 156.73298 62.817627 156.170399 \nC 62.255046 155.607818 61.491916 155.291719 60.696307 155.291719 \nC 59.900698 155.291719 59.137567 155.607818 58.574986 156.170399 \nC 58.012406 156.73298 57.696307 157.49611 57.696307 158.291719 \nC 57.696307 159.087329 58.012406 159.850459 58.574986 160.41304 \nC 59.137567 160.975621 59.900698 161.291719 60.696307 161.291719 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 184.126672 \nC 61.852451 184.126672 62.615581 183.810573 63.178162 183.247993 \nC 63.740743 182.685412 64.056842 181.922282 64.056842 181.126672 \nC 64.056842 180.331063 63.740743 179.567933 63.178162 179.005352 \nC 62.615581 178.442771 61.852451 178.126672 61.056842 178.126672 \nC 60.261233 178.126672 59.498102 178.442771 58.935521 179.005352 \nC 58.372941 179.567933 58.056842 180.331063 58.056842 181.126672 \nC 58.056842 181.922282 58.372941 182.685412 58.935521 183.247993 \nC 59.498102 183.810573 60.261233 184.126672 61.056842 184.126672 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 163.127802 \nC 63.006163 163.127802 63.769293 162.811703 64.331874 162.249122 \nC 64.894455 161.686541 65.210554 160.923411 65.210554 160.127802 \nC 65.210554 159.332192 64.894455 158.569062 64.331874 158.006481 \nC 63.769293 157.443901 63.006163 157.127802 62.210554 157.127802 \nC 61.414944 157.127802 60.651814 157.443901 60.089233 158.006481 \nC 59.526653 158.569062 59.210554 159.332192 59.210554 160.127802 \nC 59.210554 160.923411 59.526653 161.686541 60.089233 162.249122 \nC 60.651814 162.811703 61.414944 163.127802 62.210554 163.127802 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 199.354421 \nC 67.044155 199.354421 67.807285 199.038322 68.369866 198.475741 \nC 68.932447 197.91316 69.248546 197.15003 69.248546 196.354421 \nC 69.248546 195.558811 68.932447 194.795681 68.369866 194.2331 \nC 67.807285 193.67052 67.044155 193.354421 66.248546 193.354421 \nC 65.452936 193.354421 64.689806 193.67052 64.127225 194.2331 \nC 63.564645 194.795681 63.248546 195.558811 63.248546 196.354421 \nC 63.248546 197.15003 63.564645 197.91316 64.127225 198.475741 \nC 64.689806 199.038322 65.452936 199.354421 66.248546 199.354421 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 174.853897 \nC 82.04241 174.853897 82.805541 174.537798 83.368121 173.975217 \nC 83.930702 173.412637 84.246801 172.649506 84.246801 171.853897 \nC 84.246801 171.058288 83.930702 170.295157 83.368121 169.732577 \nC 82.805541 169.169996 82.04241 168.853897 81.246801 168.853897 \nC 80.451192 168.853897 79.688061 169.169996 79.125481 169.732577 \nC 78.5629 170.295157 78.246801 171.058288 78.246801 171.853897 \nC 78.246801 172.649506 78.5629 173.412637 79.125481 173.975217 \nC 79.688061 174.537798 80.451192 174.853897 81.246801 174.853897 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 163.129731 \nC 139.728008 163.129731 140.491139 162.813632 141.053719 162.251052 \nC 141.6163 161.688471 141.932399 160.925341 141.932399 160.129731 \nC 141.932399 159.334122 141.6163 158.570992 141.053719 158.008411 \nC 140.491139 157.44583 139.728008 157.129731 138.932399 157.129731 \nC 138.13679 157.129731 137.373659 157.44583 136.811079 158.008411 \nC 136.248498 158.570992 135.932399 159.334122 135.932399 160.129731 \nC 135.932399 160.925341 136.248498 161.688471 136.811079 162.251052 \nC 137.373659 162.813632 138.13679 163.129731 138.932399 163.129731 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 161.336702 \nC 365.855552 161.336702 366.618683 161.020603 367.181264 160.458023 \nC 367.743844 159.895442 368.059943 159.132311 368.059943 158.336702 \nC 368.059943 157.541093 367.743844 156.777963 367.181264 156.215382 \nC 366.618683 155.652801 365.855552 155.336702 365.059943 155.336702 \nC 364.264334 155.336702 363.501204 155.652801 362.938623 156.215382 \nC 362.376042 156.777963 362.059943 157.541093 362.059943 158.336702 \nC 362.059943 159.132311 362.376042 159.895442 362.938623 160.458023 \nC 363.501204 161.020603 364.264334 161.336702 365.059943 161.336702 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 162.535911 \nC 61.708237 162.535911 62.471367 162.219812 63.033948 161.657232 \nC 63.596529 161.094651 63.912628 160.33152 63.912628 159.535911 \nC 63.912628 158.740302 63.596529 157.977172 63.033948 157.414591 \nC 62.471367 156.85201 61.708237 156.535911 60.912628 156.535911 \nC 60.117019 156.535911 59.353888 156.85201 58.791307 157.414591 \nC 58.228727 157.977172 57.912628 158.740302 57.912628 159.535911 \nC 57.912628 160.33152 58.228727 161.094651 58.791307 161.657232 \nC 59.353888 162.219812 60.117019 162.535911 60.912628 162.535911 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 179.76649 \nC 63.006163 179.76649 63.769293 179.450391 64.331874 178.88781 \nC 64.894455 178.325229 65.210554 177.562099 65.210554 176.76649 \nC 65.210554 175.97088 64.894455 175.20775 64.331874 174.645169 \nC 63.769293 174.082589 63.006163 173.76649 62.210554 173.76649 \nC 61.414944 173.76649 60.651814 174.082589 60.089233 174.645169 \nC 59.526653 175.20775 59.210554 175.97088 59.210554 176.76649 \nC 59.210554 177.562099 59.526653 178.325229 60.089233 178.88781 \nC 60.651814 179.450391 61.414944 179.76649 62.210554 179.76649 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 186.116462 \nC 65.602015 186.116462 66.365145 185.800363 66.927726 185.237782 \nC 67.490307 184.675201 67.806406 183.912071 67.806406 183.116462 \nC 67.806406 182.320852 67.490307 181.557722 66.927726 180.995141 \nC 66.365145 180.432561 65.602015 180.116462 64.806406 180.116462 \nC 64.010796 180.116462 63.247666 180.432561 62.685085 180.995141 \nC 62.122505 181.557722 61.806406 182.320852 61.806406 183.116462 \nC 61.806406 183.912071 62.122505 184.675201 62.685085 185.237782 \nC 63.247666 185.800363 64.010796 186.116462 64.806406 186.116462 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 25.947396 \nC 70.793719 25.947396 71.556849 25.631297 72.11943 25.068716 \nC 72.682011 24.506136 72.998109 23.743005 72.998109 22.947396 \nC 72.998109 22.151787 72.682011 21.388656 72.11943 20.826076 \nC 71.556849 20.263495 70.793719 19.947396 69.998109 19.947396 \nC 69.2025 19.947396 68.43937 20.263495 67.876789 20.826076 \nC 67.314208 21.388656 66.998109 22.151787 66.998109 22.947396 \nC 66.998109 23.743005 67.314208 24.506136 67.876789 25.068716 \nC 68.43937 25.631297 69.2025 25.947396 69.998109 25.947396 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 21.532881 \nC 81.177126 21.532881 81.940257 21.216783 82.502837 20.654202 \nC 83.065418 20.091621 83.381517 19.328491 83.381517 18.532881 \nC 83.381517 17.737272 83.065418 16.974142 82.502837 16.411561 \nC 81.940257 15.84898 81.177126 15.532881 80.381517 15.532881 \nC 79.585908 15.532881 78.822778 15.84898 78.260197 16.411561 \nC 77.697616 16.974142 77.381517 17.737272 77.381517 18.532881 \nC 77.381517 19.328491 77.697616 20.091621 78.260197 20.654202 \nC 78.822778 21.216783 79.585908 21.532881 80.381517 21.532881 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.518025 \nC 101.943942 21.518025 102.707072 21.201926 103.269653 20.639345 \nC 103.832233 20.076764 104.148332 19.313634 104.148332 18.518025 \nC 104.148332 17.722415 103.832233 16.959285 103.269653 16.396704 \nC 102.707072 15.834124 101.943942 15.518025 101.148332 15.518025 \nC 100.352723 15.518025 99.589593 15.834124 99.027012 16.396704 \nC 98.464431 16.959285 98.148332 17.722415 98.148332 18.518025 \nC 98.148332 19.313634 98.464431 20.076764 99.027012 20.639345 \nC 99.589593 21.201926 100.352723 21.518025 101.148332 21.518025 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 147.770061 \nL 60.696307 146.270061 \nL 62.196307 147.770061 \nL 63.696307 146.270061 \nL 62.196307 144.770061 \nL 63.696307 143.270061 \nL 62.196307 141.770061 \nL 60.696307 143.270061 \nL 59.196307 141.770061 \nL 57.696307 143.270061 \nL 59.196307 144.770061 \nL 57.696307 146.270061 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 176.992607 \nL 61.056842 175.492607 \nL 62.556842 176.992607 \nL 64.056842 175.492607 \nL 62.556842 173.992607 \nL 64.056842 172.492607 \nL 62.556842 170.992607 \nL 61.056842 172.492607 \nL 59.556842 170.992607 \nL 58.056842 172.492607 \nL 59.556842 173.992607 \nL 58.056842 175.492607 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 162.974663 \nL 62.210554 161.474663 \nL 63.710554 162.974663 \nL 65.210554 161.474663 \nL 63.710554 159.974663 \nL 65.210554 158.474663 \nL 63.710554 156.974663 \nL 62.210554 158.474663 \nL 60.710554 156.974663 \nL 59.210554 158.474663 \nL 60.710554 159.974663 \nL 59.210554 161.474663 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 164.904087 \nL 66.248546 163.404087 \nL 67.748546 164.904087 \nL 69.248546 163.404087 \nL 67.748546 161.904087 \nL 69.248546 160.404087 \nL 67.748546 158.904087 \nL 66.248546 160.404087 \nL 64.748546 158.904087 \nL 63.248546 160.404087 \nL 64.748546 161.904087 \nL 63.248546 163.404087 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 155.403233 \nL 81.246801 153.903233 \nL 82.746801 155.403233 \nL 84.246801 153.903233 \nL 82.746801 152.403233 \nL 84.246801 150.903233 \nL 82.746801 149.403233 \nL 81.246801 150.903233 \nL 79.746801 149.403233 \nL 78.246801 150.903233 \nL 79.746801 152.403233 \nL 78.246801 153.903233 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 137.432399 188.402856 \nL 138.932399 186.902856 \nL 140.432399 188.402856 \nL 141.932399 186.902856 \nL 140.432399 185.402856 \nL 141.932399 183.902856 \nL 140.432399 182.402856 \nL 138.932399 183.902856 \nL 137.432399 182.402856 \nL 135.932399 183.902856 \nL 137.432399 185.402856 \nL 135.932399 186.902856 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 163.329529 \nL 365.059943 161.829529 \nL 366.559943 163.329529 \nL 368.059943 161.829529 \nL 366.559943 160.329529 \nL 368.059943 158.829529 \nL 366.559943 157.329529 \nL 365.059943 158.829529 \nL 363.559943 157.329529 \nL 362.059943 158.829529 \nL 363.559943 160.329529 \nL 362.059943 161.829529 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 175.963711 \nL 60.912628 174.463711 \nL 62.412628 175.963711 \nL 63.912628 174.463711 \nL 62.412628 172.963711 \nL 63.912628 171.463711 \nL 62.412628 169.963711 \nL 60.912628 171.463711 \nL 59.412628 169.963711 \nL 57.912628 171.463711 \nL 59.412628 172.963711 \nL 57.912628 174.463711 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 164.334089 \nL 62.210554 162.834089 \nL 63.710554 164.334089 \nL 65.210554 162.834089 \nL 63.710554 161.334089 \nL 65.210554 159.834089 \nL 63.710554 158.334089 \nL 62.210554 159.834089 \nL 60.710554 158.334089 \nL 59.210554 159.834089 \nL 60.710554 161.334089 \nL 59.210554 162.834089 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 194.232834 \nL 64.806406 192.732834 \nL 66.306406 194.232834 \nL 67.806406 192.732834 \nL 66.306406 191.232834 \nL 67.806406 189.732834 \nL 66.306406 188.232834 \nL 64.806406 189.732834 \nL 63.306406 188.232834 \nL 61.806406 189.732834 \nL 63.306406 191.232834 \nL 61.806406 192.732834 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 217.756364 \nL 69.998109 216.256364 \nL 71.498109 217.756364 \nL 72.998109 216.256364 \nL 71.498109 214.756364 \nL 72.998109 213.256364 \nL 71.498109 211.756364 \nL 69.998109 213.256364 \nL 68.498109 211.756364 \nL 66.998109 213.256364 \nL 68.498109 214.756364 \nL 66.998109 216.256364 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 166.275543 \nL 80.381517 164.775543 \nL 81.881517 166.275543 \nL 83.381517 164.775543 \nL 81.881517 163.275543 \nL 83.381517 161.775543 \nL 81.881517 160.275543 \nL 80.381517 161.775543 \nL 78.881517 160.275543 \nL 77.381517 161.775543 \nL 78.881517 163.275543 \nL 77.381517 164.775543 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 99.648332 185.839705 \nL 101.148332 184.339705 \nL 102.648332 185.839705 \nL 104.148332 184.339705 \nL 102.648332 182.839705 \nL 104.148332 181.339705 \nL 102.648332 179.839705 \nL 101.148332 181.339705 \nL 99.648332 179.839705 \nL 98.148332 181.339705 \nL 99.648332 182.839705 \nL 98.148332 184.339705 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 161.791892 \nC 63.006163 161.791892 63.769293 161.475793 64.331874 160.913212 \nC 64.894455 160.350631 65.210554 159.587501 65.210554 158.791892 \nC 65.210554 157.996283 64.894455 157.233152 64.331874 156.670571 \nC 63.769293 156.107991 63.006163 155.791892 62.210554 155.791892 \nC 61.414944 155.791892 60.651814 156.107991 60.089233 156.670571 \nC 59.526653 157.233152 59.210554 157.996283 59.210554 158.791892 \nC 59.210554 159.587501 59.526653 160.350631 60.089233 160.913212 \nC 60.651814 161.475793 61.414944 161.791892 62.210554 161.791892 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 191.254509 \nC 67.044155 191.254509 67.807285 190.938411 68.369866 190.37583 \nC 68.932447 189.813249 69.248546 189.050119 69.248546 188.254509 \nC 69.248546 187.4589 68.932447 186.69577 68.369866 186.133189 \nC 67.807285 185.570608 67.044155 185.254509 66.248546 185.254509 \nC 65.452936 185.254509 64.689806 185.570608 64.127225 186.133189 \nC 63.564645 186.69577 63.248546 187.4589 63.248546 188.254509 \nC 63.248546 189.050119 63.564645 189.813249 64.127225 190.37583 \nC 64.689806 190.938411 65.452936 191.254509 66.248546 191.254509 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 185.862132 \nC 82.04241 185.862132 82.805541 185.546033 83.368121 184.983452 \nC 83.930702 184.420871 84.246801 183.657741 84.246801 182.862132 \nC 84.246801 182.066522 83.930702 181.303392 83.368121 180.740811 \nC 82.805541 180.178231 82.04241 179.862132 81.246801 179.862132 \nC 80.451192 179.862132 79.688061 180.178231 79.125481 180.740811 \nC 78.5629 181.303392 78.246801 182.066522 78.246801 182.862132 \nC 78.246801 183.657741 78.5629 184.420871 79.125481 184.983452 \nC 79.688061 185.546033 80.451192 185.862132 81.246801 185.862132 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 161.280426 \nC 139.728008 161.280426 140.491139 160.964327 141.053719 160.401747 \nC 141.6163 159.839166 141.932399 159.076036 141.932399 158.280426 \nC 141.932399 157.484817 141.6163 156.721687 141.053719 156.159106 \nC 140.491139 155.596525 139.728008 155.280426 138.932399 155.280426 \nC 138.13679 155.280426 137.373659 155.596525 136.811079 156.159106 \nC 136.248498 156.721687 135.932399 157.484817 135.932399 158.280426 \nC 135.932399 159.076036 136.248498 159.839166 136.811079 160.401747 \nC 137.373659 160.964327 138.13679 161.280426 138.932399 161.280426 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 162.81843 \nC 365.855552 162.81843 366.618683 162.502331 367.181264 161.939751 \nC 367.743844 161.37717 368.059943 160.61404 368.059943 159.81843 \nC 368.059943 159.022821 367.743844 158.259691 367.181264 157.69711 \nC 366.618683 157.134529 365.855552 156.81843 365.059943 156.81843 \nC 364.264334 156.81843 363.501204 157.134529 362.938623 157.69711 \nC 362.376042 158.259691 362.059943 159.022821 362.059943 159.81843 \nC 362.059943 160.61404 362.376042 161.37717 362.938623 161.939751 \nC 363.501204 162.502331 364.264334 162.81843 365.059943 162.81843 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 161.338454 \nC 61.708237 161.338454 62.471367 161.022355 63.033948 160.459774 \nC 63.596529 159.897194 63.912628 159.134063 63.912628 158.338454 \nC 63.912628 157.542845 63.596529 156.779714 63.033948 156.217134 \nC 62.471367 155.654553 61.708237 155.338454 60.912628 155.338454 \nC 60.117019 155.338454 59.353888 155.654553 58.791307 156.217134 \nC 58.228727 156.779714 57.912628 157.542845 57.912628 158.338454 \nC 57.912628 159.134063 58.228727 159.897194 58.791307 160.459774 \nC 59.353888 161.022355 60.117019 161.338454 60.912628 161.338454 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 186.320251 \nC 63.006163 186.320251 63.769293 186.004152 64.331874 185.441571 \nC 64.894455 184.878991 65.210554 184.11586 65.210554 183.320251 \nC 65.210554 182.524642 64.894455 181.761511 64.331874 181.198931 \nC 63.769293 180.63635 63.006163 180.320251 62.210554 180.320251 \nC 61.414944 180.320251 60.651814 180.63635 60.089233 181.198931 \nC 59.526653 181.761511 59.210554 182.524642 59.210554 183.320251 \nC 59.210554 184.11586 59.526653 184.878991 60.089233 185.441571 \nC 60.651814 186.004152 61.414944 186.320251 62.210554 186.320251 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 210.863986 \nC 65.602015 210.863986 66.365145 210.547887 66.927726 209.985306 \nC 67.490307 209.422725 67.806406 208.659595 67.806406 207.863986 \nC 67.806406 207.068376 67.490307 206.305246 66.927726 205.742665 \nC 66.365145 205.180085 65.602015 204.863986 64.806406 204.863986 \nC 64.010796 204.863986 63.247666 205.180085 62.685085 205.742665 \nC 62.122505 206.305246 61.806406 207.068376 61.806406 207.863986 \nC 61.806406 208.659595 62.122505 209.422725 62.685085 209.985306 \nC 63.247666 210.547887 64.010796 210.863986 64.806406 210.863986 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 20.083636 \nC 70.793719 20.083636 71.556849 19.767537 72.11943 19.204957 \nC 72.682011 18.642376 72.998109 17.879246 72.998109 17.083636 \nC 72.998109 16.288027 72.682011 15.524897 72.11943 14.962316 \nC 71.556849 14.399735 70.793719 14.083636 69.998109 14.083636 \nC 69.2025 14.083636 68.43937 14.399735 67.876789 14.962316 \nC 67.314208 15.524897 66.998109 16.288027 66.998109 17.083636 \nC 66.998109 17.879246 67.314208 18.642376 67.876789 19.204957 \nC 68.43937 19.767537 69.2025 20.083636 69.998109 20.083636 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 125.800361 \nC 81.177126 125.800361 81.940257 125.484262 82.502837 124.921681 \nC 83.065418 124.359101 83.381517 123.59597 83.381517 122.800361 \nC 83.381517 122.004752 83.065418 121.241621 82.502837 120.679041 \nC 81.940257 120.11646 81.177126 119.800361 80.381517 119.800361 \nC 79.585908 119.800361 78.822778 120.11646 78.260197 120.679041 \nC 77.697616 121.241621 77.381517 122.004752 77.381517 122.800361 \nC 77.381517 123.59597 77.697616 124.359101 78.260197 124.921681 \nC 78.822778 125.484262 79.585908 125.800361 80.381517 125.800361 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.518101 \nC 101.943942 21.518101 102.707072 21.202002 103.269653 20.639421 \nC 103.832233 20.07684 104.148332 19.31371 104.148332 18.518101 \nC 104.148332 17.722491 103.832233 16.959361 103.269653 16.39678 \nC 102.707072 15.8342 101.943942 15.518101 101.148332 15.518101 \nC 100.352723 15.518101 99.589593 15.8342 99.027012 16.39678 \nC 98.464431 16.959361 98.148332 17.722491 98.148332 18.518101 \nC 98.148332 19.31371 98.464431 20.07684 99.027012 20.639421 \nC 99.589593 21.202002 100.352723 21.518101 101.148332 21.518101 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 125.902254 \nL 60.696307 124.402254 \nL 62.196307 125.902254 \nL 63.696307 124.402254 \nL 62.196307 122.902254 \nL 63.696307 121.402254 \nL 62.196307 119.902254 \nL 60.696307 121.402254 \nL 59.196307 119.902254 \nL 57.696307 121.402254 \nL 59.196307 122.902254 \nL 57.696307 124.402254 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 176.470774 \nL 61.056842 174.970774 \nL 62.556842 176.470774 \nL 64.056842 174.970774 \nL 62.556842 173.470774 \nL 64.056842 171.970774 \nL 62.556842 170.470774 \nL 61.056842 171.970774 \nL 59.556842 170.470774 \nL 58.056842 171.970774 \nL 59.556842 173.470774 \nL 58.056842 174.970774 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 183.423937 \nL 62.210554 181.923937 \nL 63.710554 183.423937 \nL 65.210554 181.923937 \nL 63.710554 180.423937 \nL 65.210554 178.923937 \nL 63.710554 177.423937 \nL 62.210554 178.923937 \nL 60.710554 177.423937 \nL 59.210554 178.923937 \nL 60.710554 180.423937 \nL 59.210554 181.923937 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 154.475218 \nL 66.248546 152.975218 \nL 67.748546 154.475218 \nL 69.248546 152.975218 \nL 67.748546 151.475218 \nL 69.248546 149.975218 \nL 67.748546 148.475218 \nL 66.248546 149.975218 \nL 64.748546 148.475218 \nL 63.248546 149.975218 \nL 64.748546 151.475218 \nL 63.248546 152.975218 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 181.110408 \nL 81.246801 179.610408 \nL 82.746801 181.110408 \nL 84.246801 179.610408 \nL 82.746801 178.110408 \nL 84.246801 176.610408 \nL 82.746801 175.110408 \nL 81.246801 176.610408 \nL 79.746801 175.110408 \nL 78.246801 176.610408 \nL 79.746801 178.110408 \nL 78.246801 179.610408 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 160.134708 \nL 365.059943 158.634708 \nL 366.559943 160.134708 \nL 368.059943 158.634708 \nL 366.559943 157.134708 \nL 368.059943 155.634708 \nL 366.559943 154.134708 \nL 365.059943 155.634708 \nL 363.559943 154.134708 \nL 362.059943 155.634708 \nL 363.559943 157.134708 \nL 362.059943 158.634708 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 163.352318 \nL 60.912628 161.852318 \nL 62.412628 163.352318 \nL 63.912628 161.852318 \nL 62.412628 160.352318 \nL 63.912628 158.852318 \nL 62.412628 157.352318 \nL 60.912628 158.852318 \nL 59.412628 157.352318 \nL 57.912628 158.852318 \nL 59.412628 160.352318 \nL 57.912628 161.852318 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 188.104117 \nL 62.210554 186.604117 \nL 63.710554 188.104117 \nL 65.210554 186.604117 \nL 63.710554 185.104117 \nL 65.210554 183.604117 \nL 63.710554 182.104117 \nL 62.210554 183.604117 \nL 60.710554 182.104117 \nL 59.210554 183.604117 \nL 60.710554 185.104117 \nL 59.210554 186.604117 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 172.27385 \nL 64.806406 170.77385 \nL 66.306406 172.27385 \nL 67.806406 170.77385 \nL 66.306406 169.27385 \nL 67.806406 167.77385 \nL 66.306406 166.27385 \nL 64.806406 167.77385 \nL 63.306406 166.27385 \nL 61.806406 167.77385 \nL 63.306406 169.27385 \nL 61.806406 170.77385 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 177.621627 \nL 69.998109 176.121627 \nL 71.498109 177.621627 \nL 72.998109 176.121627 \nL 71.498109 174.621627 \nL 72.998109 173.121627 \nL 71.498109 171.621627 \nL 69.998109 173.121627 \nL 68.498109 171.621627 \nL 66.998109 173.121627 \nL 68.498109 174.621627 \nL 66.998109 176.121627 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 194.023824 \nL 80.381517 192.523824 \nL 81.881517 194.023824 \nL 83.381517 192.523824 \nL 81.881517 191.023824 \nL 83.381517 189.523824 \nL 81.881517 188.023824 \nL 80.381517 189.523824 \nL 78.881517 188.023824 \nL 77.381517 189.523824 \nL 78.881517 191.023824 \nL 77.381517 192.523824 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 122.269227 \nC 61.491916 122.269227 62.255046 121.953128 62.817627 121.390547 \nC 63.380208 120.827967 63.696307 120.064836 63.696307 119.269227 \nC 63.696307 118.473618 63.380208 117.710488 62.817627 117.147907 \nC 62.255046 116.585326 61.491916 116.269227 60.696307 116.269227 \nC 59.900698 116.269227 59.137567 116.585326 58.574986 117.147907 \nC 58.012406 117.710488 57.696307 118.473618 57.696307 119.269227 \nC 57.696307 120.064836 58.012406 120.827967 58.574986 121.390547 \nC 59.137567 121.953128 59.900698 122.269227 60.696307 122.269227 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 155.825905 \nC 61.852451 155.825905 62.615581 155.509806 63.178162 154.947225 \nC 63.740743 154.384644 64.056842 153.621514 64.056842 152.825905 \nC 64.056842 152.030296 63.740743 151.267165 63.178162 150.704584 \nC 62.615581 150.142004 61.852451 149.825905 61.056842 149.825905 \nC 60.261233 149.825905 59.498102 150.142004 58.935521 150.704584 \nC 58.372941 151.267165 58.056842 152.030296 58.056842 152.825905 \nC 58.056842 153.621514 58.372941 154.384644 58.935521 154.947225 \nC 59.498102 155.509806 60.261233 155.825905 61.056842 155.825905 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 190.127707 \nC 63.006163 190.127707 63.769293 189.811608 64.331874 189.249027 \nC 64.894455 188.686446 65.210554 187.923316 65.210554 187.127707 \nC 65.210554 186.332097 64.894455 185.568967 64.331874 185.006386 \nC 63.769293 184.443805 63.006163 184.127707 62.210554 184.127707 \nC 61.414944 184.127707 60.651814 184.443805 60.089233 185.006386 \nC 59.526653 185.568967 59.210554 186.332097 59.210554 187.127707 \nC 59.210554 187.923316 59.526653 188.686446 60.089233 189.249027 \nC 60.651814 189.811608 61.414944 190.127707 62.210554 190.127707 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 167.316527 \nC 67.044155 167.316527 67.807285 167.000428 68.369866 166.437847 \nC 68.932447 165.875267 69.248546 165.112136 69.248546 164.316527 \nC 69.248546 163.520918 68.932447 162.757787 68.369866 162.195207 \nC 67.807285 161.632626 67.044155 161.316527 66.248546 161.316527 \nC 65.452936 161.316527 64.689806 161.632626 64.127225 162.195207 \nC 63.564645 162.757787 63.248546 163.520918 63.248546 164.316527 \nC 63.248546 165.112136 63.564645 165.875267 64.127225 166.437847 \nC 64.689806 167.000428 65.452936 167.316527 66.248546 167.316527 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 189.273259 \nC 82.04241 189.273259 82.805541 188.95716 83.368121 188.39458 \nC 83.930702 187.831999 84.246801 187.068869 84.246801 186.273259 \nC 84.246801 185.47765 83.930702 184.71452 83.368121 184.151939 \nC 82.805541 183.589358 82.04241 183.273259 81.246801 183.273259 \nC 80.451192 183.273259 79.688061 183.589358 79.125481 184.151939 \nC 78.5629 184.71452 78.246801 185.47765 78.246801 186.273259 \nC 78.246801 187.068869 78.5629 187.831999 79.125481 188.39458 \nC 79.688061 188.95716 80.451192 189.273259 81.246801 189.273259 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 160.470637 \nC 139.728008 160.470637 140.491139 160.154538 141.053719 159.591957 \nC 141.6163 159.029376 141.932399 158.266246 141.932399 157.470637 \nC 141.932399 156.675028 141.6163 155.911897 141.053719 155.349316 \nC 140.491139 154.786736 139.728008 154.470637 138.932399 154.470637 \nC 138.13679 154.470637 137.373659 154.786736 136.811079 155.349316 \nC 136.248498 155.911897 135.932399 156.675028 135.932399 157.470637 \nC 135.932399 158.266246 136.248498 159.029376 136.811079 159.591957 \nC 137.373659 160.154538 138.13679 160.470637 138.932399 160.470637 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 166.273841 \nC 365.855552 166.273841 366.618683 165.957742 367.181264 165.395161 \nC 367.743844 164.83258 368.059943 164.06945 368.059943 163.273841 \nC 368.059943 162.478231 367.743844 161.715101 367.181264 161.15252 \nC 366.618683 160.58994 365.855552 160.273841 365.059943 160.273841 \nC 364.264334 160.273841 363.501204 160.58994 362.938623 161.15252 \nC 362.376042 161.715101 362.059943 162.478231 362.059943 163.273841 \nC 362.059943 164.06945 362.376042 164.83258 362.938623 165.395161 \nC 363.501204 165.957742 364.264334 166.273841 365.059943 166.273841 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 162.613765 \nC 61.708237 162.613765 62.471367 162.297667 63.033948 161.735086 \nC 63.596529 161.172505 63.912628 160.409375 63.912628 159.613765 \nC 63.912628 158.818156 63.596529 158.055026 63.033948 157.492445 \nC 62.471367 156.929864 61.708237 156.613765 60.912628 156.613765 \nC 60.117019 156.613765 59.353888 156.929864 58.791307 157.492445 \nC 58.228727 158.055026 57.912628 158.818156 57.912628 159.613765 \nC 57.912628 160.409375 58.228727 161.172505 58.791307 161.735086 \nC 59.353888 162.297667 60.117019 162.613765 60.912628 162.613765 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 184.862977 \nC 63.006163 184.862977 63.769293 184.546878 64.331874 183.984298 \nC 64.894455 183.421717 65.210554 182.658587 65.210554 181.862977 \nC 65.210554 181.067368 64.894455 180.304238 64.331874 179.741657 \nC 63.769293 179.179076 63.006163 178.862977 62.210554 178.862977 \nC 61.414944 178.862977 60.651814 179.179076 60.089233 179.741657 \nC 59.526653 180.304238 59.210554 181.067368 59.210554 181.862977 \nC 59.210554 182.658587 59.526653 183.421717 60.089233 183.984298 \nC 60.651814 184.546878 61.414944 184.862977 62.210554 184.862977 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 183.653178 \nC 65.602015 183.653178 66.365145 183.337079 66.927726 182.774498 \nC 67.490307 182.211917 67.806406 181.448787 67.806406 180.653178 \nC 67.806406 179.857568 67.490307 179.094438 66.927726 178.531857 \nC 66.365145 177.969277 65.602015 177.653178 64.806406 177.653178 \nC 64.010796 177.653178 63.247666 177.969277 62.685085 178.531857 \nC 62.122505 179.094438 61.806406 179.857568 61.806406 180.653178 \nC 61.806406 181.448787 62.122505 182.211917 62.685085 182.774498 \nC 63.247666 183.337079 64.010796 183.653178 64.806406 183.653178 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 166.850208 \nC 70.793719 166.850208 71.556849 166.53411 72.11943 165.971529 \nC 72.682011 165.408948 72.998109 164.645818 72.998109 163.850208 \nC 72.998109 163.054599 72.682011 162.291469 72.11943 161.728888 \nC 71.556849 161.166307 70.793719 160.850208 69.998109 160.850208 \nC 69.2025 160.850208 68.43937 161.166307 67.876789 161.728888 \nC 67.314208 162.291469 66.998109 163.054599 66.998109 163.850208 \nC 66.998109 164.645818 67.314208 165.408948 67.876789 165.971529 \nC 68.43937 166.53411 69.2025 166.850208 69.998109 166.850208 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 21.523609 \nC 81.177126 21.523609 81.940257 21.20751 82.502837 20.644929 \nC 83.065418 20.082349 83.381517 19.319218 83.381517 18.523609 \nC 83.381517 17.728 83.065418 16.96487 82.502837 16.402289 \nC 81.940257 15.839708 81.177126 15.523609 80.381517 15.523609 \nC 79.585908 15.523609 78.822778 15.839708 78.260197 16.402289 \nC 77.697616 16.96487 77.381517 17.728 77.381517 18.523609 \nC 77.381517 19.319218 77.697616 20.082349 78.260197 20.644929 \nC 78.822778 21.20751 79.585908 21.523609 80.381517 21.523609 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.517979 \nC 101.943942 21.517979 102.707072 21.20188 103.269653 20.639299 \nC 103.832233 20.076719 104.148332 19.313588 104.148332 18.517979 \nC 104.148332 17.72237 103.832233 16.959239 103.269653 16.396659 \nC 102.707072 15.834078 101.943942 15.517979 101.148332 15.517979 \nC 100.352723 15.517979 99.589593 15.834078 99.027012 16.396659 \nC 98.464431 16.959239 98.148332 17.72237 98.148332 18.517979 \nC 98.148332 19.313588 98.464431 20.076719 99.027012 20.639299 \nC 99.589593 21.20188 100.352723 21.517979 101.148332 21.517979 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 164.145959 \nL 60.696307 162.645959 \nL 62.196307 164.145959 \nL 63.696307 162.645959 \nL 62.196307 161.145959 \nL 63.696307 159.645959 \nL 62.196307 158.145959 \nL 60.696307 159.645959 \nL 59.196307 158.145959 \nL 57.696307 159.645959 \nL 59.196307 161.145959 \nL 57.696307 162.645959 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 191.245732 \nL 61.056842 189.745732 \nL 62.556842 191.245732 \nL 64.056842 189.745732 \nL 62.556842 188.245732 \nL 64.056842 186.745732 \nL 62.556842 185.245732 \nL 61.056842 186.745732 \nL 59.556842 185.245732 \nL 58.056842 186.745732 \nL 59.556842 188.245732 \nL 58.056842 189.745732 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 189.904755 \nL 62.210554 188.404755 \nL 63.710554 189.904755 \nL 65.210554 188.404755 \nL 63.710554 186.904755 \nL 65.210554 185.404755 \nL 63.710554 183.904755 \nL 62.210554 185.404755 \nL 60.710554 183.904755 \nL 59.210554 185.404755 \nL 60.710554 186.904755 \nL 59.210554 188.404755 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 190.996572 \nL 66.248546 189.496572 \nL 67.748546 190.996572 \nL 69.248546 189.496572 \nL 67.748546 187.996572 \nL 69.248546 186.496572 \nL 67.748546 184.996572 \nL 66.248546 186.496572 \nL 64.748546 184.996572 \nL 63.248546 186.496572 \nL 64.748546 187.996572 \nL 63.248546 189.496572 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 162.912975 \nL 81.246801 161.412975 \nL 82.746801 162.912975 \nL 84.246801 161.412975 \nL 82.746801 159.912975 \nL 84.246801 158.412975 \nL 82.746801 156.912975 \nL 81.246801 158.412975 \nL 79.746801 156.912975 \nL 78.246801 158.412975 \nL 79.746801 159.912975 \nL 78.246801 161.412975 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 137.432399 163.035359 \nL 138.932399 161.535359 \nL 140.432399 163.035359 \nL 141.932399 161.535359 \nL 140.432399 160.035359 \nL 141.932399 158.535359 \nL 140.432399 157.035359 \nL 138.932399 158.535359 \nL 137.432399 157.035359 \nL 135.932399 158.535359 \nL 137.432399 160.035359 \nL 135.932399 161.535359 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 164.092025 \nL 365.059943 162.592025 \nL 366.559943 164.092025 \nL 368.059943 162.592025 \nL 366.559943 161.092025 \nL 368.059943 159.592025 \nL 366.559943 158.092025 \nL 365.059943 159.592025 \nL 363.559943 158.092025 \nL 362.059943 159.592025 \nL 363.559943 161.092025 \nL 362.059943 162.592025 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 163.511572 \nL 60.912628 162.011572 \nL 62.412628 163.511572 \nL 63.912628 162.011572 \nL 62.412628 160.511572 \nL 63.912628 159.011572 \nL 62.412628 157.511572 \nL 60.912628 159.011572 \nL 59.412628 157.511572 \nL 57.912628 159.011572 \nL 59.412628 160.511572 \nL 57.912628 162.011572 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 165.555141 \nL 62.210554 164.055141 \nL 63.710554 165.555141 \nL 65.210554 164.055141 \nL 63.710554 162.555141 \nL 65.210554 161.055141 \nL 63.710554 159.555141 \nL 62.210554 161.055141 \nL 60.710554 159.555141 \nL 59.210554 161.055141 \nL 60.710554 162.555141 \nL 59.210554 164.055141 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 166.194501 \nL 64.806406 164.694501 \nL 66.306406 166.194501 \nL 67.806406 164.694501 \nL 66.306406 163.194501 \nL 67.806406 161.694501 \nL 66.306406 160.194501 \nL 64.806406 161.694501 \nL 63.306406 160.194501 \nL 61.806406 161.694501 \nL 63.306406 163.194501 \nL 61.806406 164.694501 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 197.84981 \nL 69.998109 196.34981 \nL 71.498109 197.84981 \nL 72.998109 196.34981 \nL 71.498109 194.84981 \nL 72.998109 193.34981 \nL 71.498109 191.84981 \nL 69.998109 193.34981 \nL 68.498109 191.84981 \nL 66.998109 193.34981 \nL 68.498109 194.84981 \nL 66.998109 196.34981 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 165.046677 \nL 80.381517 163.546677 \nL 81.881517 165.046677 \nL 83.381517 163.546677 \nL 81.881517 162.046677 \nL 83.381517 160.546677 \nL 81.881517 159.046677 \nL 80.381517 160.546677 \nL 78.881517 159.046677 \nL 77.381517 160.546677 \nL 78.881517 162.046677 \nL 77.381517 163.546677 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 99.648332 178.221053 \nL 101.148332 176.721053 \nL 102.648332 178.221053 \nL 104.148332 176.721053 \nL 102.648332 175.221053 \nL 104.148332 173.721053 \nL 102.648332 172.221053 \nL 101.148332 173.721053 \nL 99.648332 172.221053 \nL 98.148332 173.721053 \nL 99.648332 175.221053 \nL 98.148332 176.721053 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 158.976615 \nC 61.491916 158.976615 62.255046 158.660516 62.817627 158.097935 \nC 63.380208 157.535354 63.696307 156.772224 63.696307 155.976615 \nC 63.696307 155.181005 63.380208 154.417875 62.817627 153.855294 \nC 62.255046 153.292714 61.491916 152.976615 60.696307 152.976615 \nC 59.900698 152.976615 59.137567 153.292714 58.574986 153.855294 \nC 58.012406 154.417875 57.696307 155.181005 57.696307 155.976615 \nC 57.696307 156.772224 58.012406 157.535354 58.574986 158.097935 \nC 59.137567 158.660516 59.900698 158.976615 60.696307 158.976615 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 160.563384 \nC 61.852451 160.563384 62.615581 160.247285 63.178162 159.684704 \nC 63.740743 159.122123 64.056842 158.358993 64.056842 157.563384 \nC 64.056842 156.767774 63.740743 156.004644 63.178162 155.442063 \nC 62.615581 154.879483 61.852451 154.563384 61.056842 154.563384 \nC 60.261233 154.563384 59.498102 154.879483 58.935521 155.442063 \nC 58.372941 156.004644 58.056842 156.767774 58.056842 157.563384 \nC 58.056842 158.358993 58.372941 159.122123 58.935521 159.684704 \nC 59.498102 160.247285 60.261233 160.563384 61.056842 160.563384 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 162.295696 \nC 63.006163 162.295696 63.769293 161.979597 64.331874 161.417016 \nC 64.894455 160.854436 65.210554 160.091305 65.210554 159.295696 \nC 65.210554 158.500087 64.894455 157.736956 64.331874 157.174376 \nC 63.769293 156.611795 63.006163 156.295696 62.210554 156.295696 \nC 61.414944 156.295696 60.651814 156.611795 60.089233 157.174376 \nC 59.526653 157.736956 59.210554 158.500087 59.210554 159.295696 \nC 59.210554 160.091305 59.526653 160.854436 60.089233 161.417016 \nC 60.651814 161.979597 61.414944 162.295696 62.210554 162.295696 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 160.54422 \nC 67.044155 160.54422 67.807285 160.228121 68.369866 159.66554 \nC 68.932447 159.102959 69.248546 158.339829 69.248546 157.54422 \nC 69.248546 156.74861 68.932447 155.98548 68.369866 155.422899 \nC 67.807285 154.860319 67.044155 154.54422 66.248546 154.54422 \nC 65.452936 154.54422 64.689806 154.860319 64.127225 155.422899 \nC 63.564645 155.98548 63.248546 156.74861 63.248546 157.54422 \nC 63.248546 158.339829 63.564645 159.102959 64.127225 159.66554 \nC 64.689806 160.228121 65.452936 160.54422 66.248546 160.54422 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 164.861928 \nC 82.04241 164.861928 82.805541 164.545829 83.368121 163.983248 \nC 83.930702 163.420668 84.246801 162.657537 84.246801 161.861928 \nC 84.246801 161.066319 83.930702 160.303188 83.368121 159.740608 \nC 82.805541 159.178027 82.04241 158.861928 81.246801 158.861928 \nC 80.451192 158.861928 79.688061 159.178027 79.125481 159.740608 \nC 78.5629 160.303188 78.246801 161.066319 78.246801 161.861928 \nC 78.246801 162.657537 78.5629 163.420668 79.125481 163.983248 \nC 79.688061 164.545829 80.451192 164.861928 81.246801 164.861928 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 163.150498 \nC 139.728008 163.150498 140.491139 162.834399 141.053719 162.271818 \nC 141.6163 161.709238 141.932399 160.946107 141.932399 160.150498 \nC 141.932399 159.354889 141.6163 158.591758 141.053719 158.029178 \nC 140.491139 157.466597 139.728008 157.150498 138.932399 157.150498 \nC 138.13679 157.150498 137.373659 157.466597 136.811079 158.029178 \nC 136.248498 158.591758 135.932399 159.354889 135.932399 160.150498 \nC 135.932399 160.946107 136.248498 161.709238 136.811079 162.271818 \nC 137.373659 162.834399 138.13679 163.150498 138.932399 163.150498 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 182.054967 \nC 365.855552 182.054967 366.618683 181.738868 367.181264 181.176287 \nC 367.743844 180.613706 368.059943 179.850576 368.059943 179.054967 \nC 368.059943 178.259357 367.743844 177.496227 367.181264 176.933646 \nC 366.618683 176.371066 365.855552 176.054967 365.059943 176.054967 \nC 364.264334 176.054967 363.501204 176.371066 362.938623 176.933646 \nC 362.376042 177.496227 362.059943 178.259357 362.059943 179.054967 \nC 362.059943 179.850576 362.376042 180.613706 362.938623 181.176287 \nC 363.501204 181.738868 364.264334 182.054967 365.059943 182.054967 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 162.875377 \nC 61.708237 162.875377 62.471367 162.559278 63.033948 161.996697 \nC 63.596529 161.434116 63.912628 160.670986 63.912628 159.875377 \nC 63.912628 159.079767 63.596529 158.316637 63.033948 157.754056 \nC 62.471367 157.191476 61.708237 156.875377 60.912628 156.875377 \nC 60.117019 156.875377 59.353888 157.191476 58.791307 157.754056 \nC 58.228727 158.316637 57.912628 159.079767 57.912628 159.875377 \nC 57.912628 160.670986 58.228727 161.434116 58.791307 161.996697 \nC 59.353888 162.559278 60.117019 162.875377 60.912628 162.875377 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 162.802742 \nC 63.006163 162.802742 63.769293 162.486643 64.331874 161.924062 \nC 64.894455 161.361481 65.210554 160.598351 65.210554 159.802742 \nC 65.210554 159.007132 64.894455 158.244002 64.331874 157.681421 \nC 63.769293 157.118841 63.006163 156.802742 62.210554 156.802742 \nC 61.414944 156.802742 60.651814 157.118841 60.089233 157.681421 \nC 59.526653 158.244002 59.210554 159.007132 59.210554 159.802742 \nC 59.210554 160.598351 59.526653 161.361481 60.089233 161.924062 \nC 60.651814 162.486643 61.414944 162.802742 62.210554 162.802742 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 165.957368 \nC 65.602015 165.957368 66.365145 165.641269 66.927726 165.078688 \nC 67.490307 164.516107 67.806406 163.752977 67.806406 162.957368 \nC 67.806406 162.161758 67.490307 161.398628 66.927726 160.836047 \nC 66.365145 160.273467 65.602015 159.957368 64.806406 159.957368 \nC 64.010796 159.957368 63.247666 160.273467 62.685085 160.836047 \nC 62.122505 161.398628 61.806406 162.161758 61.806406 162.957368 \nC 61.806406 163.752977 62.122505 164.516107 62.685085 165.078688 \nC 63.247666 165.641269 64.010796 165.957368 64.806406 165.957368 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 52.983528 \nC 70.793719 52.983528 71.556849 52.667429 72.11943 52.104849 \nC 72.682011 51.542268 72.998109 50.779138 72.998109 49.983528 \nC 72.998109 49.187919 72.682011 48.424789 72.11943 47.862208 \nC 71.556849 47.299627 70.793719 46.983528 69.998109 46.983528 \nC 69.2025 46.983528 68.43937 47.299627 67.876789 47.862208 \nC 67.314208 48.424789 66.998109 49.187919 66.998109 49.983528 \nC 66.998109 50.779138 67.314208 51.542268 67.876789 52.104849 \nC 68.43937 52.667429 69.2025 52.983528 69.998109 52.983528 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 21.528292 \nC 81.177126 21.528292 81.940257 21.212193 82.502837 20.649612 \nC 83.065418 20.087031 83.381517 19.323901 83.381517 18.528292 \nC 83.381517 17.732683 83.065418 16.969552 82.502837 16.406972 \nC 81.940257 15.844391 81.177126 15.528292 80.381517 15.528292 \nC 79.585908 15.528292 78.822778 15.844391 78.260197 16.406972 \nC 77.697616 16.969552 77.381517 17.732683 77.381517 18.528292 \nC 77.381517 19.323901 77.697616 20.087031 78.260197 20.649612 \nC 78.822778 21.212193 79.585908 21.528292 80.381517 21.528292 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.51802 \nC 101.943942 21.51802 102.707072 21.201921 103.269653 20.63934 \nC 103.832233 20.076759 104.148332 19.313629 104.148332 18.51802 \nC 104.148332 17.72241 103.832233 16.95928 103.269653 16.396699 \nC 102.707072 15.834119 101.943942 15.51802 101.148332 15.51802 \nC 100.352723 15.51802 99.589593 15.834119 99.027012 16.396699 \nC 98.464431 16.95928 98.148332 17.72241 98.148332 18.51802 \nC 98.148332 19.313629 98.464431 20.076759 99.027012 20.63934 \nC 99.589593 21.201921 100.352723 21.51802 101.148332 21.51802 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 154.752216 \nL 60.696307 153.252216 \nL 62.196307 154.752216 \nL 63.696307 153.252216 \nL 62.196307 151.752216 \nL 63.696307 150.252216 \nL 62.196307 148.752216 \nL 60.696307 150.252216 \nL 59.196307 148.752216 \nL 57.696307 150.252216 \nL 59.196307 151.752216 \nL 57.696307 153.252216 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 184.671402 \nL 61.056842 183.171402 \nL 62.556842 184.671402 \nL 64.056842 183.171402 \nL 62.556842 181.671402 \nL 64.056842 180.171402 \nL 62.556842 178.671402 \nL 61.056842 180.171402 \nL 59.556842 178.671402 \nL 58.056842 180.171402 \nL 59.556842 181.671402 \nL 58.056842 183.171402 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 184.963143 \nL 62.210554 183.463143 \nL 63.710554 184.963143 \nL 65.210554 183.463143 \nL 63.710554 181.963143 \nL 65.210554 180.463143 \nL 63.710554 178.963143 \nL 62.210554 180.463143 \nL 60.710554 178.963143 \nL 59.210554 180.463143 \nL 60.710554 181.963143 \nL 59.210554 183.463143 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 187.317961 \nL 66.248546 185.817961 \nL 67.748546 187.317961 \nL 69.248546 185.817961 \nL 67.748546 184.317961 \nL 69.248546 182.817961 \nL 67.748546 181.317961 \nL 66.248546 182.817961 \nL 64.748546 181.317961 \nL 63.248546 182.817961 \nL 64.748546 184.317961 \nL 63.248546 185.817961 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 187.093445 \nL 81.246801 185.593445 \nL 82.746801 187.093445 \nL 84.246801 185.593445 \nL 82.746801 184.093445 \nL 84.246801 182.593445 \nL 82.746801 181.093445 \nL 81.246801 182.593445 \nL 79.746801 181.093445 \nL 78.246801 182.593445 \nL 79.746801 184.093445 \nL 78.246801 185.593445 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 137.432399 159.492244 \nL 138.932399 157.992244 \nL 140.432399 159.492244 \nL 141.932399 157.992244 \nL 140.432399 156.492244 \nL 141.932399 154.992244 \nL 140.432399 153.492244 \nL 138.932399 154.992244 \nL 137.432399 153.492244 \nL 135.932399 154.992244 \nL 137.432399 156.492244 \nL 135.932399 157.992244 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 163.404679 \nL 365.059943 161.904679 \nL 366.559943 163.404679 \nL 368.059943 161.904679 \nL 366.559943 160.404679 \nL 368.059943 158.904679 \nL 366.559943 157.404679 \nL 365.059943 158.904679 \nL 363.559943 157.404679 \nL 362.059943 158.904679 \nL 363.559943 160.404679 \nL 362.059943 161.904679 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 160.12642 \nL 60.912628 158.62642 \nL 62.412628 160.12642 \nL 63.912628 158.62642 \nL 62.412628 157.12642 \nL 63.912628 155.62642 \nL 62.412628 154.12642 \nL 60.912628 155.62642 \nL 59.412628 154.12642 \nL 57.912628 155.62642 \nL 59.412628 157.12642 \nL 57.912628 158.62642 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 191.518447 \nL 62.210554 190.018447 \nL 63.710554 191.518447 \nL 65.210554 190.018447 \nL 63.710554 188.518447 \nL 65.210554 187.018447 \nL 63.710554 185.518447 \nL 62.210554 187.018447 \nL 60.710554 185.518447 \nL 59.210554 187.018447 \nL 60.710554 188.518447 \nL 59.210554 190.018447 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 166.420535 \nL 64.806406 164.920535 \nL 66.306406 166.420535 \nL 67.806406 164.920535 \nL 66.306406 163.420535 \nL 67.806406 161.920535 \nL 66.306406 160.420535 \nL 64.806406 161.920535 \nL 63.306406 160.420535 \nL 61.806406 161.920535 \nL 63.306406 163.420535 \nL 61.806406 164.920535 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 166.333856 \nL 69.998109 164.833856 \nL 71.498109 166.333856 \nL 72.998109 164.833856 \nL 71.498109 163.333856 \nL 72.998109 161.833856 \nL 71.498109 160.333856 \nL 69.998109 161.833856 \nL 68.498109 160.333856 \nL 66.998109 161.833856 \nL 68.498109 163.333856 \nL 66.998109 164.833856 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 196.458859 \nL 80.381517 194.958859 \nL 81.881517 196.458859 \nL 83.381517 194.958859 \nL 81.881517 193.458859 \nL 83.381517 191.958859 \nL 81.881517 190.458859 \nL 80.381517 191.958859 \nL 78.881517 190.458859 \nL 77.381517 191.958859 \nL 78.881517 193.458859 \nL 77.381517 194.958859 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 99.648332 183.884348 \nL 101.148332 182.384348 \nL 102.648332 183.884348 \nL 104.148332 182.384348 \nL 102.648332 180.884348 \nL 104.148332 179.384348 \nL 102.648332 177.884348 \nL 101.148332 179.384348 \nL 99.648332 177.884348 \nL 98.148332 179.384348 \nL 99.648332 180.884348 \nL 98.148332 182.384348 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 161.625229 \nC 61.491916 161.625229 62.255046 161.30913 62.817627 160.746549 \nC 63.380208 160.183969 63.696307 159.420838 63.696307 158.625229 \nC 63.696307 157.82962 63.380208 157.06649 62.817627 156.503909 \nC 62.255046 155.941328 61.491916 155.625229 60.696307 155.625229 \nC 59.900698 155.625229 59.137567 155.941328 58.574986 156.503909 \nC 58.012406 157.06649 57.696307 157.82962 57.696307 158.625229 \nC 57.696307 159.420838 58.012406 160.183969 58.574986 160.746549 \nC 59.137567 161.30913 59.900698 161.625229 60.696307 161.625229 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 161.961802 \nC 61.852451 161.961802 62.615581 161.645703 63.178162 161.083123 \nC 63.740743 160.520542 64.056842 159.757412 64.056842 158.961802 \nC 64.056842 158.166193 63.740743 157.403063 63.178162 156.840482 \nC 62.615581 156.277901 61.852451 155.961802 61.056842 155.961802 \nC 60.261233 155.961802 59.498102 156.277901 58.935521 156.840482 \nC 58.372941 157.403063 58.056842 158.166193 58.056842 158.961802 \nC 58.056842 159.757412 58.372941 160.520542 58.935521 161.083123 \nC 59.498102 161.645703 60.261233 161.961802 61.056842 161.961802 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 184.479233 \nC 63.006163 184.479233 63.769293 184.163134 64.331874 183.600553 \nC 64.894455 183.037973 65.210554 182.274842 65.210554 181.479233 \nC 65.210554 180.683624 64.894455 179.920493 64.331874 179.357913 \nC 63.769293 178.795332 63.006163 178.479233 62.210554 178.479233 \nC 61.414944 178.479233 60.651814 178.795332 60.089233 179.357913 \nC 59.526653 179.920493 59.210554 180.683624 59.210554 181.479233 \nC 59.210554 182.274842 59.526653 183.037973 60.089233 183.600553 \nC 60.651814 184.163134 61.414944 184.479233 62.210554 184.479233 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 66.248546 159.479682 \nC 67.044155 159.479682 67.807285 159.163583 68.369866 158.601002 \nC 68.932447 158.038421 69.248546 157.275291 69.248546 156.479682 \nC 69.248546 155.684072 68.932447 154.920942 68.369866 154.358361 \nC 67.807285 153.795781 67.044155 153.479682 66.248546 153.479682 \nC 65.452936 153.479682 64.689806 153.795781 64.127225 154.358361 \nC 63.564645 154.920942 63.248546 155.684072 63.248546 156.479682 \nC 63.248546 157.275291 63.564645 158.038421 64.127225 158.601002 \nC 64.689806 159.163583 65.452936 159.479682 66.248546 159.479682 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 160.692353 \nC 82.04241 160.692353 82.805541 160.376254 83.368121 159.813674 \nC 83.930702 159.251093 84.246801 158.487963 84.246801 157.692353 \nC 84.246801 156.896744 83.930702 156.133614 83.368121 155.571033 \nC 82.805541 155.008452 82.04241 154.692353 81.246801 154.692353 \nC 80.451192 154.692353 79.688061 155.008452 79.125481 155.571033 \nC 78.5629 156.133614 78.246801 156.896744 78.246801 157.692353 \nC 78.246801 158.487963 78.5629 159.251093 79.125481 159.813674 \nC 79.688061 160.376254 80.451192 160.692353 81.246801 160.692353 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 192.38262 \nC 139.728008 192.38262 140.491139 192.066521 141.053719 191.503941 \nC 141.6163 190.94136 141.932399 190.17823 141.932399 189.38262 \nC 141.932399 188.587011 141.6163 187.823881 141.053719 187.2613 \nC 140.491139 186.698719 139.728008 186.38262 138.932399 186.38262 \nC 138.13679 186.38262 137.373659 186.698719 136.811079 187.2613 \nC 136.248498 187.823881 135.932399 188.587011 135.932399 189.38262 \nC 135.932399 190.17823 136.248498 190.94136 136.811079 191.503941 \nC 137.373659 192.066521 138.13679 192.38262 138.932399 192.38262 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 162.41994 \nC 365.855552 162.41994 366.618683 162.103842 367.181264 161.541261 \nC 367.743844 160.97868 368.059943 160.21555 368.059943 159.41994 \nC 368.059943 158.624331 367.743844 157.861201 367.181264 157.29862 \nC 366.618683 156.736039 365.855552 156.41994 365.059943 156.41994 \nC 364.264334 156.41994 363.501204 156.736039 362.938623 157.29862 \nC 362.376042 157.861201 362.059943 158.624331 362.059943 159.41994 \nC 362.059943 160.21555 362.376042 160.97868 362.938623 161.541261 \nC 363.501204 162.103842 364.264334 162.41994 365.059943 162.41994 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 175.132225 \nC 61.708237 175.132225 62.471367 174.816126 63.033948 174.253545 \nC 63.596529 173.690964 63.912628 172.927834 63.912628 172.132225 \nC 63.912628 171.336615 63.596529 170.573485 63.033948 170.010904 \nC 62.471367 169.448323 61.708237 169.132225 60.912628 169.132225 \nC 60.117019 169.132225 59.353888 169.448323 58.791307 170.010904 \nC 58.228727 170.573485 57.912628 171.336615 57.912628 172.132225 \nC 57.912628 172.927834 58.228727 173.690964 58.791307 174.253545 \nC 59.353888 174.816126 60.117019 175.132225 60.912628 175.132225 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 164.848845 \nC 63.006163 164.848845 63.769293 164.532746 64.331874 163.970165 \nC 64.894455 163.407585 65.210554 162.644454 65.210554 161.848845 \nC 65.210554 161.053236 64.894455 160.290105 64.331874 159.727525 \nC 63.769293 159.164944 63.006163 158.848845 62.210554 158.848845 \nC 61.414944 158.848845 60.651814 159.164944 60.089233 159.727525 \nC 59.526653 160.290105 59.210554 161.053236 59.210554 161.848845 \nC 59.210554 162.644454 59.526653 163.407585 60.089233 163.970165 \nC 60.651814 164.532746 61.414944 164.848845 62.210554 164.848845 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 132.51817 \nC 65.602015 132.51817 66.365145 132.202071 66.927726 131.639491 \nC 67.490307 131.07691 67.806406 130.31378 67.806406 129.51817 \nC 67.806406 128.722561 67.490307 127.959431 66.927726 127.39685 \nC 66.365145 126.834269 65.602015 126.51817 64.806406 126.51817 \nC 64.010796 126.51817 63.247666 126.834269 62.685085 127.39685 \nC 62.122505 127.959431 61.806406 128.722561 61.806406 129.51817 \nC 61.806406 130.31378 62.122505 131.07691 62.685085 131.639491 \nC 63.247666 132.202071 64.010796 132.51817 64.806406 132.51817 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 69.998109 190.391418 \nC 70.793719 190.391418 71.556849 190.075319 72.11943 189.512738 \nC 72.682011 188.950157 72.998109 188.187027 72.998109 187.391418 \nC 72.998109 186.595808 72.682011 185.832678 72.11943 185.270097 \nC 71.556849 184.707517 70.793719 184.391418 69.998109 184.391418 \nC 69.2025 184.391418 68.43937 184.707517 67.876789 185.270097 \nC 67.314208 185.832678 66.998109 186.595808 66.998109 187.391418 \nC 66.998109 188.187027 67.314208 188.950157 67.876789 189.512738 \nC 68.43937 190.075319 69.2025 190.391418 69.998109 190.391418 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 21.52437 \nC 81.177126 21.52437 81.940257 21.208271 82.502837 20.64569 \nC 83.065418 20.083109 83.381517 19.319979 83.381517 18.52437 \nC 83.381517 17.72876 83.065418 16.96563 82.502837 16.403049 \nC 81.940257 15.840469 81.177126 15.52437 80.381517 15.52437 \nC 79.585908 15.52437 78.822778 15.840469 78.260197 16.403049 \nC 77.697616 16.96563 77.381517 17.72876 77.381517 18.52437 \nC 77.381517 19.319979 77.697616 20.083109 78.260197 20.64569 \nC 78.822778 21.208271 79.585908 21.52437 80.381517 21.52437 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.517933 \nC 101.943942 21.517933 102.707072 21.201835 103.269653 20.639254 \nC 103.832233 20.076673 104.148332 19.313543 104.148332 18.517933 \nC 104.148332 17.722324 103.832233 16.959194 103.269653 16.396613 \nC 102.707072 15.834032 101.943942 15.517933 101.148332 15.517933 \nC 100.352723 15.517933 99.589593 15.834032 99.027012 16.396613 \nC 98.464431 16.959194 98.148332 17.722324 98.148332 18.517933 \nC 98.148332 19.313543 98.464431 20.076673 99.027012 20.639254 \nC 99.589593 21.201835 100.352723 21.517933 101.148332 21.517933 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 158.755599 \nL 60.696307 157.255599 \nL 62.196307 158.755599 \nL 63.696307 157.255599 \nL 62.196307 155.755599 \nL 63.696307 154.255599 \nL 62.196307 152.755599 \nL 60.696307 154.255599 \nL 59.196307 152.755599 \nL 57.696307 154.255599 \nL 59.196307 155.755599 \nL 57.696307 157.255599 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 181.578628 \nL 61.056842 180.078628 \nL 62.556842 181.578628 \nL 64.056842 180.078628 \nL 62.556842 178.578628 \nL 64.056842 177.078628 \nL 62.556842 175.578628 \nL 61.056842 177.078628 \nL 59.556842 175.578628 \nL 58.056842 177.078628 \nL 59.556842 178.578628 \nL 58.056842 180.078628 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 187.921379 \nL 62.210554 186.421379 \nL 63.710554 187.921379 \nL 65.210554 186.421379 \nL 63.710554 184.921379 \nL 65.210554 183.421379 \nL 63.710554 181.921379 \nL 62.210554 183.421379 \nL 60.710554 181.921379 \nL 59.210554 183.421379 \nL 60.710554 184.921379 \nL 59.210554 186.421379 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 157.284214 \nL 66.248546 155.784214 \nL 67.748546 157.284214 \nL 69.248546 155.784214 \nL 67.748546 154.284214 \nL 69.248546 152.784214 \nL 67.748546 151.284214 \nL 66.248546 152.784214 \nL 64.748546 151.284214 \nL 63.248546 152.784214 \nL 64.748546 154.284214 \nL 63.248546 155.784214 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 163.047626 \nL 81.246801 161.547626 \nL 82.746801 163.047626 \nL 84.246801 161.547626 \nL 82.746801 160.047626 \nL 84.246801 158.547626 \nL 82.746801 157.047626 \nL 81.246801 158.547626 \nL 79.746801 157.047626 \nL 78.246801 158.547626 \nL 79.746801 160.047626 \nL 78.246801 161.547626 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 137.432399 162.48573 \nL 138.932399 160.98573 \nL 140.432399 162.48573 \nL 141.932399 160.98573 \nL 140.432399 159.48573 \nL 141.932399 157.98573 \nL 140.432399 156.48573 \nL 138.932399 157.98573 \nL 137.432399 156.48573 \nL 135.932399 157.98573 \nL 137.432399 159.48573 \nL 135.932399 160.98573 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 164.374765 \nL 365.059943 162.874765 \nL 366.559943 164.374765 \nL 368.059943 162.874765 \nL 366.559943 161.374765 \nL 368.059943 159.874765 \nL 366.559943 158.374765 \nL 365.059943 159.874765 \nL 363.559943 158.374765 \nL 362.059943 159.874765 \nL 363.559943 161.374765 \nL 362.059943 162.874765 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 162.904166 \nL 60.912628 161.404166 \nL 62.412628 162.904166 \nL 63.912628 161.404166 \nL 62.412628 159.904166 \nL 63.912628 158.404166 \nL 62.412628 156.904166 \nL 60.912628 158.404166 \nL 59.412628 156.904166 \nL 57.912628 158.404166 \nL 59.412628 159.904166 \nL 57.912628 161.404166 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 185.52307 \nL 62.210554 184.02307 \nL 63.710554 185.52307 \nL 65.210554 184.02307 \nL 63.710554 182.52307 \nL 65.210554 181.02307 \nL 63.710554 179.52307 \nL 62.210554 181.02307 \nL 60.710554 179.52307 \nL 59.210554 181.02307 \nL 60.710554 182.52307 \nL 59.210554 184.02307 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 166.031173 \nL 64.806406 164.531173 \nL 66.306406 166.031173 \nL 67.806406 164.531173 \nL 66.306406 163.031173 \nL 67.806406 161.531173 \nL 66.306406 160.031173 \nL 64.806406 161.531173 \nL 63.306406 160.031173 \nL 61.806406 161.531173 \nL 63.306406 163.031173 \nL 61.806406 164.531173 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 183.062714 \nL 69.998109 181.562714 \nL 71.498109 183.062714 \nL 72.998109 181.562714 \nL 71.498109 180.062714 \nL 72.998109 178.562714 \nL 71.498109 177.062714 \nL 69.998109 178.562714 \nL 68.498109 177.062714 \nL 66.998109 178.562714 \nL 68.498109 180.062714 \nL 66.998109 181.562714 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 178.71158 \nL 80.381517 177.21158 \nL 81.881517 178.71158 \nL 83.381517 177.21158 \nL 81.881517 175.71158 \nL 83.381517 174.21158 \nL 81.881517 172.71158 \nL 80.381517 174.21158 \nL 78.881517 172.71158 \nL 77.381517 174.21158 \nL 78.881517 175.71158 \nL 77.381517 177.21158 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 99.648332 197.153706 \nL 101.148332 195.653706 \nL 102.648332 197.153706 \nL 104.148332 195.653706 \nL 102.648332 194.153706 \nL 104.148332 192.653706 \nL 102.648332 191.153706 \nL 101.148332 192.653706 \nL 99.648332 191.153706 \nL 98.148332 192.653706 \nL 99.648332 194.153706 \nL 98.148332 195.653706 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.696307 157.764409 \nC 61.491916 157.764409 62.255046 157.44831 62.817627 156.885729 \nC 63.380208 156.323149 63.696307 155.560018 63.696307 154.764409 \nC 63.696307 153.9688 63.380208 153.205669 62.817627 152.643089 \nC 62.255046 152.080508 61.491916 151.764409 60.696307 151.764409 \nC 59.900698 151.764409 59.137567 152.080508 58.574986 152.643089 \nC 58.012406 153.205669 57.696307 153.9688 57.696307 154.764409 \nC 57.696307 155.560018 58.012406 156.323149 58.574986 156.885729 \nC 59.137567 157.44831 59.900698 157.764409 60.696307 157.764409 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 61.056842 164.766707 \nC 61.852451 164.766707 62.615581 164.450609 63.178162 163.888028 \nC 63.740743 163.325447 64.056842 162.562317 64.056842 161.766707 \nC 64.056842 160.971098 63.740743 160.207968 63.178162 159.645387 \nC 62.615581 159.082806 61.852451 158.766707 61.056842 158.766707 \nC 60.261233 158.766707 59.498102 159.082806 58.935521 159.645387 \nC 58.372941 160.207968 58.056842 160.971098 58.056842 161.766707 \nC 58.056842 162.562317 58.372941 163.325447 58.935521 163.888028 \nC 59.498102 164.450609 60.261233 164.766707 61.056842 164.766707 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 183.123903 \nC 63.006163 183.123903 63.769293 182.807804 64.331874 182.245223 \nC 64.894455 181.682642 65.210554 180.919512 65.210554 180.123903 \nC 65.210554 179.328294 64.894455 178.565163 64.331874 178.002583 \nC 63.769293 177.440002 63.006163 177.123903 62.210554 177.123903 \nC 61.414944 177.123903 60.651814 177.440002 60.089233 178.002583 \nC 59.526653 178.565163 59.210554 179.328294 59.210554 180.123903 \nC 59.210554 180.919512 59.526653 181.682642 60.089233 182.245223 \nC 60.651814 182.807804 61.414944 183.123903 62.210554 183.123903 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 81.246801 156.197729 \nC 82.04241 156.197729 82.805541 155.88163 83.368121 155.31905 \nC 83.930702 154.756469 84.246801 153.993339 84.246801 153.197729 \nC 84.246801 152.40212 83.930702 151.63899 83.368121 151.076409 \nC 82.805541 150.513828 82.04241 150.197729 81.246801 150.197729 \nC 80.451192 150.197729 79.688061 150.513828 79.125481 151.076409 \nC 78.5629 151.63899 78.246801 152.40212 78.246801 153.197729 \nC 78.246801 153.993339 78.5629 154.756469 79.125481 155.31905 \nC 79.688061 155.88163 80.451192 156.197729 81.246801 156.197729 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 138.932399 164.091993 \nC 139.728008 164.091993 140.491139 163.775894 141.053719 163.213313 \nC 141.6163 162.650732 141.932399 161.887602 141.932399 161.091993 \nC 141.932399 160.296383 141.6163 159.533253 141.053719 158.970672 \nC 140.491139 158.408092 139.728008 158.091993 138.932399 158.091993 \nC 138.13679 158.091993 137.373659 158.408092 136.811079 158.970672 \nC 136.248498 159.533253 135.932399 160.296383 135.932399 161.091993 \nC 135.932399 161.887602 136.248498 162.650732 136.811079 163.213313 \nC 137.373659 163.775894 138.13679 164.091993 138.932399 164.091993 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 365.059943 181.959054 \nC 365.855552 181.959054 366.618683 181.642955 367.181264 181.080375 \nC 367.743844 180.517794 368.059943 179.754664 368.059943 178.959054 \nC 368.059943 178.163445 367.743844 177.400315 367.181264 176.837734 \nC 366.618683 176.275153 365.855552 175.959054 365.059943 175.959054 \nC 364.264334 175.959054 363.501204 176.275153 362.938623 176.837734 \nC 362.376042 177.400315 362.059943 178.163445 362.059943 178.959054 \nC 362.059943 179.754664 362.376042 180.517794 362.938623 181.080375 \nC 363.501204 181.642955 364.264334 181.959054 365.059943 181.959054 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.912628 177.056237 \nC 61.708237 177.056237 62.471367 176.740138 63.033948 176.177557 \nC 63.596529 175.614976 63.912628 174.851846 63.912628 174.056237 \nC 63.912628 173.260628 63.596529 172.497497 63.033948 171.934917 \nC 62.471367 171.372336 61.708237 171.056237 60.912628 171.056237 \nC 60.117019 171.056237 59.353888 171.372336 58.791307 171.934917 \nC 58.228727 172.497497 57.912628 173.260628 57.912628 174.056237 \nC 57.912628 174.851846 58.228727 175.614976 58.791307 176.177557 \nC 59.353888 176.740138 60.117019 177.056237 60.912628 177.056237 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 62.210554 179.335485 \nC 63.006163 179.335485 63.769293 179.019386 64.331874 178.456806 \nC 64.894455 177.894225 65.210554 177.131095 65.210554 176.335485 \nC 65.210554 175.539876 64.894455 174.776746 64.331874 174.214165 \nC 63.769293 173.651584 63.006163 173.335485 62.210554 173.335485 \nC 61.414944 173.335485 60.651814 173.651584 60.089233 174.214165 \nC 59.526653 174.776746 59.210554 175.539876 59.210554 176.335485 \nC 59.210554 177.131095 59.526653 177.894225 60.089233 178.456806 \nC 60.651814 179.019386 61.414944 179.335485 62.210554 179.335485 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.806406 188.368384 \nC 65.602015 188.368384 66.365145 188.052285 66.927726 187.489704 \nC 67.490307 186.927123 67.806406 186.163993 67.806406 185.368384 \nC 67.806406 184.572774 67.490307 183.809644 66.927726 183.247063 \nC 66.365145 182.684483 65.602015 182.368384 64.806406 182.368384 \nC 64.010796 182.368384 63.247666 182.684483 62.685085 183.247063 \nC 62.122505 183.809644 61.806406 184.572774 61.806406 185.368384 \nC 61.806406 186.163993 62.122505 186.927123 62.685085 187.489704 \nC 63.247666 188.052285 64.010796 188.368384 64.806406 188.368384 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 80.381517 21.526223 \nC 81.177126 21.526223 81.940257 21.210124 82.502837 20.647543 \nC 83.065418 20.084963 83.381517 19.321832 83.381517 18.526223 \nC 83.381517 17.730614 83.065418 16.967483 82.502837 16.404903 \nC 81.940257 15.842322 81.177126 15.526223 80.381517 15.526223 \nC 79.585908 15.526223 78.822778 15.842322 78.260197 16.404903 \nC 77.697616 16.967483 77.381517 17.730614 77.381517 18.526223 \nC 77.381517 19.321832 77.697616 20.084963 78.260197 20.647543 \nC 78.822778 21.210124 79.585908 21.526223 80.381517 21.526223 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 101.148332 21.517928 \nC 101.943942 21.517928 102.707072 21.201829 103.269653 20.639249 \nC 103.832233 20.076668 104.148332 19.313538 104.148332 18.517928 \nC 104.148332 17.722319 103.832233 16.959189 103.269653 16.396608 \nC 102.707072 15.834027 101.943942 15.517928 101.148332 15.517928 \nC 100.352723 15.517928 99.589593 15.834027 99.027012 16.396608 \nC 98.464431 16.959189 98.148332 17.722319 98.148332 18.517928 \nC 98.148332 19.313538 98.464431 20.076668 99.027012 20.639249 \nC 99.589593 21.201829 100.352723 21.517928 101.148332 21.517928 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.196307 161.193267 \nL 60.696307 159.693267 \nL 62.196307 161.193267 \nL 63.696307 159.693267 \nL 62.196307 158.193267 \nL 63.696307 156.693267 \nL 62.196307 155.193267 \nL 60.696307 156.693267 \nL 59.196307 155.193267 \nL 57.696307 156.693267 \nL 59.196307 158.193267 \nL 57.696307 159.693267 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.556842 179.072041 \nL 61.056842 177.572041 \nL 62.556842 179.072041 \nL 64.056842 177.572041 \nL 62.556842 176.072041 \nL 64.056842 174.572041 \nL 62.556842 173.072041 \nL 61.056842 174.572041 \nL 59.556842 173.072041 \nL 58.056842 174.572041 \nL 59.556842 176.072041 \nL 58.056842 177.572041 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 160.419849 \nL 62.210554 158.919849 \nL 63.710554 160.419849 \nL 65.210554 158.919849 \nL 63.710554 157.419849 \nL 65.210554 155.919849 \nL 63.710554 154.419849 \nL 62.210554 155.919849 \nL 60.710554 154.419849 \nL 59.210554 155.919849 \nL 60.710554 157.419849 \nL 59.210554 158.919849 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 64.748546 163.055527 \nL 66.248546 161.555527 \nL 67.748546 163.055527 \nL 69.248546 161.555527 \nL 67.748546 160.055527 \nL 69.248546 158.555527 \nL 67.748546 157.055527 \nL 66.248546 158.555527 \nL 64.748546 157.055527 \nL 63.248546 158.555527 \nL 64.748546 160.055527 \nL 63.248546 161.555527 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 79.746801 184.760056 \nL 81.246801 183.260056 \nL 82.746801 184.760056 \nL 84.246801 183.260056 \nL 82.746801 181.760056 \nL 84.246801 180.260056 \nL 82.746801 178.760056 \nL 81.246801 180.260056 \nL 79.746801 178.760056 \nL 78.246801 180.260056 \nL 79.746801 181.760056 \nL 78.246801 183.260056 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 137.432399 162.682889 \nL 138.932399 161.182889 \nL 140.432399 162.682889 \nL 141.932399 161.182889 \nL 140.432399 159.682889 \nL 141.932399 158.182889 \nL 140.432399 156.682889 \nL 138.932399 158.182889 \nL 137.432399 156.682889 \nL 135.932399 158.182889 \nL 137.432399 159.682889 \nL 135.932399 161.182889 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 363.559943 163.646643 \nL 365.059943 162.146643 \nL 366.559943 163.646643 \nL 368.059943 162.146643 \nL 366.559943 160.646643 \nL 368.059943 159.146643 \nL 366.559943 157.646643 \nL 365.059943 159.146643 \nL 363.559943 157.646643 \nL 362.059943 159.146643 \nL 363.559943 160.646643 \nL 362.059943 162.146643 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 59.412628 163.24637 \nL 60.912628 161.74637 \nL 62.412628 163.24637 \nL 63.912628 161.74637 \nL 62.412628 160.24637 \nL 63.912628 158.74637 \nL 62.412628 157.24637 \nL 60.912628 158.74637 \nL 59.412628 157.24637 \nL 57.912628 158.74637 \nL 59.412628 160.24637 \nL 57.912628 161.74637 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 60.710554 190.517681 \nL 62.210554 189.017681 \nL 63.710554 190.517681 \nL 65.210554 189.017681 \nL 63.710554 187.517681 \nL 65.210554 186.017681 \nL 63.710554 184.517681 \nL 62.210554 186.017681 \nL 60.710554 184.517681 \nL 59.210554 186.017681 \nL 60.710554 187.517681 \nL 59.210554 189.017681 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 63.306406 195.600393 \nL 64.806406 194.100393 \nL 66.306406 195.600393 \nL 67.806406 194.100393 \nL 66.306406 192.600393 \nL 67.806406 191.100393 \nL 66.306406 189.600393 \nL 64.806406 191.100393 \nL 63.306406 189.600393 \nL 61.806406 191.100393 \nL 63.306406 192.600393 \nL 61.806406 194.100393 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 68.498109 205.284249 \nL 69.998109 203.784249 \nL 71.498109 205.284249 \nL 72.998109 203.784249 \nL 71.498109 202.284249 \nL 72.998109 200.784249 \nL 71.498109 199.284249 \nL 69.998109 200.784249 \nL 68.498109 199.284249 \nL 66.998109 200.784249 \nL 68.498109 202.284249 \nL 66.998109 203.784249 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 78.881517 176.678238 \nL 80.381517 175.178238 \nL 81.881517 176.678238 \nL 83.381517 175.178238 \nL 81.881517 173.678238 \nL 83.381517 172.178238 \nL 81.881517 170.678238 \nL 80.381517 172.178238 \nL 78.881517 170.678238 \nL 77.381517 172.178238 \nL 78.881517 173.678238 \nL 77.381517 175.178238 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p86e708a7ae)\" d=\"M 99.648332 197.132268 \nL 101.148332 195.632268 \nL 102.648332 197.132268 \nL 104.148332 195.632268 \nL 102.648332 194.132268 \nL 104.148332 192.632268 \nL 102.648332 191.132268 \nL 101.148332 192.632268 \nL 99.648332 191.132268 \nL 98.148332 192.632268 \nL 99.648332 194.132268 \nL 98.148332 195.632268 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n   </g>\n   <g id=\"PathCollection_2\"/>\n   <g id=\"PathCollection_3\"/>\n   <g id=\"PathCollection_4\"/>\n   <g id=\"PathCollection_5\"/>\n   <g id=\"PathCollection_6\"/>\n   <g id=\"PathCollection_7\"/>\n   <g id=\"PathCollection_8\"/>\n   <g id=\"PathCollection_9\"/>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m5f781a0edc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.461959\" xlink:href=\"#m5f781a0edc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(57.280709 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"105.528833\" xlink:href=\"#m5f781a0edc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2500 -->\n      <g transform=\"translate(92.803833 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"150.595706\" xlink:href=\"#m5f781a0edc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5000 -->\n      <g transform=\"translate(137.870706 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"195.662579\" xlink:href=\"#m5f781a0edc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 7500 -->\n      <g transform=\"translate(182.937579 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.729453\" xlink:href=\"#m5f781a0edc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10000 -->\n      <g transform=\"translate(224.823203 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.796326\" xlink:href=\"#m5f781a0edc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12500 -->\n      <g transform=\"translate(269.890076 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8632\" xlink:href=\"#m5f781a0edc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 15000 -->\n      <g transform=\"translate(314.95695 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"375.930073\" xlink:href=\"#m5f781a0edc\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 17500 -->\n      <g transform=\"translate(360.023823 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- parameters -->\n     <g transform=\"translate(183.876563 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"165.869141\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"324.560547\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"386.083984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"425.292969\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"486.816406\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"527.929688\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m41464670c6\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m41464670c6\" y=\"170.591119\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(20.878125 174.390338)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m41464670c6\" y=\"94.472315\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 98.271534)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m41464670c6\" y=\"18.353512\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 22.15273)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m3acd4f8146\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"223.79588\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"210.392024\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"200.881837\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"193.505162\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"187.477981\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"182.382071\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"177.967793\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"174.074125\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"147.677076\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"134.27322\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"124.763033\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"117.386359\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"111.359177\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"106.263267\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"101.84899\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"97.955321\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"71.558272\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"58.154416\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"48.644229\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"41.267555\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"35.240373\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"30.144463\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"25.730186\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3acd4f8146\" y=\"21.836517\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_13\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 125.577812)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 179.170313 219.64 \nL 246.585938 219.64 \nQ 248.585938 219.64 248.585938 217.64 \nL 248.585938 71.85875 \nQ 248.585938 69.85875 246.585938 69.85875 \nL 179.170313 69.85875 \nQ 177.170313 69.85875 177.170313 71.85875 \nL 177.170313 217.64 \nQ 177.170313 219.64 179.170313 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_14\">\n     <!-- layers -->\n     <g transform=\"translate(189.170313 81.457187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"209.765625\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"250.878906\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_10\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m5e68fc5089\" style=\"stroke:#edd1cb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#edd1cb;stroke:#edd1cb;\" x=\"191.170313\" xlink:href=\"#m5e68fc5089\" y=\"93.510312\"/>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- 1 -->\n     <g transform=\"translate(209.170313 96.135312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_11\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m2f2a1d7800\" style=\"stroke:#ebcac5;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ebcac5;stroke:#ebcac5;\" x=\"191.170313\" xlink:href=\"#m2f2a1d7800\" y=\"108.188437\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- 2 -->\n     <g transform=\"translate(209.170313 110.813437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_12\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m9adeed5f4c\" style=\"stroke:#e5bcbb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#e5bcbb;stroke:#e5bcbb;\" x=\"191.170313\" xlink:href=\"#m9adeed5f4c\" y=\"122.866562\"/>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- 4 -->\n     <g transform=\"translate(209.170313 125.491562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-34\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_13\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mee7d8bfac5\" style=\"stroke:#d89faa;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#d89faa;stroke:#d89faa;\" x=\"191.170313\" xlink:href=\"#mee7d8bfac5\" y=\"137.544688\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- 8 -->\n     <g transform=\"translate(209.170313 140.169688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-38\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_14\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m198721e052\" style=\"stroke:#ae6b91;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ae6b91;stroke:#ae6b91;\" x=\"191.170313\" xlink:href=\"#m198721e052\" y=\"152.222812\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- 16 -->\n     <g transform=\"translate(209.170313 154.847812)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_15\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"me5e687efa0\" style=\"stroke:#2d1e3e;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#2d1e3e;stroke:#2d1e3e;\" x=\"191.170313\" xlink:href=\"#me5e687efa0\" y=\"166.900937\"/>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- 32 -->\n     <g transform=\"translate(209.170313 169.525937)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- model -->\n     <g transform=\"translate(189.170313 184.204062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_16\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m8cc11efb7a\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"191.170313\" xlink:href=\"#m8cc11efb7a\" y=\"196.257187\"/>\n     </g>\n    </g>\n    <g id=\"text_22\">\n     <!-- FFNN -->\n     <g transform=\"translate(209.170313 198.882187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_17\">\n     <defs>\n      <path d=\"M -1.5 3 \nL 0 1.5 \nL 1.5 3 \nL 3 1.5 \nL 1.5 0 \nL 3 -1.5 \nL 1.5 -3 \nL 0 -1.5 \nL -1.5 -3 \nL -3 -1.5 \nL -1.5 0 \nL -3 1.5 \nz\n\" id=\"m7820280257\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"191.170313\" xlink:href=\"#m7820280257\" y=\"210.935312\"/>\n     </g>\n    </g>\n    <g id=\"text_23\">\n     <!-- ResNet -->\n     <g transform=\"translate(209.170313 213.560312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p86e708a7ae\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "218 260\n"
     ]
    }
   ],
   "source": [
    "print(len(d_results[d_results.loss < 2]), len(d_results))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}