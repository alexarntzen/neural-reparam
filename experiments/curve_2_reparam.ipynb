{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x11a2e2570>"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from neural_reparam.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from neural_reparam.ResNET import ResNET, init_zero\n",
    "import experiments.curves_2 as c2\n",
    "\n",
    "# make reproducible\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_2/\"\n",
    "SET_NAME = \"exp_3\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "\n",
    "FOLDS = 1\n",
    "N = 128  # training points internal\n",
    "loss_func = get_elastic_metric_loss(r=c2.r, constrain_cost=1e3, verbose=False)\n",
    "no_penalty_loss_func = get_elastic_metric_loss(r=c2.r, constrain_cost=0, verbose=False)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [2],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"model\": [FFNN, ResNET],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [10],\n",
    "    \"learning_rate\": [1],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = c2.q(x_train.detach())\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59858102\n",
      "Training Loss:  0.5985713\n",
      "Training Loss:  0.5985713\n",
      "Training Loss:  0.59853274\n",
      "Training Loss:  0.59853274\n",
      "Training Loss:  0.59853274\n",
      "Training Loss:  0.59853274\n",
      "Training Loss:  0.59853274\n",
      "Training Loss:  0.59853274\n",
      "Training Loss:  0.59853274\n",
      "Final training Loss:  0.59853274\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59668064\n",
      "Training Loss:  0.59668058\n",
      "Training Loss:  0.59668058\n",
      "Training Loss:  0.59668052\n",
      "Training Loss:  0.59668052\n",
      "Training Loss:  0.59668052\n",
      "Training Loss:  0.59668052\n",
      "Training Loss:  0.59668052\n",
      "Training Loss:  0.59668052\n",
      "Training Loss:  0.59668058\n",
      "Final training Loss:  0.59668058\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Training Loss:  0.59714878\n",
      "Final training Loss:  0.59714878\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59388077\n",
      "Training Loss:  0.59336686\n",
      "Training Loss:  0.59336686\n",
      "Training Loss:  0.59336686\n",
      "Training Loss:  0.59336686\n",
      "Training Loss:  0.59336686\n",
      "Training Loss:  0.59336686\n",
      "Training Loss:  0.59336686\n",
      "Training Loss:  0.59336686\n",
      "Training Loss:  0.59336686\n",
      "Final training Loss:  0.59336686\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72161549\n",
      "Training Loss:  0.60107285\n",
      "Training Loss:  0.60078037\n",
      "Training Loss:  0.60078037\n",
      "Training Loss:  0.60078037\n",
      "Training Loss:  0.60078037\n",
      "Training Loss:  0.60078037\n",
      "Training Loss:  0.60078037\n",
      "Training Loss:  0.60078037\n",
      "Training Loss:  0.60078037\n",
      "Final training Loss:  0.60078037\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Training Loss:  0.63090134\n",
      "Final training Loss:  0.63090134\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Training Loss:  0.66260946\n",
      "Final training Loss:  0.66260946\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61640632\n",
      "Training Loss:  0.61631078\n",
      "Training Loss:  0.61631078\n",
      "Training Loss:  0.61631066\n",
      "Training Loss:  0.61631066\n",
      "Training Loss:  0.61631066\n",
      "Training Loss:  0.61631066\n",
      "Training Loss:  0.61631066\n",
      "Training Loss:  0.61631066\n",
      "Training Loss:  0.61631066\n",
      "Final training Loss:  0.61631066\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62819964\n",
      "Training Loss:  0.62819964\n",
      "Training Loss:  0.62819964\n",
      "Training Loss:  0.62819964\n",
      "Training Loss:  0.62819964\n",
      "Training Loss:  0.62819952\n",
      "Training Loss:  0.62819952\n",
      "Training Loss:  0.62819952\n",
      "Training Loss:  0.62819952\n",
      "Training Loss:  0.62819952\n",
      "Final training Loss:  0.62819952\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62442964\n",
      "Training Loss:  0.62442952\n",
      "Training Loss:  0.62442952\n",
      "Training Loss:  0.62425041\n",
      "Training Loss:  0.62425041\n",
      "Training Loss:  0.62425041\n",
      "Training Loss:  0.62425041\n",
      "Training Loss:  0.62425041\n",
      "Training Loss:  0.62425041\n",
      "Training Loss:  0.62425035\n",
      "Final training Loss:  0.62425035\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59768814\n",
      "Training Loss:  0.59768814\n",
      "Training Loss:  0.59768814\n",
      "Training Loss:  0.59768814\n",
      "Training Loss:  0.59768814\n",
      "Training Loss:  0.59762323\n",
      "Training Loss:  0.59762323\n",
      "Training Loss:  0.59762323\n",
      "Training Loss:  0.59762323\n",
      "Training Loss:  0.59762323\n",
      "Final training Loss:  0.59762323\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6065492\n",
      "Training Loss:  0.6065492\n",
      "Training Loss:  0.6065492\n",
      "Training Loss:  0.59788364\n",
      "Training Loss:  0.59788364\n",
      "Training Loss:  0.59788364\n",
      "Training Loss:  0.59788364\n",
      "Training Loss:  0.59788364\n",
      "Training Loss:  0.59788364\n",
      "Training Loss:  0.59788364\n",
      "Final training Loss:  0.59788364\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Training Loss:  500.99523926\n",
      "Final training Loss:  500.99523926\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Final training Loss:  501.0\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Training Loss:  0.60546726\n",
      "Final training Loss:  0.60546726\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59574157\n",
      "Training Loss:  0.59574157\n",
      "Training Loss:  0.59574157\n",
      "Training Loss:  0.59574157\n",
      "Training Loss:  0.59574157\n",
      "Training Loss:  0.59574157\n",
      "Training Loss:  0.59571755\n",
      "Training Loss:  0.59571755\n",
      "Training Loss:  0.59571755\n",
      "Training Loss:  0.59571755\n",
      "Final training Loss:  0.59571755\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73091686\n",
      "Training Loss:  0.61467057\n",
      "Training Loss:  0.61467057\n",
      "Training Loss:  0.61467057\n",
      "Training Loss:  0.61467057\n",
      "Training Loss:  0.61467057\n",
      "Training Loss:  0.61467057\n",
      "Training Loss:  0.61467057\n",
      "Training Loss:  0.61467057\n",
      "Training Loss:  0.61467057\n",
      "Final training Loss:  0.61467057\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72493887\n",
      "Training Loss:  0.72493887\n",
      "Training Loss:  0.72493893\n",
      "Training Loss:  0.61345106\n",
      "Training Loss:  0.61344516\n",
      "Training Loss:  0.61344516\n",
      "Training Loss:  0.61344516\n",
      "Training Loss:  0.61344516\n",
      "Training Loss:  0.61344516\n",
      "Training Loss:  0.61344516\n",
      "Final training Loss:  0.61344516\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72976768\n",
      "Training Loss:  0.61448258\n",
      "Training Loss:  0.61448258\n",
      "Training Loss:  0.61448258\n",
      "Training Loss:  0.61448258\n",
      "Training Loss:  0.61448258\n",
      "Training Loss:  0.61448258\n",
      "Training Loss:  0.61448258\n",
      "Training Loss:  0.61448258\n",
      "Training Loss:  0.61448258\n",
      "Final training Loss:  0.61448258\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65611011\n",
      "Training Loss:  0.65611011\n",
      "Training Loss:  0.65611011\n",
      "Training Loss:  0.65587002\n",
      "Training Loss:  0.65572834\n",
      "Training Loss:  0.6557278\n",
      "Training Loss:  0.6557278\n",
      "Training Loss:  0.6557278\n",
      "Training Loss:  0.6557278\n",
      "Training Loss:  0.6557278\n",
      "Final training Loss:  0.6557278\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Training Loss:  0.67557263\n",
      "Final training Loss:  0.67557263\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7270292\n",
      "Training Loss:  0.67005873\n",
      "Training Loss:  0.67005873\n",
      "Training Loss:  0.67005873\n",
      "Training Loss:  0.67005873\n",
      "Training Loss:  0.67005873\n",
      "Training Loss:  0.66945881\n",
      "Training Loss:  0.66945881\n",
      "Training Loss:  0.66945881\n",
      "Training Loss:  0.66945881\n",
      "Final training Loss:  0.66945881\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Training Loss:  0.65337545\n",
      "Final training Loss:  0.65337545\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73011887\n",
      "Training Loss:  0.64377403\n",
      "Training Loss:  0.64377391\n",
      "Training Loss:  0.64369923\n",
      "Training Loss:  0.64369923\n",
      "Training Loss:  0.64369917\n",
      "Training Loss:  0.64369917\n",
      "Training Loss:  0.64369917\n",
      "Training Loss:  0.64369917\n",
      "Training Loss:  0.64369917\n",
      "Final training Loss:  0.64369917\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64259273\n",
      "Training Loss:  0.64259273\n",
      "Training Loss:  0.64259273\n",
      "Training Loss:  0.64259273\n",
      "Training Loss:  0.64259273\n",
      "Training Loss:  0.64259267\n",
      "Training Loss:  0.64259261\n",
      "Training Loss:  0.64259261\n",
      "Training Loss:  0.64259261\n",
      "Training Loss:  0.64259261\n",
      "Final training Loss:  0.64259261\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6363557\n",
      "Training Loss:  0.63605088\n",
      "Training Loss:  0.63605088\n",
      "Training Loss:  0.63605088\n",
      "Training Loss:  0.63605088\n",
      "Training Loss:  0.63605088\n",
      "Training Loss:  0.63605088\n",
      "Training Loss:  0.63605088\n",
      "Training Loss:  0.63605088\n",
      "Training Loss:  0.63605088\n",
      "Final training Loss:  0.63605088\n",
      "\n",
      "Running model (trial=0, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Training Loss:  0.73577374\n",
      "Final training Loss:  0.73577374\n",
      "\n",
      "Running model (trial=0, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59920239\n",
      "Training Loss:  0.59920228\n",
      "Training Loss:  0.59920228\n",
      "Training Loss:  0.59920228\n",
      "Training Loss:  0.59920228\n",
      "Training Loss:  0.59920228\n",
      "Training Loss:  0.59920228\n",
      "Training Loss:  0.59920228\n",
      "Training Loss:  0.59920228\n",
      "Training Loss:  0.59920228\n",
      "Final training Loss:  0.59920228\n",
      "\n",
      "Running model (trial=1, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Training Loss:  0.6044367\n",
      "Final training Loss:  0.6044367\n",
      "\n",
      "Running model (trial=1, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Training Loss:  0.59629899\n",
      "Final training Loss:  0.59629899\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Training Loss:  0.59387743\n",
      "Final training Loss:  0.59387743\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Training Loss:  0.5929333\n",
      "Final training Loss:  0.5929333\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68186873\n",
      "Training Loss:  0.68128312\n",
      "Training Loss:  0.68128312\n",
      "Training Loss:  0.68128312\n",
      "Training Loss:  0.68128312\n",
      "Training Loss:  0.68128312\n",
      "Training Loss:  0.68128312\n",
      "Training Loss:  0.68128312\n",
      "Training Loss:  0.68128312\n",
      "Training Loss:  0.68128312\n",
      "Final training Loss:  0.68128312\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59465641\n",
      "Training Loss:  0.59465623\n",
      "Training Loss:  0.59465623\n",
      "Training Loss:  0.59465623\n",
      "Training Loss:  0.59465623\n",
      "Training Loss:  0.59465623\n",
      "Training Loss:  0.59465623\n",
      "Training Loss:  0.59465623\n",
      "Training Loss:  0.59465623\n",
      "Training Loss:  0.59465623\n",
      "Final training Loss:  0.59465623\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64027697\n",
      "Training Loss:  0.64027703\n",
      "Training Loss:  0.64027703\n",
      "Training Loss:  0.64027703\n",
      "Training Loss:  0.64027697\n",
      "Training Loss:  0.64027697\n",
      "Training Loss:  0.64027697\n",
      "Training Loss:  0.64027703\n",
      "Training Loss:  0.64027697\n",
      "Training Loss:  0.64027697\n",
      "Final training Loss:  0.64027697\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64177644\n",
      "Training Loss:  0.64177632\n",
      "Training Loss:  0.64177632\n",
      "Training Loss:  0.64177632\n",
      "Training Loss:  0.64177632\n",
      "Training Loss:  0.64177632\n",
      "Training Loss:  0.64177632\n",
      "Training Loss:  0.64177632\n",
      "Training Loss:  0.64177632\n",
      "Training Loss:  0.64177632\n",
      "Final training Loss:  0.64177632\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60449564\n",
      "Training Loss:  0.60449564\n",
      "Training Loss:  0.60449558\n",
      "Training Loss:  0.60449558\n",
      "Training Loss:  0.60449558\n",
      "Training Loss:  0.60449558\n",
      "Training Loss:  0.60449558\n",
      "Training Loss:  0.60449558\n",
      "Training Loss:  0.60449558\n",
      "Training Loss:  0.60449558\n",
      "Final training Loss:  0.60449558\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59334707\n",
      "Training Loss:  0.59334707\n",
      "Training Loss:  0.59334707\n",
      "Training Loss:  0.59334707\n",
      "Training Loss:  0.59334707\n",
      "Training Loss:  0.59323388\n",
      "Training Loss:  0.59323388\n",
      "Training Loss:  0.59316272\n",
      "Training Loss:  0.59316272\n",
      "Training Loss:  0.59316272\n",
      "Final training Loss:  0.59316272\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60963392\n",
      "Training Loss:  0.60963392\n",
      "Training Loss:  0.60963362\n",
      "Training Loss:  0.60963362\n",
      "Training Loss:  0.60963356\n",
      "Training Loss:  0.60963356\n",
      "Training Loss:  0.60960716\n",
      "Training Loss:  0.60960716\n",
      "Training Loss:  0.60960716\n",
      "Training Loss:  0.60960716\n",
      "Final training Loss:  0.60960716\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Training Loss:  0.60012001\n",
      "Final training Loss:  0.60012001\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59466827\n",
      "Training Loss:  0.59466827\n",
      "Training Loss:  0.59466827\n",
      "Training Loss:  0.59465832\n",
      "Training Loss:  0.59465814\n",
      "Training Loss:  0.59465814\n",
      "Training Loss:  0.59465814\n",
      "Training Loss:  0.59465814\n",
      "Training Loss:  0.59465814\n",
      "Training Loss:  0.59465814\n",
      "Final training Loss:  0.59465814\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Final training Loss:  500.99996948\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59765857\n",
      "Training Loss:  0.59765857\n",
      "Training Loss:  0.59765857\n",
      "Training Loss:  0.59765631\n",
      "Training Loss:  0.59765631\n",
      "Training Loss:  0.59765631\n",
      "Training Loss:  0.59765631\n",
      "Training Loss:  0.59765631\n",
      "Training Loss:  0.59765631\n",
      "Training Loss:  0.59765631\n",
      "Final training Loss:  0.59765631\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71990424\n",
      "Training Loss:  0.59725082\n",
      "Training Loss:  0.59725082\n",
      "Training Loss:  0.59725076\n",
      "Training Loss:  0.59725076\n",
      "Training Loss:  0.59725076\n",
      "Training Loss:  0.59725076\n",
      "Training Loss:  0.59725076\n",
      "Training Loss:  0.59725076\n",
      "Training Loss:  0.59725076\n",
      "Final training Loss:  0.59725076\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Training Loss:  0.60190225\n",
      "Final training Loss:  0.60190225\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63338882\n",
      "Training Loss:  0.63338882\n",
      "Training Loss:  0.63338882\n",
      "Training Loss:  0.63338882\n",
      "Training Loss:  0.63338882\n",
      "Training Loss:  0.6331591\n",
      "Training Loss:  0.6331591\n",
      "Training Loss:  0.6331591\n",
      "Training Loss:  0.6331591\n",
      "Training Loss:  0.6331591\n",
      "Final training Loss:  0.6331591\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Training Loss:  0.6203146\n",
      "Final training Loss:  0.6203146\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73011863\n",
      "Training Loss:  0.61765188\n",
      "Training Loss:  0.61765188\n",
      "Training Loss:  0.61765188\n",
      "Training Loss:  0.61765188\n",
      "Training Loss:  0.61765188\n",
      "Training Loss:  0.61765188\n",
      "Training Loss:  0.61765188\n",
      "Training Loss:  0.61765188\n",
      "Training Loss:  0.61765188\n",
      "Final training Loss:  0.61765188\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7284146\n",
      "Training Loss:  0.68068415\n",
      "Training Loss:  0.68068415\n",
      "Training Loss:  0.68068415\n",
      "Training Loss:  0.68068415\n",
      "Training Loss:  0.68068415\n",
      "Training Loss:  0.68068421\n",
      "Training Loss:  0.68068415\n",
      "Training Loss:  0.68061292\n",
      "Training Loss:  0.68061292\n",
      "Final training Loss:  0.68061292\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67436707\n",
      "Training Loss:  0.67436695\n",
      "Training Loss:  0.67436695\n",
      "Training Loss:  0.67436689\n",
      "Training Loss:  0.67436689\n",
      "Training Loss:  0.67436689\n",
      "Training Loss:  0.67436689\n",
      "Training Loss:  0.67436689\n",
      "Training Loss:  0.67436689\n",
      "Training Loss:  0.67436689\n",
      "Final training Loss:  0.67436689\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64246476\n",
      "Training Loss:  0.64246476\n",
      "Training Loss:  0.64243412\n",
      "Training Loss:  0.64238226\n",
      "Training Loss:  0.64238226\n",
      "Training Loss:  0.64238226\n",
      "Training Loss:  0.64238226\n",
      "Training Loss:  0.64238226\n",
      "Training Loss:  0.64238226\n",
      "Training Loss:  0.64238226\n",
      "Final training Loss:  0.64238226\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59870833\n",
      "Training Loss:  0.59870833\n",
      "Training Loss:  0.5987078\n",
      "Training Loss:  0.59870648\n",
      "Training Loss:  0.59870613\n",
      "Training Loss:  0.59870613\n",
      "Training Loss:  0.59870613\n",
      "Training Loss:  0.59870583\n",
      "Training Loss:  0.59870583\n",
      "Training Loss:  0.59870565\n",
      "Final training Loss:  0.59870565\n",
      "\n",
      "Running model (trial=1, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60817546\n",
      "Training Loss:  0.6081754\n",
      "Training Loss:  0.6081754\n",
      "Training Loss:  0.6081754\n",
      "Training Loss:  0.6081754\n",
      "Training Loss:  0.6081754\n",
      "Training Loss:  0.6081754\n",
      "Training Loss:  0.6081754\n",
      "Training Loss:  0.6081754\n",
      "Training Loss:  0.6081754\n",
      "Final training Loss:  0.6081754\n",
      "\n",
      "Running model (trial=1, mod=53, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59876502\n",
      "Training Loss:  0.59876484\n",
      "Training Loss:  0.59876484\n",
      "Training Loss:  0.59876484\n",
      "Training Loss:  0.59876484\n",
      "Training Loss:  0.59876484\n",
      "Training Loss:  0.59876484\n",
      "Training Loss:  0.59876484\n",
      "Training Loss:  0.59876484\n",
      "Training Loss:  0.59876484\n",
      "Final training Loss:  0.59876484\n",
      "\n",
      "Running model (trial=1, mod=54, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62752926\n",
      "Training Loss:  0.6258136\n",
      "Training Loss:  0.6258136\n",
      "Training Loss:  0.6258136\n",
      "Training Loss:  0.6258136\n",
      "Training Loss:  0.6258136\n",
      "Training Loss:  0.6258136\n",
      "Training Loss:  0.6258136\n",
      "Training Loss:  0.6258136\n",
      "Training Loss:  0.6258136\n",
      "Final training Loss:  0.6258136\n",
      "\n",
      "Running model (trial=1, mod=55, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Training Loss:  0.64127195\n",
      "Final training Loss:  0.64127195\n",
      "\n",
      "Running model (trial=2, mod=56, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Training Loss:  0.59656173\n",
      "Final training Loss:  0.59656173\n",
      "\n",
      "Running model (trial=2, mod=57, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59746248\n",
      "Training Loss:  0.597453\n",
      "Training Loss:  0.597453\n",
      "Training Loss:  0.597453\n",
      "Training Loss:  0.597453\n",
      "Training Loss:  0.597453\n",
      "Training Loss:  0.597453\n",
      "Training Loss:  0.597453\n",
      "Training Loss:  0.597453\n",
      "Training Loss:  0.597453\n",
      "Final training Loss:  0.597453\n",
      "\n",
      "Running model (trial=2, mod=58, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6152817\n",
      "Training Loss:  0.61528158\n",
      "Training Loss:  0.61528158\n",
      "Training Loss:  0.61528152\n",
      "Training Loss:  0.61528152\n",
      "Training Loss:  0.61528152\n",
      "Training Loss:  0.61528152\n",
      "Training Loss:  0.61528152\n",
      "Training Loss:  0.61528152\n",
      "Training Loss:  0.61528152\n",
      "Final training Loss:  0.61528152\n",
      "\n",
      "Running model (trial=2, mod=59, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Training Loss:  0.62383497\n",
      "Final training Loss:  0.62383497\n",
      "\n",
      "Running model (trial=2, mod=60, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59524423\n",
      "Training Loss:  0.59524423\n",
      "Training Loss:  0.59502572\n",
      "Training Loss:  0.59502572\n",
      "Training Loss:  0.59502572\n",
      "Training Loss:  0.59502572\n",
      "Training Loss:  0.59502572\n",
      "Training Loss:  0.59502578\n",
      "Training Loss:  0.59502578\n",
      "Training Loss:  0.59502566\n",
      "Final training Loss:  0.59502566\n",
      "\n",
      "Running model (trial=2, mod=61, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68989873\n",
      "Training Loss:  0.68974173\n",
      "Training Loss:  0.68974173\n",
      "Training Loss:  0.68974167\n",
      "Training Loss:  0.68974167\n",
      "Training Loss:  0.68974167\n",
      "Training Loss:  0.68974167\n",
      "Training Loss:  0.68974167\n",
      "Training Loss:  0.68974167\n",
      "Training Loss:  0.68974167\n",
      "Final training Loss:  0.68974167\n",
      "\n",
      "Running model (trial=2, mod=62, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Training Loss:  0.62834156\n",
      "Final training Loss:  0.62834156\n",
      "\n",
      "Running model (trial=2, mod=63, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Training Loss:  0.62506121\n",
      "Final training Loss:  0.62506121\n",
      "\n",
      "Running model (trial=2, mod=64, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64171726\n",
      "Training Loss:  0.64171726\n",
      "Training Loss:  0.64171726\n",
      "Training Loss:  0.64171726\n",
      "Training Loss:  0.61273384\n",
      "Training Loss:  0.61273384\n",
      "Training Loss:  0.61273384\n",
      "Training Loss:  0.61273384\n",
      "Training Loss:  0.61273384\n",
      "Training Loss:  0.61273384\n",
      "Final training Loss:  0.61273384\n",
      "\n",
      "Running model (trial=2, mod=65, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59534568\n",
      "Training Loss:  0.59534568\n",
      "Training Loss:  0.5953154\n",
      "Training Loss:  0.5953154\n",
      "Training Loss:  0.5953154\n",
      "Training Loss:  0.5953154\n",
      "Training Loss:  0.5953154\n",
      "Training Loss:  0.5953154\n",
      "Training Loss:  0.5953154\n",
      "Training Loss:  0.5953154\n",
      "Final training Loss:  0.5953154\n",
      "\n",
      "Running model (trial=2, mod=66, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60478133\n",
      "Training Loss:  0.59416038\n",
      "Training Loss:  0.59416038\n",
      "Training Loss:  0.59416038\n",
      "Training Loss:  0.59416038\n",
      "Training Loss:  0.59416038\n",
      "Training Loss:  0.59416038\n",
      "Training Loss:  0.59416038\n",
      "Training Loss:  0.59416038\n",
      "Training Loss:  0.59416038\n",
      "Final training Loss:  0.59416038\n",
      "\n",
      "Running model (trial=2, mod=67, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60512441\n",
      "Training Loss:  0.60512441\n",
      "Training Loss:  0.60512441\n",
      "Training Loss:  0.59773427\n",
      "Training Loss:  0.59773421\n",
      "Training Loss:  0.59773421\n",
      "Training Loss:  0.59773421\n",
      "Training Loss:  0.59773421\n",
      "Training Loss:  0.59773421\n",
      "Training Loss:  0.59773421\n",
      "Final training Loss:  0.59773421\n",
      "\n",
      "Running model (trial=2, mod=68, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60451722\n",
      "Training Loss:  0.60019267\n",
      "Training Loss:  0.60019267\n",
      "Training Loss:  0.60019267\n",
      "Training Loss:  0.60019267\n",
      "Training Loss:  0.60019267\n",
      "Training Loss:  0.5977385\n",
      "Training Loss:  0.5977385\n",
      "Training Loss:  0.5977385\n",
      "Training Loss:  0.5977385\n",
      "Final training Loss:  0.5977385\n",
      "\n",
      "Running model (trial=2, mod=69, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Final training Loss:  501.0\n",
      "\n",
      "Running model (trial=2, mod=70, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6220026\n",
      "Training Loss:  0.6220026\n",
      "Training Loss:  0.6220026\n",
      "Training Loss:  0.62200254\n",
      "Training Loss:  0.62200254\n",
      "Training Loss:  0.62200254\n",
      "Training Loss:  0.62200254\n",
      "Training Loss:  0.62200254\n",
      "Training Loss:  0.62200254\n",
      "Training Loss:  0.62200254\n",
      "Final training Loss:  0.62200254\n",
      "\n",
      "Running model (trial=2, mod=71, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60785127\n",
      "Training Loss:  0.60784978\n",
      "Training Loss:  0.60784978\n",
      "Training Loss:  0.60784978\n",
      "Training Loss:  0.60784978\n",
      "Training Loss:  0.60761994\n",
      "Training Loss:  0.60761994\n",
      "Training Loss:  0.60761994\n",
      "Training Loss:  0.60761994\n",
      "Training Loss:  0.60761994\n",
      "Final training Loss:  0.60761994\n",
      "\n",
      "Running model (trial=2, mod=72, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Training Loss:  0.60088784\n",
      "Final training Loss:  0.60088784\n",
      "\n",
      "Running model (trial=2, mod=73, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7256642\n",
      "Training Loss:  0.62043977\n",
      "Training Loss:  0.62043977\n",
      "Training Loss:  0.62043977\n",
      "Training Loss:  0.62043977\n",
      "Training Loss:  0.62043977\n",
      "Training Loss:  0.62043977\n",
      "Training Loss:  0.62043977\n",
      "Training Loss:  0.62043977\n",
      "Training Loss:  0.62043977\n",
      "Final training Loss:  0.62043977\n",
      "\n",
      "Running model (trial=2, mod=74, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Training Loss:  0.65853202\n",
      "Final training Loss:  0.65853202\n",
      "\n",
      "Running model (trial=2, mod=75, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Training Loss:  0.67717874\n",
      "Final training Loss:  0.67717874\n",
      "\n",
      "Running model (trial=2, mod=76, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.687738\n",
      "Training Loss:  0.68773782\n",
      "Training Loss:  0.68773782\n",
      "Training Loss:  0.68773782\n",
      "Training Loss:  0.68773782\n",
      "Training Loss:  0.68773782\n",
      "Training Loss:  0.68773782\n",
      "Training Loss:  0.68773782\n",
      "Training Loss:  0.68773782\n",
      "Training Loss:  0.68773782\n",
      "Final training Loss:  0.68773782\n",
      "\n",
      "Running model (trial=2, mod=77, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72976321\n",
      "Training Loss:  0.68278331\n",
      "Training Loss:  0.68278331\n",
      "Training Loss:  0.68278331\n",
      "Training Loss:  0.68278331\n",
      "Training Loss:  0.68278331\n",
      "Training Loss:  0.68278331\n",
      "Training Loss:  0.68278331\n",
      "Training Loss:  0.68278331\n",
      "Training Loss:  0.68278331\n",
      "Final training Loss:  0.68278331\n",
      "\n",
      "Running model (trial=2, mod=78, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Training Loss:  0.66575962\n",
      "Final training Loss:  0.66575962\n",
      "\n",
      "Running model (trial=2, mod=79, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64409667\n",
      "Training Loss:  0.64409667\n",
      "Training Loss:  0.64345157\n",
      "Training Loss:  0.64345157\n",
      "Training Loss:  0.64345157\n",
      "Training Loss:  0.64345157\n",
      "Training Loss:  0.64345157\n",
      "Training Loss:  0.64345157\n",
      "Training Loss:  0.64345157\n",
      "Training Loss:  0.64345157\n",
      "Final training Loss:  0.64345157\n",
      "\n",
      "Running model (trial=2, mod=80, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72444636\n",
      "Training Loss:  0.60081017\n",
      "Training Loss:  0.60081017\n",
      "Training Loss:  0.60081017\n",
      "Training Loss:  0.60081011\n",
      "Training Loss:  0.60080987\n",
      "Training Loss:  0.60080987\n",
      "Training Loss:  0.60080987\n",
      "Training Loss:  0.60080987\n",
      "Training Loss:  0.60080987\n",
      "Final training Loss:  0.60080987\n",
      "\n",
      "Running model (trial=2, mod=81, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62640589\n",
      "Training Loss:  0.62640589\n",
      "Training Loss:  0.62640589\n",
      "Training Loss:  0.62640589\n",
      "Training Loss:  0.62640589\n",
      "Training Loss:  0.62616962\n",
      "Training Loss:  0.62616962\n",
      "Training Loss:  0.62616962\n",
      "Training Loss:  0.62616962\n",
      "Training Loss:  0.62616962\n",
      "Final training Loss:  0.62616962\n",
      "\n",
      "Running model (trial=2, mod=82, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Training Loss:  0.6224345\n",
      "Final training Loss:  0.6224345\n",
      "\n",
      "Running model (trial=2, mod=83, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Training Loss:  0.67416483\n",
      "Final training Loss:  0.67416483\n",
      "\n",
      "Running model (trial=3, mod=84, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60344976\n",
      "Training Loss:  0.60344976\n",
      "Training Loss:  0.6034497\n",
      "Training Loss:  0.60344976\n",
      "Training Loss:  0.60344976\n",
      "Training Loss:  0.59715605\n",
      "Training Loss:  0.59715605\n",
      "Training Loss:  0.59715605\n",
      "Training Loss:  0.59715605\n",
      "Training Loss:  0.59715605\n",
      "Final training Loss:  0.59715605\n",
      "\n",
      "Running model (trial=3, mod=85, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5943681\n",
      "Training Loss:  0.59403241\n",
      "Training Loss:  0.59403241\n",
      "Training Loss:  0.59403241\n",
      "Training Loss:  0.59403241\n",
      "Training Loss:  0.59403241\n",
      "Training Loss:  0.59403241\n",
      "Training Loss:  0.59403241\n",
      "Training Loss:  0.59403241\n",
      "Training Loss:  0.59403241\n",
      "Final training Loss:  0.59403241\n",
      "\n",
      "Running model (trial=3, mod=86, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63125741\n",
      "Training Loss:  0.63125712\n",
      "Training Loss:  0.631257\n",
      "Training Loss:  0.63105351\n",
      "Training Loss:  0.63105351\n",
      "Training Loss:  0.63105351\n",
      "Training Loss:  0.63105351\n",
      "Training Loss:  0.63105351\n",
      "Training Loss:  0.63105351\n",
      "Training Loss:  0.63105351\n",
      "Final training Loss:  0.63105351\n",
      "\n",
      "Running model (trial=3, mod=87, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59410167\n",
      "Training Loss:  0.59410167\n",
      "Training Loss:  0.59400886\n",
      "Training Loss:  0.59400886\n",
      "Training Loss:  0.59400886\n",
      "Training Loss:  0.59400886\n",
      "Training Loss:  0.59400886\n",
      "Training Loss:  0.59400886\n",
      "Training Loss:  0.59400886\n",
      "Training Loss:  0.59400886\n",
      "Final training Loss:  0.59400886\n",
      "\n",
      "Running model (trial=3, mod=88, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60180837\n",
      "Training Loss:  0.60180831\n",
      "Training Loss:  0.60180831\n",
      "Training Loss:  0.60180831\n",
      "Training Loss:  0.60180831\n",
      "Training Loss:  0.60180831\n",
      "Training Loss:  0.60180831\n",
      "Training Loss:  0.60180831\n",
      "Training Loss:  0.60180831\n",
      "Training Loss:  0.60180831\n",
      "Final training Loss:  0.60180831\n",
      "\n",
      "Running model (trial=3, mod=89, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64045691\n",
      "Training Loss:  0.64045691\n",
      "Training Loss:  0.64045691\n",
      "Training Loss:  0.64045691\n",
      "Training Loss:  0.64039332\n",
      "Training Loss:  0.64039332\n",
      "Training Loss:  0.64039332\n",
      "Training Loss:  0.64039332\n",
      "Training Loss:  0.64039332\n",
      "Training Loss:  0.64039332\n",
      "Final training Loss:  0.64039332\n",
      "\n",
      "Running model (trial=3, mod=90, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64689654\n",
      "Training Loss:  0.64689654\n",
      "Training Loss:  0.64689648\n",
      "Training Loss:  0.64689648\n",
      "Training Loss:  0.64689648\n",
      "Training Loss:  0.64689648\n",
      "Training Loss:  0.64689654\n",
      "Training Loss:  0.64689654\n",
      "Training Loss:  0.64689654\n",
      "Training Loss:  0.64689654\n",
      "Final training Loss:  0.64689654\n",
      "\n",
      "Running model (trial=3, mod=91, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72702879\n",
      "Training Loss:  0.61249459\n",
      "Training Loss:  0.61249435\n",
      "Training Loss:  0.61249435\n",
      "Training Loss:  0.61246854\n",
      "Training Loss:  0.61246794\n",
      "Training Loss:  0.61246794\n",
      "Training Loss:  0.61246794\n",
      "Training Loss:  0.61246771\n",
      "Training Loss:  0.61246771\n",
      "Final training Loss:  0.61246771\n",
      "\n",
      "Running model (trial=3, mod=92, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59393251\n",
      "Training Loss:  0.59393197\n",
      "Training Loss:  0.59393197\n",
      "Training Loss:  0.59390831\n",
      "Training Loss:  0.59390795\n",
      "Training Loss:  0.59390795\n",
      "Training Loss:  0.59388137\n",
      "Training Loss:  0.59388107\n",
      "Training Loss:  0.59388107\n",
      "Training Loss:  0.59388089\n",
      "Final training Loss:  0.59388089\n",
      "\n",
      "Running model (trial=3, mod=93, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Training Loss:  0.59631395\n",
      "Final training Loss:  0.59631395\n",
      "\n",
      "Running model (trial=3, mod=94, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59394956\n",
      "Training Loss:  0.59393454\n",
      "Training Loss:  0.59393454\n",
      "Training Loss:  0.59393454\n",
      "Training Loss:  0.59393454\n",
      "Training Loss:  0.59393454\n",
      "Training Loss:  0.59393454\n",
      "Training Loss:  0.59393454\n",
      "Training Loss:  0.59393454\n",
      "Training Loss:  0.59393454\n",
      "Final training Loss:  0.59393454\n",
      "\n",
      "Running model (trial=3, mod=95, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Training Loss:  0.60417992\n",
      "Final training Loss:  0.60417992\n",
      "\n",
      "Running model (trial=3, mod=96, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60500818\n",
      "Training Loss:  0.60500818\n",
      "Training Loss:  0.60500818\n",
      "Training Loss:  0.60500807\n",
      "Training Loss:  0.6008479\n",
      "Training Loss:  0.6008479\n",
      "Training Loss:  0.6008479\n",
      "Training Loss:  0.6008479\n",
      "Training Loss:  0.6008479\n",
      "Training Loss:  0.6008479\n",
      "Final training Loss:  0.6008479\n",
      "\n",
      "Running model (trial=3, mod=97, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Final training Loss:  501.0\n",
      "\n",
      "Running model (trial=3, mod=98, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59632635\n",
      "Training Loss:  0.59620112\n",
      "Final training Loss:  0.59620112\n",
      "\n",
      "Running model (trial=3, mod=99, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Training Loss:  0.6626116\n",
      "Final training Loss:  0.6626116\n",
      "\n",
      "Running model (trial=3, mod=100, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60665429\n",
      "Training Loss:  0.60665429\n",
      "Training Loss:  0.60665429\n",
      "Training Loss:  0.60665429\n",
      "Training Loss:  0.60665423\n",
      "Training Loss:  0.60665423\n",
      "Training Loss:  0.60665423\n",
      "Training Loss:  0.60665423\n",
      "Training Loss:  0.60665423\n",
      "Training Loss:  0.60665417\n",
      "Final training Loss:  0.60665417\n",
      "\n",
      "Running model (trial=3, mod=101, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73041511\n",
      "Training Loss:  0.62376356\n",
      "Training Loss:  0.62376344\n",
      "Training Loss:  0.62376344\n",
      "Training Loss:  0.62376344\n",
      "Training Loss:  0.62376344\n",
      "Training Loss:  0.62376344\n",
      "Training Loss:  0.62376344\n",
      "Training Loss:  0.62376344\n",
      "Training Loss:  0.62376344\n",
      "Final training Loss:  0.62376344\n",
      "\n",
      "Running model (trial=3, mod=102, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7305395\n",
      "Training Loss:  0.61763859\n",
      "Training Loss:  0.61763859\n",
      "Training Loss:  0.61763859\n",
      "Training Loss:  0.61763859\n",
      "Training Loss:  0.61763859\n",
      "Training Loss:  0.61763859\n",
      "Training Loss:  0.61763859\n",
      "Training Loss:  0.61763859\n",
      "Training Loss:  0.61763859\n",
      "Final training Loss:  0.61763859\n",
      "\n",
      "Running model (trial=3, mod=103, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Training Loss:  0.66548747\n",
      "Final training Loss:  0.66548747\n",
      "\n",
      "Running model (trial=3, mod=104, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66732574\n",
      "Training Loss:  0.66732574\n",
      "Training Loss:  0.66732574\n",
      "Training Loss:  0.66732574\n",
      "Training Loss:  0.66732574\n",
      "Training Loss:  0.66732568\n",
      "Training Loss:  0.66732568\n",
      "Training Loss:  0.66732568\n",
      "Training Loss:  0.66732568\n",
      "Training Loss:  0.66732568\n",
      "Final training Loss:  0.66732568\n",
      "\n",
      "Running model (trial=3, mod=105, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Training Loss:  0.6827144\n",
      "Final training Loss:  0.6827144\n",
      "\n",
      "Running model (trial=3, mod=106, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72555643\n",
      "Training Loss:  0.63866538\n",
      "Training Loss:  0.63864905\n",
      "Training Loss:  0.63864905\n",
      "Training Loss:  0.63864905\n",
      "Training Loss:  0.63864905\n",
      "Training Loss:  0.63864905\n",
      "Training Loss:  0.63864905\n",
      "Training Loss:  0.63864905\n",
      "Training Loss:  0.63864905\n",
      "Final training Loss:  0.63864905\n",
      "\n",
      "Running model (trial=3, mod=107, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Training Loss:  0.65007478\n",
      "Final training Loss:  0.65007478\n",
      "\n",
      "Running model (trial=3, mod=108, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63051248\n",
      "Training Loss:  0.63051248\n",
      "Training Loss:  0.63051248\n",
      "Training Loss:  0.63051248\n",
      "Training Loss:  0.63051218\n",
      "Training Loss:  0.63051218\n",
      "Training Loss:  0.63051218\n",
      "Training Loss:  0.63051218\n",
      "Training Loss:  0.63051218\n",
      "Training Loss:  0.63051218\n",
      "Final training Loss:  0.63051218\n",
      "\n",
      "Running model (trial=3, mod=109, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Training Loss:  0.65125728\n",
      "Final training Loss:  0.65125728\n",
      "\n",
      "Running model (trial=3, mod=110, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Training Loss:  0.59167635\n",
      "Final training Loss:  0.59167635\n",
      "\n",
      "Running model (trial=3, mod=111, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Training Loss:  0.66667348\n",
      "Final training Loss:  0.66667348\n",
      "\n",
      "Running model (trial=4, mod=112, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60305613\n",
      "Training Loss:  0.60305613\n",
      "Training Loss:  0.60305613\n",
      "Training Loss:  0.60305613\n",
      "Training Loss:  0.60305613\n",
      "Training Loss:  0.60305613\n",
      "Training Loss:  0.60305613\n",
      "Training Loss:  0.60305625\n",
      "Training Loss:  0.59816647\n",
      "Training Loss:  0.59816647\n",
      "Final training Loss:  0.59816647\n",
      "\n",
      "Running model (trial=4, mod=113, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5959025\n",
      "Training Loss:  0.595873\n",
      "Training Loss:  0.595873\n",
      "Training Loss:  0.595873\n",
      "Training Loss:  0.595873\n",
      "Training Loss:  0.595873\n",
      "Training Loss:  0.595873\n",
      "Training Loss:  0.595873\n",
      "Training Loss:  0.595873\n",
      "Training Loss:  0.595873\n",
      "Final training Loss:  0.595873\n",
      "\n",
      "Running model (trial=4, mod=114, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Training Loss:  0.59674484\n",
      "Final training Loss:  0.59674484\n",
      "\n",
      "Running model (trial=4, mod=115, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59987676\n",
      "Training Loss:  0.5998767\n",
      "Training Loss:  0.5998767\n",
      "Training Loss:  0.5998767\n",
      "Training Loss:  0.5998767\n",
      "Training Loss:  0.5998767\n",
      "Training Loss:  0.5998767\n",
      "Training Loss:  0.5998767\n",
      "Training Loss:  0.5998767\n",
      "Training Loss:  0.5998767\n",
      "Final training Loss:  0.5998767\n",
      "\n",
      "Running model (trial=4, mod=116, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60608375\n",
      "Training Loss:  0.60608375\n",
      "Training Loss:  0.60608375\n",
      "Training Loss:  0.60608375\n",
      "Training Loss:  0.60608375\n",
      "Training Loss:  0.60608363\n",
      "Training Loss:  0.60608363\n",
      "Training Loss:  0.60608363\n",
      "Training Loss:  0.60608363\n",
      "Training Loss:  0.60608363\n",
      "Final training Loss:  0.60608363\n",
      "\n",
      "Running model (trial=4, mod=117, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Training Loss:  0.65794748\n",
      "Final training Loss:  0.65794748\n",
      "\n",
      "Running model (trial=4, mod=118, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72635788\n",
      "Training Loss:  0.66294187\n",
      "Training Loss:  0.66294187\n",
      "Training Loss:  0.66294187\n",
      "Training Loss:  0.66294187\n",
      "Training Loss:  0.66294187\n",
      "Training Loss:  0.66294187\n",
      "Training Loss:  0.66294187\n",
      "Training Loss:  0.66294187\n",
      "Training Loss:  0.66294187\n",
      "Final training Loss:  0.66294187\n",
      "\n",
      "Running model (trial=4, mod=119, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68988347\n",
      "Training Loss:  0.68988353\n",
      "Training Loss:  0.68988353\n",
      "Training Loss:  0.68723518\n",
      "Training Loss:  0.68723518\n",
      "Training Loss:  0.68723518\n",
      "Training Loss:  0.68723518\n",
      "Training Loss:  0.68723518\n",
      "Training Loss:  0.68723518\n",
      "Training Loss:  0.68723518\n",
      "Final training Loss:  0.68723518\n",
      "\n",
      "Running model (trial=4, mod=120, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Training Loss:  0.59610152\n",
      "Final training Loss:  0.59610152\n",
      "\n",
      "Running model (trial=4, mod=121, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Training Loss:  0.59460354\n",
      "Final training Loss:  0.59460354\n",
      "\n",
      "Running model (trial=4, mod=122, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59885907\n",
      "Training Loss:  0.59885907\n",
      "Training Loss:  0.59885895\n",
      "Training Loss:  0.59885895\n",
      "Training Loss:  0.59885895\n",
      "Training Loss:  0.59885889\n",
      "Training Loss:  0.59885883\n",
      "Training Loss:  0.59885877\n",
      "Training Loss:  0.59885877\n",
      "Training Loss:  0.59885877\n",
      "Final training Loss:  0.59885877\n",
      "\n",
      "Running model (trial=4, mod=123, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60590392\n",
      "Training Loss:  0.60590398\n",
      "Training Loss:  0.59685194\n",
      "Training Loss:  0.59659743\n",
      "Training Loss:  0.59659743\n",
      "Training Loss:  0.59659743\n",
      "Training Loss:  0.59659743\n",
      "Training Loss:  0.59659743\n",
      "Training Loss:  0.59659743\n",
      "Training Loss:  0.59659743\n",
      "Final training Loss:  0.59659743\n",
      "\n",
      "Running model (trial=4, mod=124, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60811013\n",
      "Training Loss:  0.59776044\n",
      "Training Loss:  0.59776044\n",
      "Training Loss:  0.59776044\n",
      "Training Loss:  0.59776044\n",
      "Training Loss:  0.59773004\n",
      "Training Loss:  0.59771067\n",
      "Training Loss:  0.59771067\n",
      "Training Loss:  0.59771067\n",
      "Training Loss:  0.59771067\n",
      "Final training Loss:  0.59771067\n",
      "\n",
      "Running model (trial=4, mod=125, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Final training Loss:  501.0\n",
      "\n",
      "Running model (trial=4, mod=126, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Training Loss:  0.6062153\n",
      "Final training Loss:  0.6062153\n",
      "\n",
      "Running model (trial=4, mod=127, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65505588\n",
      "Training Loss:  0.65499282\n",
      "Training Loss:  0.65499282\n",
      "Training Loss:  0.65499282\n",
      "Training Loss:  0.65499282\n",
      "Training Loss:  0.6549924\n",
      "Training Loss:  0.65498769\n",
      "Training Loss:  0.65497094\n",
      "Training Loss:  0.65496993\n",
      "Training Loss:  0.65496993\n",
      "Final training Loss:  0.65496993\n",
      "\n",
      "Running model (trial=4, mod=128, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65495801\n",
      "Training Loss:  0.65495801\n",
      "Training Loss:  0.65495104\n",
      "Training Loss:  0.65495104\n",
      "Training Loss:  0.65495104\n",
      "Training Loss:  0.65495104\n",
      "Training Loss:  0.65495104\n",
      "Training Loss:  0.65495104\n",
      "Training Loss:  0.65495104\n",
      "Training Loss:  0.65495104\n",
      "Final training Loss:  0.65495104\n",
      "\n",
      "Running model (trial=4, mod=129, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Training Loss:  0.6168707\n",
      "Final training Loss:  0.6168707\n",
      "\n",
      "Running model (trial=4, mod=130, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Training Loss:  0.65352839\n",
      "Final training Loss:  0.65352839\n",
      "\n",
      "Running model (trial=4, mod=131, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63891155\n",
      "Training Loss:  0.63891155\n",
      "Training Loss:  0.63891125\n",
      "Training Loss:  0.63891125\n",
      "Training Loss:  0.63891125\n",
      "Training Loss:  0.63891125\n",
      "Training Loss:  0.63891125\n",
      "Training Loss:  0.63891125\n",
      "Training Loss:  0.63891125\n",
      "Training Loss:  0.63891125\n",
      "Final training Loss:  0.63891125\n",
      "\n",
      "Running model (trial=4, mod=132, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66952246\n",
      "Training Loss:  0.66952217\n",
      "Training Loss:  0.66952217\n",
      "Training Loss:  0.66952217\n",
      "Training Loss:  0.66952217\n",
      "Training Loss:  0.66952217\n",
      "Training Loss:  0.66952217\n",
      "Training Loss:  0.66952217\n",
      "Training Loss:  0.66952217\n",
      "Training Loss:  0.66952217\n",
      "Final training Loss:  0.66952217\n",
      "\n",
      "Running model (trial=4, mod=133, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65009338\n",
      "Training Loss:  0.65009344\n",
      "Training Loss:  0.65009344\n",
      "Training Loss:  0.65009338\n",
      "Training Loss:  0.65009338\n",
      "Training Loss:  0.65009338\n",
      "Training Loss:  0.65009338\n",
      "Training Loss:  0.65009338\n",
      "Training Loss:  0.65009338\n",
      "Training Loss:  0.65009338\n",
      "Final training Loss:  0.65009338\n",
      "\n",
      "Running model (trial=4, mod=134, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68536574\n",
      "Training Loss:  0.68002844\n",
      "Training Loss:  0.68002844\n",
      "Training Loss:  0.68002844\n",
      "Training Loss:  0.68002844\n",
      "Training Loss:  0.68002844\n",
      "Training Loss:  0.68002844\n",
      "Training Loss:  0.68002844\n",
      "Training Loss:  0.68002844\n",
      "Training Loss:  0.68002844\n",
      "Final training Loss:  0.68002844\n",
      "\n",
      "Running model (trial=4, mod=135, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73215115\n",
      "Training Loss:  0.60999846\n",
      "Training Loss:  0.60999846\n",
      "Training Loss:  0.60999846\n",
      "Training Loss:  0.60999846\n",
      "Training Loss:  0.60999846\n",
      "Training Loss:  0.60999846\n",
      "Training Loss:  0.60999846\n",
      "Training Loss:  0.60999846\n",
      "Training Loss:  0.60999846\n",
      "Final training Loss:  0.60999846\n",
      "\n",
      "Running model (trial=4, mod=136, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5942136\n",
      "Training Loss:  0.5942136\n",
      "Training Loss:  0.5942136\n",
      "Training Loss:  0.5942136\n",
      "Training Loss:  0.5942136\n",
      "Training Loss:  0.5942136\n",
      "Training Loss:  0.5942136\n",
      "Training Loss:  0.59410328\n",
      "Training Loss:  0.59410328\n",
      "Training Loss:  0.59410328\n",
      "Final training Loss:  0.59410328\n",
      "\n",
      "Running model (trial=4, mod=137, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Training Loss:  0.65384364\n",
      "Final training Loss:  0.65384364\n",
      "\n",
      "Running model (trial=4, mod=138, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Training Loss:  0.64539361\n",
      "Final training Loss:  0.64539361\n",
      "\n",
      "Running model (trial=4, mod=139, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Training Loss:  0.60020828\n",
      "Final training Loss:  0.60020828\n",
      "\n",
      "Running model (trial=5, mod=140, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60215569\n",
      "Training Loss:  0.60215557\n",
      "Training Loss:  0.59676504\n",
      "Training Loss:  0.59676498\n",
      "Training Loss:  0.59673786\n",
      "Training Loss:  0.59673786\n",
      "Training Loss:  0.59673786\n",
      "Training Loss:  0.59673786\n",
      "Training Loss:  0.5967378\n",
      "Training Loss:  0.5967378\n",
      "Final training Loss:  0.5967378\n",
      "\n",
      "Running model (trial=5, mod=141, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59633386\n",
      "Training Loss:  0.59633386\n",
      "Training Loss:  0.59633386\n",
      "Training Loss:  0.59633386\n",
      "Training Loss:  0.59633386\n",
      "Training Loss:  0.59629065\n",
      "Training Loss:  0.59629065\n",
      "Training Loss:  0.59629065\n",
      "Training Loss:  0.59629065\n",
      "Training Loss:  0.59629065\n",
      "Final training Loss:  0.59629065\n",
      "\n",
      "Running model (trial=5, mod=142, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Training Loss:  0.59410375\n",
      "Final training Loss:  0.59410375\n",
      "\n",
      "Running model (trial=5, mod=143, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60260737\n",
      "Training Loss:  0.6025368\n",
      "Training Loss:  0.6025368\n",
      "Training Loss:  0.6025368\n",
      "Training Loss:  0.6025368\n",
      "Training Loss:  0.6025368\n",
      "Training Loss:  0.6025368\n",
      "Training Loss:  0.6025368\n",
      "Training Loss:  0.6025368\n",
      "Training Loss:  0.6025368\n",
      "Final training Loss:  0.6025368\n",
      "\n",
      "Running model (trial=5, mod=144, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Training Loss:  0.59431261\n",
      "Final training Loss:  0.59431261\n",
      "\n",
      "Running model (trial=5, mod=145, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Training Loss:  0.66933244\n",
      "Final training Loss:  0.66933244\n",
      "\n",
      "Running model (trial=5, mod=146, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Training Loss:  0.6317696\n",
      "Final training Loss:  0.6317696\n",
      "\n",
      "Running model (trial=5, mod=147, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72611821\n",
      "Training Loss:  0.64852661\n",
      "Training Loss:  0.64662671\n",
      "Training Loss:  0.64662653\n",
      "Training Loss:  0.64662653\n",
      "Training Loss:  0.64662653\n",
      "Training Loss:  0.64662653\n",
      "Training Loss:  0.64662653\n",
      "Training Loss:  0.64662653\n",
      "Training Loss:  0.64662653\n",
      "Final training Loss:  0.64662653\n",
      "\n",
      "Running model (trial=5, mod=148, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64008069\n",
      "Training Loss:  0.64008069\n",
      "Training Loss:  0.64008063\n",
      "Training Loss:  0.64008063\n",
      "Training Loss:  0.64008063\n",
      "Training Loss:  0.64008063\n",
      "Training Loss:  0.64008063\n",
      "Training Loss:  0.64008063\n",
      "Training Loss:  0.64008063\n",
      "Training Loss:  0.64008063\n",
      "Final training Loss:  0.64008063\n",
      "\n",
      "Running model (trial=5, mod=149, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.5933944\n",
      "Training Loss:  0.59336954\n",
      "Final training Loss:  0.59336954\n",
      "\n",
      "Running model (trial=5, mod=150, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60490048\n",
      "Training Loss:  0.60490054\n",
      "Training Loss:  0.60060936\n",
      "Training Loss:  0.60060936\n",
      "Training Loss:  0.60060936\n",
      "Training Loss:  0.60060936\n",
      "Training Loss:  0.60060936\n",
      "Training Loss:  0.60060936\n",
      "Training Loss:  0.60060936\n",
      "Training Loss:  0.60060936\n",
      "Final training Loss:  0.60060936\n",
      "\n",
      "Running model (trial=5, mod=151, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Training Loss:  0.60373354\n",
      "Final training Loss:  0.60373354\n",
      "\n",
      "Running model (trial=5, mod=152, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99856567\n",
      "Training Loss:  0.59451169\n",
      "Training Loss:  0.59451169\n",
      "Training Loss:  0.59451169\n",
      "Training Loss:  0.59451169\n",
      "Training Loss:  0.59451169\n",
      "Training Loss:  0.59362078\n",
      "Training Loss:  0.59362078\n",
      "Training Loss:  0.59362078\n",
      "Training Loss:  0.5935725\n",
      "Final training Loss:  0.5935725\n",
      "\n",
      "Running model (trial=5, mod=153, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Final training Loss:  501.0\n",
      "\n",
      "Running model (trial=5, mod=154, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59389788\n",
      "Training Loss:  0.59377736\n",
      "Training Loss:  0.59372401\n",
      "Training Loss:  0.59367317\n",
      "Training Loss:  0.59367317\n",
      "Training Loss:  0.59367317\n",
      "Training Loss:  0.59367317\n",
      "Training Loss:  0.59367317\n",
      "Training Loss:  0.59367317\n",
      "Training Loss:  0.59365302\n",
      "Final training Loss:  0.59365302\n",
      "\n",
      "Running model (trial=5, mod=155, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Training Loss:  0.59393877\n",
      "Final training Loss:  0.59393877\n",
      "\n",
      "Running model (trial=5, mod=156, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62849462\n",
      "Training Loss:  0.62849462\n",
      "Training Loss:  0.62849462\n",
      "Training Loss:  0.62849462\n",
      "Training Loss:  0.62849432\n",
      "Training Loss:  0.62849432\n",
      "Training Loss:  0.62849432\n",
      "Training Loss:  0.62849432\n",
      "Training Loss:  0.62849432\n",
      "Training Loss:  0.62849432\n",
      "Final training Loss:  0.62849432\n",
      "\n",
      "Running model (trial=5, mod=157, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Training Loss:  0.64680314\n",
      "Final training Loss:  0.64680314\n",
      "\n",
      "Running model (trial=5, mod=158, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Training Loss:  0.60966033\n",
      "Final training Loss:  0.60966033\n",
      "\n",
      "Running model (trial=5, mod=159, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Training Loss:  0.64495867\n",
      "Final training Loss:  0.64495867\n",
      "\n",
      "Running model (trial=5, mod=160, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73150986\n",
      "Training Loss:  0.69229114\n",
      "Training Loss:  0.69229114\n",
      "Training Loss:  0.69229102\n",
      "Training Loss:  0.69229102\n",
      "Training Loss:  0.69229102\n",
      "Training Loss:  0.69229102\n",
      "Training Loss:  0.69229102\n",
      "Training Loss:  0.69229102\n",
      "Training Loss:  0.69229102\n",
      "Final training Loss:  0.69229102\n",
      "\n",
      "Running model (trial=5, mod=161, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6849947\n",
      "Training Loss:  0.6849947\n",
      "Training Loss:  0.68496197\n",
      "Training Loss:  0.68496197\n",
      "Training Loss:  0.68496197\n",
      "Training Loss:  0.68496197\n",
      "Training Loss:  0.68496197\n",
      "Training Loss:  0.68496197\n",
      "Training Loss:  0.68496197\n",
      "Training Loss:  0.68496197\n",
      "Final training Loss:  0.68496197\n",
      "\n",
      "Running model (trial=5, mod=162, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Training Loss:  0.60654026\n",
      "Final training Loss:  0.60654026\n",
      "\n",
      "Running model (trial=5, mod=163, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73006356\n",
      "Training Loss:  0.66677171\n",
      "Training Loss:  0.66677171\n",
      "Training Loss:  0.66677171\n",
      "Training Loss:  0.66677171\n",
      "Training Loss:  0.66677171\n",
      "Training Loss:  0.66676545\n",
      "Training Loss:  0.66676545\n",
      "Training Loss:  0.66676545\n",
      "Training Loss:  0.66676545\n",
      "Final training Loss:  0.66676545\n",
      "\n",
      "Running model (trial=5, mod=164, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Training Loss:  0.59813541\n",
      "Final training Loss:  0.59813541\n",
      "\n",
      "Running model (trial=5, mod=165, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67562205\n",
      "Training Loss:  0.67562205\n",
      "Training Loss:  0.67562205\n",
      "Training Loss:  0.67562205\n",
      "Training Loss:  0.67562205\n",
      "Training Loss:  0.67562205\n",
      "Training Loss:  0.67562205\n",
      "Training Loss:  0.67562205\n",
      "Training Loss:  0.67538184\n",
      "Training Loss:  0.67538184\n",
      "Final training Loss:  0.67538184\n",
      "\n",
      "Running model (trial=5, mod=166, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Training Loss:  0.59486365\n",
      "Final training Loss:  0.59486365\n",
      "\n",
      "Running model (trial=5, mod=167, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Training Loss:  0.63104564\n",
      "Final training Loss:  0.63104564\n",
      "\n",
      "Running model (trial=6, mod=168, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60145897\n",
      "Training Loss:  0.59778184\n",
      "Training Loss:  0.59778184\n",
      "Training Loss:  0.59778184\n",
      "Training Loss:  0.59778184\n",
      "Training Loss:  0.59772992\n",
      "Training Loss:  0.59772992\n",
      "Training Loss:  0.59772992\n",
      "Training Loss:  0.59772992\n",
      "Training Loss:  0.59772992\n",
      "Final training Loss:  0.59772992\n",
      "\n",
      "Running model (trial=6, mod=169, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60319591\n",
      "Training Loss:  0.60319591\n",
      "Training Loss:  0.59770769\n",
      "Training Loss:  0.59770769\n",
      "Training Loss:  0.59770769\n",
      "Training Loss:  0.59770769\n",
      "Training Loss:  0.59770769\n",
      "Training Loss:  0.59770769\n",
      "Training Loss:  0.59770769\n",
      "Training Loss:  0.59770769\n",
      "Final training Loss:  0.59770769\n",
      "\n",
      "Running model (trial=6, mod=170, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Training Loss:  0.60318547\n",
      "Final training Loss:  0.60318547\n",
      "\n",
      "Running model (trial=6, mod=171, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6151368\n",
      "Training Loss:  0.61506784\n",
      "Training Loss:  0.61506784\n",
      "Training Loss:  0.61506784\n",
      "Training Loss:  0.61506784\n",
      "Training Loss:  0.61506784\n",
      "Training Loss:  0.61506784\n",
      "Training Loss:  0.61506784\n",
      "Training Loss:  0.61506784\n",
      "Training Loss:  0.61506784\n",
      "Final training Loss:  0.61506784\n",
      "\n",
      "Running model (trial=6, mod=172, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Training Loss:  0.63698441\n",
      "Final training Loss:  0.63698441\n",
      "\n",
      "Running model (trial=6, mod=173, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64295858\n",
      "Training Loss:  0.64295858\n",
      "Training Loss:  0.64295858\n",
      "Training Loss:  0.64205229\n",
      "Training Loss:  0.64197624\n",
      "Training Loss:  0.64185816\n",
      "Training Loss:  0.64185816\n",
      "Training Loss:  0.64185816\n",
      "Training Loss:  0.64185816\n",
      "Training Loss:  0.64185816\n",
      "Final training Loss:  0.64185816\n",
      "\n",
      "Running model (trial=6, mod=174, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Training Loss:  0.64743483\n",
      "Final training Loss:  0.64743483\n",
      "\n",
      "Running model (trial=6, mod=175, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60484952\n",
      "Training Loss:  0.60473812\n",
      "Training Loss:  0.60473812\n",
      "Training Loss:  0.60473812\n",
      "Training Loss:  0.60462546\n",
      "Training Loss:  0.60462546\n",
      "Training Loss:  0.60462517\n",
      "Training Loss:  0.60462499\n",
      "Training Loss:  0.60462499\n",
      "Training Loss:  0.60462499\n",
      "Final training Loss:  0.60462499\n",
      "\n",
      "Running model (trial=6, mod=176, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Training Loss:  0.65098566\n",
      "Final training Loss:  0.65098566\n",
      "\n",
      "Running model (trial=6, mod=177, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60615426\n",
      "Training Loss:  0.60615414\n",
      "Training Loss:  0.60615414\n",
      "Training Loss:  0.60615414\n",
      "Training Loss:  0.60615414\n",
      "Training Loss:  0.60615414\n",
      "Training Loss:  0.60615414\n",
      "Training Loss:  0.60615414\n",
      "Training Loss:  0.60615414\n",
      "Training Loss:  0.60615414\n",
      "Final training Loss:  0.60615414\n",
      "\n",
      "Running model (trial=6, mod=178, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59852844\n",
      "Training Loss:  0.59852844\n",
      "Training Loss:  0.59852844\n",
      "Training Loss:  0.59852844\n",
      "Training Loss:  0.59852844\n",
      "Training Loss:  0.59852844\n",
      "Training Loss:  0.59852844\n",
      "Training Loss:  0.59852844\n",
      "Training Loss:  0.59834468\n",
      "Training Loss:  0.59834462\n",
      "Final training Loss:  0.59834462\n",
      "\n",
      "Running model (trial=6, mod=179, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59915692\n",
      "Training Loss:  0.5991568\n",
      "Training Loss:  0.5991568\n",
      "Training Loss:  0.5991568\n",
      "Training Loss:  0.59839135\n",
      "Training Loss:  0.59839135\n",
      "Training Loss:  0.59839135\n",
      "Training Loss:  0.59839135\n",
      "Training Loss:  0.59839135\n",
      "Training Loss:  0.59839135\n",
      "Final training Loss:  0.59839135\n",
      "\n",
      "Running model (trial=6, mod=180, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.6044336\n",
      "Training Loss:  0.60443366\n",
      "Final training Loss:  0.60443366\n",
      "\n",
      "Running model (trial=6, mod=181, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Final training Loss:  501.0\n",
      "\n",
      "Running model (trial=6, mod=182, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59900934\n",
      "Training Loss:  0.59898525\n",
      "Training Loss:  0.59898525\n",
      "Training Loss:  0.59888005\n",
      "Training Loss:  0.59888005\n",
      "Training Loss:  0.59888005\n",
      "Training Loss:  0.59888005\n",
      "Training Loss:  0.59888005\n",
      "Training Loss:  0.59888005\n",
      "Training Loss:  0.59888005\n",
      "Final training Loss:  0.59888005\n",
      "\n",
      "Running model (trial=6, mod=183, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Training Loss:  0.60442579\n",
      "Final training Loss:  0.60442579\n",
      "\n",
      "Running model (trial=6, mod=184, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73248136\n",
      "Training Loss:  0.71458197\n",
      "Training Loss:  0.71458197\n",
      "Training Loss:  0.71458197\n",
      "Training Loss:  0.71394473\n",
      "Training Loss:  0.71394473\n",
      "Training Loss:  0.71394473\n",
      "Training Loss:  0.71394473\n",
      "Training Loss:  0.71394473\n",
      "Training Loss:  0.71394473\n",
      "Final training Loss:  0.71394473\n",
      "\n",
      "Running model (trial=6, mod=185, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6569832\n",
      "Training Loss:  0.65695012\n",
      "Training Loss:  0.65695012\n",
      "Training Loss:  0.65695012\n",
      "Training Loss:  0.65694892\n",
      "Training Loss:  0.65694892\n",
      "Training Loss:  0.65694892\n",
      "Training Loss:  0.65694875\n",
      "Training Loss:  0.65691465\n",
      "Training Loss:  0.65691465\n",
      "Final training Loss:  0.65691465\n",
      "\n",
      "Running model (trial=6, mod=186, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73304385\n",
      "Training Loss:  0.6653651\n",
      "Training Loss:  0.6653651\n",
      "Training Loss:  0.6653651\n",
      "Training Loss:  0.6653651\n",
      "Training Loss:  0.66536504\n",
      "Training Loss:  0.66536504\n",
      "Training Loss:  0.66536504\n",
      "Training Loss:  0.66536492\n",
      "Training Loss:  0.66536492\n",
      "Final training Loss:  0.66536492\n",
      "\n",
      "Running model (trial=6, mod=187, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69907343\n",
      "Training Loss:  0.69907343\n",
      "Training Loss:  0.69878983\n",
      "Training Loss:  0.69878966\n",
      "Training Loss:  0.69875681\n",
      "Training Loss:  0.69875669\n",
      "Training Loss:  0.69873482\n",
      "Training Loss:  0.69873482\n",
      "Training Loss:  0.69873482\n",
      "Training Loss:  0.69873482\n",
      "Final training Loss:  0.69873482\n",
      "\n",
      "Running model (trial=6, mod=188, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66585523\n",
      "Training Loss:  0.66569722\n",
      "Training Loss:  0.66569722\n",
      "Training Loss:  0.66569722\n",
      "Training Loss:  0.66569722\n",
      "Training Loss:  0.66569722\n",
      "Training Loss:  0.66569722\n",
      "Training Loss:  0.66569722\n",
      "Training Loss:  0.66569722\n",
      "Training Loss:  0.66569722\n",
      "Final training Loss:  0.66569722\n",
      "\n",
      "Running model (trial=6, mod=189, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72817391\n",
      "Training Loss:  0.68032068\n",
      "Training Loss:  0.68032068\n",
      "Training Loss:  0.68032068\n",
      "Training Loss:  0.68032068\n",
      "Training Loss:  0.68032068\n",
      "Training Loss:  0.68032068\n",
      "Training Loss:  0.68032068\n",
      "Training Loss:  0.68032068\n",
      "Training Loss:  0.68032068\n",
      "Final training Loss:  0.68032068\n",
      "\n",
      "Running model (trial=6, mod=190, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73616457\n",
      "Training Loss:  0.73616463\n",
      "Training Loss:  0.66033465\n",
      "Training Loss:  0.66033465\n",
      "Training Loss:  0.66033465\n",
      "Training Loss:  0.66033465\n",
      "Training Loss:  0.66033465\n",
      "Training Loss:  0.66033465\n",
      "Training Loss:  0.66033465\n",
      "Training Loss:  0.66033465\n",
      "Final training Loss:  0.66033465\n",
      "\n",
      "Running model (trial=6, mod=191, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69534189\n",
      "Training Loss:  0.69534189\n",
      "Training Loss:  0.69534189\n",
      "Training Loss:  0.69457179\n",
      "Training Loss:  0.69457179\n",
      "Training Loss:  0.69457179\n",
      "Training Loss:  0.69419277\n",
      "Training Loss:  0.69419277\n",
      "Training Loss:  0.69419277\n",
      "Training Loss:  0.69419277\n",
      "Final training Loss:  0.69419277\n",
      "\n",
      "Running model (trial=6, mod=192, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Training Loss:  0.65209019\n",
      "Final training Loss:  0.65209019\n",
      "\n",
      "Running model (trial=6, mod=193, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68392992\n",
      "Training Loss:  0.68392992\n",
      "Training Loss:  0.6837709\n",
      "Training Loss:  0.6837709\n",
      "Training Loss:  0.68377095\n",
      "Training Loss:  0.68377084\n",
      "Training Loss:  0.68377084\n",
      "Training Loss:  0.68377084\n",
      "Training Loss:  0.68377084\n",
      "Training Loss:  0.68377084\n",
      "Final training Loss:  0.68377084\n",
      "\n",
      "Running model (trial=6, mod=194, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.610183\n",
      "Training Loss:  0.61018264\n",
      "Training Loss:  0.61018264\n",
      "Training Loss:  0.61018264\n",
      "Training Loss:  0.61018264\n",
      "Training Loss:  0.61018264\n",
      "Training Loss:  0.61018264\n",
      "Training Loss:  0.61018264\n",
      "Training Loss:  0.61018264\n",
      "Training Loss:  0.61018264\n",
      "Final training Loss:  0.61018264\n",
      "\n",
      "Running model (trial=6, mod=195, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Training Loss:  0.67590719\n",
      "Final training Loss:  0.67590719\n",
      "\n",
      "Running model (trial=7, mod=196, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60330462\n",
      "Training Loss:  0.60330451\n",
      "Training Loss:  0.60330451\n",
      "Training Loss:  0.60330451\n",
      "Training Loss:  0.60330451\n",
      "Training Loss:  0.60330451\n",
      "Training Loss:  0.60287601\n",
      "Training Loss:  0.60287601\n",
      "Training Loss:  0.60287601\n",
      "Training Loss:  0.60287607\n",
      "Final training Loss:  0.60287607\n",
      "\n",
      "Running model (trial=7, mod=197, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59386069\n",
      "Training Loss:  0.59386069\n",
      "Training Loss:  0.59386069\n",
      "Training Loss:  0.59386069\n",
      "Training Loss:  0.59386069\n",
      "Training Loss:  0.59386069\n",
      "Training Loss:  0.59384525\n",
      "Training Loss:  0.59383875\n",
      "Training Loss:  0.59383875\n",
      "Training Loss:  0.59383875\n",
      "Final training Loss:  0.59383875\n",
      "\n",
      "Running model (trial=7, mod=198, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59846145\n",
      "Training Loss:  0.59846145\n",
      "Training Loss:  0.59838051\n",
      "Training Loss:  0.59838051\n",
      "Training Loss:  0.59838051\n",
      "Training Loss:  0.59838051\n",
      "Training Loss:  0.59838051\n",
      "Training Loss:  0.59838051\n",
      "Training Loss:  0.59838051\n",
      "Training Loss:  0.59838051\n",
      "Final training Loss:  0.59838051\n",
      "\n",
      "Running model (trial=7, mod=199, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Training Loss:  0.60868478\n",
      "Final training Loss:  0.60868478\n",
      "\n",
      "Running model (trial=7, mod=200, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73120171\n",
      "Training Loss:  0.61647165\n",
      "Training Loss:  0.61647165\n",
      "Training Loss:  0.61647165\n",
      "Training Loss:  0.61647165\n",
      "Training Loss:  0.61647165\n",
      "Training Loss:  0.61647165\n",
      "Training Loss:  0.61647165\n",
      "Training Loss:  0.61647165\n",
      "Training Loss:  0.61647165\n",
      "Final training Loss:  0.61647165\n",
      "\n",
      "Running model (trial=7, mod=201, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66810036\n",
      "Training Loss:  0.66810036\n",
      "Training Loss:  0.66810036\n",
      "Training Loss:  0.66810036\n",
      "Training Loss:  0.66809994\n",
      "Training Loss:  0.66809994\n",
      "Training Loss:  0.66809994\n",
      "Training Loss:  0.66809994\n",
      "Training Loss:  0.66809994\n",
      "Training Loss:  0.66809905\n",
      "Final training Loss:  0.66809905\n",
      "\n",
      "Running model (trial=7, mod=202, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63202876\n",
      "Training Loss:  0.63177669\n",
      "Training Loss:  0.63177669\n",
      "Training Loss:  0.63177669\n",
      "Training Loss:  0.63177669\n",
      "Training Loss:  0.63177669\n",
      "Training Loss:  0.63177669\n",
      "Training Loss:  0.63166517\n",
      "Training Loss:  0.63166517\n",
      "Training Loss:  0.63166517\n",
      "Final training Loss:  0.63166517\n",
      "\n",
      "Running model (trial=7, mod=203, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6919598\n",
      "Training Loss:  0.6919598\n",
      "Training Loss:  0.6919598\n",
      "Training Loss:  0.69190025\n",
      "Training Loss:  0.69190025\n",
      "Training Loss:  0.69190025\n",
      "Training Loss:  0.69190025\n",
      "Training Loss:  0.69190025\n",
      "Training Loss:  0.69190001\n",
      "Training Loss:  0.69190007\n",
      "Final training Loss:  0.69190007\n",
      "\n",
      "Running model (trial=7, mod=204, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61034942\n",
      "Training Loss:  0.60990155\n",
      "Training Loss:  0.60990155\n",
      "Training Loss:  0.60990155\n",
      "Training Loss:  0.60990155\n",
      "Training Loss:  0.60990155\n",
      "Training Loss:  0.60990155\n",
      "Training Loss:  0.60990155\n",
      "Training Loss:  0.60990155\n",
      "Training Loss:  0.60990155\n",
      "Final training Loss:  0.60990155\n",
      "\n",
      "Running model (trial=7, mod=205, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60785139\n",
      "Training Loss:  0.60785139\n",
      "Training Loss:  0.60785139\n",
      "Training Loss:  0.60785139\n",
      "Training Loss:  0.60777336\n",
      "Training Loss:  0.60774028\n",
      "Training Loss:  0.60774028\n",
      "Training Loss:  0.60774028\n",
      "Training Loss:  0.60774028\n",
      "Training Loss:  0.60774028\n",
      "Final training Loss:  0.60774028\n",
      "\n",
      "Running model (trial=7, mod=206, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Training Loss:  0.59724599\n",
      "Final training Loss:  0.59724599\n",
      "\n",
      "Running model (trial=7, mod=207, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59356058\n",
      "Training Loss:  0.59356058\n",
      "Training Loss:  0.59356058\n",
      "Training Loss:  0.59350622\n",
      "Training Loss:  0.59350622\n",
      "Training Loss:  0.59350622\n",
      "Training Loss:  0.59350622\n",
      "Training Loss:  0.59350622\n",
      "Training Loss:  0.59350622\n",
      "Training Loss:  0.59350622\n",
      "Final training Loss:  0.59350622\n",
      "\n",
      "Running model (trial=7, mod=208, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60572547\n",
      "Training Loss:  0.59748518\n",
      "Training Loss:  0.59748518\n",
      "Training Loss:  0.59748518\n",
      "Training Loss:  0.59748518\n",
      "Training Loss:  0.59748518\n",
      "Training Loss:  0.59748518\n",
      "Training Loss:  0.59748518\n",
      "Training Loss:  0.59748518\n",
      "Training Loss:  0.59748518\n",
      "Final training Loss:  0.59748518\n",
      "\n",
      "Running model (trial=7, mod=209, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Final training Loss:  500.99996948\n",
      "\n",
      "Running model (trial=7, mod=210, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59397656\n",
      "Training Loss:  0.59397656\n",
      "Training Loss:  0.59397656\n",
      "Training Loss:  0.59397632\n",
      "Training Loss:  0.59397632\n",
      "Training Loss:  0.59397608\n",
      "Training Loss:  0.59397608\n",
      "Training Loss:  0.59397608\n",
      "Training Loss:  0.59397608\n",
      "Training Loss:  0.59397608\n",
      "Final training Loss:  0.59397608\n",
      "\n",
      "Running model (trial=7, mod=211, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.74002302\n",
      "Training Loss:  0.74002308\n",
      "Training Loss:  0.63892657\n",
      "Training Loss:  0.63892657\n",
      "Training Loss:  0.63892657\n",
      "Training Loss:  0.6385988\n",
      "Training Loss:  0.63859832\n",
      "Training Loss:  0.63859707\n",
      "Training Loss:  0.63859254\n",
      "Training Loss:  0.63859254\n",
      "Final training Loss:  0.63859254\n",
      "\n",
      "Running model (trial=7, mod=212, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72797126\n",
      "Training Loss:  0.60541034\n",
      "Training Loss:  0.60541034\n",
      "Training Loss:  0.60541034\n",
      "Training Loss:  0.60541034\n",
      "Training Loss:  0.60541034\n",
      "Training Loss:  0.60541034\n",
      "Training Loss:  0.60541034\n",
      "Training Loss:  0.59805691\n",
      "Training Loss:  0.59805626\n",
      "Final training Loss:  0.59805626\n",
      "\n",
      "Running model (trial=7, mod=213, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Training Loss:  0.610686\n",
      "Final training Loss:  0.610686\n",
      "\n",
      "Running model (trial=7, mod=214, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Training Loss:  0.6366635\n",
      "Final training Loss:  0.6366635\n",
      "\n",
      "Running model (trial=7, mod=215, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Training Loss:  0.61357582\n",
      "Final training Loss:  0.61357582\n",
      "\n",
      "Running model (trial=7, mod=216, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Training Loss:  0.66408682\n",
      "Final training Loss:  0.66408682\n",
      "\n",
      "Running model (trial=7, mod=217, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73386294\n",
      "Training Loss:  0.68860567\n",
      "Training Loss:  0.68860567\n",
      "Training Loss:  0.68860567\n",
      "Training Loss:  0.68860567\n",
      "Training Loss:  0.68860567\n",
      "Training Loss:  0.68860567\n",
      "Training Loss:  0.68860567\n",
      "Training Loss:  0.68860567\n",
      "Training Loss:  0.68860567\n",
      "Final training Loss:  0.68860567\n",
      "\n",
      "Running model (trial=7, mod=218, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Training Loss:  0.63007426\n",
      "Final training Loss:  0.63007426\n",
      "\n",
      "Running model (trial=7, mod=219, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60422969\n",
      "Training Loss:  0.60389757\n",
      "Training Loss:  0.60389757\n",
      "Training Loss:  0.60389757\n",
      "Training Loss:  0.60389757\n",
      "Training Loss:  0.60389757\n",
      "Training Loss:  0.60389757\n",
      "Training Loss:  0.60389757\n",
      "Training Loss:  0.60389757\n",
      "Training Loss:  0.60389709\n",
      "Final training Loss:  0.60389709\n",
      "\n",
      "Running model (trial=7, mod=220, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59443629\n",
      "Training Loss:  0.59443533\n",
      "Training Loss:  0.59443533\n",
      "Training Loss:  0.59443533\n",
      "Training Loss:  0.59443533\n",
      "Training Loss:  0.59443533\n",
      "Training Loss:  0.59443533\n",
      "Training Loss:  0.59443533\n",
      "Training Loss:  0.59443533\n",
      "Training Loss:  0.59443533\n",
      "Final training Loss:  0.59443533\n",
      "\n",
      "Running model (trial=7, mod=221, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59197134\n",
      "Training Loss:  0.59197134\n",
      "Training Loss:  0.59197134\n",
      "Training Loss:  0.59197134\n",
      "Training Loss:  0.59197134\n",
      "Training Loss:  0.59197134\n",
      "Training Loss:  0.59197134\n",
      "Training Loss:  0.59186345\n",
      "Training Loss:  0.59186345\n",
      "Training Loss:  0.59186345\n",
      "Final training Loss:  0.59186345\n",
      "\n",
      "Running model (trial=7, mod=222, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71799415\n",
      "Training Loss:  0.6150279\n",
      "Training Loss:  0.6150279\n",
      "Training Loss:  0.6150279\n",
      "Training Loss:  0.6150279\n",
      "Training Loss:  0.6150279\n",
      "Training Loss:  0.6150279\n",
      "Training Loss:  0.6150279\n",
      "Training Loss:  0.6150279\n",
      "Training Loss:  0.6150279\n",
      "Final training Loss:  0.6150279\n",
      "\n",
      "Running model (trial=7, mod=223, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Training Loss:  1.31234407\n",
      "Final training Loss:  1.31234407\n",
      "\n",
      "Running model (trial=8, mod=224, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59574854\n",
      "Training Loss:  0.59564102\n",
      "Training Loss:  0.59561169\n",
      "Training Loss:  0.59561169\n",
      "Training Loss:  0.59561169\n",
      "Training Loss:  0.59561169\n",
      "Training Loss:  0.59561163\n",
      "Training Loss:  0.59561163\n",
      "Training Loss:  0.59561163\n",
      "Training Loss:  0.59561163\n",
      "Final training Loss:  0.59561163\n",
      "\n",
      "Running model (trial=8, mod=225, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Training Loss:  0.59370887\n",
      "Final training Loss:  0.59370887\n",
      "\n",
      "Running model (trial=8, mod=226, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Training Loss:  0.62641692\n",
      "Final training Loss:  0.62641692\n",
      "\n",
      "Running model (trial=8, mod=227, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Training Loss:  0.59958071\n",
      "Final training Loss:  0.59958071\n",
      "\n",
      "Running model (trial=8, mod=228, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59948105\n",
      "Training Loss:  0.59948099\n",
      "Training Loss:  0.59948099\n",
      "Training Loss:  0.59948099\n",
      "Training Loss:  0.59948099\n",
      "Training Loss:  0.59948099\n",
      "Training Loss:  0.59948099\n",
      "Training Loss:  0.59948099\n",
      "Training Loss:  0.59948099\n",
      "Training Loss:  0.59948099\n",
      "Final training Loss:  0.59948099\n",
      "\n",
      "Running model (trial=8, mod=229, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72463596\n",
      "Training Loss:  0.68481892\n",
      "Training Loss:  0.68460679\n",
      "Training Loss:  0.68460679\n",
      "Training Loss:  0.68424219\n",
      "Training Loss:  0.68424219\n",
      "Training Loss:  0.68424219\n",
      "Training Loss:  0.68424213\n",
      "Training Loss:  0.68424213\n",
      "Training Loss:  0.68424213\n",
      "Final training Loss:  0.68424213\n",
      "\n",
      "Running model (trial=8, mod=230, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Training Loss:  0.69275552\n",
      "Final training Loss:  0.69275552\n",
      "\n",
      "Running model (trial=8, mod=231, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Training Loss:  0.63782603\n",
      "Final training Loss:  0.63782603\n",
      "\n",
      "Running model (trial=8, mod=232, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72541666\n",
      "Training Loss:  0.60349226\n",
      "Training Loss:  0.60349226\n",
      "Training Loss:  0.6034922\n",
      "Training Loss:  0.6034922\n",
      "Training Loss:  0.6034922\n",
      "Training Loss:  0.6034922\n",
      "Training Loss:  0.6034922\n",
      "Training Loss:  0.6034922\n",
      "Training Loss:  0.6034922\n",
      "Final training Loss:  0.6034922\n",
      "\n",
      "Running model (trial=8, mod=233, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59493941\n",
      "Training Loss:  0.59481919\n",
      "Training Loss:  0.59481919\n",
      "Training Loss:  0.59481919\n",
      "Training Loss:  0.59481919\n",
      "Training Loss:  0.59481919\n",
      "Training Loss:  0.59481919\n",
      "Training Loss:  0.59481919\n",
      "Training Loss:  0.59476066\n",
      "Training Loss:  0.59469843\n",
      "Final training Loss:  0.59469843\n",
      "\n",
      "Running model (trial=8, mod=234, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Training Loss:  0.5961436\n",
      "Final training Loss:  0.5961436\n",
      "\n",
      "Running model (trial=8, mod=235, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Training Loss:  0.5999254\n",
      "Final training Loss:  0.5999254\n",
      "\n",
      "Running model (trial=8, mod=236, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99667358\n",
      "Training Loss:  0.59985125\n",
      "Training Loss:  0.59985125\n",
      "Training Loss:  0.59985125\n",
      "Training Loss:  0.59985125\n",
      "Training Loss:  0.59985125\n",
      "Training Loss:  0.59985125\n",
      "Training Loss:  0.59985125\n",
      "Training Loss:  0.59985125\n",
      "Training Loss:  0.59985125\n",
      "Final training Loss:  0.59985125\n",
      "\n",
      "Running model (trial=8, mod=237, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Training Loss:  500.99996948\n",
      "Final training Loss:  500.99996948\n",
      "\n",
      "Running model (trial=8, mod=238, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60149074\n",
      "Training Loss:  0.60146457\n",
      "Training Loss:  0.60146457\n",
      "Training Loss:  0.60146457\n",
      "Training Loss:  0.60146457\n",
      "Training Loss:  0.60146457\n",
      "Training Loss:  0.60146457\n",
      "Training Loss:  0.60146457\n",
      "Training Loss:  0.60146457\n",
      "Training Loss:  0.60146457\n",
      "Final training Loss:  0.60146457\n",
      "\n",
      "Running model (trial=8, mod=239, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Training Loss:  0.62108803\n",
      "Final training Loss:  0.62108803\n",
      "\n",
      "Running model (trial=8, mod=240, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59771633\n",
      "Training Loss:  0.59763008\n",
      "Training Loss:  0.59763008\n",
      "Training Loss:  0.59763008\n",
      "Training Loss:  0.5974735\n",
      "Training Loss:  0.5974735\n",
      "Training Loss:  0.5974735\n",
      "Training Loss:  0.5974735\n",
      "Training Loss:  0.5974735\n",
      "Training Loss:  0.5974732\n",
      "Final training Loss:  0.5974732\n",
      "\n",
      "Running model (trial=8, mod=241, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60084748\n",
      "Training Loss:  0.60084748\n",
      "Training Loss:  0.60084748\n",
      "Training Loss:  0.60084748\n",
      "Training Loss:  0.60084748\n",
      "Training Loss:  0.60084748\n",
      "Training Loss:  0.60084748\n",
      "Training Loss:  0.60084742\n",
      "Training Loss:  0.60084736\n",
      "Training Loss:  0.60084724\n",
      "Final training Loss:  0.60084724\n",
      "\n",
      "Running model (trial=8, mod=242, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72709054\n",
      "Training Loss:  0.63881391\n",
      "Training Loss:  0.6378054\n",
      "Training Loss:  0.63774037\n",
      "Training Loss:  0.63774037\n",
      "Training Loss:  0.63774037\n",
      "Training Loss:  0.63774037\n",
      "Training Loss:  0.63774037\n",
      "Training Loss:  0.63774037\n",
      "Training Loss:  0.63773829\n",
      "Final training Loss:  0.63773829\n",
      "\n",
      "Running model (trial=8, mod=243, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67603612\n",
      "Training Loss:  0.67603457\n",
      "Training Loss:  0.67603356\n",
      "Training Loss:  0.67603356\n",
      "Training Loss:  0.67591012\n",
      "Training Loss:  0.67591012\n",
      "Training Loss:  0.67591012\n",
      "Training Loss:  0.67591012\n",
      "Training Loss:  0.67591012\n",
      "Training Loss:  0.67591012\n",
      "Final training Loss:  0.67591012\n",
      "\n",
      "Running model (trial=8, mod=244, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68612272\n",
      "Training Loss:  0.68612272\n",
      "Training Loss:  0.68612272\n",
      "Training Loss:  0.68612272\n",
      "Training Loss:  0.68612266\n",
      "Training Loss:  0.67710632\n",
      "Training Loss:  0.67710632\n",
      "Training Loss:  0.67710632\n",
      "Training Loss:  0.67710632\n",
      "Training Loss:  0.67710632\n",
      "Final training Loss:  0.67710632\n",
      "\n",
      "Running model (trial=8, mod=245, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Training Loss:  0.67989188\n",
      "Final training Loss:  0.67989188\n",
      "\n",
      "Running model (trial=8, mod=246, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Training Loss:  0.60522681\n",
      "Final training Loss:  0.60522681\n",
      "\n",
      "Running model (trial=8, mod=247, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62414742\n",
      "Training Loss:  0.62414742\n",
      "Training Loss:  0.62414742\n",
      "Training Loss:  0.62414742\n",
      "Training Loss:  0.62414742\n",
      "Training Loss:  0.62414742\n",
      "Training Loss:  0.62414742\n",
      "Training Loss:  0.62414742\n",
      "Training Loss:  0.62414724\n",
      "Training Loss:  0.62414724\n",
      "Final training Loss:  0.62414724\n",
      "\n",
      "Running model (trial=8, mod=248, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61208969\n",
      "Training Loss:  0.61208969\n",
      "Training Loss:  0.61117023\n",
      "Training Loss:  0.61116987\n",
      "Training Loss:  0.61116946\n",
      "Training Loss:  0.61116946\n",
      "Training Loss:  0.61116946\n",
      "Training Loss:  0.61116946\n",
      "Training Loss:  0.61116946\n",
      "Training Loss:  0.61116946\n",
      "Final training Loss:  0.61116946\n",
      "\n",
      "Running model (trial=8, mod=249, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Training Loss:  0.61052185\n",
      "Final training Loss:  0.61052185\n",
      "\n",
      "Running model (trial=8, mod=250, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63695896\n",
      "Training Loss:  0.63695896\n",
      "Training Loss:  0.63695896\n",
      "Training Loss:  0.63695896\n",
      "Training Loss:  0.63695896\n",
      "Training Loss:  0.63695896\n",
      "Training Loss:  0.63695896\n",
      "Training Loss:  0.63695896\n",
      "Training Loss:  0.6369589\n",
      "Training Loss:  0.6369589\n",
      "Final training Loss:  0.6369589\n",
      "\n",
      "Running model (trial=8, mod=251, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Training Loss:  0.60148871\n",
      "Final training Loss:  0.60148871\n",
      "\n",
      "Running model (trial=9, mod=252, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6032784\n",
      "Training Loss:  0.6032784\n",
      "Training Loss:  0.60327834\n",
      "Training Loss:  0.60327834\n",
      "Training Loss:  0.60327834\n",
      "Training Loss:  0.60327834\n",
      "Training Loss:  0.60327834\n",
      "Training Loss:  0.60327834\n",
      "Training Loss:  0.6032784\n",
      "Training Loss:  0.59520698\n",
      "Final training Loss:  0.59520698\n",
      "\n",
      "Running model (trial=9, mod=253, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Training Loss:  0.59765571\n",
      "Final training Loss:  0.59765571\n",
      "\n",
      "Running model (trial=9, mod=254, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Training Loss:  0.59583998\n",
      "Final training Loss:  0.59583998\n",
      "\n",
      "Running model (trial=9, mod=255, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59413606\n",
      "Training Loss:  0.59373736\n",
      "Training Loss:  0.59373742\n",
      "Training Loss:  0.59373742\n",
      "Training Loss:  0.59373742\n",
      "Training Loss:  0.59373742\n",
      "Training Loss:  0.59373742\n",
      "Training Loss:  0.59373742\n",
      "Training Loss:  0.59373742\n",
      "Training Loss:  0.59373742\n",
      "Final training Loss:  0.59373742\n",
      "\n",
      "Running model (trial=9, mod=256, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60749966\n",
      "Training Loss:  0.60749966\n",
      "Training Loss:  0.60749966\n",
      "Training Loss:  0.60749751\n",
      "Training Loss:  0.60749751\n",
      "Training Loss:  0.60749751\n",
      "Training Loss:  0.60749751\n",
      "Training Loss:  0.60749751\n",
      "Training Loss:  0.60749751\n",
      "Training Loss:  0.60749751\n",
      "Final training Loss:  0.60749751\n",
      "\n",
      "Running model (trial=9, mod=257, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Training Loss:  0.67688227\n",
      "Final training Loss:  0.67688227\n",
      "\n",
      "Running model (trial=9, mod=258, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64332628\n",
      "Training Loss:  0.64323461\n",
      "Training Loss:  0.64323461\n",
      "Training Loss:  0.64323461\n",
      "Training Loss:  0.64323467\n",
      "Training Loss:  0.64323461\n",
      "Training Loss:  0.64323461\n",
      "Training Loss:  0.64323461\n",
      "Training Loss:  0.64323461\n",
      "Training Loss:  0.64323461\n",
      "Final training Loss:  0.64323461\n",
      "\n",
      "Running model (trial=9, mod=259, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68181545\n",
      "Training Loss:  0.68127042\n",
      "Training Loss:  0.68127042\n",
      "Training Loss:  0.68127042\n",
      "Training Loss:  0.68127042\n",
      "Training Loss:  0.68127042\n",
      "Training Loss:  0.68127042\n",
      "Training Loss:  0.68127042\n",
      "Training Loss:  0.68127042\n",
      "Training Loss:  0.68127042\n",
      "Final training Loss:  0.68127042\n",
      "\n",
      "Running model (trial=9, mod=260, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64333773\n",
      "Training Loss:  0.64315164\n",
      "Training Loss:  0.64313763\n",
      "Training Loss:  0.64313763\n",
      "Training Loss:  0.64313763\n",
      "Training Loss:  0.64313763\n",
      "Training Loss:  0.64313763\n",
      "Training Loss:  0.64313763\n",
      "Training Loss:  0.64313763\n",
      "Training Loss:  0.64313763\n",
      "Final training Loss:  0.64313763\n",
      "\n",
      "Running model (trial=9, mod=261, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60093373\n",
      "Training Loss:  0.60074353\n",
      "Training Loss:  0.60074347\n",
      "Training Loss:  0.60074347\n",
      "Training Loss:  0.60074347\n",
      "Training Loss:  0.60074347\n",
      "Training Loss:  0.60074347\n",
      "Training Loss:  0.60074347\n",
      "Training Loss:  0.60074347\n",
      "Training Loss:  0.60074347\n",
      "Final training Loss:  0.60074347\n",
      "\n",
      "Running model (trial=9, mod=262, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5957554\n",
      "Training Loss:  0.59566975\n",
      "Training Loss:  0.59566975\n",
      "Training Loss:  0.59566975\n",
      "Training Loss:  0.59566975\n",
      "Training Loss:  0.59566975\n",
      "Training Loss:  0.59566975\n",
      "Training Loss:  0.59566975\n",
      "Training Loss:  0.59566975\n",
      "Training Loss:  0.59566975\n",
      "Final training Loss:  0.59566975\n",
      "\n",
      "Running model (trial=9, mod=263, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60533935\n",
      "Training Loss:  0.60533935\n",
      "Training Loss:  0.60533935\n",
      "Training Loss:  0.60533935\n",
      "Training Loss:  0.60533935\n",
      "Training Loss:  0.60533935\n",
      "Training Loss:  0.59820968\n",
      "Training Loss:  0.59820968\n",
      "Training Loss:  0.59820968\n",
      "Training Loss:  0.59820968\n",
      "Final training Loss:  0.59820968\n",
      "\n",
      "Running model (trial=9, mod=264, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99786377\n",
      "Training Loss:  500.90991211\n",
      "Training Loss:  500.90991211\n",
      "Training Loss:  500.90991211\n",
      "Training Loss:  500.90991211\n",
      "Training Loss:  500.90991211\n",
      "Training Loss:  500.90991211\n",
      "Training Loss:  500.90991211\n",
      "Training Loss:  500.90991211\n",
      "Training Loss:  500.90991211\n",
      "Final training Loss:  500.90991211\n",
      "\n",
      "Running model (trial=9, mod=265, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Training Loss:  501.0\n",
      "Final training Loss:  501.0\n",
      "\n",
      "Running model (trial=9, mod=266, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Training Loss:  0.59315932\n",
      "Final training Loss:  0.59315932\n",
      "\n",
      "Running model (trial=9, mod=267, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5977118\n",
      "Training Loss:  0.5977118\n",
      "Training Loss:  0.5977118\n",
      "Training Loss:  0.5977118\n",
      "Training Loss:  0.59771174\n",
      "Training Loss:  0.59771174\n",
      "Training Loss:  0.59771174\n",
      "Training Loss:  0.59771174\n",
      "Training Loss:  0.59771174\n",
      "Training Loss:  0.59771174\n",
      "Final training Loss:  0.59771174\n",
      "\n",
      "Running model (trial=9, mod=268, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61335319\n",
      "Training Loss:  0.61335319\n",
      "Training Loss:  0.61335182\n",
      "Training Loss:  0.61335182\n",
      "Training Loss:  0.61335182\n",
      "Training Loss:  0.61335182\n",
      "Training Loss:  0.61335182\n",
      "Training Loss:  0.61335182\n",
      "Training Loss:  0.61335182\n",
      "Training Loss:  0.61335182\n",
      "Final training Loss:  0.61335182\n",
      "\n",
      "Running model (trial=9, mod=269, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Training Loss:  0.6356566\n",
      "Final training Loss:  0.6356566\n",
      "\n",
      "Running model (trial=9, mod=270, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60315496\n",
      "Training Loss:  0.6031549\n",
      "Training Loss:  0.6031549\n",
      "Training Loss:  0.60315484\n",
      "Training Loss:  0.60315484\n",
      "Training Loss:  0.60315484\n",
      "Training Loss:  0.60315484\n",
      "Training Loss:  0.60315472\n",
      "Training Loss:  0.60315472\n",
      "Training Loss:  0.60315472\n",
      "Final training Loss:  0.60315472\n",
      "\n",
      "Running model (trial=9, mod=271, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Training Loss:  0.66301358\n",
      "Final training Loss:  0.66301358\n",
      "\n",
      "Running model (trial=9, mod=272, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73158872\n",
      "Training Loss:  0.69295686\n",
      "Training Loss:  0.69295686\n",
      "Training Loss:  0.69295686\n",
      "Training Loss:  0.69295686\n",
      "Training Loss:  0.69295686\n",
      "Training Loss:  0.69295686\n",
      "Training Loss:  0.69295686\n",
      "Training Loss:  0.69295686\n",
      "Training Loss:  0.69295686\n",
      "Final training Loss:  0.69295686\n",
      "\n",
      "Running model (trial=9, mod=273, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Training Loss:  0.68421769\n",
      "Final training Loss:  0.68421769\n",
      "\n",
      "Running model (trial=9, mod=274, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.612207\n",
      "Training Loss:  0.6122067\n",
      "Final training Loss:  0.6122067\n",
      "\n",
      "Running model (trial=9, mod=275, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7355606\n",
      "Training Loss:  0.61695886\n",
      "Training Loss:  0.61695886\n",
      "Training Loss:  0.59484112\n",
      "Training Loss:  0.59484112\n",
      "Training Loss:  0.59484112\n",
      "Training Loss:  0.59484112\n",
      "Training Loss:  0.59484112\n",
      "Training Loss:  0.59484112\n",
      "Training Loss:  0.59484112\n",
      "Final training Loss:  0.59484112\n",
      "\n",
      "Running model (trial=9, mod=276, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60783118\n",
      "Training Loss:  0.60783112\n",
      "Training Loss:  0.60783112\n",
      "Training Loss:  0.60783112\n",
      "Training Loss:  0.60783112\n",
      "Training Loss:  0.60783112\n",
      "Training Loss:  0.60783112\n",
      "Training Loss:  0.60783112\n",
      "Training Loss:  0.60783112\n",
      "Training Loss:  0.60783112\n",
      "Final training Loss:  0.60783112\n",
      "\n",
      "Running model (trial=9, mod=277, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Training Loss:  0.67539108\n",
      "Final training Loss:  0.67539108\n",
      "\n",
      "Running model (trial=9, mod=278, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Training Loss:  0.65034258\n",
      "Final training Loss:  0.65034258\n",
      "\n",
      "Running model (trial=9, mod=279, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12aef7e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12af03e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Training Loss:  0.60027301\n",
      "Final training Loss:  0.60027301\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "x_train_ = x_train.detach()\n",
    "x_sorted, indices = torch.sort(x_train_, dim=0)\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": x_sorted,\n",
    "    \"x_train\": x_sorted,\n",
    "    \"y_train\": c2.ksi(x_sorted),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"analytical solution\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: no_penalty_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "\n",
    "# make data frame\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type})\n",
    "loss_array -= c2.DIST_R_Q\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 419.878125 262.19625\" width=\"419.878125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-03T13:35:58.237986</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 419.878125 262.19625 \nL 419.878125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 77.878125 224.64 \nL 412.678125 224.64 \nL 412.678125 7.2 \nL 77.878125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m8cb39bd501\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"194.054947\" xlink:href=\"#m8cb39bd501\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(185.254947 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"338.494106\" xlink:href=\"#m8cb39bd501\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(329.694106 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m19d8acf935\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"93.096307\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"118.53078\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"136.576826\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"150.574427\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"162.0113\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"171.681038\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"180.057346\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"187.445773\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"237.535466\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"262.969939\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"281.015986\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"295.013586\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"306.450459\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"316.120197\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"324.496505\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"331.884932\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"381.974625\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"407.409099\" xlink:href=\"#m19d8acf935\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Neurons -->\n     <g transform=\"translate(224.516406 252.916563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"199.707031\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"238.570312\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"299.751953\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"363.130859\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m272a6ee153\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"77.878125\" xlink:href=\"#m272a6ee153\" y=\"203.003936\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{6\\times10^{-1}}$ -->\n      <g transform=\"translate(30.378125 206.803154)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 4488 3438 \nL 3059 2003 \nL 4488 575 \nL 4116 197 \nL 2681 1631 \nL 1247 197 \nL 878 575 \nL 2303 2003 \nL 878 3438 \nL 1247 3816 \nL 2681 2381 \nL 4116 3816 \nL 4488 3438 \nz\n\" id=\"DejaVuSans-d7\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"77.878125\" xlink:href=\"#m272a6ee153\" y=\"155.21121\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{6.2\\times10^{-1}}$ -->\n      <g transform=\"translate(21.378125 159.010429)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 684 794 \nL 1344 794 \nL 1344 0 \nL 684 0 \nL 684 794 \nz\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use transform=\"translate(89.910156 0.684375)\" xlink:href=\"#DejaVuSans-32\"/>\n       <use transform=\"translate(173.015625 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(276.287109 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(339.910156 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(404.490234 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(463.142578 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"77.878125\" xlink:href=\"#m272a6ee153\" y=\"108.935973\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{6.4\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 112.735191)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use transform=\"translate(95.410156 0.684375)\" xlink:href=\"#DejaVuSans-34\"/>\n       <use transform=\"translate(178.515625 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(281.787109 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(345.410156 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(409.990234 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(468.642578 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"77.878125\" xlink:href=\"#m272a6ee153\" y=\"64.084817\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{6.6\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 67.884036)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use transform=\"translate(95.410156 0.684375)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(178.515625 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(281.787109 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(345.410156 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(409.990234 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(468.642578 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"77.878125\" xlink:href=\"#m272a6ee153\" y=\"20.572704\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{6.8\\times10^{-1}}$ -->\n      <g transform=\"translate(21.078125 24.371923)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-2e\"/>\n       <use transform=\"translate(92.785156 0.684375)\" xlink:href=\"#DejaVuSans-38\"/>\n       <use transform=\"translate(175.890625 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(279.162109 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(342.785156 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(407.365234 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(466.017578 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- $\\Delta E$ -->\n     <g transform=\"translate(14.798437 122.52)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 906 525 \nL 3472 525 \nL 2188 4044 \nz\nM 50 0 \nL 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 50 0 \nz\n\" id=\"DejaVuSans-394\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-394\"/>\n      <use transform=\"translate(68.408203 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 93.096307 210.33307 \nL 93.096307 204.442337 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 136.576826 214.756364 \nL 136.576826 211.651447 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 180.057346 204.638711 \nL 180.057346 188.499865 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 223.537865 207.89265 \nL 223.537865 188.603347 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 267.018385 194.151893 \nL 267.018385 143.665315 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 310.498904 100.490014 \nL 310.498904 51.781244 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 353.979424 107.443686 \nL 353.979424 73.988075 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 397.459943 128.767448 \nL 397.459943 70.436807 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 93.096307 209.363794 \nL 93.096307 193.413236 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 136.576826 186.642522 \nL 136.576826 140.617629 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 180.057346 160.195513 \nL 180.057346 115.710412 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 223.537865 161.976796 \nL 223.537865 129.203967 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 267.018385 148.685743 \nL 267.018385 108.293301 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 310.498904 99.248691 \nL 310.498904 54.555105 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 353.979424 37.471583 \nL 353.979424 18.462119 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 397.459943 36.257944 \nL 397.459943 17.083636 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 93.096307 207.367409 \nL 136.576826 213.151119 \nL 180.057346 196.802083 \nL 223.537865 198.062036 \nL 267.018385 170.322276 \nL 310.498904 76.206794 \nL 353.979424 91.071435 \nL 397.459943 99.836722 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path clip-path=\"url(#pdbcfd8bf37)\" d=\"M 93.096307 201.711148 \nL 136.576826 163.750069 \nL 180.057346 138.026976 \nL 223.537865 145.322252 \nL 267.018385 129.32839 \nL 310.498904 76.941377 \nL 353.979424 27.938988 \nL 397.459943 26.678961 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_28\"/>\n   <g id=\"line2d_29\"/>\n   <g id=\"line2d_30\"/>\n   <g id=\"line2d_31\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 77.878125 224.64 \nL 77.878125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 412.678125 224.64 \nL 412.678125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 77.878125 224.64 \nL 412.678125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 77.878125 7.2 \nL 412.678125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 84.878125 59.234375 \nL 154.646875 59.234375 \nQ 156.646875 59.234375 156.646875 57.234375 \nL 156.646875 14.2 \nQ 156.646875 12.2 154.646875 12.2 \nL 84.878125 12.2 \nQ 82.878125 12.2 82.878125 14.2 \nL 82.878125 57.234375 \nQ 82.878125 59.234375 84.878125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- model -->\n     <g transform=\"translate(104.192969 23.798438)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_32\">\n     <path d=\"M 86.878125 34.976563 \nL 106.878125 34.976563 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_33\"/>\n    <g id=\"text_11\">\n     <!-- FFNN -->\n     <g transform=\"translate(114.878125 38.476563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_34\">\n     <path d=\"M 86.878125 49.654688 \nL 106.878125 49.654688 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_35\"/>\n    <g id=\"text_12\">\n     <!-- ResNET -->\n     <g transform=\"translate(114.878125 53.154688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pdbcfd8bf37\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"77.878125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\" , bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "0 180\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 1]), len(d_results_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 410.378125 262.19625\" width=\"410.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-03T13:35:59.943275</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 410.378125 262.19625 \nL 410.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 68.378125 224.64 \nL 403.178125 224.64 \nL 403.178125 7.2 \nL 68.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mbc47aab694\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"83.596307\" xlink:href=\"#mbc47aab694\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(74.796307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.81113\" xlink:href=\"#mbc47aab694\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(277.01113 239.238437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m548a7f4db5\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"74.343464\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"144.469034\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"180.077297\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"205.341761\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"224.938402\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"240.950024\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"254.487657\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"266.214489\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"276.558287\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"346.683857\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"382.29212\" xlink:href=\"#m548a7f4db5\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Layers -->\n     <g transform=\"translate(219.232031 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"55.712891\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"116.992188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"176.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"237.695312\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"278.808594\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9564cf8d3b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"68.378125\" xlink:href=\"#m9564cf8d3b\" y=\"17.080285\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(43.778125 20.879503)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"maf9da29b96\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#maf9da29b96\" y=\"212.489885\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{6\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 216.289104)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 4488 3438 \nL 3059 2003 \nL 4488 575 \nL 4116 197 \nL 2681 1631 \nL 1247 197 \nL 878 575 \nL 2303 2003 \nL 878 3438 \nL 1247 3816 \nL 2681 2381 \nL 4116 3816 \nL 4488 3438 \nz\n\" id=\"DejaVuSans-d7\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#maf9da29b96\" y=\"153.521577\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{7\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 157.320796)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-37\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#maf9da29b96\" y=\"102.440905\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{8\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 106.240124)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-38\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#maf9da29b96\" y=\"57.384559\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{9\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 61.183778)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-39\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- $\\Delta E$ -->\n     <g transform=\"translate(14.798437 122.52)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 906 525 \nL 3472 525 \nL 2188 4044 \nz\nM 50 0 \nL 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 50 0 \nz\n\" id=\"DejaVuSans-394\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-394\"/>\n      <use transform=\"translate(68.408203 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 83.596307 206.45455 \nL 83.596307 196.768833 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 144.469034 212.93119 \nL 144.469034 208.684142 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 205.341761 214.756364 \nL 205.341761 212.699757 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 266.214489 213.918549 \nL 266.214489 212.400523 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 327.087216 212.29145 \nL 327.087216 124.345331 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 387.959943 17.086441 \nL 387.959943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 83.596307 194.153641 \nL 83.596307 182.573821 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 144.469034 201.423162 \nL 144.469034 188.927827 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 205.341761 208.933499 \nL 205.341761 199.196523 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 266.214489 195.578204 \nL 266.214489 180.673607 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 327.087216 201.070754 \nL 327.087216 182.934973 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 387.959943 198.080868 \nL 387.959943 179.079007 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_19\">\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 83.596307 201.507718 \nL 144.469034 210.862194 \nL 205.341761 213.750101 \nL 266.214489 213.203217 \nL 327.087216 166.26854 \nL 387.959943 17.085004 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_20\">\n    <path clip-path=\"url(#pe46621966b)\" d=\"M 83.596307 188.541071 \nL 144.469034 195.436528 \nL 205.341761 204.294096 \nL 266.214489 188.209105 \nL 327.087216 192.467413 \nL 387.959943 188.285327 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_21\"/>\n   <g id=\"line2d_22\"/>\n   <g id=\"line2d_23\"/>\n   <g id=\"line2d_24\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 68.378125 224.64 \nL 68.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 403.178125 224.64 \nL 403.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 68.378125 224.64 \nL 403.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 68.378125 7.2 \nL 403.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 75.378125 59.234375 \nL 145.146875 59.234375 \nQ 147.146875 59.234375 147.146875 57.234375 \nL 147.146875 14.2 \nQ 147.146875 12.2 145.146875 12.2 \nL 75.378125 12.2 \nQ 73.378125 12.2 73.378125 14.2 \nL 73.378125 57.234375 \nQ 73.378125 59.234375 75.378125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- model -->\n     <g transform=\"translate(94.692969 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_25\">\n     <path d=\"M 77.378125 34.976562 \nL 97.378125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_26\"/>\n    <g id=\"text_11\">\n     <!-- FFNN -->\n     <g transform=\"translate(105.378125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_27\">\n     <path d=\"M 77.378125 49.654687 \nL 97.378125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_28\"/>\n    <g id=\"text_12\">\n     <!-- ResNET -->\n     <g transform=\"translate(105.378125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe46621966b\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"68.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Layers\")\n",
    "fig_layers.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\" , bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "0 140\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 1]), len(d_results_layer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 410.378125 262.19625\" width=\"410.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-31T14:41:02.627835</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 410.378125 262.19625 \nL 410.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 68.378125 224.64 \nL 403.178125 224.64 \nL 403.178125 7.2 \nL 68.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 213.261642 \nC 84.391916 213.261642 85.155046 212.945544 85.717627 212.382963 \nC 86.280208 211.820382 86.596307 211.057252 86.596307 210.261642 \nC 86.596307 209.466033 86.280208 208.702903 85.717627 208.140322 \nC 85.155046 207.577741 84.391916 207.261642 83.596307 207.261642 \nC 82.800698 207.261642 82.037567 207.577741 81.474986 208.140322 \nC 80.912406 208.702903 80.596307 209.466033 80.596307 210.261642 \nC 80.596307 211.057252 80.912406 211.820382 81.474986 212.382963 \nC 82.037567 212.945544 82.800698 213.261642 83.596307 213.261642 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 214.413508 \nC 84.470115 214.413508 85.233245 214.097409 85.795826 213.534828 \nC 86.358407 212.972248 86.674506 212.209117 86.674506 211.413508 \nC 86.674506 210.617899 86.358407 209.854768 85.795826 209.292188 \nC 85.233245 208.729607 84.470115 208.413508 83.674506 208.413508 \nC 82.878896 208.413508 82.115766 208.729607 81.553185 209.292188 \nC 80.990605 209.854768 80.674506 210.617899 80.674506 211.413508 \nC 80.674506 212.209117 80.990605 212.972248 81.553185 213.534828 \nC 82.115766 214.097409 82.878896 214.413508 83.674506 214.413508 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 214.205581 \nC 84.736911 214.205581 85.500041 213.889482 86.062622 213.326902 \nC 86.625203 212.764321 86.941302 212.001191 86.941302 211.205581 \nC 86.941302 210.409972 86.625203 209.646842 86.062622 209.084261 \nC 85.500041 208.52168 84.736911 208.205581 83.941302 208.205581 \nC 83.145692 208.205581 82.382562 208.52168 81.819981 209.084261 \nC 81.257401 209.646842 80.941302 210.409972 80.941302 211.205581 \nC 80.941302 212.001191 81.257401 212.764321 81.819981 213.326902 \nC 82.382562 213.889482 83.145692 214.205581 83.941302 214.205581 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 216.490238 \nC 85.712096 216.490238 86.475227 216.174139 87.037807 215.611559 \nC 87.600388 215.048978 87.916487 214.285848 87.916487 213.490238 \nC 87.916487 212.694629 87.600388 211.931499 87.037807 211.368918 \nC 86.475227 210.806337 85.712096 210.490238 84.916487 210.490238 \nC 84.120878 210.490238 83.357747 210.806337 82.795167 211.368918 \nC 82.232586 211.931499 81.916487 212.694629 81.916487 213.490238 \nC 81.916487 214.285848 82.232586 215.048978 82.795167 215.611559 \nC 83.357747 216.174139 84.120878 216.490238 84.916487 216.490238 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 211.945468 \nC 89.428841 211.945468 90.191971 211.629369 90.754552 211.066789 \nC 91.317132 210.504208 91.633231 209.741077 91.633231 208.945468 \nC 91.633231 208.149859 91.317132 207.386729 90.754552 206.824148 \nC 90.191971 206.261567 89.428841 205.945468 88.633231 205.945468 \nC 87.837622 205.945468 87.074492 206.261567 86.511911 206.824148 \nC 85.94933 207.386729 85.633231 208.149859 85.633231 208.945468 \nC 85.633231 209.741077 85.94933 210.504208 86.511911 211.066789 \nC 87.074492 211.629369 87.837622 211.945468 88.633231 211.945468 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 193.423385 \nC 103.927823 193.423385 104.690954 193.107286 105.253534 192.544705 \nC 105.816115 191.982125 106.132214 191.218994 106.132214 190.423385 \nC 106.132214 189.627776 105.816115 188.864645 105.253534 188.302065 \nC 104.690954 187.739484 103.927823 187.423385 103.132214 187.423385 \nC 102.336605 187.423385 101.573474 187.739484 101.010894 188.302065 \nC 100.448313 188.864645 100.132214 189.627776 100.132214 190.423385 \nC 100.132214 191.218994 100.448313 191.982125 101.010894 192.544705 \nC 101.573474 193.107286 102.336605 193.423385 103.132214 193.423385 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 175.951385 \nC 161.187765 175.951385 161.950895 175.635286 162.513476 175.072705 \nC 163.076056 174.510124 163.392155 173.746994 163.392155 172.951385 \nC 163.392155 172.155775 163.076056 171.392645 162.513476 170.830064 \nC 161.950895 170.267484 161.187765 169.951385 160.392155 169.951385 \nC 159.596546 169.951385 158.833416 170.267484 158.270835 170.830064 \nC 157.708254 171.392645 157.392155 172.155775 157.392155 172.951385 \nC 157.392155 173.746994 157.708254 174.510124 158.270835 175.072705 \nC 158.833416 175.635286 159.596546 175.951385 160.392155 175.951385 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 202.402691 \nC 388.755552 202.402691 389.518683 202.086592 390.081264 201.524011 \nC 390.643844 200.961431 390.959943 200.1983 390.959943 199.402691 \nC 390.959943 198.607082 390.643844 197.843952 390.081264 197.281371 \nC 389.518683 196.71879 388.755552 196.402691 387.959943 196.402691 \nC 387.164334 196.402691 386.401204 196.71879 385.838623 197.281371 \nC 385.276042 197.843952 384.959943 198.607082 384.959943 199.402691 \nC 384.959943 200.1983 385.276042 200.961431 385.838623 201.524011 \nC 386.401204 202.086592 387.164334 202.402691 387.959943 202.402691 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 195.588781 \nC 84.571313 195.588781 85.334444 195.272682 85.897024 194.710101 \nC 86.459605 194.14752 86.775704 193.38439 86.775704 192.588781 \nC 86.775704 191.793171 86.459605 191.030041 85.897024 190.46746 \nC 85.334444 189.90488 84.571313 189.588781 83.775704 189.588781 \nC 82.980095 189.588781 82.216965 189.90488 81.654384 190.46746 \nC 81.091803 191.030041 80.775704 191.793171 80.775704 192.588781 \nC 80.775704 193.38439 81.091803 194.14752 81.654384 194.710101 \nC 82.216965 195.272682 82.980095 195.588781 83.775704 195.588781 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 197.614115 \nC 84.736911 197.614115 85.500041 197.298016 86.062622 196.735435 \nC 86.625203 196.172854 86.941302 195.409724 86.941302 194.614115 \nC 86.941302 193.818505 86.625203 193.055375 86.062622 192.492794 \nC 85.500041 191.930214 84.736911 191.614115 83.941302 191.614115 \nC 83.145692 191.614115 82.382562 191.930214 81.819981 192.492794 \nC 81.257401 193.055375 80.941302 193.818505 80.941302 194.614115 \nC 80.941302 195.409724 81.257401 196.172854 81.819981 196.735435 \nC 82.382562 197.298016 83.145692 197.614115 83.941302 197.614115 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 213.828127 \nC 85.068106 213.828127 85.831236 213.512028 86.393817 212.949447 \nC 86.956398 212.386866 87.272497 211.623736 87.272497 210.828127 \nC 87.272497 210.032518 86.956398 209.269387 86.393817 208.706807 \nC 85.831236 208.144226 85.068106 207.828127 84.272497 207.828127 \nC 83.476887 207.828127 82.713757 208.144226 82.151176 208.706807 \nC 81.588596 209.269387 81.272497 210.032518 81.272497 210.828127 \nC 81.272497 211.623736 81.588596 212.386866 82.151176 212.949447 \nC 82.713757 213.512028 83.476887 213.828127 84.272497 213.828127 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 213.664601 \nC 85.730496 213.664601 86.493626 213.348502 87.056207 212.785921 \nC 87.618788 212.22334 87.934887 211.46021 87.934887 210.664601 \nC 87.934887 209.868991 87.618788 209.105861 87.056207 208.54328 \nC 86.493626 207.9807 85.730496 207.664601 84.934887 207.664601 \nC 84.139277 207.664601 83.376147 207.9807 82.813566 208.54328 \nC 82.250986 209.105861 81.934887 209.868991 81.934887 210.664601 \nC 81.934887 211.46021 82.250986 212.22334 82.813566 212.785921 \nC 83.376147 213.348502 84.139277 213.664601 84.934887 213.664601 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 20.863006 \nC 87.055276 20.863006 87.818406 20.546907 88.380987 19.984326 \nC 88.943568 19.421745 89.259667 18.658615 89.259667 17.863006 \nC 89.259667 17.067396 88.943568 16.304266 88.380987 15.741685 \nC 87.818406 15.179105 87.055276 14.863006 86.259667 14.863006 \nC 85.464058 14.863006 84.700927 15.179105 84.138347 15.741685 \nC 83.575766 16.304266 83.259667 17.067396 83.259667 17.863006 \nC 83.259667 18.658615 83.575766 19.421745 84.138347 19.984326 \nC 84.700927 20.546907 85.464058 20.863006 86.259667 20.863006 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.086642 \nC 89.704836 20.086642 90.467967 19.770543 91.030547 19.207962 \nC 91.593128 18.645381 91.909227 17.882251 91.909227 17.086642 \nC 91.909227 16.291032 91.593128 15.527902 91.030547 14.965321 \nC 90.467967 14.40274 89.704836 14.086642 88.909227 14.086642 \nC 88.113618 14.086642 87.350488 14.40274 86.787907 14.965321 \nC 86.225326 15.527902 85.909227 16.291032 85.909227 17.086642 \nC 85.909227 17.882251 86.225326 18.645381 86.787907 19.207962 \nC 87.350488 19.770543 88.113618 20.086642 88.909227 20.086642 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 209.058452 \nL 83.596307 207.558452 \nL 85.096307 209.058452 \nL 86.596307 207.558452 \nL 85.096307 206.058452 \nL 86.596307 204.558452 \nL 85.096307 203.058452 \nL 83.596307 204.558452 \nL 82.096307 203.058452 \nL 80.596307 204.558452 \nL 82.096307 206.058452 \nL 80.596307 207.558452 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 215.012339 \nL 83.674506 213.512339 \nL 85.174506 215.012339 \nL 86.674506 213.512339 \nL 85.174506 212.012339 \nL 86.674506 210.512339 \nL 85.174506 209.012339 \nL 83.674506 210.512339 \nL 82.174506 209.012339 \nL 80.674506 210.512339 \nL 82.174506 212.012339 \nL 80.674506 213.512339 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 203.830035 \nL 83.941302 202.330035 \nL 85.441302 203.830035 \nL 86.941302 202.330035 \nL 85.441302 200.830035 \nL 86.941302 199.330035 \nL 85.441302 197.830035 \nL 83.941302 199.330035 \nL 82.441302 197.830035 \nL 80.941302 199.330035 \nL 82.441302 200.830035 \nL 80.941302 202.330035 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 208.162184 \nL 84.916487 206.662184 \nL 86.416487 208.162184 \nL 87.916487 206.662184 \nL 86.416487 205.162184 \nL 87.916487 203.662184 \nL 86.416487 202.162184 \nL 84.916487 203.662184 \nL 83.416487 202.162184 \nL 81.916487 203.662184 \nL 83.416487 205.162184 \nL 81.916487 206.662184 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 203.534265 \nL 88.633231 202.034265 \nL 90.133231 203.534265 \nL 91.633231 202.034265 \nL 90.133231 200.534265 \nL 91.633231 199.034265 \nL 90.133231 197.534265 \nL 88.633231 199.034265 \nL 87.133231 197.534265 \nL 85.633231 199.034265 \nL 87.133231 200.534265 \nL 85.633231 202.034265 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 178.97857 \nL 103.132214 177.47857 \nL 104.632214 178.97857 \nL 106.132214 177.47857 \nL 104.632214 175.97857 \nL 106.132214 174.47857 \nL 104.632214 172.97857 \nL 103.132214 174.47857 \nL 101.632214 172.97857 \nL 100.132214 174.47857 \nL 101.632214 175.97857 \nL 100.132214 177.47857 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 167.810082 \nL 160.392155 166.310082 \nL 161.892155 167.810082 \nL 163.392155 166.310082 \nL 161.892155 164.810082 \nL 163.392155 163.310082 \nL 161.892155 161.810082 \nL 160.392155 163.310082 \nL 158.892155 161.810082 \nL 157.392155 163.310082 \nL 158.892155 164.810082 \nL 157.392155 166.310082 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 171.429197 \nL 387.959943 169.929197 \nL 389.459943 171.429197 \nL 390.959943 169.929197 \nL 389.459943 168.429197 \nL 390.959943 166.929197 \nL 389.459943 165.429197 \nL 387.959943 166.929197 \nL 386.459943 165.429197 \nL 384.959943 166.929197 \nL 386.459943 168.429197 \nL 384.959943 169.929197 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 180.926599 \nL 83.775704 179.426599 \nL 85.275704 180.926599 \nL 86.775704 179.426599 \nL 85.275704 177.926599 \nL 86.775704 176.426599 \nL 85.275704 174.926599 \nL 83.775704 176.426599 \nL 82.275704 174.926599 \nL 80.775704 176.426599 \nL 82.275704 177.926599 \nL 80.775704 179.426599 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 186.034325 \nL 83.941302 184.534325 \nL 85.441302 186.034325 \nL 86.941302 184.534325 \nL 85.441302 183.034325 \nL 86.941302 181.534325 \nL 85.441302 180.034325 \nL 83.941302 181.534325 \nL 82.441302 180.034325 \nL 80.941302 181.534325 \nL 82.441302 183.034325 \nL 80.941302 184.534325 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 187.203213 \nL 84.272497 185.703213 \nL 85.772497 187.203213 \nL 87.272497 185.703213 \nL 85.772497 184.203213 \nL 87.272497 182.703213 \nL 85.772497 181.203213 \nL 84.272497 182.703213 \nL 82.772497 181.203213 \nL 81.272497 182.703213 \nL 82.772497 184.203213 \nL 81.272497 185.703213 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 191.049206 \nL 84.934887 189.549206 \nL 86.434887 191.049206 \nL 87.934887 189.549206 \nL 86.434887 188.049206 \nL 87.934887 186.549206 \nL 86.434887 185.049206 \nL 84.934887 186.549206 \nL 83.434887 185.049206 \nL 81.934887 186.549206 \nL 83.434887 188.049206 \nL 81.934887 189.549206 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 135.562765 \nL 86.259667 134.062765 \nL 87.759667 135.562765 \nL 89.259667 134.062765 \nL 87.759667 132.562765 \nL 89.259667 131.062765 \nL 87.759667 129.562765 \nL 86.259667 131.062765 \nL 84.759667 129.562765 \nL 83.259667 131.062765 \nL 84.759667 132.562765 \nL 83.259667 134.062765 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 213.613584 \nL 88.909227 212.113584 \nL 90.409227 213.613584 \nL 91.909227 212.113584 \nL 90.409227 210.613584 \nL 91.909227 209.113584 \nL 90.409227 207.613584 \nL 88.909227 209.113584 \nL 87.409227 207.613584 \nL 85.909227 209.113584 \nL 87.409227 210.613584 \nL 85.909227 212.113584 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 209.540003 \nC 84.391916 209.540003 85.155046 209.223904 85.717627 208.661323 \nC 86.280208 208.098742 86.596307 207.335612 86.596307 206.540003 \nC 86.596307 205.744394 86.280208 204.981263 85.717627 204.418683 \nC 85.155046 203.856102 84.391916 203.540003 83.596307 203.540003 \nC 82.800698 203.540003 82.037567 203.856102 81.474986 204.418683 \nC 80.912406 204.981263 80.596307 205.744394 80.596307 206.540003 \nC 80.596307 207.335612 80.912406 208.098742 81.474986 208.661323 \nC 82.037567 209.223904 82.800698 209.540003 83.596307 209.540003 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 214.650549 \nC 84.470115 214.650549 85.233245 214.33445 85.795826 213.771869 \nC 86.358407 213.209288 86.674506 212.446158 86.674506 211.650549 \nC 86.674506 210.854939 86.358407 210.091809 85.795826 209.529228 \nC 85.233245 208.966648 84.470115 208.650549 83.674506 208.650549 \nC 82.878896 208.650549 82.115766 208.966648 81.553185 209.529228 \nC 80.990605 210.091809 80.674506 210.854939 80.674506 211.650549 \nC 80.674506 212.446158 80.990605 213.209288 81.553185 213.771869 \nC 82.115766 214.33445 82.878896 214.650549 83.674506 214.650549 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 216.158519 \nC 84.736911 216.158519 85.500041 215.842421 86.062622 215.27984 \nC 86.625203 214.717259 86.941302 213.954129 86.941302 213.158519 \nC 86.941302 212.36291 86.625203 211.59978 86.062622 211.037199 \nC 85.500041 210.474618 84.736911 210.158519 83.941302 210.158519 \nC 83.145692 210.158519 82.382562 210.474618 81.819981 211.037199 \nC 81.257401 211.59978 80.941302 212.36291 80.941302 213.158519 \nC 80.941302 213.954129 81.257401 214.717259 81.819981 215.27984 \nC 82.382562 215.842421 83.145692 216.158519 83.941302 216.158519 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 216.856905 \nC 85.712096 216.856905 86.475227 216.540806 87.037807 215.978225 \nC 87.600388 215.415644 87.916487 214.652514 87.916487 213.856905 \nC 87.916487 213.061295 87.600388 212.298165 87.037807 211.735584 \nC 86.475227 211.173004 85.712096 210.856905 84.916487 210.856905 \nC 84.120878 210.856905 83.357747 211.173004 82.795167 211.735584 \nC 82.232586 212.298165 81.916487 213.061295 81.916487 213.856905 \nC 81.916487 214.652514 82.232586 215.415644 82.795167 215.978225 \nC 83.357747 216.540806 84.120878 216.856905 84.916487 216.856905 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 164.487225 \nC 89.428841 164.487225 90.191971 164.171126 90.754552 163.608545 \nC 91.317132 163.045964 91.633231 162.282834 91.633231 161.487225 \nC 91.633231 160.691616 91.317132 159.928485 90.754552 159.365905 \nC 90.191971 158.803324 89.428841 158.487225 88.633231 158.487225 \nC 87.837622 158.487225 87.074492 158.803324 86.511911 159.365905 \nC 85.94933 159.928485 85.633231 160.691616 85.633231 161.487225 \nC 85.633231 162.282834 85.94933 163.045964 86.511911 163.608545 \nC 87.074492 164.171126 87.837622 164.487225 88.633231 164.487225 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 216.056078 \nC 103.927823 216.056078 104.690954 215.739979 105.253534 215.177399 \nC 105.816115 214.614818 106.132214 213.851688 106.132214 213.056078 \nC 106.132214 212.260469 105.816115 211.497339 105.253534 210.934758 \nC 104.690954 210.372177 103.927823 210.056078 103.132214 210.056078 \nC 102.336605 210.056078 101.573474 210.372177 101.010894 210.934758 \nC 100.448313 211.497339 100.132214 212.260469 100.132214 213.056078 \nC 100.132214 213.851688 100.448313 214.614818 101.010894 215.177399 \nC 101.573474 215.739979 102.336605 216.056078 103.132214 216.056078 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 188.766769 \nC 161.187765 188.766769 161.950895 188.45067 162.513476 187.888089 \nC 163.076056 187.325508 163.392155 186.562378 163.392155 185.766769 \nC 163.392155 184.971159 163.076056 184.208029 162.513476 183.645448 \nC 161.950895 183.082868 161.187765 182.766769 160.392155 182.766769 \nC 159.596546 182.766769 158.833416 183.082868 158.270835 183.645448 \nC 157.708254 184.208029 157.392155 184.971159 157.392155 185.766769 \nC 157.392155 186.562378 157.708254 187.325508 158.270835 187.888089 \nC 158.833416 188.45067 159.596546 188.766769 160.392155 188.766769 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 187.136082 \nC 388.755552 187.136082 389.518683 186.819983 390.081264 186.257402 \nC 390.643844 185.694821 390.959943 184.931691 390.959943 184.136082 \nC 390.959943 183.340472 390.643844 182.577342 390.081264 182.014761 \nC 389.518683 181.452181 388.755552 181.136082 387.959943 181.136082 \nC 387.164334 181.136082 386.401204 181.452181 385.838623 182.014761 \nC 385.276042 182.577342 384.959943 183.340472 384.959943 184.136082 \nC 384.959943 184.931691 385.276042 185.694821 385.838623 186.257402 \nC 386.401204 186.819983 387.164334 187.136082 387.959943 187.136082 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 209.68055 \nC 84.571313 209.68055 85.334444 209.364451 85.897024 208.80187 \nC 86.459605 208.239289 86.775704 207.476159 86.775704 206.68055 \nC 86.775704 205.88494 86.459605 205.12181 85.897024 204.559229 \nC 85.334444 203.996649 84.571313 203.68055 83.775704 203.68055 \nC 82.980095 203.68055 82.216965 203.996649 81.654384 204.559229 \nC 81.091803 205.12181 80.775704 205.88494 80.775704 206.68055 \nC 80.775704 207.476159 81.091803 208.239289 81.654384 208.80187 \nC 82.216965 209.364451 82.980095 209.68055 83.775704 209.68055 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 216.611589 \nC 84.736911 216.611589 85.500041 216.29549 86.062622 215.732909 \nC 86.625203 215.170328 86.941302 214.407198 86.941302 213.611589 \nC 86.941302 212.81598 86.625203 212.052849 86.062622 211.490269 \nC 85.500041 210.927688 84.736911 210.611589 83.941302 210.611589 \nC 83.145692 210.611589 82.382562 210.927688 81.819981 211.490269 \nC 81.257401 212.052849 80.941302 212.81598 80.941302 213.611589 \nC 80.941302 214.407198 81.257401 215.170328 81.819981 215.732909 \nC 82.382562 216.29549 83.145692 216.611589 83.941302 216.611589 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 206.735358 \nC 85.068106 206.735358 85.831236 206.419259 86.393817 205.856678 \nC 86.956398 205.294097 87.272497 204.530967 87.272497 203.735358 \nC 87.272497 202.939749 86.956398 202.176618 86.393817 201.614037 \nC 85.831236 201.051457 85.068106 200.735358 84.272497 200.735358 \nC 83.476887 200.735358 82.713757 201.051457 82.151176 201.614037 \nC 81.588596 202.176618 81.272497 202.939749 81.272497 203.735358 \nC 81.272497 204.530967 81.588596 205.294097 82.151176 205.856678 \nC 82.713757 206.419259 83.476887 206.735358 84.272497 206.735358 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 212.249011 \nC 85.730496 212.249011 86.493626 211.932912 87.056207 211.370331 \nC 87.618788 210.80775 87.934887 210.04462 87.934887 209.249011 \nC 87.934887 208.453401 87.618788 207.690271 87.056207 207.12769 \nC 86.493626 206.56511 85.730496 206.249011 84.934887 206.249011 \nC 84.139277 206.249011 83.376147 206.56511 82.813566 207.12769 \nC 82.250986 207.690271 81.934887 208.453401 81.934887 209.249011 \nC 81.934887 210.04462 82.250986 210.80775 82.813566 211.370331 \nC 83.376147 211.932912 84.139277 212.249011 84.934887 212.249011 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 215.789703 \nC 87.055276 215.789703 87.818406 215.473604 88.380987 214.911023 \nC 88.943568 214.348443 89.259667 213.585312 89.259667 212.789703 \nC 89.259667 211.994094 88.943568 211.230963 88.380987 210.668383 \nC 87.818406 210.105802 87.055276 209.789703 86.259667 209.789703 \nC 85.464058 209.789703 84.700927 210.105802 84.138347 210.668383 \nC 83.575766 211.230963 83.259667 211.994094 83.259667 212.789703 \nC 83.259667 213.585312 83.575766 214.348443 84.138347 214.911023 \nC 84.700927 215.473604 85.464058 215.789703 86.259667 215.789703 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.090185 \nC 89.704836 20.090185 90.467967 19.774086 91.030547 19.211505 \nC 91.593128 18.648925 91.909227 17.885794 91.909227 17.090185 \nC 91.909227 16.294576 91.593128 15.531445 91.030547 14.968865 \nC 90.467967 14.406284 89.704836 14.090185 88.909227 14.090185 \nC 88.113618 14.090185 87.350488 14.406284 86.787907 14.968865 \nC 86.225326 15.531445 85.909227 16.294576 85.909227 17.090185 \nC 85.909227 17.885794 86.225326 18.648925 86.787907 19.211505 \nC 87.350488 19.774086 88.113618 20.090185 88.909227 20.090185 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 213.79405 \nL 83.596307 212.29405 \nL 85.096307 213.79405 \nL 86.596307 212.29405 \nL 85.096307 210.79405 \nL 86.596307 209.29405 \nL 85.096307 207.79405 \nL 83.596307 209.29405 \nL 82.096307 207.79405 \nL 80.596307 209.29405 \nL 82.096307 210.79405 \nL 80.596307 212.29405 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 214.096609 \nL 83.674506 212.596609 \nL 85.174506 214.096609 \nL 86.674506 212.596609 \nL 85.174506 211.096609 \nL 86.674506 209.596609 \nL 85.174506 208.096609 \nL 83.674506 209.596609 \nL 82.174506 208.096609 \nL 80.674506 209.596609 \nL 82.174506 211.096609 \nL 80.674506 212.596609 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 211.196645 \nL 83.941302 209.696645 \nL 85.441302 211.196645 \nL 86.941302 209.696645 \nL 85.441302 208.196645 \nL 86.941302 206.696645 \nL 85.441302 205.196645 \nL 83.941302 206.696645 \nL 82.441302 205.196645 \nL 80.941302 206.696645 \nL 82.441302 208.196645 \nL 80.941302 209.696645 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 193.894756 \nL 84.916487 192.394756 \nL 86.416487 193.894756 \nL 87.916487 192.394756 \nL 86.416487 190.894756 \nL 87.916487 189.394756 \nL 86.416487 187.894756 \nL 84.916487 189.394756 \nL 83.416487 187.894756 \nL 81.916487 189.394756 \nL 83.416487 190.894756 \nL 81.916487 192.394756 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 201.132459 \nL 88.633231 199.632459 \nL 90.133231 201.132459 \nL 91.633231 199.632459 \nL 90.133231 198.132459 \nL 91.633231 196.632459 \nL 90.133231 195.132459 \nL 88.633231 196.632459 \nL 87.133231 195.132459 \nL 85.633231 196.632459 \nL 87.133231 198.132459 \nL 85.633231 199.632459 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 202.094544 \nL 103.132214 200.594544 \nL 104.632214 202.094544 \nL 106.132214 200.594544 \nL 104.632214 199.094544 \nL 106.132214 197.594544 \nL 104.632214 196.094544 \nL 103.132214 197.594544 \nL 101.632214 196.094544 \nL 100.132214 197.594544 \nL 101.632214 199.094544 \nL 100.132214 200.594544 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 164.857083 \nL 160.392155 163.357083 \nL 161.892155 164.857083 \nL 163.392155 163.357083 \nL 161.892155 161.857083 \nL 163.392155 160.357083 \nL 161.892155 158.857083 \nL 160.392155 160.357083 \nL 158.892155 158.857083 \nL 157.392155 160.357083 \nL 158.892155 161.857083 \nL 157.392155 163.357083 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 168.660638 \nL 387.959943 167.160638 \nL 389.459943 168.660638 \nL 390.959943 167.160638 \nL 389.459943 165.660638 \nL 390.959943 164.160638 \nL 389.459943 162.660638 \nL 387.959943 164.160638 \nL 386.459943 162.660638 \nL 384.959943 164.160638 \nL 386.459943 165.660638 \nL 384.959943 167.160638 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 186.616674 \nL 83.775704 185.116674 \nL 85.275704 186.616674 \nL 86.775704 185.116674 \nL 85.275704 183.616674 \nL 86.775704 182.116674 \nL 85.275704 180.616674 \nL 83.775704 182.116674 \nL 82.275704 180.616674 \nL 80.775704 182.116674 \nL 82.275704 183.616674 \nL 80.775704 185.116674 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 213.137321 \nL 83.941302 211.637321 \nL 85.441302 213.137321 \nL 86.941302 211.637321 \nL 85.441302 210.137321 \nL 86.941302 208.637321 \nL 85.441302 207.137321 \nL 83.941302 208.637321 \nL 82.441302 207.137321 \nL 80.941302 208.637321 \nL 82.441302 210.137321 \nL 80.941302 211.637321 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 208.128062 \nL 84.272497 206.628062 \nL 85.772497 208.128062 \nL 87.272497 206.628062 \nL 85.772497 205.128062 \nL 87.272497 203.628062 \nL 85.772497 202.128062 \nL 84.272497 203.628062 \nL 82.772497 202.128062 \nL 81.272497 203.628062 \nL 82.772497 205.128062 \nL 81.272497 206.628062 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 213.521696 \nL 84.934887 212.021696 \nL 86.434887 213.521696 \nL 87.934887 212.021696 \nL 86.434887 210.521696 \nL 87.934887 209.021696 \nL 86.434887 207.521696 \nL 84.934887 209.021696 \nL 83.434887 207.521696 \nL 81.934887 209.021696 \nL 83.434887 210.521696 \nL 81.934887 212.021696 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 197.500535 \nL 86.259667 196.000535 \nL 87.759667 197.500535 \nL 89.259667 196.000535 \nL 87.759667 194.500535 \nL 89.259667 193.000535 \nL 87.759667 191.500535 \nL 86.259667 193.000535 \nL 84.759667 191.500535 \nL 83.259667 193.000535 \nL 84.759667 194.500535 \nL 83.259667 196.000535 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 187.627256 \nL 88.909227 186.127256 \nL 90.409227 187.627256 \nL 91.909227 186.127256 \nL 90.409227 184.627256 \nL 91.909227 183.127256 \nL 90.409227 181.627256 \nL 88.909227 183.127256 \nL 87.409227 181.627256 \nL 85.909227 183.127256 \nL 87.409227 184.627256 \nL 85.909227 186.127256 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 214.496102 \nC 84.391916 214.496102 85.155046 214.180003 85.717627 213.617422 \nC 86.280208 213.054842 86.596307 212.291711 86.596307 211.496102 \nC 86.596307 210.700493 86.280208 209.937362 85.717627 209.374782 \nC 85.155046 208.812201 84.391916 208.496102 83.596307 208.496102 \nC 82.800698 208.496102 82.037567 208.812201 81.474986 209.374782 \nC 80.912406 209.937362 80.596307 210.700493 80.596307 211.496102 \nC 80.596307 212.291711 80.912406 213.054842 81.474986 213.617422 \nC 82.037567 214.180003 82.800698 214.496102 83.596307 214.496102 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 213.9297 \nC 84.470115 213.9297 85.233245 213.613601 85.795826 213.051021 \nC 86.358407 212.48844 86.674506 211.72531 86.674506 210.9297 \nC 86.674506 210.134091 86.358407 209.370961 85.795826 208.80838 \nC 85.233245 208.245799 84.470115 207.9297 83.674506 207.9297 \nC 82.878896 207.9297 82.115766 208.245799 81.553185 208.80838 \nC 80.990605 209.370961 80.674506 210.134091 80.674506 210.9297 \nC 80.674506 211.72531 80.990605 212.48844 81.553185 213.051021 \nC 82.115766 213.613601 82.878896 213.9297 83.674506 213.9297 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 203.106877 \nC 84.736911 203.106877 85.500041 202.790778 86.062622 202.228197 \nC 86.625203 201.665617 86.941302 200.902486 86.941302 200.106877 \nC 86.941302 199.311268 86.625203 198.548137 86.062622 197.985557 \nC 85.500041 197.422976 84.736911 197.106877 83.941302 197.106877 \nC 83.145692 197.106877 82.382562 197.422976 81.819981 197.985557 \nC 81.257401 198.548137 80.941302 199.311268 80.941302 200.106877 \nC 80.941302 200.902486 81.257401 201.665617 81.819981 202.228197 \nC 82.382562 202.790778 83.145692 203.106877 83.941302 203.106877 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 197.655314 \nC 85.712096 197.655314 86.475227 197.339215 87.037807 196.776634 \nC 87.600388 196.214053 87.916487 195.450923 87.916487 194.655314 \nC 87.916487 193.859705 87.600388 193.096574 87.037807 192.533994 \nC 86.475227 191.971413 85.712096 191.655314 84.916487 191.655314 \nC 84.120878 191.655314 83.357747 191.971413 82.795167 192.533994 \nC 82.232586 193.096574 81.916487 193.859705 81.916487 194.655314 \nC 81.916487 195.450923 82.232586 196.214053 82.795167 196.776634 \nC 83.357747 197.339215 84.120878 197.655314 84.916487 197.655314 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 215.703091 \nC 89.428841 215.703091 90.191971 215.386992 90.754552 214.824411 \nC 91.317132 214.26183 91.633231 213.4987 91.633231 212.703091 \nC 91.633231 211.907481 91.317132 211.144351 90.754552 210.58177 \nC 90.191971 210.01919 89.428841 209.703091 88.633231 209.703091 \nC 87.837622 209.703091 87.074492 210.01919 86.511911 210.58177 \nC 85.94933 211.144351 85.633231 211.907481 85.633231 212.703091 \nC 85.633231 213.4987 85.94933 214.26183 86.511911 214.824411 \nC 87.074492 215.386992 87.837622 215.703091 88.633231 215.703091 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 159.862805 \nC 103.927823 159.862805 104.690954 159.546706 105.253534 158.984125 \nC 105.816115 158.421544 106.132214 157.658414 106.132214 156.862805 \nC 106.132214 156.067195 105.816115 155.304065 105.253534 154.741484 \nC 104.690954 154.178904 103.927823 153.862805 103.132214 153.862805 \nC 102.336605 153.862805 101.573474 154.178904 101.010894 154.741484 \nC 100.448313 155.304065 100.132214 156.067195 100.132214 156.862805 \nC 100.132214 157.658414 100.448313 158.421544 101.010894 158.984125 \nC 101.573474 159.546706 102.336605 159.862805 103.132214 159.862805 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 195.763629 \nC 161.187765 195.763629 161.950895 195.44753 162.513476 194.88495 \nC 163.076056 194.322369 163.392155 193.559239 163.392155 192.763629 \nC 163.392155 191.96802 163.076056 191.20489 162.513476 190.642309 \nC 161.950895 190.079728 161.187765 189.763629 160.392155 189.763629 \nC 159.596546 189.763629 158.833416 190.079728 158.270835 190.642309 \nC 157.708254 191.20489 157.392155 191.96802 157.392155 192.763629 \nC 157.392155 193.559239 157.708254 194.322369 158.270835 194.88495 \nC 158.833416 195.44753 159.596546 195.763629 160.392155 195.763629 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 198.064602 \nC 388.755552 198.064602 389.518683 197.748503 390.081264 197.185922 \nC 390.643844 196.623341 390.959943 195.860211 390.959943 195.064602 \nC 390.959943 194.268992 390.643844 193.505862 390.081264 192.943281 \nC 389.518683 192.380701 388.755552 192.064602 387.959943 192.064602 \nC 387.164334 192.064602 386.401204 192.380701 385.838623 192.943281 \nC 385.276042 193.505862 384.959943 194.268992 384.959943 195.064602 \nC 384.959943 195.860211 385.276042 196.623341 385.838623 197.185922 \nC 386.401204 197.748503 387.164334 198.064602 387.959943 198.064602 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 204.482163 \nC 84.571313 204.482163 85.334444 204.166064 85.897024 203.603483 \nC 86.459605 203.040903 86.775704 202.277772 86.775704 201.482163 \nC 86.775704 200.686554 86.459605 199.923423 85.897024 199.360843 \nC 85.334444 198.798262 84.571313 198.482163 83.775704 198.482163 \nC 82.980095 198.482163 82.216965 198.798262 81.654384 199.360843 \nC 81.091803 199.923423 80.775704 200.686554 80.775704 201.482163 \nC 80.775704 202.277772 81.091803 203.040903 81.654384 203.603483 \nC 82.216965 204.166064 82.980095 204.482163 83.775704 204.482163 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 215.238983 \nC 84.736911 215.238983 85.500041 214.922884 86.062622 214.360303 \nC 86.625203 213.797722 86.941302 213.034592 86.941302 212.238983 \nC 86.941302 211.443373 86.625203 210.680243 86.062622 210.117662 \nC 85.500041 209.555081 84.736911 209.238983 83.941302 209.238983 \nC 83.145692 209.238983 82.382562 209.555081 81.819981 210.117662 \nC 81.257401 210.680243 80.941302 211.443373 80.941302 212.238983 \nC 80.941302 213.034592 81.257401 213.797722 81.819981 214.360303 \nC 82.382562 214.922884 83.145692 215.238983 83.941302 215.238983 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 216.015079 \nC 85.068106 216.015079 85.831236 215.69898 86.393817 215.1364 \nC 86.956398 214.573819 87.272497 213.810689 87.272497 213.015079 \nC 87.272497 212.21947 86.956398 211.45634 86.393817 210.893759 \nC 85.831236 210.331178 85.068106 210.015079 84.272497 210.015079 \nC 83.476887 210.015079 82.713757 210.331178 82.151176 210.893759 \nC 81.588596 211.45634 81.272497 212.21947 81.272497 213.015079 \nC 81.272497 213.810689 81.588596 214.573819 82.151176 215.1364 \nC 82.713757 215.69898 83.476887 216.015079 84.272497 216.015079 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 213.763541 \nC 85.730496 213.763541 86.493626 213.447442 87.056207 212.884861 \nC 87.618788 212.322281 87.934887 211.55915 87.934887 210.763541 \nC 87.934887 209.967932 87.618788 209.204801 87.056207 208.642221 \nC 86.493626 208.07964 85.730496 207.763541 84.934887 207.763541 \nC 84.139277 207.763541 83.376147 208.07964 82.813566 208.642221 \nC 82.250986 209.204801 81.934887 209.967932 81.934887 210.763541 \nC 81.934887 211.55915 82.250986 212.322281 82.813566 212.884861 \nC 83.376147 213.447442 84.139277 213.763541 84.934887 213.763541 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 213.79011 \nC 87.055276 213.79011 87.818406 213.474011 88.380987 212.91143 \nC 88.943568 212.348849 89.259667 211.585719 89.259667 210.79011 \nC 89.259667 209.9945 88.943568 209.23137 88.380987 208.668789 \nC 87.818406 208.106208 87.055276 207.79011 86.259667 207.79011 \nC 85.464058 207.79011 84.700927 208.106208 84.138347 208.668789 \nC 83.575766 209.23137 83.259667 209.9945 83.259667 210.79011 \nC 83.259667 211.585719 83.575766 212.348849 84.138347 212.91143 \nC 84.700927 213.474011 85.464058 213.79011 86.259667 213.79011 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.086888 \nC 89.704836 20.086888 90.467967 19.770789 91.030547 19.208209 \nC 91.593128 18.645628 91.909227 17.882498 91.909227 17.086888 \nC 91.909227 16.291279 91.593128 15.528149 91.030547 14.965568 \nC 90.467967 14.402987 89.704836 14.086888 88.909227 14.086888 \nC 88.113618 14.086888 87.350488 14.402987 86.787907 14.965568 \nC 86.225326 15.528149 85.909227 16.291279 85.909227 17.086888 \nC 85.909227 17.882498 86.225326 18.645628 86.787907 19.208209 \nC 87.350488 19.770789 88.113618 20.086888 88.909227 20.086888 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 199.155318 \nL 83.596307 197.655318 \nL 85.096307 199.155318 \nL 86.596307 197.655318 \nL 85.096307 196.155318 \nL 86.596307 194.655318 \nL 85.096307 193.155318 \nL 83.596307 194.655318 \nL 82.096307 193.155318 \nL 80.596307 194.655318 \nL 82.096307 196.155318 \nL 80.596307 197.655318 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 209.239601 \nL 83.674506 207.739601 \nL 85.174506 209.239601 \nL 86.674506 207.739601 \nL 85.174506 206.239601 \nL 86.674506 204.739601 \nL 85.174506 203.239601 \nL 83.674506 204.739601 \nL 82.174506 203.239601 \nL 80.674506 204.739601 \nL 82.174506 206.239601 \nL 80.674506 207.739601 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 212.609583 \nL 83.941302 211.109583 \nL 85.441302 212.609583 \nL 86.941302 211.109583 \nL 85.441302 209.609583 \nL 86.941302 208.109583 \nL 85.441302 206.609583 \nL 83.941302 208.109583 \nL 82.441302 206.609583 \nL 80.941302 208.109583 \nL 82.441302 209.609583 \nL 80.941302 211.109583 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 200.890944 \nL 84.916487 199.390944 \nL 86.416487 200.890944 \nL 87.916487 199.390944 \nL 86.416487 197.890944 \nL 87.916487 196.390944 \nL 86.416487 194.890944 \nL 84.916487 196.390944 \nL 83.416487 194.890944 \nL 81.916487 196.390944 \nL 83.416487 197.890944 \nL 81.916487 199.390944 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 177.280807 \nL 88.633231 175.780807 \nL 90.133231 177.280807 \nL 91.633231 175.780807 \nL 90.133231 174.280807 \nL 91.633231 172.780807 \nL 90.133231 171.280807 \nL 88.633231 172.780807 \nL 87.133231 171.280807 \nL 85.633231 172.780807 \nL 87.133231 174.280807 \nL 85.633231 175.780807 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 166.85294 \nL 103.132214 165.35294 \nL 104.632214 166.85294 \nL 106.132214 165.35294 \nL 104.632214 163.85294 \nL 106.132214 162.35294 \nL 104.632214 160.85294 \nL 103.132214 162.35294 \nL 101.632214 160.85294 \nL 100.132214 162.35294 \nL 101.632214 163.85294 \nL 100.132214 165.35294 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 161.138802 \nL 160.392155 159.638802 \nL 161.892155 161.138802 \nL 163.392155 159.638802 \nL 161.892155 158.138802 \nL 163.392155 156.638802 \nL 161.892155 155.138802 \nL 160.392155 156.638802 \nL 158.892155 155.138802 \nL 157.392155 156.638802 \nL 158.892155 158.138802 \nL 157.392155 159.638802 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 163.965562 \nL 387.959943 162.465562 \nL 389.459943 163.965562 \nL 390.959943 162.465562 \nL 389.459943 160.965562 \nL 390.959943 159.465562 \nL 389.459943 157.965562 \nL 387.959943 159.465562 \nL 386.459943 157.965562 \nL 384.959943 159.465562 \nL 386.459943 160.965562 \nL 384.959943 162.465562 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 173.19219 \nL 83.775704 171.69219 \nL 85.275704 173.19219 \nL 86.775704 171.69219 \nL 85.275704 170.19219 \nL 86.775704 168.69219 \nL 85.275704 167.19219 \nL 83.775704 168.69219 \nL 82.275704 167.19219 \nL 80.775704 168.69219 \nL 82.275704 170.19219 \nL 80.775704 171.69219 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 186.625263 \nL 83.941302 185.125263 \nL 85.441302 186.625263 \nL 86.941302 185.125263 \nL 85.441302 183.625263 \nL 86.941302 182.125263 \nL 85.441302 180.625263 \nL 83.941302 182.125263 \nL 82.441302 180.625263 \nL 80.941302 182.125263 \nL 82.441302 183.625263 \nL 80.941302 185.125263 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 212.577224 \nL 84.272497 211.077224 \nL 85.772497 212.577224 \nL 87.272497 211.077224 \nL 85.772497 209.577224 \nL 87.272497 208.077224 \nL 85.772497 206.577224 \nL 84.272497 208.077224 \nL 82.772497 206.577224 \nL 81.272497 208.077224 \nL 82.772497 209.577224 \nL 81.272497 211.077224 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 197.62587 \nL 84.934887 196.12587 \nL 86.434887 197.62587 \nL 87.934887 196.12587 \nL 86.434887 194.62587 \nL 87.934887 193.12587 \nL 86.434887 191.62587 \nL 84.934887 193.12587 \nL 83.434887 191.62587 \nL 81.934887 193.12587 \nL 83.434887 194.62587 \nL 81.934887 196.12587 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 198.785877 \nL 86.259667 197.285877 \nL 87.759667 198.785877 \nL 89.259667 197.285877 \nL 87.759667 195.785877 \nL 89.259667 194.285877 \nL 87.759667 192.785877 \nL 86.259667 194.285877 \nL 84.759667 192.785877 \nL 83.259667 194.285877 \nL 84.759667 195.785877 \nL 83.259667 197.285877 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 168.67159 \nL 88.909227 167.17159 \nL 90.409227 168.67159 \nL 91.909227 167.17159 \nL 90.409227 165.67159 \nL 91.909227 164.17159 \nL 90.409227 162.67159 \nL 88.909227 164.17159 \nL 87.409227 162.67159 \nL 85.909227 164.17159 \nL 87.409227 165.67159 \nL 85.909227 167.17159 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 214.278585 \nC 84.391916 214.278585 85.155046 213.962486 85.717627 213.399905 \nC 86.280208 212.837325 86.596307 212.074194 86.596307 211.278585 \nC 86.596307 210.482976 86.280208 209.719845 85.717627 209.157265 \nC 85.155046 208.594684 84.391916 208.278585 83.596307 208.278585 \nC 82.800698 208.278585 82.037567 208.594684 81.474986 209.157265 \nC 80.912406 209.719845 80.596307 210.482976 80.596307 211.278585 \nC 80.596307 212.074194 80.912406 212.837325 81.474986 213.399905 \nC 82.037567 213.962486 82.800698 214.278585 83.596307 214.278585 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 216.057098 \nC 84.470115 216.057098 85.233245 215.740999 85.795826 215.178418 \nC 86.358407 214.615837 86.674506 213.852707 86.674506 213.057098 \nC 86.674506 212.261488 86.358407 211.498358 85.795826 210.935777 \nC 85.233245 210.373197 84.470115 210.057098 83.674506 210.057098 \nC 82.878896 210.057098 82.115766 210.373197 81.553185 210.935777 \nC 80.990605 211.498358 80.674506 212.261488 80.674506 213.057098 \nC 80.674506 213.852707 80.990605 214.615837 81.553185 215.178418 \nC 82.115766 215.740999 82.878896 216.057098 83.674506 216.057098 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 193.30854 \nC 84.736911 193.30854 85.500041 192.992441 86.062622 192.42986 \nC 86.625203 191.867279 86.941302 191.104149 86.941302 190.30854 \nC 86.941302 189.512931 86.625203 188.7498 86.062622 188.187219 \nC 85.500041 187.624639 84.736911 187.30854 83.941302 187.30854 \nC 83.145692 187.30854 82.382562 187.624639 81.819981 188.187219 \nC 81.257401 188.7498 80.941302 189.512931 80.941302 190.30854 \nC 80.941302 191.104149 81.257401 191.867279 81.819981 192.42986 \nC 82.382562 192.992441 83.145692 193.30854 83.941302 193.30854 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 216.233716 \nC 85.712096 216.233716 86.475227 215.917617 87.037807 215.355037 \nC 87.600388 214.792456 87.916487 214.029325 87.916487 213.233716 \nC 87.916487 212.438107 87.600388 211.674977 87.037807 211.112396 \nC 86.475227 210.549815 85.712096 210.233716 84.916487 210.233716 \nC 84.120878 210.233716 83.357747 210.549815 82.795167 211.112396 \nC 82.232586 211.674977 81.916487 212.438107 81.916487 213.233716 \nC 81.916487 214.029325 82.232586 214.792456 82.795167 215.355037 \nC 83.357747 215.917617 84.120878 216.233716 84.916487 216.233716 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 211.316933 \nC 89.428841 211.316933 90.191971 211.000834 90.754552 210.438254 \nC 91.317132 209.875673 91.633231 209.112543 91.633231 208.316933 \nC 91.633231 207.521324 91.317132 206.758194 90.754552 206.195613 \nC 90.191971 205.633032 89.428841 205.316933 88.633231 205.316933 \nC 87.837622 205.316933 87.074492 205.633032 86.511911 206.195613 \nC 85.94933 206.758194 85.633231 207.521324 85.633231 208.316933 \nC 85.633231 209.112543 85.94933 209.875673 86.511911 210.438254 \nC 87.074492 211.000834 87.837622 211.316933 88.633231 211.316933 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 187.830245 \nC 103.927823 187.830245 104.690954 187.514146 105.253534 186.951565 \nC 105.816115 186.388984 106.132214 185.625854 106.132214 184.830245 \nC 106.132214 184.034635 105.816115 183.271505 105.253534 182.708924 \nC 104.690954 182.146343 103.927823 181.830245 103.132214 181.830245 \nC 102.336605 181.830245 101.573474 182.146343 101.010894 182.708924 \nC 100.448313 183.271505 100.132214 184.034635 100.132214 184.830245 \nC 100.132214 185.625854 100.448313 186.388984 101.010894 186.951565 \nC 101.573474 187.514146 102.336605 187.830245 103.132214 187.830245 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 184.871628 \nC 161.187765 184.871628 161.950895 184.555529 162.513476 183.992948 \nC 163.076056 183.430368 163.392155 182.667237 163.392155 181.871628 \nC 163.392155 181.076019 163.076056 180.312888 162.513476 179.750308 \nC 161.950895 179.187727 161.187765 178.871628 160.392155 178.871628 \nC 159.596546 178.871628 158.833416 179.187727 158.270835 179.750308 \nC 157.708254 180.312888 157.392155 181.076019 157.392155 181.871628 \nC 157.392155 182.667237 157.708254 183.430368 158.270835 183.992948 \nC 158.833416 184.555529 159.596546 184.871628 160.392155 184.871628 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 205.282589 \nC 388.755552 205.282589 389.518683 204.96649 390.081264 204.403909 \nC 390.643844 203.841328 390.959943 203.078198 390.959943 202.282589 \nC 390.959943 201.486979 390.643844 200.723849 390.081264 200.161268 \nC 389.518683 199.598688 388.755552 199.282589 387.959943 199.282589 \nC 387.164334 199.282589 386.401204 199.598688 385.838623 200.161268 \nC 385.276042 200.723849 384.959943 201.486979 384.959943 202.282589 \nC 384.959943 203.078198 385.276042 203.841328 385.838623 204.403909 \nC 386.401204 204.96649 387.164334 205.282589 387.959943 205.282589 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 216.19436 \nC 84.571313 216.19436 85.334444 215.878261 85.897024 215.31568 \nC 86.459605 214.753099 86.775704 213.989969 86.775704 213.19436 \nC 86.775704 212.39875 86.459605 211.63562 85.897024 211.073039 \nC 85.334444 210.510459 84.571313 210.19436 83.775704 210.19436 \nC 82.980095 210.19436 82.216965 210.510459 81.654384 211.073039 \nC 81.091803 211.63562 80.775704 212.39875 80.775704 213.19436 \nC 80.775704 213.989969 81.091803 214.753099 81.654384 215.31568 \nC 82.216965 215.878261 82.980095 216.19436 83.775704 216.19436 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 214.629486 \nC 84.736911 214.629486 85.500041 214.313387 86.062622 213.750806 \nC 86.625203 213.188225 86.941302 212.425095 86.941302 211.629486 \nC 86.941302 210.833877 86.625203 210.070746 86.062622 209.508165 \nC 85.500041 208.945585 84.736911 208.629486 83.941302 208.629486 \nC 83.145692 208.629486 82.382562 208.945585 81.819981 209.508165 \nC 81.257401 210.070746 80.941302 210.833877 80.941302 211.629486 \nC 80.941302 212.425095 81.257401 213.188225 81.819981 213.750806 \nC 82.382562 214.313387 83.145692 214.629486 83.941302 214.629486 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 216.134087 \nC 85.068106 216.134087 85.831236 215.817988 86.393817 215.255407 \nC 86.956398 214.692826 87.272497 213.929696 87.272497 213.134087 \nC 87.272497 212.338477 86.956398 211.575347 86.393817 211.012766 \nC 85.831236 210.450186 85.068106 210.134087 84.272497 210.134087 \nC 83.476887 210.134087 82.713757 210.450186 82.151176 211.012766 \nC 81.588596 211.575347 81.272497 212.338477 81.272497 213.134087 \nC 81.272497 213.929696 81.588596 214.692826 82.151176 215.255407 \nC 82.713757 215.817988 83.476887 216.134087 84.272497 216.134087 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 209.795788 \nC 85.730496 209.795788 86.493626 209.479689 87.056207 208.917108 \nC 87.618788 208.354527 87.934887 207.591397 87.934887 206.795788 \nC 87.934887 206.000178 87.618788 205.237048 87.056207 204.674467 \nC 86.493626 204.111886 85.730496 203.795788 84.934887 203.795788 \nC 84.139277 203.795788 83.376147 204.111886 82.813566 204.674467 \nC 82.250986 205.237048 81.934887 206.000178 81.934887 206.795788 \nC 81.934887 207.591397 82.250986 208.354527 82.813566 208.917108 \nC 83.376147 209.479689 84.139277 209.795788 84.934887 209.795788 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 211.879636 \nC 87.055276 211.879636 87.818406 211.563537 88.380987 211.000957 \nC 88.943568 210.438376 89.259667 209.675246 89.259667 208.879636 \nC 89.259667 208.084027 88.943568 207.320897 88.380987 206.758316 \nC 87.818406 206.195735 87.055276 205.879636 86.259667 205.879636 \nC 85.464058 205.879636 84.700927 206.195735 84.138347 206.758316 \nC 83.575766 207.320897 83.259667 208.084027 83.259667 208.879636 \nC 83.259667 209.675246 83.575766 210.438376 84.138347 211.000957 \nC 84.700927 211.563537 85.464058 211.879636 86.259667 211.879636 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.085161 \nC 89.704836 20.085161 90.467967 19.769062 91.030547 19.206482 \nC 91.593128 18.643901 91.909227 17.880771 91.909227 17.085161 \nC 91.909227 16.289552 91.593128 15.526422 91.030547 14.963841 \nC 90.467967 14.40126 89.704836 14.085161 88.909227 14.085161 \nC 88.113618 14.085161 87.350488 14.40126 86.787907 14.963841 \nC 86.225326 15.526422 85.909227 16.289552 85.909227 17.085161 \nC 85.909227 17.880771 86.225326 18.643901 86.787907 19.206482 \nC 87.350488 19.769062 88.113618 20.085161 88.909227 20.085161 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 214.780864 \nL 83.596307 213.280864 \nL 85.096307 214.780864 \nL 86.596307 213.280864 \nL 85.096307 211.780864 \nL 86.596307 210.280864 \nL 85.096307 208.780864 \nL 83.596307 210.280864 \nL 82.096307 208.780864 \nL 80.596307 210.280864 \nL 82.096307 211.780864 \nL 80.596307 213.280864 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 176.069165 \nL 83.674506 174.569165 \nL 85.174506 176.069165 \nL 86.674506 174.569165 \nL 85.174506 173.069165 \nL 86.674506 171.569165 \nL 85.174506 170.069165 \nL 83.674506 171.569165 \nL 82.174506 170.069165 \nL 80.674506 171.569165 \nL 82.174506 173.069165 \nL 80.674506 174.569165 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 208.143366 \nL 83.941302 206.643366 \nL 85.441302 208.143366 \nL 86.941302 206.643366 \nL 85.441302 205.143366 \nL 86.941302 203.643366 \nL 85.441302 202.143366 \nL 83.941302 203.643366 \nL 82.441302 202.143366 \nL 80.941302 203.643366 \nL 82.441302 205.143366 \nL 80.941302 206.643366 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 197.836125 \nL 84.916487 196.336125 \nL 86.416487 197.836125 \nL 87.916487 196.336125 \nL 86.416487 194.836125 \nL 87.916487 193.336125 \nL 86.416487 191.836125 \nL 84.916487 193.336125 \nL 83.416487 191.836125 \nL 81.916487 193.336125 \nL 83.416487 194.836125 \nL 81.916487 196.336125 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 201.630162 \nL 88.633231 200.130162 \nL 90.133231 201.630162 \nL 91.633231 200.130162 \nL 90.133231 198.630162 \nL 91.633231 197.130162 \nL 90.133231 195.630162 \nL 88.633231 197.130162 \nL 87.133231 195.630162 \nL 85.633231 197.130162 \nL 87.133231 198.630162 \nL 85.633231 200.130162 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 173.968144 \nL 103.132214 172.468144 \nL 104.632214 173.968144 \nL 106.132214 172.468144 \nL 104.632214 170.968144 \nL 106.132214 169.468144 \nL 104.632214 167.968144 \nL 103.132214 169.468144 \nL 101.632214 167.968144 \nL 100.132214 169.468144 \nL 101.632214 170.968144 \nL 100.132214 172.468144 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 174.061734 \nL 160.392155 172.561734 \nL 161.892155 174.061734 \nL 163.392155 172.561734 \nL 161.892155 171.061734 \nL 163.392155 169.561734 \nL 161.892155 168.061734 \nL 160.392155 169.561734 \nL 158.892155 168.061734 \nL 157.392155 169.561734 \nL 158.892155 171.061734 \nL 157.392155 172.561734 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 164.253937 \nL 387.959943 162.753937 \nL 389.459943 164.253937 \nL 390.959943 162.753937 \nL 389.459943 161.253937 \nL 390.959943 159.753937 \nL 389.459943 158.253937 \nL 387.959943 159.753937 \nL 386.459943 158.253937 \nL 384.959943 159.753937 \nL 386.459943 161.253937 \nL 384.959943 162.753937 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 188.806448 \nL 83.775704 187.306448 \nL 85.275704 188.806448 \nL 86.775704 187.306448 \nL 85.275704 185.806448 \nL 86.775704 184.306448 \nL 85.275704 182.806448 \nL 83.775704 184.306448 \nL 82.275704 182.806448 \nL 80.775704 184.306448 \nL 82.275704 185.806448 \nL 80.775704 187.306448 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 182.159807 \nL 83.941302 180.659807 \nL 85.441302 182.159807 \nL 86.941302 180.659807 \nL 85.441302 179.159807 \nL 86.941302 177.659807 \nL 85.441302 176.159807 \nL 83.941302 177.659807 \nL 82.441302 176.159807 \nL 80.941302 177.659807 \nL 82.441302 179.159807 \nL 80.941302 180.659807 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 194.854046 \nL 84.272497 193.354046 \nL 85.772497 194.854046 \nL 87.272497 193.354046 \nL 85.772497 191.854046 \nL 87.272497 190.354046 \nL 85.772497 188.854046 \nL 84.272497 190.354046 \nL 82.772497 188.854046 \nL 81.272497 190.354046 \nL 82.772497 191.854046 \nL 81.272497 193.354046 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 182.368013 \nL 84.934887 180.868013 \nL 86.434887 182.368013 \nL 87.934887 180.868013 \nL 86.434887 179.368013 \nL 87.934887 177.868013 \nL 86.434887 176.368013 \nL 84.934887 177.868013 \nL 83.434887 176.368013 \nL 81.934887 177.868013 \nL 83.434887 179.368013 \nL 81.934887 180.868013 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 217.756364 \nL 86.259667 216.256364 \nL 87.759667 217.756364 \nL 89.259667 216.256364 \nL 87.759667 214.756364 \nL 89.259667 213.256364 \nL 87.759667 211.756364 \nL 86.259667 213.256364 \nL 84.759667 211.756364 \nL 83.259667 213.256364 \nL 84.759667 214.756364 \nL 83.259667 216.256364 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 173.263012 \nL 88.909227 171.763012 \nL 90.409227 173.263012 \nL 91.909227 171.763012 \nL 90.409227 170.263012 \nL 91.909227 168.763012 \nL 90.409227 167.263012 \nL 88.909227 168.763012 \nL 87.409227 167.263012 \nL 85.909227 168.763012 \nL 87.409227 170.263012 \nL 85.909227 171.763012 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 213.534446 \nC 84.391916 213.534446 85.155046 213.218347 85.717627 212.655766 \nC 86.280208 212.093186 86.596307 211.330055 86.596307 210.534446 \nC 86.596307 209.738837 86.280208 208.975706 85.717627 208.413126 \nC 85.155046 207.850545 84.391916 207.534446 83.596307 207.534446 \nC 82.800698 207.534446 82.037567 207.850545 81.474986 208.413126 \nC 80.912406 208.975706 80.596307 209.738837 80.596307 210.534446 \nC 80.596307 211.330055 80.912406 212.093186 81.474986 212.655766 \nC 82.037567 213.218347 82.800698 213.534446 83.596307 213.534446 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 214.951426 \nC 84.470115 214.951426 85.233245 214.635327 85.795826 214.072747 \nC 86.358407 213.510166 86.674506 212.747036 86.674506 211.951426 \nC 86.674506 211.155817 86.358407 210.392687 85.795826 209.830106 \nC 85.233245 209.267525 84.470115 208.951426 83.674506 208.951426 \nC 82.878896 208.951426 82.115766 209.267525 81.553185 209.830106 \nC 80.990605 210.392687 80.674506 211.155817 80.674506 211.951426 \nC 80.674506 212.747036 80.990605 213.510166 81.553185 214.072747 \nC 82.115766 214.635327 82.878896 214.951426 83.674506 214.951426 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 214.472153 \nC 84.736911 214.472153 85.500041 214.156054 86.062622 213.593473 \nC 86.625203 213.030892 86.941302 212.267762 86.941302 211.472153 \nC 86.941302 210.676543 86.625203 209.913413 86.062622 209.350832 \nC 85.500041 208.788252 84.736911 208.472153 83.941302 208.472153 \nC 83.145692 208.472153 82.382562 208.788252 81.819981 209.350832 \nC 81.257401 209.913413 80.941302 210.676543 80.941302 211.472153 \nC 80.941302 212.267762 81.257401 213.030892 81.819981 213.593473 \nC 82.382562 214.156054 83.145692 214.472153 83.941302 214.472153 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 212.376404 \nC 85.712096 212.376404 86.475227 212.060305 87.037807 211.497724 \nC 87.600388 210.935143 87.916487 210.172013 87.916487 209.376404 \nC 87.916487 208.580794 87.600388 207.817664 87.037807 207.255083 \nC 86.475227 206.692502 85.712096 206.376404 84.916487 206.376404 \nC 84.120878 206.376404 83.357747 206.692502 82.795167 207.255083 \nC 82.232586 207.817664 81.916487 208.580794 81.916487 209.376404 \nC 81.916487 210.172013 82.232586 210.935143 82.795167 211.497724 \nC 83.357747 212.060305 84.120878 212.376404 84.916487 212.376404 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 208.999126 \nC 89.428841 208.999126 90.191971 208.683027 90.754552 208.120446 \nC 91.317132 207.557866 91.633231 206.794735 91.633231 205.999126 \nC 91.633231 205.203517 91.317132 204.440386 90.754552 203.877806 \nC 90.191971 203.315225 89.428841 202.999126 88.633231 202.999126 \nC 87.837622 202.999126 87.074492 203.315225 86.511911 203.877806 \nC 85.94933 204.440386 85.633231 205.203517 85.633231 205.999126 \nC 85.633231 206.794735 85.94933 207.557866 86.511911 208.120446 \nC 87.074492 208.683027 87.837622 208.999126 88.633231 208.999126 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 179.703041 \nC 103.927823 179.703041 104.690954 179.386942 105.253534 178.824361 \nC 105.816115 178.26178 106.132214 177.49865 106.132214 176.703041 \nC 106.132214 175.907431 105.816115 175.144301 105.253534 174.58172 \nC 104.690954 174.01914 103.927823 173.703041 103.132214 173.703041 \nC 102.336605 173.703041 101.573474 174.01914 101.010894 174.58172 \nC 100.448313 175.144301 100.132214 175.907431 100.132214 176.703041 \nC 100.132214 177.49865 100.448313 178.26178 101.010894 178.824361 \nC 101.573474 179.386942 102.336605 179.703041 103.132214 179.703041 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 174.778549 \nC 161.187765 174.778549 161.950895 174.46245 162.513476 173.89987 \nC 163.076056 173.337289 163.392155 172.574159 163.392155 171.778549 \nC 163.392155 170.98294 163.076056 170.21981 162.513476 169.657229 \nC 161.950895 169.094648 161.187765 168.778549 160.392155 168.778549 \nC 159.596546 168.778549 158.833416 169.094648 158.270835 169.657229 \nC 157.708254 170.21981 157.392155 170.98294 157.392155 171.778549 \nC 157.392155 172.574159 157.708254 173.337289 158.270835 173.89987 \nC 158.833416 174.46245 159.596546 174.778549 160.392155 174.778549 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 161.216919 \nC 388.755552 161.216919 389.518683 160.90082 390.081264 160.33824 \nC 390.643844 159.775659 390.959943 159.012528 390.959943 158.216919 \nC 390.959943 157.42131 390.643844 156.65818 390.081264 156.095599 \nC 389.518683 155.533018 388.755552 155.216919 387.959943 155.216919 \nC 387.164334 155.216919 386.401204 155.533018 385.838623 156.095599 \nC 385.276042 156.65818 384.959943 157.42131 384.959943 158.216919 \nC 384.959943 159.012528 385.276042 159.775659 385.838623 160.33824 \nC 386.401204 160.90082 387.164334 161.216919 387.959943 161.216919 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 214.796931 \nC 84.571313 214.796931 85.334444 214.480832 85.897024 213.918252 \nC 86.459605 213.355671 86.775704 212.592541 86.775704 211.796931 \nC 86.775704 211.001322 86.459605 210.238192 85.897024 209.675611 \nC 85.334444 209.11303 84.571313 208.796931 83.775704 208.796931 \nC 82.980095 208.796931 82.216965 209.11303 81.654384 209.675611 \nC 81.091803 210.238192 80.775704 211.001322 80.775704 211.796931 \nC 80.775704 212.592541 81.091803 213.355671 81.654384 213.918252 \nC 82.216965 214.480832 82.980095 214.796931 83.775704 214.796931 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 215.943927 \nC 84.736911 215.943927 85.500041 215.627828 86.062622 215.065247 \nC 86.625203 214.502667 86.941302 213.739536 86.941302 212.943927 \nC 86.941302 212.148318 86.625203 211.385187 86.062622 210.822607 \nC 85.500041 210.260026 84.736911 209.943927 83.941302 209.943927 \nC 83.145692 209.943927 82.382562 210.260026 81.819981 210.822607 \nC 81.257401 211.385187 80.941302 212.148318 80.941302 212.943927 \nC 80.941302 213.739536 81.257401 214.502667 81.819981 215.065247 \nC 82.382562 215.627828 83.145692 215.943927 83.941302 215.943927 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 213.011431 \nC 85.068106 213.011431 85.831236 212.695332 86.393817 212.132751 \nC 86.956398 211.57017 87.272497 210.80704 87.272497 210.011431 \nC 87.272497 209.215822 86.956398 208.452691 86.393817 207.890111 \nC 85.831236 207.32753 85.068106 207.011431 84.272497 207.011431 \nC 83.476887 207.011431 82.713757 207.32753 82.151176 207.890111 \nC 81.588596 208.452691 81.272497 209.215822 81.272497 210.011431 \nC 81.272497 210.80704 81.588596 211.57017 82.151176 212.132751 \nC 82.713757 212.695332 83.476887 213.011431 84.272497 213.011431 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 214.455949 \nC 85.730496 214.455949 86.493626 214.13985 87.056207 213.57727 \nC 87.618788 213.014689 87.934887 212.251559 87.934887 211.455949 \nC 87.934887 210.66034 87.618788 209.89721 87.056207 209.334629 \nC 86.493626 208.772048 85.730496 208.455949 84.934887 208.455949 \nC 84.139277 208.455949 83.376147 208.772048 82.813566 209.334629 \nC 82.250986 209.89721 81.934887 210.66034 81.934887 211.455949 \nC 81.934887 212.251559 82.250986 213.014689 82.813566 213.57727 \nC 83.376147 214.13985 84.139277 214.455949 84.934887 214.455949 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 213.756937 \nC 87.055276 213.756937 87.818406 213.440838 88.380987 212.878257 \nC 88.943568 212.315676 89.259667 211.552546 89.259667 210.756937 \nC 89.259667 209.961327 88.943568 209.198197 88.380987 208.635616 \nC 87.818406 208.073036 87.055276 207.756937 86.259667 207.756937 \nC 85.464058 207.756937 84.700927 208.073036 84.138347 208.635616 \nC 83.575766 209.198197 83.259667 209.961327 83.259667 210.756937 \nC 83.259667 211.552546 83.575766 212.315676 84.138347 212.878257 \nC 84.700927 213.440838 85.464058 213.756937 86.259667 213.756937 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.085991 \nC 89.704836 20.085991 90.467967 19.769892 91.030547 19.207312 \nC 91.593128 18.644731 91.909227 17.8816 91.909227 17.085991 \nC 91.909227 16.290382 91.593128 15.527252 91.030547 14.964671 \nC 90.467967 14.40209 89.704836 14.085991 88.909227 14.085991 \nC 88.113618 14.085991 87.350488 14.40209 86.787907 14.964671 \nC 86.225326 15.527252 85.909227 16.290382 85.909227 17.085991 \nC 85.909227 17.8816 86.225326 18.644731 86.787907 19.207312 \nC 87.350488 19.769892 88.113618 20.085991 88.909227 20.085991 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 208.453772 \nL 83.596307 206.953772 \nL 85.096307 208.453772 \nL 86.596307 206.953772 \nL 85.096307 205.453772 \nL 86.596307 203.953772 \nL 85.096307 202.453772 \nL 83.596307 203.953772 \nL 82.096307 202.453772 \nL 80.596307 203.953772 \nL 82.096307 205.453772 \nL 80.596307 206.953772 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 180.306563 \nL 83.674506 178.806563 \nL 85.174506 180.306563 \nL 86.674506 178.806563 \nL 85.174506 177.306563 \nL 86.674506 175.806563 \nL 85.174506 174.306563 \nL 83.674506 175.806563 \nL 82.174506 174.306563 \nL 80.674506 175.806563 \nL 82.174506 177.306563 \nL 80.674506 178.806563 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 179.355836 \nL 83.941302 177.855836 \nL 85.441302 179.355836 \nL 86.941302 177.855836 \nL 85.441302 176.355836 \nL 86.941302 174.855836 \nL 85.441302 173.355836 \nL 83.941302 174.855836 \nL 82.441302 173.355836 \nL 80.941302 174.855836 \nL 82.441302 176.355836 \nL 80.941302 177.855836 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 202.646053 \nL 84.916487 201.146053 \nL 86.416487 202.646053 \nL 87.916487 201.146053 \nL 86.416487 199.646053 \nL 87.916487 198.146053 \nL 86.416487 196.646053 \nL 84.916487 198.146053 \nL 83.416487 196.646053 \nL 81.916487 198.146053 \nL 83.416487 199.646053 \nL 81.916487 201.146053 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 180.51488 \nL 88.633231 179.01488 \nL 90.133231 180.51488 \nL 91.633231 179.01488 \nL 90.133231 177.51488 \nL 91.633231 176.01488 \nL 90.133231 174.51488 \nL 88.633231 176.01488 \nL 87.133231 174.51488 \nL 85.633231 176.01488 \nL 87.133231 177.51488 \nL 85.633231 179.01488 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 189.063098 \nL 103.132214 187.563098 \nL 104.632214 189.063098 \nL 106.132214 187.563098 \nL 104.632214 186.063098 \nL 106.132214 184.563098 \nL 104.632214 183.063098 \nL 103.132214 184.563098 \nL 101.632214 183.063098 \nL 100.132214 184.563098 \nL 101.632214 186.063098 \nL 100.132214 187.563098 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 171.618128 \nL 160.392155 170.118128 \nL 161.892155 171.618128 \nL 163.392155 170.118128 \nL 161.892155 168.618128 \nL 163.392155 167.118128 \nL 161.892155 165.618128 \nL 160.392155 167.118128 \nL 158.892155 165.618128 \nL 157.392155 167.118128 \nL 158.892155 168.618128 \nL 157.392155 170.118128 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 183.09282 \nL 387.959943 181.59282 \nL 389.459943 183.09282 \nL 390.959943 181.59282 \nL 389.459943 180.09282 \nL 390.959943 178.59282 \nL 389.459943 177.09282 \nL 387.959943 178.59282 \nL 386.459943 177.09282 \nL 384.959943 178.59282 \nL 386.459943 180.09282 \nL 384.959943 181.59282 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 166.466422 \nL 83.775704 164.966422 \nL 85.275704 166.466422 \nL 86.775704 164.966422 \nL 85.275704 163.466422 \nL 86.775704 161.966422 \nL 85.275704 160.466422 \nL 83.775704 161.966422 \nL 82.275704 160.466422 \nL 80.775704 161.966422 \nL 82.275704 163.466422 \nL 80.775704 164.966422 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 206.991712 \nL 83.941302 205.491712 \nL 85.441302 206.991712 \nL 86.941302 205.491712 \nL 85.441302 203.991712 \nL 86.941302 202.491712 \nL 85.441302 200.991712 \nL 83.941302 202.491712 \nL 82.441302 200.991712 \nL 80.941302 202.491712 \nL 82.441302 203.991712 \nL 80.941302 205.491712 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 216.018628 \nL 84.272497 214.518628 \nL 85.772497 216.018628 \nL 87.272497 214.518628 \nL 85.772497 213.018628 \nL 87.272497 211.518628 \nL 85.772497 210.018628 \nL 84.272497 211.518628 \nL 82.772497 210.018628 \nL 81.272497 211.518628 \nL 82.772497 213.018628 \nL 81.272497 214.518628 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 180.271855 \nL 84.934887 178.771855 \nL 86.434887 180.271855 \nL 87.934887 178.771855 \nL 86.434887 177.271855 \nL 87.934887 175.771855 \nL 86.434887 174.271855 \nL 84.934887 175.771855 \nL 83.434887 174.271855 \nL 81.934887 175.771855 \nL 83.434887 177.271855 \nL 81.934887 178.771855 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 185.506549 \nL 86.259667 184.006549 \nL 87.759667 185.506549 \nL 89.259667 184.006549 \nL 87.759667 182.506549 \nL 89.259667 181.006549 \nL 87.759667 179.506549 \nL 86.259667 181.006549 \nL 84.759667 179.506549 \nL 83.259667 181.006549 \nL 84.759667 182.506549 \nL 83.259667 184.006549 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 213.118778 \nL 88.909227 211.618778 \nL 90.409227 213.118778 \nL 91.909227 211.618778 \nL 90.409227 210.118778 \nL 91.909227 208.618778 \nL 90.409227 207.118778 \nL 88.909227 208.618778 \nL 87.409227 207.118778 \nL 85.909227 208.618778 \nL 87.409227 210.118778 \nL 85.909227 211.618778 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 214.380092 \nC 84.391916 214.380092 85.155046 214.063993 85.717627 213.501413 \nC 86.280208 212.938832 86.596307 212.175702 86.596307 211.380092 \nC 86.596307 210.584483 86.280208 209.821353 85.717627 209.258772 \nC 85.155046 208.696191 84.391916 208.380092 83.596307 208.380092 \nC 82.800698 208.380092 82.037567 208.696191 81.474986 209.258772 \nC 80.912406 209.821353 80.596307 210.584483 80.596307 211.380092 \nC 80.596307 212.175702 80.912406 212.938832 81.474986 213.501413 \nC 82.037567 214.063993 82.800698 214.380092 83.596307 214.380092 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 214.656943 \nC 84.470115 214.656943 85.233245 214.340844 85.795826 213.778263 \nC 86.358407 213.215683 86.674506 212.452552 86.674506 211.656943 \nC 86.674506 210.861334 86.358407 210.098204 85.795826 209.535623 \nC 85.233245 208.973042 84.470115 208.656943 83.674506 208.656943 \nC 82.878896 208.656943 82.115766 208.973042 81.553185 209.535623 \nC 80.990605 210.098204 80.674506 210.861334 80.674506 211.656943 \nC 80.674506 212.452552 80.990605 213.215683 81.553185 213.778263 \nC 82.115766 214.340844 82.878896 214.656943 83.674506 214.656943 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 216.035767 \nC 84.736911 216.035767 85.500041 215.719668 86.062622 215.157087 \nC 86.625203 214.594507 86.941302 213.831376 86.941302 213.035767 \nC 86.941302 212.240158 86.625203 211.477027 86.062622 210.914447 \nC 85.500041 210.351866 84.736911 210.035767 83.941302 210.035767 \nC 83.145692 210.035767 82.382562 210.351866 81.819981 210.914447 \nC 81.257401 211.477027 80.941302 212.240158 80.941302 213.035767 \nC 80.941302 213.831376 81.257401 214.594507 81.819981 215.157087 \nC 82.382562 215.719668 83.145692 216.035767 83.941302 216.035767 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 210.903781 \nC 85.712096 210.903781 86.475227 210.587682 87.037807 210.025101 \nC 87.600388 209.46252 87.916487 208.69939 87.916487 207.903781 \nC 87.916487 207.108171 87.600388 206.345041 87.037807 205.78246 \nC 86.475227 205.21988 85.712096 204.903781 84.916487 204.903781 \nC 84.120878 204.903781 83.357747 205.21988 82.795167 205.78246 \nC 82.232586 206.345041 81.916487 207.108171 81.916487 207.903781 \nC 81.916487 208.69939 82.232586 209.46252 82.795167 210.025101 \nC 83.357747 210.587682 84.120878 210.903781 84.916487 210.903781 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 215.93819 \nC 89.428841 215.93819 90.191971 215.622091 90.754552 215.059511 \nC 91.317132 214.49693 91.633231 213.7338 91.633231 212.93819 \nC 91.633231 212.142581 91.317132 211.379451 90.754552 210.81687 \nC 90.191971 210.254289 89.428841 209.93819 88.633231 209.93819 \nC 87.837622 209.93819 87.074492 210.254289 86.511911 210.81687 \nC 85.94933 211.379451 85.633231 212.142581 85.633231 212.93819 \nC 85.633231 213.7338 85.94933 214.49693 86.511911 215.059511 \nC 87.074492 215.622091 87.837622 215.93819 88.633231 215.93819 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 171.173739 \nC 103.927823 171.173739 104.690954 170.85764 105.253534 170.295059 \nC 105.816115 169.732478 106.132214 168.969348 106.132214 168.173739 \nC 106.132214 167.378129 105.816115 166.614999 105.253534 166.052418 \nC 104.690954 165.489837 103.927823 165.173739 103.132214 165.173739 \nC 102.336605 165.173739 101.573474 165.489837 101.010894 166.052418 \nC 100.448313 166.614999 100.132214 167.378129 100.132214 168.173739 \nC 100.132214 168.969348 100.448313 169.732478 101.010894 170.295059 \nC 101.573474 170.85764 102.336605 171.173739 103.132214 171.173739 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 193.372872 \nC 161.187765 193.372872 161.950895 193.056773 162.513476 192.494192 \nC 163.076056 191.931611 163.392155 191.168481 163.392155 190.372872 \nC 163.392155 189.577262 163.076056 188.814132 162.513476 188.251551 \nC 161.950895 187.68897 161.187765 187.372872 160.392155 187.372872 \nC 159.596546 187.372872 158.833416 187.68897 158.270835 188.251551 \nC 157.708254 188.814132 157.392155 189.577262 157.392155 190.372872 \nC 157.392155 191.168481 157.708254 191.931611 158.270835 192.494192 \nC 158.833416 193.056773 159.596546 193.372872 160.392155 193.372872 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 184.496987 \nC 388.755552 184.496987 389.518683 184.180888 390.081264 183.618307 \nC 390.643844 183.055727 390.959943 182.292596 390.959943 181.496987 \nC 390.959943 180.701378 390.643844 179.938248 390.081264 179.375667 \nC 389.518683 178.813086 388.755552 178.496987 387.959943 178.496987 \nC 387.164334 178.496987 386.401204 178.813086 385.838623 179.375667 \nC 385.276042 179.938248 384.959943 180.701378 384.959943 181.496987 \nC 384.959943 182.292596 385.276042 183.055727 385.838623 183.618307 \nC 386.401204 184.180888 387.164334 184.496987 387.959943 184.496987 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 188.111606 \nC 84.571313 188.111606 85.334444 187.795507 85.897024 187.232926 \nC 86.459605 186.670345 86.775704 185.907215 86.775704 185.111606 \nC 86.775704 184.315996 86.459605 183.552866 85.897024 182.990285 \nC 85.334444 182.427705 84.571313 182.111606 83.775704 182.111606 \nC 82.980095 182.111606 82.216965 182.427705 81.654384 182.990285 \nC 81.091803 183.552866 80.775704 184.315996 80.775704 185.111606 \nC 80.775704 185.907215 81.091803 186.670345 81.654384 187.232926 \nC 82.216965 187.795507 82.980095 188.111606 83.775704 188.111606 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 216.554084 \nC 84.736911 216.554084 85.500041 216.237985 86.062622 215.675405 \nC 86.625203 215.112824 86.941302 214.349693 86.941302 213.554084 \nC 86.941302 212.758475 86.625203 211.995345 86.062622 211.432764 \nC 85.500041 210.870183 84.736911 210.554084 83.941302 210.554084 \nC 83.145692 210.554084 82.382562 210.870183 81.819981 211.432764 \nC 81.257401 211.995345 80.941302 212.758475 80.941302 213.554084 \nC 80.941302 214.349693 81.257401 215.112824 81.819981 215.675405 \nC 82.382562 216.237985 83.145692 216.554084 83.941302 216.554084 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 211.946514 \nC 85.068106 211.946514 85.831236 211.630415 86.393817 211.067834 \nC 86.956398 210.505253 87.272497 209.742123 87.272497 208.946514 \nC 87.272497 208.150905 86.956398 207.387774 86.393817 206.825193 \nC 85.831236 206.262613 85.068106 205.946514 84.272497 205.946514 \nC 83.476887 205.946514 82.713757 206.262613 82.151176 206.825193 \nC 81.588596 207.387774 81.272497 208.150905 81.272497 208.946514 \nC 81.272497 209.742123 81.588596 210.505253 82.151176 211.067834 \nC 82.713757 211.630415 83.476887 211.946514 84.272497 211.946514 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 210.269918 \nC 85.730496 210.269918 86.493626 209.953819 87.056207 209.391239 \nC 87.618788 208.828658 87.934887 208.065528 87.934887 207.269918 \nC 87.934887 206.474309 87.618788 205.711179 87.056207 205.148598 \nC 86.493626 204.586017 85.730496 204.269918 84.934887 204.269918 \nC 84.139277 204.269918 83.376147 204.586017 82.813566 205.148598 \nC 82.250986 205.711179 81.934887 206.474309 81.934887 207.269918 \nC 81.934887 208.065528 82.250986 208.828658 82.813566 209.391239 \nC 83.376147 209.953819 84.139277 210.269918 84.934887 210.269918 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 216.403691 \nC 87.055276 216.403691 87.818406 216.087592 88.380987 215.525011 \nC 88.943568 214.962431 89.259667 214.1993 89.259667 213.403691 \nC 89.259667 212.608082 88.943568 211.844952 88.380987 211.282371 \nC 87.818406 210.71979 87.055276 210.403691 86.259667 210.403691 \nC 85.464058 210.403691 84.700927 210.71979 84.138347 211.282371 \nC 83.575766 211.844952 83.259667 212.608082 83.259667 213.403691 \nC 83.259667 214.1993 83.575766 214.962431 84.138347 215.525011 \nC 84.700927 216.087592 85.464058 216.403691 86.259667 216.403691 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.089019 \nC 89.704836 20.089019 90.467967 19.77292 91.030547 19.210339 \nC 91.593128 18.647758 91.909227 17.884628 91.909227 17.089019 \nC 91.909227 16.293409 91.593128 15.530279 91.030547 14.967698 \nC 90.467967 14.405118 89.704836 14.089019 88.909227 14.089019 \nC 88.113618 14.089019 87.350488 14.405118 86.787907 14.967698 \nC 86.225326 15.530279 85.909227 16.293409 85.909227 17.089019 \nC 85.909227 17.884628 86.225326 18.647758 86.787907 19.210339 \nC 87.350488 19.77292 88.113618 20.089019 88.909227 20.089019 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 216.294081 \nL 83.596307 214.794081 \nL 85.096307 216.294081 \nL 86.596307 214.794081 \nL 85.096307 213.294081 \nL 86.596307 211.794081 \nL 85.096307 210.294081 \nL 83.596307 211.794081 \nL 82.096307 210.294081 \nL 80.596307 211.794081 \nL 82.096307 213.294081 \nL 80.596307 214.794081 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 216.184653 \nL 83.674506 214.684653 \nL 85.174506 216.184653 \nL 86.674506 214.684653 \nL 85.174506 213.184653 \nL 86.674506 211.684653 \nL 85.174506 210.184653 \nL 83.674506 211.684653 \nL 82.174506 210.184653 \nL 80.674506 211.684653 \nL 82.174506 213.184653 \nL 80.674506 214.684653 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 195.234826 \nL 83.941302 193.734826 \nL 85.441302 195.234826 \nL 86.941302 193.734826 \nL 85.441302 192.234826 \nL 86.941302 190.734826 \nL 85.441302 189.234826 \nL 83.941302 190.734826 \nL 82.441302 189.234826 \nL 80.941302 190.734826 \nL 82.441302 192.234826 \nL 80.941302 193.734826 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 184.19372 \nL 84.916487 182.69372 \nL 86.416487 184.19372 \nL 87.916487 182.69372 \nL 86.416487 181.19372 \nL 87.916487 179.69372 \nL 86.416487 178.19372 \nL 84.916487 179.69372 \nL 83.416487 178.19372 \nL 81.916487 179.69372 \nL 83.416487 181.19372 \nL 81.916487 182.69372 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 206.423827 \nL 88.633231 204.923827 \nL 90.133231 206.423827 \nL 91.633231 204.923827 \nL 90.133231 203.423827 \nL 91.633231 201.923827 \nL 90.133231 200.423827 \nL 88.633231 201.923827 \nL 87.133231 200.423827 \nL 85.633231 201.923827 \nL 87.133231 203.423827 \nL 85.633231 204.923827 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 185.331495 \nL 103.132214 183.831495 \nL 104.632214 185.331495 \nL 106.132214 183.831495 \nL 104.632214 182.331495 \nL 106.132214 180.831495 \nL 104.632214 179.331495 \nL 103.132214 180.831495 \nL 101.632214 179.331495 \nL 100.132214 180.831495 \nL 101.632214 182.331495 \nL 100.132214 183.831495 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 158.525696 \nL 160.392155 157.025696 \nL 161.892155 158.525696 \nL 163.392155 157.025696 \nL 161.892155 155.525696 \nL 163.392155 154.025696 \nL 161.892155 152.525696 \nL 160.392155 154.025696 \nL 158.892155 152.525696 \nL 157.392155 154.025696 \nL 158.892155 155.525696 \nL 157.392155 157.025696 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 162.499784 \nL 387.959943 160.999784 \nL 389.459943 162.499784 \nL 390.959943 160.999784 \nL 389.459943 159.499784 \nL 390.959943 157.999784 \nL 389.459943 156.499784 \nL 387.959943 157.999784 \nL 386.459943 156.499784 \nL 384.959943 157.999784 \nL 386.459943 159.499784 \nL 384.959943 160.999784 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 208.244008 \nL 83.775704 206.744008 \nL 85.275704 208.244008 \nL 86.775704 206.744008 \nL 85.275704 205.244008 \nL 86.775704 203.744008 \nL 85.275704 202.244008 \nL 83.775704 203.744008 \nL 82.275704 202.244008 \nL 80.775704 203.744008 \nL 82.275704 205.244008 \nL 80.775704 206.744008 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 172.629396 \nL 83.941302 171.129396 \nL 85.441302 172.629396 \nL 86.941302 171.129396 \nL 85.441302 169.629396 \nL 86.941302 168.129396 \nL 85.441302 166.629396 \nL 83.941302 168.129396 \nL 82.441302 166.629396 \nL 80.941302 168.129396 \nL 82.441302 169.629396 \nL 80.941302 171.129396 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 213.891372 \nL 84.272497 212.391372 \nL 85.772497 213.891372 \nL 87.272497 212.391372 \nL 85.772497 210.891372 \nL 87.272497 209.391372 \nL 85.772497 207.891372 \nL 84.272497 209.391372 \nL 82.772497 207.891372 \nL 81.272497 209.391372 \nL 82.772497 210.891372 \nL 81.272497 212.391372 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 167.931288 \nL 84.934887 166.431288 \nL 86.434887 167.931288 \nL 87.934887 166.431288 \nL 86.434887 164.931288 \nL 87.934887 163.431288 \nL 86.434887 161.931288 \nL 84.934887 163.431288 \nL 83.434887 161.931288 \nL 81.934887 163.431288 \nL 83.434887 164.931288 \nL 81.934887 166.431288 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 215.619741 \nL 86.259667 214.119741 \nL 87.759667 215.619741 \nL 89.259667 214.119741 \nL 87.759667 212.619741 \nL 89.259667 211.119741 \nL 87.759667 209.619741 \nL 86.259667 211.119741 \nL 84.759667 209.619741 \nL 83.259667 211.119741 \nL 84.759667 212.619741 \nL 83.259667 214.119741 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 194.176594 \nL 88.909227 192.676594 \nL 90.409227 194.176594 \nL 91.909227 192.676594 \nL 90.409227 191.176594 \nL 91.909227 189.676594 \nL 90.409227 188.176594 \nL 88.909227 189.676594 \nL 87.409227 188.176594 \nL 85.909227 189.676594 \nL 87.409227 191.176594 \nL 85.909227 192.676594 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 213.738438 \nC 84.391916 213.738438 85.155046 213.422339 85.717627 212.859758 \nC 86.280208 212.297177 86.596307 211.534047 86.596307 210.738438 \nC 86.596307 209.942828 86.280208 209.179698 85.717627 208.617117 \nC 85.155046 208.054537 84.391916 207.738438 83.596307 207.738438 \nC 82.800698 207.738438 82.037567 208.054537 81.474986 208.617117 \nC 80.912406 209.179698 80.596307 209.942828 80.596307 210.738438 \nC 80.596307 211.534047 80.912406 212.297177 81.474986 212.859758 \nC 82.037567 213.422339 82.800698 213.738438 83.596307 213.738438 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 213.767669 \nC 84.470115 213.767669 85.233245 213.45157 85.795826 212.888989 \nC 86.358407 212.326408 86.674506 211.563278 86.674506 210.767669 \nC 86.674506 209.972059 86.358407 209.208929 85.795826 208.646348 \nC 85.233245 208.083768 84.470115 207.767669 83.674506 207.767669 \nC 82.878896 207.767669 82.115766 208.083768 81.553185 208.646348 \nC 80.990605 209.208929 80.674506 209.972059 80.674506 210.767669 \nC 80.674506 211.563278 80.990605 212.326408 81.553185 212.888989 \nC 82.115766 213.45157 82.878896 213.767669 83.674506 213.767669 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 210.646721 \nC 84.736911 210.646721 85.500041 210.330622 86.062622 209.768041 \nC 86.625203 209.205461 86.941302 208.44233 86.941302 207.646721 \nC 86.941302 206.851112 86.625203 206.087981 86.062622 205.525401 \nC 85.500041 204.96282 84.736911 204.646721 83.941302 204.646721 \nC 83.145692 204.646721 82.382562 204.96282 81.819981 205.525401 \nC 81.257401 206.087981 80.941302 206.851112 80.941302 207.646721 \nC 80.941302 208.44233 81.257401 209.205461 81.819981 209.768041 \nC 82.382562 210.330622 83.145692 210.646721 83.941302 210.646721 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 203.217127 \nC 85.712096 203.217127 86.475227 202.901028 87.037807 202.338447 \nC 87.600388 201.775866 87.916487 201.012736 87.916487 200.217127 \nC 87.916487 199.421517 87.600388 198.658387 87.037807 198.095806 \nC 86.475227 197.533226 85.712096 197.217127 84.916487 197.217127 \nC 84.120878 197.217127 83.357747 197.533226 82.795167 198.095806 \nC 82.232586 198.658387 81.916487 199.421517 81.916487 200.217127 \nC 81.916487 201.012736 82.232586 201.775866 82.795167 202.338447 \nC 83.357747 202.901028 84.120878 203.217127 84.916487 203.217127 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 189.792057 \nC 89.428841 189.792057 90.191971 189.475959 90.754552 188.913378 \nC 91.317132 188.350797 91.633231 187.587667 91.633231 186.792057 \nC 91.633231 185.996448 91.317132 185.233318 90.754552 184.670737 \nC 90.191971 184.108156 89.428841 183.792057 88.633231 183.792057 \nC 87.837622 183.792057 87.074492 184.108156 86.511911 184.670737 \nC 85.94933 185.233318 85.633231 185.996448 85.633231 186.792057 \nC 85.633231 187.587667 85.94933 188.350797 86.511911 188.913378 \nC 87.074492 189.475959 87.837622 189.792057 88.633231 189.792057 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 186.977182 \nC 103.927823 186.977182 104.690954 186.661083 105.253534 186.098503 \nC 105.816115 185.535922 106.132214 184.772791 106.132214 183.977182 \nC 106.132214 183.181573 105.816115 182.418443 105.253534 181.855862 \nC 104.690954 181.293281 103.927823 180.977182 103.132214 180.977182 \nC 102.336605 180.977182 101.573474 181.293281 101.010894 181.855862 \nC 100.448313 182.418443 100.132214 183.181573 100.132214 183.977182 \nC 100.132214 184.772791 100.448313 185.535922 101.010894 186.098503 \nC 101.573474 186.661083 102.336605 186.977182 103.132214 186.977182 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 184.030545 \nC 161.187765 184.030545 161.950895 183.714446 162.513476 183.151865 \nC 163.076056 182.589284 163.392155 181.826154 163.392155 181.030545 \nC 163.392155 180.234936 163.076056 179.471805 162.513476 178.909225 \nC 161.950895 178.346644 161.187765 178.030545 160.392155 178.030545 \nC 159.596546 178.030545 158.833416 178.346644 158.270835 178.909225 \nC 157.708254 179.471805 157.392155 180.234936 157.392155 181.030545 \nC 157.392155 181.826154 157.708254 182.589284 158.270835 183.151865 \nC 158.833416 183.714446 159.596546 184.030545 160.392155 184.030545 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 209.487872 \nC 388.755552 209.487872 389.518683 209.171774 390.081264 208.609193 \nC 390.643844 208.046612 390.959943 207.283482 390.959943 206.487872 \nC 390.959943 205.692263 390.643844 204.929133 390.081264 204.366552 \nC 389.518683 203.803971 388.755552 203.487872 387.959943 203.487872 \nC 387.164334 203.487872 386.401204 203.803971 385.838623 204.366552 \nC 385.276042 204.929133 384.959943 205.692263 384.959943 206.487872 \nC 384.959943 207.283482 385.276042 208.046612 385.838623 208.609193 \nC 386.401204 209.171774 387.164334 209.487872 387.959943 209.487872 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 182.735287 \nC 84.571313 182.735287 85.334444 182.419188 85.897024 181.856607 \nC 86.459605 181.294026 86.775704 180.530896 86.775704 179.735287 \nC 86.775704 178.939678 86.459605 178.176547 85.897024 177.613966 \nC 85.334444 177.051386 84.571313 176.735287 83.775704 176.735287 \nC 82.980095 176.735287 82.216965 177.051386 81.654384 177.613966 \nC 81.091803 178.176547 80.775704 178.939678 80.775704 179.735287 \nC 80.775704 180.530896 81.091803 181.294026 81.654384 181.856607 \nC 82.216965 182.419188 82.980095 182.735287 83.775704 182.735287 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 208.969522 \nC 84.736911 208.969522 85.500041 208.653423 86.062622 208.090842 \nC 86.625203 207.528262 86.941302 206.765131 86.941302 205.969522 \nC 86.941302 205.173913 86.625203 204.410782 86.062622 203.848202 \nC 85.500041 203.285621 84.736911 202.969522 83.941302 202.969522 \nC 83.145692 202.969522 82.382562 203.285621 81.819981 203.848202 \nC 81.257401 204.410782 80.941302 205.173913 80.941302 205.969522 \nC 80.941302 206.765131 81.257401 207.528262 81.819981 208.090842 \nC 82.382562 208.653423 83.145692 208.969522 83.941302 208.969522 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 213.449139 \nC 85.068106 213.449139 85.831236 213.13304 86.393817 212.570459 \nC 86.956398 212.007878 87.272497 211.244748 87.272497 210.449139 \nC 87.272497 209.653529 86.956398 208.890399 86.393817 208.327818 \nC 85.831236 207.765237 85.068106 207.449139 84.272497 207.449139 \nC 83.476887 207.449139 82.713757 207.765237 82.151176 208.327818 \nC 81.588596 208.890399 81.272497 209.653529 81.272497 210.449139 \nC 81.272497 211.244748 81.588596 212.007878 82.151176 212.570459 \nC 82.713757 213.13304 83.476887 213.449139 84.272497 213.449139 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 213.397701 \nC 85.730496 213.397701 86.493626 213.081602 87.056207 212.519021 \nC 87.618788 211.95644 87.934887 211.19331 87.934887 210.397701 \nC 87.934887 209.602092 87.618788 208.838961 87.056207 208.276381 \nC 86.493626 207.7138 85.730496 207.397701 84.934887 207.397701 \nC 84.139277 207.397701 83.376147 207.7138 82.813566 208.276381 \nC 82.250986 208.838961 81.934887 209.602092 81.934887 210.397701 \nC 81.934887 211.19331 82.250986 211.95644 82.813566 212.519021 \nC 83.376147 213.081602 84.139277 213.397701 84.934887 213.397701 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 209.5426 \nC 87.055276 209.5426 87.818406 209.226501 88.380987 208.663921 \nC 88.943568 208.10134 89.259667 207.33821 89.259667 206.5426 \nC 89.259667 205.746991 88.943568 204.983861 88.380987 204.42128 \nC 87.818406 203.858699 87.055276 203.5426 86.259667 203.5426 \nC 85.464058 203.5426 84.700927 203.858699 84.138347 204.42128 \nC 83.575766 204.983861 83.259667 205.746991 83.259667 206.5426 \nC 83.259667 207.33821 83.575766 208.10134 84.138347 208.663921 \nC 84.700927 209.226501 85.464058 209.5426 86.259667 209.5426 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.084376 \nC 89.704836 20.084376 90.467967 19.768278 91.030547 19.205697 \nC 91.593128 18.643116 91.909227 17.879986 91.909227 17.084376 \nC 91.909227 16.288767 91.593128 15.525637 91.030547 14.963056 \nC 90.467967 14.400475 89.704836 14.084376 88.909227 14.084376 \nC 88.113618 14.084376 87.350488 14.400475 86.787907 14.963056 \nC 86.225326 15.525637 85.909227 16.288767 85.909227 17.084376 \nC 85.909227 17.879986 86.225326 18.643116 86.787907 19.205697 \nC 87.350488 19.768278 88.113618 20.084376 88.909227 20.084376 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 213.101734 \nL 83.596307 211.601734 \nL 85.096307 213.101734 \nL 86.596307 211.601734 \nL 85.096307 210.101734 \nL 86.596307 208.601734 \nL 85.096307 207.101734 \nL 83.596307 208.601734 \nL 82.096307 207.101734 \nL 80.596307 208.601734 \nL 82.096307 210.101734 \nL 80.596307 211.601734 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 209.620866 \nL 83.674506 208.120866 \nL 85.174506 209.620866 \nL 86.674506 208.120866 \nL 85.174506 206.620866 \nL 86.674506 205.120866 \nL 85.174506 203.620866 \nL 83.674506 205.120866 \nL 82.174506 203.620866 \nL 80.674506 205.120866 \nL 82.174506 206.620866 \nL 80.674506 208.120866 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 146.881943 \nL 83.941302 145.381943 \nL 85.441302 146.881943 \nL 86.941302 145.381943 \nL 85.441302 143.881943 \nL 86.941302 142.381943 \nL 85.441302 140.881943 \nL 83.941302 142.381943 \nL 82.441302 140.881943 \nL 80.941302 142.381943 \nL 82.441302 143.881943 \nL 80.941302 145.381943 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 178.948021 \nL 84.916487 177.448021 \nL 86.416487 178.948021 \nL 87.916487 177.448021 \nL 86.416487 175.948021 \nL 87.916487 174.448021 \nL 86.416487 172.948021 \nL 84.916487 174.448021 \nL 83.416487 172.948021 \nL 81.916487 174.448021 \nL 83.416487 175.948021 \nL 81.916487 177.448021 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 173.379717 \nL 88.633231 171.879717 \nL 90.133231 173.379717 \nL 91.633231 171.879717 \nL 90.133231 170.379717 \nL 91.633231 168.879717 \nL 90.133231 167.379717 \nL 88.633231 168.879717 \nL 87.133231 167.379717 \nL 85.633231 168.879717 \nL 87.133231 170.379717 \nL 85.633231 171.879717 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 155.095993 \nL 103.132214 153.595993 \nL 104.632214 155.095993 \nL 106.132214 153.595993 \nL 104.632214 152.095993 \nL 106.132214 150.595993 \nL 104.632214 149.095993 \nL 103.132214 150.595993 \nL 101.632214 149.095993 \nL 100.132214 150.595993 \nL 101.632214 152.095993 \nL 100.132214 153.595993 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 173.232283 \nL 160.392155 171.732283 \nL 161.892155 173.232283 \nL 163.392155 171.732283 \nL 161.892155 170.232283 \nL 163.392155 168.732283 \nL 161.892155 167.232283 \nL 160.392155 168.732283 \nL 158.892155 167.232283 \nL 157.392155 168.732283 \nL 158.892155 170.232283 \nL 157.392155 171.732283 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 165.153826 \nL 387.959943 163.653826 \nL 389.459943 165.153826 \nL 390.959943 163.653826 \nL 389.459943 162.153826 \nL 390.959943 160.653826 \nL 389.459943 159.153826 \nL 387.959943 160.653826 \nL 386.459943 159.153826 \nL 384.959943 160.653826 \nL 386.459943 162.153826 \nL 384.959943 163.653826 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 176.305379 \nL 83.775704 174.805379 \nL 85.275704 176.305379 \nL 86.775704 174.805379 \nL 85.275704 173.305379 \nL 86.775704 171.805379 \nL 85.275704 170.305379 \nL 83.775704 171.805379 \nL 82.275704 170.305379 \nL 80.775704 171.805379 \nL 82.275704 173.305379 \nL 80.775704 174.805379 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 157.524956 \nL 83.941302 156.024956 \nL 85.441302 157.524956 \nL 86.941302 156.024956 \nL 85.441302 154.524956 \nL 86.941302 153.024956 \nL 85.441302 151.524956 \nL 83.941302 153.024956 \nL 82.441302 151.524956 \nL 80.941302 153.024956 \nL 82.441302 154.524956 \nL 80.941302 156.024956 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 181.238566 \nL 84.272497 179.738566 \nL 85.772497 181.238566 \nL 87.272497 179.738566 \nL 85.772497 178.238566 \nL 87.272497 176.738566 \nL 85.772497 175.238566 \nL 84.272497 176.738566 \nL 82.772497 175.238566 \nL 81.272497 176.738566 \nL 82.772497 178.238566 \nL 81.272497 179.738566 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 163.166085 \nL 84.934887 161.666085 \nL 86.434887 163.166085 \nL 87.934887 161.666085 \nL 86.434887 160.166085 \nL 87.934887 158.666085 \nL 86.434887 157.166085 \nL 84.934887 158.666085 \nL 83.434887 157.166085 \nL 81.934887 158.666085 \nL 83.434887 160.166085 \nL 81.934887 161.666085 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 206.173597 \nL 86.259667 204.673597 \nL 87.759667 206.173597 \nL 89.259667 204.673597 \nL 87.759667 203.173597 \nL 89.259667 201.673597 \nL 87.759667 200.173597 \nL 86.259667 201.673597 \nL 84.759667 200.173597 \nL 83.259667 201.673597 \nL 84.759667 203.173597 \nL 83.259667 204.673597 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 167.623086 \nL 88.909227 166.123086 \nL 90.409227 167.623086 \nL 91.909227 166.123086 \nL 90.409227 164.623086 \nL 91.909227 163.123086 \nL 90.409227 161.623086 \nL 88.909227 163.123086 \nL 87.409227 161.623086 \nL 85.909227 163.123086 \nL 87.409227 164.623086 \nL 85.909227 166.123086 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 210.512695 \nC 84.391916 210.512695 85.155046 210.196596 85.717627 209.634016 \nC 86.280208 209.071435 86.596307 208.308305 86.596307 207.512695 \nC 86.596307 206.717086 86.280208 205.953956 85.717627 205.391375 \nC 85.155046 204.828794 84.391916 204.512695 83.596307 204.512695 \nC 82.800698 204.512695 82.037567 204.828794 81.474986 205.391375 \nC 80.912406 205.953956 80.596307 206.717086 80.596307 207.512695 \nC 80.596307 208.308305 80.912406 209.071435 81.474986 209.634016 \nC 82.037567 210.196596 82.800698 210.512695 83.596307 210.512695 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 216.206635 \nC 84.470115 216.206635 85.233245 215.890536 85.795826 215.327955 \nC 86.358407 214.765374 86.674506 214.002244 86.674506 213.206635 \nC 86.674506 212.411025 86.358407 211.647895 85.795826 211.085314 \nC 85.233245 210.522733 84.470115 210.206635 83.674506 210.206635 \nC 82.878896 210.206635 82.115766 210.522733 81.553185 211.085314 \nC 80.990605 211.647895 80.674506 212.411025 80.674506 213.206635 \nC 80.674506 214.002244 80.990605 214.765374 81.553185 215.327955 \nC 82.115766 215.890536 82.878896 216.206635 83.674506 216.206635 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 213.424019 \nC 84.736911 213.424019 85.500041 213.10792 86.062622 212.545339 \nC 86.625203 211.982758 86.941302 211.219628 86.941302 210.424019 \nC 86.941302 209.628409 86.625203 208.865279 86.062622 208.302698 \nC 85.500041 207.740118 84.736911 207.424019 83.941302 207.424019 \nC 83.145692 207.424019 82.382562 207.740118 81.819981 208.302698 \nC 81.257401 208.865279 80.941302 209.628409 80.941302 210.424019 \nC 80.941302 211.219628 81.257401 211.982758 81.819981 212.545339 \nC 82.382562 213.10792 83.145692 213.424019 83.941302 213.424019 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 207.418188 \nC 85.712096 207.418188 86.475227 207.102089 87.037807 206.539508 \nC 87.600388 205.976928 87.916487 205.213797 87.916487 204.418188 \nC 87.916487 203.622579 87.600388 202.859448 87.037807 202.296868 \nC 86.475227 201.734287 85.712096 201.418188 84.916487 201.418188 \nC 84.120878 201.418188 83.357747 201.734287 82.795167 202.296868 \nC 82.232586 202.859448 81.916487 203.622579 81.916487 204.418188 \nC 81.916487 205.213797 82.232586 205.976928 82.795167 206.539508 \nC 83.357747 207.102089 84.120878 207.418188 84.916487 207.418188 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 202.23008 \nC 89.428841 202.23008 90.191971 201.913981 90.754552 201.351401 \nC 91.317132 200.78882 91.633231 200.02569 91.633231 199.23008 \nC 91.633231 198.434471 91.317132 197.671341 90.754552 197.10876 \nC 90.191971 196.546179 89.428841 196.23008 88.633231 196.23008 \nC 87.837622 196.23008 87.074492 196.546179 86.511911 197.10876 \nC 85.94933 197.671341 85.633231 198.434471 85.633231 199.23008 \nC 85.633231 200.02569 85.94933 200.78882 86.511911 201.351401 \nC 87.074492 201.913981 87.837622 202.23008 88.633231 202.23008 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 172.912439 \nC 103.927823 172.912439 104.690954 172.59634 105.253534 172.033759 \nC 105.816115 171.471178 106.132214 170.708048 106.132214 169.912439 \nC 106.132214 169.116829 105.816115 168.353699 105.253534 167.791118 \nC 104.690954 167.228538 103.927823 166.912439 103.132214 166.912439 \nC 102.336605 166.912439 101.573474 167.228538 101.010894 167.791118 \nC 100.448313 168.353699 100.132214 169.116829 100.132214 169.912439 \nC 100.132214 170.708048 100.448313 171.471178 101.010894 172.033759 \nC 101.573474 172.59634 102.336605 172.912439 103.132214 172.912439 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 192.940816 \nC 161.187765 192.940816 161.950895 192.624717 162.513476 192.062137 \nC 163.076056 191.499556 163.392155 190.736426 163.392155 189.940816 \nC 163.392155 189.145207 163.076056 188.382077 162.513476 187.819496 \nC 161.950895 187.256915 161.187765 186.940816 160.392155 186.940816 \nC 159.596546 186.940816 158.833416 187.256915 158.270835 187.819496 \nC 157.708254 188.382077 157.392155 189.145207 157.392155 189.940816 \nC 157.392155 190.736426 157.708254 191.499556 158.270835 192.062137 \nC 158.833416 192.624717 159.596546 192.940816 160.392155 192.940816 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 158.784282 \nC 388.755552 158.784282 389.518683 158.468183 390.081264 157.905602 \nC 390.643844 157.343022 390.959943 156.579891 390.959943 155.784282 \nC 390.959943 154.988673 390.643844 154.225542 390.081264 153.662962 \nC 389.518683 153.100381 388.755552 152.784282 387.959943 152.784282 \nC 387.164334 152.784282 386.401204 153.100381 385.838623 153.662962 \nC 385.276042 154.225542 384.959943 154.988673 384.959943 155.784282 \nC 384.959943 156.579891 385.276042 157.343022 385.838623 157.905602 \nC 386.401204 158.468183 387.164334 158.784282 387.959943 158.784282 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 206.601908 \nC 84.571313 206.601908 85.334444 206.285809 85.897024 205.723228 \nC 86.459605 205.160647 86.775704 204.397517 86.775704 203.601908 \nC 86.775704 202.806299 86.459605 202.043168 85.897024 201.480588 \nC 85.334444 200.918007 84.571313 200.601908 83.775704 200.601908 \nC 82.980095 200.601908 82.216965 200.918007 81.654384 201.480588 \nC 81.091803 202.043168 80.775704 202.806299 80.775704 203.601908 \nC 80.775704 204.397517 81.091803 205.160647 81.654384 205.723228 \nC 82.216965 206.285809 82.980095 206.601908 83.775704 206.601908 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 208.105217 \nC 84.736911 208.105217 85.500041 207.789118 86.062622 207.226537 \nC 86.625203 206.663956 86.941302 205.900826 86.941302 205.105217 \nC 86.941302 204.309608 86.625203 203.546477 86.062622 202.983896 \nC 85.500041 202.421316 84.736911 202.105217 83.941302 202.105217 \nC 83.145692 202.105217 82.382562 202.421316 81.819981 202.983896 \nC 81.257401 203.546477 80.941302 204.309608 80.941302 205.105217 \nC 80.941302 205.900826 81.257401 206.663956 81.819981 207.226537 \nC 82.382562 207.789118 83.145692 208.105217 83.941302 208.105217 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 214.146565 \nC 85.068106 214.146565 85.831236 213.830466 86.393817 213.267885 \nC 86.956398 212.705305 87.272497 211.942174 87.272497 211.146565 \nC 87.272497 210.350956 86.956398 209.587825 86.393817 209.025245 \nC 85.831236 208.462664 85.068106 208.146565 84.272497 208.146565 \nC 83.476887 208.146565 82.713757 208.462664 82.151176 209.025245 \nC 81.588596 209.587825 81.272497 210.350956 81.272497 211.146565 \nC 81.272497 211.942174 81.588596 212.705305 82.151176 213.267885 \nC 82.713757 213.830466 83.476887 214.146565 84.272497 214.146565 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 216.496173 \nC 85.730496 216.496173 86.493626 216.180074 87.056207 215.617493 \nC 87.618788 215.054912 87.934887 214.291782 87.934887 213.496173 \nC 87.934887 212.700563 87.618788 211.937433 87.056207 211.374852 \nC 86.493626 210.812271 85.730496 210.496173 84.934887 210.496173 \nC 84.139277 210.496173 83.376147 210.812271 82.813566 211.374852 \nC 82.250986 211.937433 81.934887 212.700563 81.934887 213.496173 \nC 81.934887 214.291782 82.250986 215.054912 82.813566 215.617493 \nC 83.376147 216.180074 84.139277 216.496173 84.934887 216.496173 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 213.922455 \nC 87.055276 213.922455 87.818406 213.606356 88.380987 213.043775 \nC 88.943568 212.481195 89.259667 211.718064 89.259667 210.922455 \nC 89.259667 210.126846 88.943568 209.363715 88.380987 208.801135 \nC 87.818406 208.238554 87.055276 207.922455 86.259667 207.922455 \nC 85.464058 207.922455 84.700927 208.238554 84.138347 208.801135 \nC 83.575766 209.363715 83.259667 210.126846 83.259667 210.922455 \nC 83.259667 211.718064 83.575766 212.481195 84.138347 213.043775 \nC 84.700927 213.606356 85.464058 213.922455 86.259667 213.922455 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.088839 \nC 89.704836 20.088839 90.467967 19.77274 91.030547 19.21016 \nC 91.593128 18.647579 91.909227 17.884449 91.909227 17.088839 \nC 91.909227 16.29323 91.593128 15.5301 91.030547 14.967519 \nC 90.467967 14.404938 89.704836 14.088839 88.909227 14.088839 \nC 88.113618 14.088839 87.350488 14.404938 86.787907 14.967519 \nC 86.225326 15.5301 85.909227 16.29323 85.909227 17.088839 \nC 85.909227 17.884449 86.225326 18.647579 86.787907 19.21016 \nC 87.350488 19.77274 88.113618 20.088839 88.909227 20.088839 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 217.0833 \nL 83.596307 215.5833 \nL 85.096307 217.0833 \nL 86.596307 215.5833 \nL 85.096307 214.0833 \nL 86.596307 212.5833 \nL 85.096307 211.0833 \nL 83.596307 212.5833 \nL 82.096307 211.0833 \nL 80.596307 212.5833 \nL 82.096307 214.0833 \nL 80.596307 215.5833 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 188.982881 \nL 83.674506 187.482881 \nL 85.174506 188.982881 \nL 86.674506 187.482881 \nL 85.174506 185.982881 \nL 86.674506 184.482881 \nL 85.174506 182.982881 \nL 83.674506 184.482881 \nL 82.174506 182.982881 \nL 80.674506 184.482881 \nL 82.174506 185.982881 \nL 80.674506 187.482881 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 214.505238 \nL 83.941302 213.005238 \nL 85.441302 214.505238 \nL 86.941302 213.005238 \nL 85.441302 211.505238 \nL 86.941302 210.005238 \nL 85.441302 208.505238 \nL 83.941302 210.005238 \nL 82.441302 208.505238 \nL 80.941302 210.005238 \nL 82.441302 211.505238 \nL 80.941302 213.005238 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 206.24344 \nL 84.916487 204.74344 \nL 86.416487 206.24344 \nL 87.916487 204.74344 \nL 86.416487 203.24344 \nL 87.916487 201.74344 \nL 86.416487 200.24344 \nL 84.916487 201.74344 \nL 83.416487 200.24344 \nL 81.916487 201.74344 \nL 83.416487 203.24344 \nL 81.916487 204.74344 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 189.994527 \nL 88.633231 188.494527 \nL 90.133231 189.994527 \nL 91.633231 188.494527 \nL 90.133231 186.994527 \nL 91.633231 185.494527 \nL 90.133231 183.994527 \nL 88.633231 185.494527 \nL 87.133231 183.994527 \nL 85.633231 185.494527 \nL 87.133231 186.994527 \nL 85.633231 188.494527 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 204.654455 \nL 103.132214 203.154455 \nL 104.632214 204.654455 \nL 106.132214 203.154455 \nL 104.632214 201.654455 \nL 106.132214 200.154455 \nL 104.632214 198.654455 \nL 103.132214 200.154455 \nL 101.632214 198.654455 \nL 100.132214 200.154455 \nL 101.632214 201.654455 \nL 100.132214 203.154455 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 174.446572 \nL 160.392155 172.946572 \nL 161.892155 174.446572 \nL 163.392155 172.946572 \nL 161.892155 171.446572 \nL 163.392155 169.946572 \nL 161.892155 168.446572 \nL 160.392155 169.946572 \nL 158.892155 168.446572 \nL 157.392155 169.946572 \nL 158.892155 171.446572 \nL 157.392155 172.946572 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 160.659132 \nL 387.959943 159.159132 \nL 389.459943 160.659132 \nL 390.959943 159.159132 \nL 389.459943 157.659132 \nL 390.959943 156.159132 \nL 389.459943 154.659132 \nL 387.959943 156.159132 \nL 386.459943 154.659132 \nL 384.959943 156.159132 \nL 386.459943 157.659132 \nL 384.959943 159.159132 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 194.75232 \nL 83.775704 193.25232 \nL 85.275704 194.75232 \nL 86.775704 193.25232 \nL 85.275704 191.75232 \nL 86.775704 190.25232 \nL 85.275704 188.75232 \nL 83.775704 190.25232 \nL 82.275704 188.75232 \nL 80.775704 190.25232 \nL 82.275704 191.75232 \nL 80.775704 193.25232 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 210.972347 \nL 83.941302 209.472347 \nL 85.441302 210.972347 \nL 86.941302 209.472347 \nL 85.441302 207.972347 \nL 86.941302 206.472347 \nL 85.441302 204.972347 \nL 83.941302 206.472347 \nL 82.441302 204.972347 \nL 80.941302 206.472347 \nL 82.441302 207.972347 \nL 80.941302 209.472347 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 217.277635 \nL 84.272497 215.777635 \nL 85.772497 217.277635 \nL 87.272497 215.777635 \nL 85.772497 214.277635 \nL 87.272497 212.777635 \nL 85.772497 211.277635 \nL 84.272497 212.777635 \nL 82.772497 211.277635 \nL 81.272497 212.777635 \nL 82.772497 214.277635 \nL 81.272497 215.777635 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 217.5752 \nL 84.934887 216.0752 \nL 86.434887 217.5752 \nL 87.934887 216.0752 \nL 86.434887 214.5752 \nL 87.934887 213.0752 \nL 86.434887 211.5752 \nL 84.934887 213.0752 \nL 83.434887 211.5752 \nL 81.934887 213.0752 \nL 83.434887 214.5752 \nL 81.934887 216.0752 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 203.026308 \nL 86.259667 201.526308 \nL 87.759667 203.026308 \nL 89.259667 201.526308 \nL 87.759667 200.026308 \nL 89.259667 198.526308 \nL 87.759667 197.026308 \nL 86.259667 198.526308 \nL 84.759667 197.026308 \nL 83.259667 198.526308 \nL 84.759667 200.026308 \nL 83.259667 201.526308 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 149.453991 \nL 88.909227 147.953991 \nL 90.409227 149.453991 \nL 91.909227 147.953991 \nL 90.409227 146.453991 \nL 91.909227 144.953991 \nL 90.409227 143.453991 \nL 88.909227 144.953991 \nL 87.409227 143.453991 \nL 85.909227 144.953991 \nL 87.409227 146.453991 \nL 85.909227 147.953991 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 215.097964 \nC 84.391916 215.097964 85.155046 214.781865 85.717627 214.219284 \nC 86.280208 213.656703 86.596307 212.893573 86.596307 212.097964 \nC 86.596307 211.302354 86.280208 210.539224 85.717627 209.976643 \nC 85.155046 209.414063 84.391916 209.097964 83.596307 209.097964 \nC 82.800698 209.097964 82.037567 209.414063 81.474986 209.976643 \nC 80.912406 210.539224 80.596307 211.302354 80.596307 212.097964 \nC 80.596307 212.893573 80.912406 213.656703 81.474986 214.219284 \nC 82.037567 214.781865 82.800698 215.097964 83.596307 215.097964 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 216.561872 \nC 84.470115 216.561872 85.233245 216.245773 85.795826 215.683192 \nC 86.358407 215.120612 86.674506 214.357481 86.674506 213.561872 \nC 86.674506 212.766263 86.358407 212.003132 85.795826 211.440552 \nC 85.233245 210.877971 84.470115 210.561872 83.674506 210.561872 \nC 82.878896 210.561872 82.115766 210.877971 81.553185 211.440552 \nC 80.990605 212.003132 80.674506 212.766263 80.674506 213.561872 \nC 80.674506 214.357481 80.990605 215.120612 81.553185 215.683192 \nC 82.115766 216.245773 82.878896 216.561872 83.674506 216.561872 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 196.074079 \nC 84.736911 196.074079 85.500041 195.757981 86.062622 195.1954 \nC 86.625203 194.632819 86.941302 193.869689 86.941302 193.074079 \nC 86.941302 192.27847 86.625203 191.51534 86.062622 190.952759 \nC 85.500041 190.390178 84.736911 190.074079 83.941302 190.074079 \nC 83.145692 190.074079 82.382562 190.390178 81.819981 190.952759 \nC 81.257401 191.51534 80.941302 192.27847 80.941302 193.074079 \nC 80.941302 193.869689 81.257401 194.632819 81.819981 195.1954 \nC 82.382562 195.757981 83.145692 196.074079 83.941302 196.074079 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 212.934105 \nC 85.712096 212.934105 86.475227 212.618006 87.037807 212.055425 \nC 87.600388 211.492844 87.916487 210.729714 87.916487 209.934105 \nC 87.916487 209.138495 87.600388 208.375365 87.037807 207.812784 \nC 86.475227 207.250204 85.712096 206.934105 84.916487 206.934105 \nC 84.120878 206.934105 83.357747 207.250204 82.795167 207.812784 \nC 82.232586 208.375365 81.916487 209.138495 81.916487 209.934105 \nC 81.916487 210.729714 82.232586 211.492844 82.795167 212.055425 \nC 83.357747 212.618006 84.120878 212.934105 84.916487 212.934105 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 212.627016 \nC 89.428841 212.627016 90.191971 212.310917 90.754552 211.748336 \nC 91.317132 211.185756 91.633231 210.422625 91.633231 209.627016 \nC 91.633231 208.831407 91.317132 208.068276 90.754552 207.505696 \nC 90.191971 206.943115 89.428841 206.627016 88.633231 206.627016 \nC 87.837622 206.627016 87.074492 206.943115 86.511911 207.505696 \nC 85.94933 208.068276 85.633231 208.831407 85.633231 209.627016 \nC 85.633231 210.422625 85.94933 211.185756 86.511911 211.748336 \nC 87.074492 212.310917 87.837622 212.627016 88.633231 212.627016 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 162.89408 \nC 103.927823 162.89408 104.690954 162.577981 105.253534 162.0154 \nC 105.816115 161.452819 106.132214 160.689689 106.132214 159.89408 \nC 106.132214 159.098471 105.816115 158.33534 105.253534 157.772759 \nC 104.690954 157.210179 103.927823 156.89408 103.132214 156.89408 \nC 102.336605 156.89408 101.573474 157.210179 101.010894 157.772759 \nC 100.448313 158.33534 100.132214 159.098471 100.132214 159.89408 \nC 100.132214 160.689689 100.448313 161.452819 101.010894 162.0154 \nC 101.573474 162.577981 102.336605 162.89408 103.132214 162.89408 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 158.710202 \nC 161.187765 158.710202 161.950895 158.394103 162.513476 157.831522 \nC 163.076056 157.268941 163.392155 156.505811 163.392155 155.710202 \nC 163.392155 154.914592 163.076056 154.151462 162.513476 153.588881 \nC 161.950895 153.026301 161.187765 152.710202 160.392155 152.710202 \nC 159.596546 152.710202 158.833416 153.026301 158.270835 153.588881 \nC 157.708254 154.151462 157.392155 154.914592 157.392155 155.710202 \nC 157.392155 156.505811 157.708254 157.268941 158.270835 157.831522 \nC 158.833416 158.394103 159.596546 158.710202 160.392155 158.710202 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 189.303255 \nC 388.755552 189.303255 389.518683 188.987156 390.081264 188.424576 \nC 390.643844 187.861995 390.959943 187.098865 390.959943 186.303255 \nC 390.959943 185.507646 390.643844 184.744516 390.081264 184.181935 \nC 389.518683 183.619354 388.755552 183.303255 387.959943 183.303255 \nC 387.164334 183.303255 386.401204 183.619354 385.838623 184.181935 \nC 385.276042 184.744516 384.959943 185.507646 384.959943 186.303255 \nC 384.959943 187.098865 385.276042 187.861995 385.838623 188.424576 \nC 386.401204 188.987156 387.164334 189.303255 387.959943 189.303255 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 210.972683 \nC 84.571313 210.972683 85.334444 210.656584 85.897024 210.094003 \nC 86.459605 209.531422 86.775704 208.768292 86.775704 207.972683 \nC 86.775704 207.177073 86.459605 206.413943 85.897024 205.851362 \nC 85.334444 205.288781 84.571313 204.972683 83.775704 204.972683 \nC 82.980095 204.972683 82.216965 205.288781 81.654384 205.851362 \nC 81.091803 206.413943 80.775704 207.177073 80.775704 207.972683 \nC 80.775704 208.768292 81.091803 209.531422 81.654384 210.094003 \nC 82.216965 210.656584 82.980095 210.972683 83.775704 210.972683 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 215.737303 \nC 84.736911 215.737303 85.500041 215.421204 86.062622 214.858624 \nC 86.625203 214.296043 86.941302 213.532912 86.941302 212.737303 \nC 86.941302 211.941694 86.625203 211.178564 86.062622 210.615983 \nC 85.500041 210.053402 84.736911 209.737303 83.941302 209.737303 \nC 83.145692 209.737303 82.382562 210.053402 81.819981 210.615983 \nC 81.257401 211.178564 80.941302 211.941694 80.941302 212.737303 \nC 80.941302 213.532912 81.257401 214.296043 81.819981 214.858624 \nC 82.382562 215.421204 83.145692 215.737303 83.941302 215.737303 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 214.958353 \nC 85.068106 214.958353 85.831236 214.642254 86.393817 214.079673 \nC 86.956398 213.517093 87.272497 212.753962 87.272497 211.958353 \nC 87.272497 211.162744 86.956398 210.399613 86.393817 209.837033 \nC 85.831236 209.274452 85.068106 208.958353 84.272497 208.958353 \nC 83.476887 208.958353 82.713757 209.274452 82.151176 209.837033 \nC 81.588596 210.399613 81.272497 211.162744 81.272497 211.958353 \nC 81.272497 212.753962 81.588596 213.517093 82.151176 214.079673 \nC 82.713757 214.642254 83.476887 214.958353 84.272497 214.958353 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 212.368851 \nC 85.730496 212.368851 86.493626 212.052753 87.056207 211.490172 \nC 87.618788 210.927591 87.934887 210.164461 87.934887 209.368851 \nC 87.934887 208.573242 87.618788 207.810112 87.056207 207.247531 \nC 86.493626 206.68495 85.730496 206.368851 84.934887 206.368851 \nC 84.139277 206.368851 83.376147 206.68495 82.813566 207.247531 \nC 82.250986 207.810112 81.934887 208.573242 81.934887 209.368851 \nC 81.934887 210.164461 82.250986 210.927591 82.813566 211.490172 \nC 83.376147 212.052753 84.139277 212.368851 84.934887 212.368851 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 212.403585 \nC 87.055276 212.403585 87.818406 212.087486 88.380987 211.524905 \nC 88.943568 210.962324 89.259667 210.199194 89.259667 209.403585 \nC 89.259667 208.607975 88.943568 207.844845 88.380987 207.282264 \nC 87.818406 206.719684 87.055276 206.403585 86.259667 206.403585 \nC 85.464058 206.403585 84.700927 206.719684 84.138347 207.282264 \nC 83.575766 207.844845 83.259667 208.607975 83.259667 209.403585 \nC 83.259667 210.199194 83.575766 210.962324 84.138347 211.524905 \nC 84.700927 212.087486 85.464058 212.403585 86.259667 212.403585 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.095837 \nC 89.704836 20.095837 90.467967 19.779738 91.030547 19.217157 \nC 91.593128 18.654576 91.909227 17.891446 91.909227 17.095837 \nC 91.909227 16.300227 91.593128 15.537097 91.030547 14.974516 \nC 90.467967 14.411936 89.704836 14.095837 88.909227 14.095837 \nC 88.113618 14.095837 87.350488 14.411936 86.787907 14.974516 \nC 86.225326 15.537097 85.909227 16.300227 85.909227 17.095837 \nC 85.909227 17.891446 86.225326 18.654576 86.787907 19.217157 \nC 87.350488 19.779738 88.113618 20.095837 88.909227 20.095837 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 211.408285 \nL 83.596307 209.908285 \nL 85.096307 211.408285 \nL 86.596307 209.908285 \nL 85.096307 208.408285 \nL 86.596307 206.908285 \nL 85.096307 205.408285 \nL 83.596307 206.908285 \nL 82.096307 205.408285 \nL 80.596307 206.908285 \nL 82.096307 208.408285 \nL 80.596307 209.908285 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 200.585699 \nL 83.674506 199.085699 \nL 85.174506 200.585699 \nL 86.674506 199.085699 \nL 85.174506 197.585699 \nL 86.674506 196.085699 \nL 85.174506 194.585699 \nL 83.674506 196.085699 \nL 82.174506 194.585699 \nL 80.674506 196.085699 \nL 82.174506 197.585699 \nL 80.674506 199.085699 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 214.38449 \nL 83.941302 212.88449 \nL 85.441302 214.38449 \nL 86.941302 212.88449 \nL 85.441302 211.38449 \nL 86.941302 209.88449 \nL 85.441302 208.38449 \nL 83.941302 209.88449 \nL 82.441302 208.38449 \nL 80.941302 209.88449 \nL 82.441302 211.38449 \nL 80.941302 212.88449 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 211.765102 \nL 84.916487 210.265102 \nL 86.416487 211.765102 \nL 87.916487 210.265102 \nL 86.416487 208.765102 \nL 87.916487 207.265102 \nL 86.416487 205.765102 \nL 84.916487 207.265102 \nL 83.416487 205.765102 \nL 81.916487 207.265102 \nL 83.416487 208.765102 \nL 81.916487 210.265102 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 189.58002 \nL 88.633231 188.08002 \nL 90.133231 189.58002 \nL 91.633231 188.08002 \nL 90.133231 186.58002 \nL 91.633231 185.08002 \nL 90.133231 183.58002 \nL 88.633231 185.08002 \nL 87.133231 183.58002 \nL 85.633231 185.08002 \nL 87.133231 186.58002 \nL 85.633231 188.08002 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 167.863522 \nL 103.132214 166.363522 \nL 104.632214 167.863522 \nL 106.132214 166.363522 \nL 104.632214 164.863522 \nL 106.132214 163.363522 \nL 104.632214 161.863522 \nL 103.132214 163.363522 \nL 101.632214 161.863522 \nL 100.132214 163.363522 \nL 101.632214 164.863522 \nL 100.132214 166.363522 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 167.556272 \nL 160.392155 166.056272 \nL 161.892155 167.556272 \nL 163.392155 166.056272 \nL 161.892155 164.556272 \nL 163.392155 163.056272 \nL 161.892155 161.556272 \nL 160.392155 163.056272 \nL 158.892155 161.556272 \nL 157.392155 163.056272 \nL 158.892155 164.556272 \nL 157.392155 166.056272 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 165.320594 \nL 387.959943 163.820594 \nL 389.459943 165.320594 \nL 390.959943 163.820594 \nL 389.459943 162.320594 \nL 390.959943 160.820594 \nL 389.459943 159.320594 \nL 387.959943 160.820594 \nL 386.459943 159.320594 \nL 384.959943 160.820594 \nL 386.459943 162.320594 \nL 384.959943 163.820594 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 209.246608 \nL 83.775704 207.746608 \nL 85.275704 209.246608 \nL 86.775704 207.746608 \nL 85.275704 206.246608 \nL 86.775704 204.746608 \nL 85.275704 203.246608 \nL 83.775704 204.746608 \nL 82.275704 203.246608 \nL 80.775704 204.746608 \nL 82.275704 206.246608 \nL 80.775704 207.746608 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 197.553008 \nL 83.941302 196.053008 \nL 85.441302 197.553008 \nL 86.941302 196.053008 \nL 85.441302 194.553008 \nL 86.941302 193.053008 \nL 85.441302 191.553008 \nL 83.941302 193.053008 \nL 82.441302 191.553008 \nL 80.941302 193.053008 \nL 82.441302 194.553008 \nL 80.941302 196.053008 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 205.372853 \nL 84.272497 203.872853 \nL 85.772497 205.372853 \nL 87.272497 203.872853 \nL 85.772497 202.372853 \nL 87.272497 200.872853 \nL 85.772497 199.372853 \nL 84.272497 200.872853 \nL 82.772497 199.372853 \nL 81.272497 200.872853 \nL 82.772497 202.372853 \nL 81.272497 203.872853 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 207.217999 \nL 84.934887 205.717999 \nL 86.434887 207.217999 \nL 87.934887 205.717999 \nL 86.434887 204.217999 \nL 87.934887 202.717999 \nL 86.434887 201.217999 \nL 84.934887 202.717999 \nL 83.434887 201.217999 \nL 81.934887 202.717999 \nL 83.434887 204.217999 \nL 81.934887 205.717999 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 190.480842 \nL 86.259667 188.980842 \nL 87.759667 190.480842 \nL 89.259667 188.980842 \nL 87.759667 187.480842 \nL 89.259667 185.980842 \nL 87.759667 184.480842 \nL 86.259667 185.980842 \nL 84.759667 184.480842 \nL 83.259667 185.980842 \nL 84.759667 187.480842 \nL 83.259667 188.980842 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 211.643775 \nL 88.909227 210.143775 \nL 90.409227 211.643775 \nL 91.909227 210.143775 \nL 90.409227 208.643775 \nL 91.909227 207.143775 \nL 90.409227 205.643775 \nL 88.909227 207.143775 \nL 87.409227 205.643775 \nL 85.909227 207.143775 \nL 87.409227 208.643775 \nL 85.909227 210.143775 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.596307 215.308457 \nC 84.391916 215.308457 85.155046 214.992358 85.717627 214.429777 \nC 86.280208 213.867196 86.596307 213.104066 86.596307 212.308457 \nC 86.596307 211.512847 86.280208 210.749717 85.717627 210.187136 \nC 85.155046 209.624556 84.391916 209.308457 83.596307 209.308457 \nC 82.800698 209.308457 82.037567 209.624556 81.474986 210.187136 \nC 80.912406 210.749717 80.596307 211.512847 80.596307 212.308457 \nC 80.596307 213.104066 80.912406 213.867196 81.474986 214.429777 \nC 82.037567 214.992358 82.800698 215.308457 83.596307 215.308457 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.674506 213.852223 \nC 84.470115 213.852223 85.233245 213.536124 85.795826 212.973543 \nC 86.358407 212.410962 86.674506 211.647832 86.674506 210.852223 \nC 86.674506 210.056613 86.358407 209.293483 85.795826 208.730902 \nC 85.233245 208.168322 84.470115 207.852223 83.674506 207.852223 \nC 82.878896 207.852223 82.115766 208.168322 81.553185 208.730902 \nC 80.990605 209.293483 80.674506 210.056613 80.674506 210.852223 \nC 80.674506 211.647832 80.990605 212.410962 81.553185 212.973543 \nC 82.115766 213.536124 82.878896 213.852223 83.674506 213.852223 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 214.939644 \nC 84.736911 214.939644 85.500041 214.623545 86.062622 214.060965 \nC 86.625203 213.498384 86.941302 212.735253 86.941302 211.939644 \nC 86.941302 211.144035 86.625203 210.380905 86.062622 209.818324 \nC 85.500041 209.255743 84.736911 208.939644 83.941302 208.939644 \nC 83.145692 208.939644 82.382562 209.255743 81.819981 209.818324 \nC 81.257401 210.380905 80.941302 211.144035 80.941302 211.939644 \nC 80.941302 212.735253 81.257401 213.498384 81.819981 214.060965 \nC 82.382562 214.623545 83.145692 214.939644 83.941302 214.939644 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.916487 216.504677 \nC 85.712096 216.504677 86.475227 216.188578 87.037807 215.625998 \nC 87.600388 215.063417 87.916487 214.300287 87.916487 213.504677 \nC 87.916487 212.709068 87.600388 211.945938 87.037807 211.383357 \nC 86.475227 210.820776 85.712096 210.504677 84.916487 210.504677 \nC 84.120878 210.504677 83.357747 210.820776 82.795167 211.383357 \nC 82.232586 211.945938 81.916487 212.709068 81.916487 213.504677 \nC 81.916487 214.300287 82.232586 215.063417 82.795167 215.625998 \nC 83.357747 216.188578 84.120878 216.504677 84.916487 216.504677 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.633231 208.541026 \nC 89.428841 208.541026 90.191971 208.224927 90.754552 207.662346 \nC 91.317132 207.099765 91.633231 206.336635 91.633231 205.541026 \nC 91.633231 204.745416 91.317132 203.982286 90.754552 203.419705 \nC 90.191971 202.857124 89.428841 202.541026 88.633231 202.541026 \nC 87.837622 202.541026 87.074492 202.857124 86.511911 203.419705 \nC 85.94933 203.982286 85.633231 204.745416 85.633231 205.541026 \nC 85.633231 206.336635 85.94933 207.099765 86.511911 207.662346 \nC 87.074492 208.224927 87.837622 208.541026 88.633231 208.541026 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 103.132214 168.049607 \nC 103.927823 168.049607 104.690954 167.733509 105.253534 167.170928 \nC 105.816115 166.608347 106.132214 165.845217 106.132214 165.049607 \nC 106.132214 164.253998 105.816115 163.490868 105.253534 162.928287 \nC 104.690954 162.365706 103.927823 162.049607 103.132214 162.049607 \nC 102.336605 162.049607 101.573474 162.365706 101.010894 162.928287 \nC 100.448313 163.490868 100.132214 164.253998 100.132214 165.049607 \nC 100.132214 165.845217 100.448313 166.608347 101.010894 167.170928 \nC 101.573474 167.733509 102.336605 168.049607 103.132214 168.049607 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 160.392155 186.214135 \nC 161.187765 186.214135 161.950895 185.898036 162.513476 185.335455 \nC 163.076056 184.772874 163.392155 184.009744 163.392155 183.214135 \nC 163.392155 182.418525 163.076056 181.655395 162.513476 181.092814 \nC 161.950895 180.530234 161.187765 180.214135 160.392155 180.214135 \nC 159.596546 180.214135 158.833416 180.530234 158.270835 181.092814 \nC 157.708254 181.655395 157.392155 182.418525 157.392155 183.214135 \nC 157.392155 184.009744 157.708254 184.772874 158.270835 185.335455 \nC 158.833416 185.898036 159.596546 186.214135 160.392155 186.214135 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 387.959943 164.576445 \nC 388.755552 164.576445 389.518683 164.260346 390.081264 163.697765 \nC 390.643844 163.135185 390.959943 162.372054 390.959943 161.576445 \nC 390.959943 160.780836 390.643844 160.017705 390.081264 159.455125 \nC 389.518683 158.892544 388.755552 158.576445 387.959943 158.576445 \nC 387.164334 158.576445 386.401204 158.892544 385.838623 159.455125 \nC 385.276042 160.017705 384.959943 160.780836 384.959943 161.576445 \nC 384.959943 162.372054 385.276042 163.135185 385.838623 163.697765 \nC 386.401204 164.260346 387.164334 164.576445 387.959943 164.576445 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.775704 187.455132 \nC 84.571313 187.455132 85.334444 187.139033 85.897024 186.576452 \nC 86.459605 186.013871 86.775704 185.250741 86.775704 184.455132 \nC 86.775704 183.659523 86.459605 182.896392 85.897024 182.333812 \nC 85.334444 181.771231 84.571313 181.455132 83.775704 181.455132 \nC 82.980095 181.455132 82.216965 181.771231 81.654384 182.333812 \nC 81.091803 182.896392 80.775704 183.659523 80.775704 184.455132 \nC 80.775704 185.250741 81.091803 186.013871 81.654384 186.576452 \nC 82.216965 187.139033 82.980095 187.455132 83.775704 187.455132 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.941302 213.266439 \nC 84.736911 213.266439 85.500041 212.95034 86.062622 212.38776 \nC 86.625203 211.825179 86.941302 211.062049 86.941302 210.266439 \nC 86.941302 209.47083 86.625203 208.7077 86.062622 208.145119 \nC 85.500041 207.582538 84.736911 207.266439 83.941302 207.266439 \nC 83.145692 207.266439 82.382562 207.582538 81.819981 208.145119 \nC 81.257401 208.7077 80.941302 209.47083 80.941302 210.266439 \nC 80.941302 211.062049 81.257401 211.825179 81.819981 212.38776 \nC 82.382562 212.95034 83.145692 213.266439 83.941302 213.266439 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.272497 215.100524 \nC 85.068106 215.100524 85.831236 214.784426 86.393817 214.221845 \nC 86.956398 213.659264 87.272497 212.896134 87.272497 212.100524 \nC 87.272497 211.304915 86.956398 210.541785 86.393817 209.979204 \nC 85.831236 209.416623 85.068106 209.100524 84.272497 209.100524 \nC 83.476887 209.100524 82.713757 209.416623 82.151176 209.979204 \nC 81.588596 210.541785 81.272497 211.304915 81.272497 212.100524 \nC 81.272497 212.896134 81.588596 213.659264 82.151176 214.221845 \nC 82.713757 214.784426 83.476887 215.100524 84.272497 215.100524 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.934887 213.441077 \nC 85.730496 213.441077 86.493626 213.124979 87.056207 212.562398 \nC 87.618788 211.999817 87.934887 211.236687 87.934887 210.441077 \nC 87.934887 209.645468 87.618788 208.882338 87.056207 208.319757 \nC 86.493626 207.757176 85.730496 207.441077 84.934887 207.441077 \nC 84.139277 207.441077 83.376147 207.757176 82.813566 208.319757 \nC 82.250986 208.882338 81.934887 209.645468 81.934887 210.441077 \nC 81.934887 211.236687 82.250986 211.999817 82.813566 212.562398 \nC 83.376147 213.124979 84.139277 213.441077 84.934887 213.441077 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 86.259667 24.906537 \nC 87.055276 24.906537 87.818406 24.590438 88.380987 24.027857 \nC 88.943568 23.465276 89.259667 22.702146 89.259667 21.906537 \nC 89.259667 21.110927 88.943568 20.347797 88.380987 19.785216 \nC 87.818406 19.222636 87.055276 18.906537 86.259667 18.906537 \nC 85.464058 18.906537 84.700927 19.222636 84.138347 19.785216 \nC 83.575766 20.347797 83.259667 21.110927 83.259667 21.906537 \nC 83.259667 22.702146 83.575766 23.465276 84.138347 24.027857 \nC 84.700927 24.590438 85.464058 24.906537 86.259667 24.906537 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 88.909227 20.083636 \nC 89.704836 20.083636 90.467967 19.767537 91.030547 19.204957 \nC 91.593128 18.642376 91.909227 17.879246 91.909227 17.083636 \nC 91.909227 16.288027 91.593128 15.524897 91.030547 14.962316 \nC 90.467967 14.399735 89.704836 14.083636 88.909227 14.083636 \nC 88.113618 14.083636 87.350488 14.399735 86.787907 14.962316 \nC 86.225326 15.524897 85.909227 16.288027 85.909227 17.083636 \nC 85.909227 17.879246 86.225326 18.642376 86.787907 19.204957 \nC 87.350488 19.767537 88.113618 20.083636 88.909227 20.083636 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.096307 216.728667 \nL 83.596307 215.228667 \nL 85.096307 216.728667 \nL 86.596307 215.228667 \nL 85.096307 213.728667 \nL 86.596307 212.228667 \nL 85.096307 210.728667 \nL 83.596307 212.228667 \nL 82.096307 210.728667 \nL 80.596307 212.228667 \nL 82.096307 213.728667 \nL 80.596307 215.228667 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.174506 214.214335 \nL 83.674506 212.714335 \nL 85.174506 214.214335 \nL 86.674506 212.714335 \nL 85.174506 211.214335 \nL 86.674506 209.714335 \nL 85.174506 208.214335 \nL 83.674506 209.714335 \nL 82.174506 208.214335 \nL 80.674506 209.714335 \nL 82.174506 211.214335 \nL 80.674506 212.714335 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 204.942969 \nL 83.941302 203.442969 \nL 85.441302 204.942969 \nL 86.941302 203.442969 \nL 85.441302 201.942969 \nL 86.941302 200.442969 \nL 85.441302 198.942969 \nL 83.941302 200.442969 \nL 82.441302 198.942969 \nL 80.941302 200.442969 \nL 82.441302 201.942969 \nL 80.941302 203.442969 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.416487 190.709165 \nL 84.916487 189.209165 \nL 86.416487 190.709165 \nL 87.916487 189.209165 \nL 86.416487 187.709165 \nL 87.916487 186.209165 \nL 86.416487 184.709165 \nL 84.916487 186.209165 \nL 83.416487 184.709165 \nL 81.916487 186.209165 \nL 83.416487 187.709165 \nL 81.916487 189.209165 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.133231 211.276746 \nL 88.633231 209.776746 \nL 90.133231 211.276746 \nL 91.633231 209.776746 \nL 90.133231 208.276746 \nL 91.633231 206.776746 \nL 90.133231 205.276746 \nL 88.633231 206.776746 \nL 87.133231 205.276746 \nL 85.633231 206.776746 \nL 87.133231 208.276746 \nL 85.633231 209.776746 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 101.632214 176.427858 \nL 103.132214 174.927858 \nL 104.632214 176.427858 \nL 106.132214 174.927858 \nL 104.632214 173.427858 \nL 106.132214 171.927858 \nL 104.632214 170.427858 \nL 103.132214 171.927858 \nL 101.632214 170.427858 \nL 100.132214 171.927858 \nL 101.632214 173.427858 \nL 100.132214 174.927858 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 158.892155 158.149091 \nL 160.392155 156.649091 \nL 161.892155 158.149091 \nL 163.392155 156.649091 \nL 161.892155 155.149091 \nL 163.392155 153.649091 \nL 161.892155 152.149091 \nL 160.392155 153.649091 \nL 158.892155 152.149091 \nL 157.392155 153.649091 \nL 158.892155 155.149091 \nL 157.392155 156.649091 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 386.459943 163.134267 \nL 387.959943 161.634267 \nL 389.459943 163.134267 \nL 390.959943 161.634267 \nL 389.459943 160.134267 \nL 390.959943 158.634267 \nL 389.459943 157.134267 \nL 387.959943 158.634267 \nL 386.459943 157.134267 \nL 384.959943 158.634267 \nL 386.459943 160.134267 \nL 384.959943 161.634267 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.275704 205.482374 \nL 83.775704 203.982374 \nL 85.275704 205.482374 \nL 86.775704 203.982374 \nL 85.275704 202.482374 \nL 86.775704 200.982374 \nL 85.275704 199.482374 \nL 83.775704 200.982374 \nL 82.275704 199.482374 \nL 80.775704 200.982374 \nL 82.275704 202.482374 \nL 80.775704 203.982374 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.441302 215.791929 \nL 83.941302 214.291929 \nL 85.441302 215.791929 \nL 86.941302 214.291929 \nL 85.441302 212.791929 \nL 86.941302 211.291929 \nL 85.441302 209.791929 \nL 83.941302 211.291929 \nL 82.441302 209.791929 \nL 80.941302 211.291929 \nL 82.441302 212.791929 \nL 80.941302 214.291929 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 82.772497 207.550894 \nL 84.272497 206.050894 \nL 85.772497 207.550894 \nL 87.272497 206.050894 \nL 85.772497 204.550894 \nL 87.272497 203.050894 \nL 85.772497 201.550894 \nL 84.272497 203.050894 \nL 82.772497 201.550894 \nL 81.272497 203.050894 \nL 82.772497 204.550894 \nL 81.272497 206.050894 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 83.434887 167.935772 \nL 84.934887 166.435772 \nL 86.434887 167.935772 \nL 87.934887 166.435772 \nL 86.434887 164.935772 \nL 87.934887 163.435772 \nL 86.434887 161.935772 \nL 84.934887 163.435772 \nL 83.434887 161.935772 \nL 81.934887 163.435772 \nL 83.434887 164.935772 \nL 81.934887 166.435772 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 84.759667 182.238548 \nL 86.259667 180.738548 \nL 87.759667 182.238548 \nL 89.259667 180.738548 \nL 87.759667 179.238548 \nL 89.259667 177.738548 \nL 87.759667 176.238548 \nL 86.259667 177.738548 \nL 84.759667 176.238548 \nL 83.259667 177.738548 \nL 84.759667 179.238548 \nL 83.259667 180.738548 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p6487185631)\" d=\"M 87.409227 212.189665 \nL 88.909227 210.689665 \nL 90.409227 212.189665 \nL 91.909227 210.689665 \nL 90.409227 209.189665 \nL 91.909227 207.689665 \nL 90.409227 206.189665 \nL 88.909227 207.689665 \nL 87.409227 206.189665 \nL 85.909227 207.689665 \nL 87.409227 209.189665 \nL 85.909227 210.689665 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n   </g>\n   <g id=\"PathCollection_2\"/>\n   <g id=\"PathCollection_3\"/>\n   <g id=\"PathCollection_4\"/>\n   <g id=\"PathCollection_5\"/>\n   <g id=\"PathCollection_6\"/>\n   <g id=\"PathCollection_7\"/>\n   <g id=\"PathCollection_8\"/>\n   <g id=\"PathCollection_9\"/>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m95549d192d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"83.552607\" xlink:href=\"#m95549d192d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(80.371357 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"129.551918\" xlink:href=\"#m95549d192d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20000 -->\n      <g transform=\"translate(113.645668 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"175.551228\" xlink:href=\"#m95549d192d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40000 -->\n      <g transform=\"translate(159.644978 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"221.550538\" xlink:href=\"#m95549d192d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60000 -->\n      <g transform=\"translate(205.644288 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"267.549849\" xlink:href=\"#m95549d192d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80000 -->\n      <g transform=\"translate(251.643599 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"313.549159\" xlink:href=\"#m95549d192d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100000 -->\n      <g transform=\"translate(294.461659 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"359.548469\" xlink:href=\"#m95549d192d\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120000 -->\n      <g transform=\"translate(340.460969 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- parameters -->\n     <g transform=\"translate(206.776562 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"165.869141\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"324.560547\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"386.083984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"425.292969\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"486.816406\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"527.929688\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"md940716d36\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"68.378125\" xlink:href=\"#md940716d36\" y=\"17.083008\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(43.778125 20.882227)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"mfe32220412\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#mfe32220412\" y=\"209.284084\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{6\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 213.083303)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 4488 3438 \nL 3059 2003 \nL 4488 575 \nL 4116 197 \nL 2681 1631 \nL 1247 197 \nL 878 575 \nL 2303 2003 \nL 878 3438 \nL 1247 3816 \nL 2681 2381 \nL 4116 3816 \nL 4488 3438 \nz\n\" id=\"DejaVuSans-d7\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#mfe32220412\" y=\"151.284005\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{7\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 155.083224)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-37\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#mfe32220412\" y=\"101.042052\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- $\\mathdefault{8\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 104.84127)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-38\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#mfe32220412\" y=\"56.725508\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- $\\mathdefault{9\\times10^{-1}}$ -->\n      <g transform=\"translate(20.878125 60.524727)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 703 97 \nL 703 672 \nQ 941 559 1184 500 \nQ 1428 441 1663 441 \nQ 2288 441 2617 861 \nQ 2947 1281 2994 2138 \nQ 2813 1869 2534 1725 \nQ 2256 1581 1919 1581 \nQ 1219 1581 811 2004 \nQ 403 2428 403 3163 \nQ 403 3881 828 4315 \nQ 1253 4750 1959 4750 \nQ 2769 4750 3195 4129 \nQ 3622 3509 3622 2328 \nQ 3622 1225 3098 567 \nQ 2575 -91 1691 -91 \nQ 1453 -91 1209 -44 \nQ 966 3 703 97 \nz\nM 1959 2075 \nQ 2384 2075 2632 2365 \nQ 2881 2656 2881 3163 \nQ 2881 3666 2632 3958 \nQ 2384 4250 1959 4250 \nQ 1534 4250 1286 3958 \nQ 1038 3666 1038 3163 \nQ 1038 2656 1286 2365 \nQ 1534 2075 1959 2075 \nz\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-39\"/>\n       <use transform=\"translate(83.105469 0.684375)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- loss -->\n     <g transform=\"translate(14.798437 125.577812)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 68.378125 224.64 \nL 68.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 403.178125 224.64 \nL 403.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 68.378125 224.64 \nL 403.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 68.378125 7.2 \nL 403.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 200.89375 219.64 \nL 270.6625 219.64 \nQ 272.6625 219.64 272.6625 217.64 \nL 272.6625 71.85875 \nQ 272.6625 69.85875 270.6625 69.85875 \nL 200.89375 69.85875 \nQ 198.89375 69.85875 198.89375 71.85875 \nL 198.89375 217.64 \nQ 198.89375 219.64 200.89375 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_15\">\n     <!-- layers -->\n     <g transform=\"translate(210.89375 81.457187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"209.765625\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"250.878906\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_10\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m08fb733f0a\" style=\"stroke:#edd1cb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#edd1cb;stroke:#edd1cb;\" x=\"212.89375\" xlink:href=\"#m08fb733f0a\" y=\"93.510312\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- 1 -->\n     <g transform=\"translate(230.89375 96.135312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_11\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m6f7584158e\" style=\"stroke:#ebcac5;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ebcac5;stroke:#ebcac5;\" x=\"212.89375\" xlink:href=\"#m6f7584158e\" y=\"108.188437\"/>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- 2 -->\n     <g transform=\"translate(230.89375 110.813437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_12\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m1f9140d249\" style=\"stroke:#e5bcbb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#e5bcbb;stroke:#e5bcbb;\" x=\"212.89375\" xlink:href=\"#m1f9140d249\" y=\"122.866562\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- 4 -->\n     <g transform=\"translate(230.89375 125.491562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_13\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m2cfc342390\" style=\"stroke:#d89faa;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#d89faa;stroke:#d89faa;\" x=\"212.89375\" xlink:href=\"#m2cfc342390\" y=\"137.544688\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- 8 -->\n     <g transform=\"translate(230.89375 140.169688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-38\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_14\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m1d0efb1d52\" style=\"stroke:#ae6b91;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ae6b91;stroke:#ae6b91;\" x=\"212.89375\" xlink:href=\"#m1d0efb1d52\" y=\"152.222812\"/>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- 16 -->\n     <g transform=\"translate(230.89375 154.847812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_15\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"ma1da0fe2cb\" style=\"stroke:#2d1e3e;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#2d1e3e;stroke:#2d1e3e;\" x=\"212.89375\" xlink:href=\"#ma1da0fe2cb\" y=\"166.900937\"/>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- 32 -->\n     <g transform=\"translate(230.89375 169.525937)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"text_22\">\n     <!-- model -->\n     <g transform=\"translate(210.89375 184.204062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_16\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m355c9c4348\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"212.89375\" xlink:href=\"#m355c9c4348\" y=\"196.257187\"/>\n     </g>\n    </g>\n    <g id=\"text_23\">\n     <!-- FFNN -->\n     <g transform=\"translate(230.89375 198.882187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_17\">\n     <defs>\n      <path d=\"M -1.5 3 \nL 0 1.5 \nL 1.5 3 \nL 3 1.5 \nL 1.5 0 \nL 3 -1.5 \nL 1.5 -3 \nL 0 -1.5 \nL -1.5 -3 \nL -3 -1.5 \nL -1.5 0 \nL -3 1.5 \nz\n\" id=\"m8dde95e39c\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"212.89375\" xlink:href=\"#m8dde95e39c\" y=\"210.935312\"/>\n     </g>\n    </g>\n    <g id=\"text_24\">\n     <!-- ResNET -->\n     <g transform=\"translate(230.89375 213.560312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p6487185631\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"68.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280 280\n"
     ]
    }
   ],
   "source": [
    "print(len(d_results[d_results.loss < 2]), len(d_results))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}