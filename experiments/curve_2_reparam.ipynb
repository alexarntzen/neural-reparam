{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from neural_reparam.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from neural_reparam.ResNet import ResNet, init_zero\n",
    "import experiments.curves_2 as c2\n",
    "\n",
    "# make reproducible\n",
    "seed = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_2/\"\n",
    "SET_NAME = \"exp_6\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "\n",
    "FOLDS = 1\n",
    "N = 256 # training points internal\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=5,\n",
    "                                                                        verbose=True)\n",
    "loss_func = get_elastic_metric_loss(r=c2.r, constrain_cost=1e3, verbose=False)\n",
    "no_penalty_loss_func = get_elastic_metric_loss(r=c2.r, constrain_cost=0, verbose=False)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [2],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"model\": [FFNN, ResNet],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N // 2],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [100],\n",
    "    \"learning_rate\": [1],\n",
    "    \"lr_scheduler\": [lr_scheduler]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = c2.q(x_train.detach())\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59218872\n",
      "################################  5  ################################\n",
      "Training Loss:  0.58922362\n",
      "################################  10  ################################\n",
      "Training Loss:  5.70864105\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  11.35127735\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  5.94324589\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  7.51574755\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  3.92681932\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  3.92673993\n",
      "################################  40  ################################\n",
      "Training Loss:  3.92673993\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  3.92673993\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  3.92673993\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  3.92673993\n",
      "################################  60  ################################\n",
      "Training Loss:  3.92673993\n",
      "################################  65  ################################\n",
      "Training Loss:  3.92673993\n",
      "################################  70  ################################\n",
      "Training Loss:  3.92673993\n",
      "################################  75  ################################\n",
      "Training Loss:  3.92673993\n",
      "################################  80  ################################\n",
      "Training Loss:  3.92673993\n",
      "################################  85  ################################\n",
      "Training Loss:  3.92673993\n",
      "################################  90  ################################\n",
      "Training Loss:  3.92673993\n",
      "################################  95  ################################\n",
      "Training Loss:  3.92673993\n",
      "Final training Loss:  3.92673993\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59227145\n",
      "################################  5  ################################\n",
      "Training Loss:  7.87599325\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  3.92084074\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59281033\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58964276\n",
      "################################  25  ################################\n",
      "Training Loss:  1.15262103\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.77019715\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59240687\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59240687\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59240687\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59240687\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59240687\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59240687\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59240687\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59240687\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59240687\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59240687\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59240687\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59240687\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59240687\n",
      "Final training Loss:  0.59240687\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  25.98102188\n",
      "################################  5  ################################\n",
      "Training Loss:  5.77153969\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59035695\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  6.98777056\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  5.92027378\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59223276\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  5.23060751\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.68698263\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.686975\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.686975\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.686975\n",
      "################################  55  ################################\n",
      "Training Loss:  0.686975\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.686975\n",
      "################################  65  ################################\n",
      "Training Loss:  0.686975\n",
      "################################  70  ################################\n",
      "Training Loss:  0.686975\n",
      "################################  75  ################################\n",
      "Training Loss:  0.686975\n",
      "################################  80  ################################\n",
      "Training Loss:  0.686975\n",
      "################################  85  ################################\n",
      "Training Loss:  0.686975\n",
      "################################  90  ################################\n",
      "Training Loss:  0.686975\n",
      "################################  95  ################################\n",
      "Training Loss:  0.686975\n",
      "Final training Loss:  0.686975\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65106422\n",
      "################################  5  ################################\n",
      "Training Loss:  0.58992732\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59169573\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  7.97293186\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.58955371\n",
      "################################  25  ################################\n",
      "Training Loss:  1.99114215\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59142947\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59142935\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59142935\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59142935\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59142935\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59142935\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59142935\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59142935\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59142935\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59142935\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59142935\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59142935\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59142935\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59142935\n",
      "Final training Loss:  0.59142935\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64267927\n",
      "################################  5  ################################\n",
      "Training Loss:  62.94193268\n",
      "################################  10  ################################\n",
      "Training Loss:  7.63109493\n",
      "################################  15  ################################\n",
      "Training Loss:  47.61635208\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59093171\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59060746\n",
      "################################  30  ################################\n",
      "Training Loss:  9.7981472\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  13.32595348\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59287286\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59370697\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59363228\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59363234\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59363234\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59363234\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59363234\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59363234\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59363234\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59363234\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59363234\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59363234\n",
      "Final training Loss:  0.59363234\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64255488\n",
      "################################  5  ################################\n",
      "Training Loss:  15.12932682\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58973879\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59056628\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  9.86070061\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.5908196\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58961767\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58960068\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58960062\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58960062\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58960062\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58960062\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58960062\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58960062\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58960062\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58960062\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58960062\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58960062\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58960062\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58960062\n",
      "Final training Loss:  0.58960062\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.86403942\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60243177\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60499281\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.31446242\n",
      "################################  5  ################################\n",
      "Training Loss:  5.21268892\n",
      "################################  10  ################################\n",
      "Training Loss:  6.57435751\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  12.25229168\n",
      "################################  20  ################################\n",
      "Training Loss:  0.5891999\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  15.93615055\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59206581\n",
      "################################  10  ################################\n",
      "Training Loss:  3.32848954\n",
      "################################  15  ################################\n",
      "Training Loss:  44.12121582\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59018314\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  10.39533234\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59692127\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60378301\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59659582\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  5.35515356\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59301794\n",
      "################################  25  ################################\n",
      "Training Loss:  2.81046462\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59988129\n",
      "################################  5  ################################\n",
      "Training Loss:  2.26431894\n",
      "################################  10  ################################\n",
      "Training Loss:  2.61290741\n",
      "################################  15  ################################\n",
      "Training Loss:  1.34271598\n",
      "################################  20  ################################\n",
      "Training Loss:  39.19378281\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.5923776\n",
      "################################  30  ################################\n",
      "Training Loss:  147.89097595\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59665495\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5945642\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59447765\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59447765\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59447765\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59447765\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59447765\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59447765\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59447765\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59447765\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59447765\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59447765\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59447765\n",
      "Final training Loss:  0.59447765\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.0065155\n",
      "################################  5  ################################\n",
      "Training Loss:  1.01763749\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59689993\n",
      "################################  15  ################################\n",
      "Training Loss:  6.82553768\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  8.63122177\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59271407\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59153593\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59112674\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59112632\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59112406\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59112394\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59112394\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59112394\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59112394\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59112394\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59112394\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59112394\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59112394\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59112394\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59112394\n",
      "Final training Loss:  0.59112394\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  5  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  500.99993896\n",
      "################################  35  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  55  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  60  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  65  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  70  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  75  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  80  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  85  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  90  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  95  ################################\n",
      "Training Loss:  500.99993896\n",
      "Final training Loss:  500.99993896\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59729832\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59480315\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59629798\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59547997\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.69571596\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  6.94201422\n",
      "################################  30  ################################\n",
      "Training Loss:  6.94199228\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  6.94183588\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  6.94183588\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  6.94183588\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  6.94183588\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  6.94183588\n",
      "################################  60  ################################\n",
      "Training Loss:  6.94183588\n",
      "################################  65  ################################\n",
      "Training Loss:  6.94183588\n",
      "################################  70  ################################\n",
      "Training Loss:  6.94183588\n",
      "################################  75  ################################\n",
      "Training Loss:  6.94183588\n",
      "################################  80  ################################\n",
      "Training Loss:  6.94183588\n",
      "################################  85  ################################\n",
      "Training Loss:  6.94183588\n",
      "################################  90  ################################\n",
      "Training Loss:  6.94183588\n",
      "################################  95  ################################\n",
      "Training Loss:  6.94183588\n",
      "Final training Loss:  6.94183588\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59809744\n",
      "################################  5  ################################\n",
      "Training Loss:  7.61989117\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.72606158\n",
      "################################  15  ################################\n",
      "Training Loss:  2.45650482\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  7.81346607\n",
      "################################  25  ################################\n",
      "Training Loss:  44.81300354\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.67924929\n",
      "################################  5  ################################\n",
      "Training Loss:  25.42311668\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.34558964\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59128571\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59388566\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59567261\n",
      "################################  20  ################################\n",
      "Training Loss:  119.97671509\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59328789\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59328789\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59328789\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59328789\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59328789\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59328789\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59328789\n",
      "Final training Loss:  0.59328789\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  17.59814453\n",
      "################################  5  ################################\n",
      "Training Loss:  59.35430145\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.60711008\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59570539\n",
      "################################  20  ################################\n",
      "Training Loss:  0.60579348\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  45.61564255\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  25.55623436\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59963483\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59963477\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59963477\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59963477\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59963477\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59963477\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59963477\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59963477\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59963477\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59963477\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59963477\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59963477\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59963477\n",
      "Final training Loss:  0.59963477\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  14.28028297\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60826445\n",
      "################################  10  ################################\n",
      "Training Loss:  0.62245381\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  16.35784531\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64449447\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.9302845\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59035659\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58929652\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  33.56653976\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  8.83612537\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.71247411\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58937484\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58937484\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58937484\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58937484\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58937484\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58937484\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58937484\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58937484\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58937484\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58937484\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58937484\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58937484\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58937484\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58937484\n",
      "Final training Loss:  0.58937484\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66002333\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59130037\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59449714\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59263802\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  9.71051311\n",
      "################################  25  ################################\n",
      "Training Loss:  48.06243896\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59889215\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  8.03217793\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  8.03241634\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  8.03244591\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  8.03244591\n",
      "################################  55  ################################\n",
      "Training Loss:  8.03244591\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.03244591\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  8.03244591\n",
      "################################  70  ################################\n",
      "Training Loss:  8.03244591\n",
      "################################  75  ################################\n",
      "Training Loss:  8.03244591\n",
      "################################  80  ################################\n",
      "Training Loss:  8.03244591\n",
      "################################  85  ################################\n",
      "Training Loss:  8.03244591\n",
      "################################  90  ################################\n",
      "Training Loss:  8.03244591\n",
      "################################  95  ################################\n",
      "Training Loss:  8.03244591\n",
      "Final training Loss:  8.03244591\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5967322\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59569633\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59957379\n",
      "################################  15  ################################\n",
      "Training Loss:  6.42175817\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  3.66662312\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  31.73328781\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58975565\n",
      "################################  35  ################################\n",
      "Training Loss:  47.13550186\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59078676\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59094149\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59094137\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59094137\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59094137\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59094137\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59094137\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59094137\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59094137\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59094137\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59094137\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59094137\n",
      "Final training Loss:  0.59094137\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61614889\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59124219\n",
      "################################  10  ################################\n",
      "Training Loss:  0.5901131\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  4.65130758\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.5907312\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.5910151\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5904789\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59047884\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59047866\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59047866\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59047866\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59047866\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59047866\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59047866\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59047866\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59047866\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59047866\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59047866\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59047866\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59047866\n",
      "Final training Loss:  0.59047866\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  45.65388107\n",
      "################################  5  ################################\n",
      "Training Loss:  1.84760582\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.58971357\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59252638\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  6.09942341\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  18.15598869\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60022819\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.60022819\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60022819\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.60022819\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60022819\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60022819\n",
      "Final training Loss:  0.60022819\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64546806\n",
      "################################  5  ################################\n",
      "Training Loss:  0.64546615\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.64324325\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.65717739\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59243\n",
      "################################  25  ################################\n",
      "Training Loss:  24.21549797\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59490234\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59490222\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59490222\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59490222\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59490222\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59490222\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59490222\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59490222\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59490222\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59490222\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59490222\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59490222\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59490222\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59490222\n",
      "Final training Loss:  0.59490222\n",
      "\n",
      "Running model (trial=1, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59282982\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59802353\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58955669\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59011352\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59324425\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  1.71802914\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.73154426\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.72946864\n",
      "################################  45  ################################\n",
      "Training Loss:  0.72946864\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.72946864\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  60  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  65  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  70  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  75  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  80  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  85  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  90  ################################\n",
      "Training Loss:  0.72946864\n",
      "################################  95  ################################\n",
      "Training Loss:  0.72946864\n",
      "Final training Loss:  0.72946864\n",
      "\n",
      "Running model (trial=1, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59435874\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59080589\n",
      "################################  10  ################################\n",
      "Training Loss:  0.5952962\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59443337\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  75.58035278\n",
      "################################  25  ################################\n",
      "Training Loss:  19.98809814\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59140593\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  3.87413669\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  3.87416744\n",
      "################################  45  ################################\n",
      "Training Loss:  3.8735342\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  3.8735342\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  3.8735342\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.8735342\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  3.8735342\n",
      "################################  70  ################################\n",
      "Training Loss:  3.8735342\n",
      "################################  75  ################################\n",
      "Training Loss:  3.8735342\n",
      "################################  80  ################################\n",
      "Training Loss:  3.8735342\n",
      "################################  85  ################################\n",
      "Training Loss:  3.8735342\n",
      "################################  90  ################################\n",
      "Training Loss:  3.8735342\n",
      "################################  95  ################################\n",
      "Training Loss:  3.8735342\n",
      "Final training Loss:  3.8735342\n",
      "\n",
      "Running model (trial=1, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.37803888\n",
      "################################  5  ################################\n",
      "Training Loss:  8.26192951\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59012586\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59203643\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  1.64026642\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  59.7255249\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59057039\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59057039\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59057039\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59057039\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59057039\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59057039\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59057039\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59057039\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59057039\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59057039\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59057039\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59057039\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59057039\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59057039\n",
      "Final training Loss:  0.59057039\n",
      "\n",
      "Running model (trial=1, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59267032\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59178388\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  6.52317286\n",
      "################################  15  ################################\n",
      "Training Loss:  7.37929201\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59508681\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.58930051\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59497303\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59497136\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59497136\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59497136\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59497136\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59497136\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59497136\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59497136\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59497136\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59497136\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59497136\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59497136\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59497136\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59497136\n",
      "Final training Loss:  0.59497136\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64100689\n",
      "################################  5  ################################\n",
      "Training Loss:  99.40685272\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  102.06890869\n",
      "################################  15  ################################\n",
      "Training Loss:  7.29759121\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.60366541\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59235758\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  5.64972639\n",
      "################################  35  ################################\n",
      "Training Loss:  5.64987469\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  5.64987469\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  5.64987469\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  5.64987469\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  5.64987469\n",
      "################################  60  ################################\n",
      "Training Loss:  5.64987469\n",
      "################################  65  ################################\n",
      "Training Loss:  5.64987469\n",
      "################################  70  ################################\n",
      "Training Loss:  5.64987469\n",
      "################################  75  ################################\n",
      "Training Loss:  5.64987469\n",
      "################################  80  ################################\n",
      "Training Loss:  5.64987469\n",
      "################################  85  ################################\n",
      "Training Loss:  5.64987469\n",
      "################################  90  ################################\n",
      "Training Loss:  5.64987469\n",
      "################################  95  ################################\n",
      "Training Loss:  5.64987469\n",
      "Final training Loss:  5.64987469\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59563708\n",
      "################################  5  ################################\n",
      "Training Loss:  0.63291883\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.61260134\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59986186\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59175348\n",
      "################################  25  ################################\n",
      "Training Loss:  18.5679493\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5922839\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.6003198\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59902596\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59902555\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59902555\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59902555\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59902555\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59902555\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59902555\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59902555\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59902555\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59902555\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59902555\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59902555\n",
      "Final training Loss:  0.59902555\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.41648793\n",
      "################################  5  ################################\n",
      "Training Loss:  0.62927437\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59596694\n",
      "################################  15  ################################\n",
      "Training Loss:  59.61531448\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  7.15679216\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59059149\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  50.28072357\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59222609\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59220618\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59220618\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59220618\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59220618\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59220618\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59220618\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59220618\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59220618\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59220618\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59220618\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59220618\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59220618\n",
      "Final training Loss:  0.59220618\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60421991\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60421991\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59613144\n",
      "################################  15  ################################\n",
      "Training Loss:  2.06541395\n",
      "################################  20  ################################\n",
      "Training Loss:  0.5898627\n",
      "################################  25  ################################\n",
      "Training Loss:  3.69108438\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.58912385\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58947504\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  135.15731812\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.52117467\n",
      "################################  5  ################################\n",
      "Training Loss:  5.96895933\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  78.57546997\n",
      "################################  15  ################################\n",
      "Training Loss:  10.23517323\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59016645\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59016639\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59016639\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59016639\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59016639\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59016639\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59016639\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59016639\n",
      "Final training Loss:  0.59016639\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59335953\n",
      "################################  5  ################################\n",
      "Training Loss:  0.91088855\n",
      "################################  10  ################################\n",
      "Training Loss:  4.93350267\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59052253\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59413272\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59756541\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60159713\n",
      "################################  35  ################################\n",
      "Training Loss:  1.06054676\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58997834\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58997822\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58997822\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58997822\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58997822\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58997822\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58997822\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58997822\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58997822\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58997822\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58997822\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58997822\n",
      "Final training Loss:  0.58997822\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63507217\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59326607\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59154379\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59846634\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.5899415\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  94.34160614\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5910483\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59104818\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59104824\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59104824\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59104824\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59104824\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59104824\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59104824\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59104824\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59104824\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59104824\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59104824\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59104824\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59104824\n",
      "Final training Loss:  0.59104824\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60379905\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59207255\n",
      "################################  10  ################################\n",
      "Training Loss:  1.45337319\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59060574\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58970618\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58977675\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59001005\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58957183\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58957165\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58957165\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58957183\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58957183\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58957183\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58957183\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58957183\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58957183\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58957183\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58957183\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58957183\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58957183\n",
      "Final training Loss:  0.58957183\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  5  ################################\n",
      "Training Loss:  501.00009155\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  501.00009155\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  501.00009155\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  501.00009155\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  501.00009155\n",
      "################################  35  ################################\n",
      "Training Loss:  501.00009155\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  501.00009155\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  501.00009155\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  55  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  60  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  65  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  70  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  75  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  80  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  85  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  90  ################################\n",
      "Training Loss:  501.00009155\n",
      "################################  95  ################################\n",
      "Training Loss:  501.00009155\n",
      "Final training Loss:  501.00009155\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59224564\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59121007\n",
      "################################  10  ################################\n",
      "Training Loss:  0.5899744\n",
      "################################  15  ################################\n",
      "Training Loss:  8.52968502\n",
      "################################  20  ################################\n",
      "Training Loss:  5.04580355\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59105438\n",
      "################################  30  ################################\n",
      "Training Loss:  7.09881926\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58958924\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  3.05859852\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.6191963\n",
      "################################  50  ################################\n",
      "Training Loss:  1.61858714\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  1.61858714\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.61858714\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  1.61858714\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  1.61858714\n",
      "################################  75  ################################\n",
      "Training Loss:  1.61858714\n",
      "################################  80  ################################\n",
      "Training Loss:  1.61858714\n",
      "################################  85  ################################\n",
      "Training Loss:  1.61858714\n",
      "################################  90  ################################\n",
      "Training Loss:  1.61858714\n",
      "################################  95  ################################\n",
      "Training Loss:  1.61858714\n",
      "Final training Loss:  1.61858714\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6866976\n",
      "################################  5  ################################\n",
      "Training Loss:  13.18108654\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59137881\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  7.46590424\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  7.04623079\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59079725\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59822553\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  9.07783508\n",
      "################################  40  ################################\n",
      "Training Loss:  9.07756615\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  9.07723808\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  9.077178\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  9.077178\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  9.077178\n",
      "################################  65  ################################\n",
      "Training Loss:  9.077178\n",
      "################################  70  ################################\n",
      "Training Loss:  9.077178\n",
      "################################  75  ################################\n",
      "Training Loss:  9.077178\n",
      "################################  80  ################################\n",
      "Training Loss:  9.077178\n",
      "################################  85  ################################\n",
      "Training Loss:  9.077178\n",
      "################################  90  ################################\n",
      "Training Loss:  9.077178\n",
      "################################  95  ################################\n",
      "Training Loss:  9.077178\n",
      "Final training Loss:  9.077178\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63465935\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59070587\n",
      "################################  10  ################################\n",
      "Training Loss:  6.30716515\n",
      "################################  15  ################################\n",
      "Training Loss:  12.91884041\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58971125\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.590361\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5906322\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59058172\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59058172\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59058172\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59058172\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59058172\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59058172\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59058172\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59058172\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59058172\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59058172\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59058172\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59058172\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59058172\n",
      "Final training Loss:  0.59058172\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69312125\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61782932\n",
      "################################  10  ################################\n",
      "Training Loss:  15.15670681\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60360438\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.60194027\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  41.76807022\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59535825\n",
      "################################  35  ################################\n",
      "Training Loss:  96.53817749\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59144074\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  13.12055111\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.60058272\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.60007501\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60007501\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60007501\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60007501\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60007501\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60007501\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60007501\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60007501\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60007501\n",
      "Final training Loss:  0.60007501\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  72.05823517\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  13.01065922\n",
      "################################  5  ################################\n",
      "Training Loss:  5.98665524\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60704416\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60704374\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.47787166\n",
      "################################  25  ################################\n",
      "Training Loss:  0.60766256\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.6076625\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.60766256\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.60766256\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60766256\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.60766256\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60766256\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60766256\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60766256\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60766256\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60766256\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60766256\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60766256\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60766256\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60766256\n",
      "Final training Loss:  0.60766256\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.23385906\n",
      "################################  5  ################################\n",
      "Training Loss:  0.62569106\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  121.55850983\n",
      "################################  15  ################################\n",
      "Training Loss:  12.36338806\n",
      "################################  20  ################################\n",
      "Training Loss:  13.69840622\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59167421\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59417051\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  9.12897873\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59202009\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59612012\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59179741\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58972317\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58972317\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58972317\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58972317\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.58972317\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58972317\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58972317\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58972317\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58972317\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58972317\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58972317\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58972317\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58972317\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58972317\n",
      "Final training Loss:  0.58972317\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70243937\n",
      "################################  5  ################################\n",
      "Training Loss:  14.50581264\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58911717\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59092546\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58878601\n",
      "################################  25  ################################\n",
      "Training Loss:  2.96977615\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  28.05931091\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  1.58531284\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58992881\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58992308\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58992314\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58992314\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58992314\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58992314\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58992314\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58992314\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58992314\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58992314\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58992314\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58992314\n",
      "Final training Loss:  0.58992314\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62303698\n",
      "################################  5  ################################\n",
      "Training Loss:  0.62303478\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.61302811\n",
      "################################  15  ################################\n",
      "Training Loss:  101.5356369\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  16.45161438\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  16.45137215\n",
      "################################  35  ################################\n",
      "Training Loss:  16.45113373\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  16.45113373\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  16.45113373\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  16.45113373\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  60  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  65  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  70  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  75  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  80  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  85  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  90  ################################\n",
      "Training Loss:  16.45113373\n",
      "################################  95  ################################\n",
      "Training Loss:  16.45113373\n",
      "Final training Loss:  16.45113373\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.95457029\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59023511\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58938766\n",
      "################################  15  ################################\n",
      "Training Loss:  3.51274657\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  8.00806522\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.24139881\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59778887\n",
      "################################  10  ################################\n",
      "Training Loss:  37.9452858\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.63183349\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.67497075\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60619956\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60619956\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59144515\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.60320449\n",
      "################################  20  ################################\n",
      "Training Loss:  8.87869644\n",
      "################################  25  ################################\n",
      "Training Loss:  37.10227203\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59029543\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59028924\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59028924\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59028924\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59028924\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59028924\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59028924\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59028947\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59028924\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59028947\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59028924\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59028924\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59028924\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59028924\n",
      "Final training Loss:  0.59028924\n",
      "\n",
      "Running model (trial=2, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59749907\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59104872\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59006721\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59099364\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58992273\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59361058\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  5.12201929\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  49.50136948\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59373254\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59373248\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59373248\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59373248\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59373248\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59373248\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59373248\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59373248\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59373248\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59373248\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59373248\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59373248\n",
      "Final training Loss:  0.59373248\n",
      "\n",
      "Running model (trial=2, mod=53, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61440039\n",
      "################################  5  ################################\n",
      "Training Loss:  113.55584717\n",
      "################################  10  ################################\n",
      "Training Loss:  4.33326101\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  9.16750526\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59118974\n",
      "################################  25  ################################\n",
      "Training Loss:  7.25469446\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58943224\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58942205\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58942181\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58942181\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58942181\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58942181\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58942181\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58942181\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58942181\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58942181\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58942181\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58942181\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58942181\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58942181\n",
      "Final training Loss:  0.58942181\n",
      "\n",
      "Running model (trial=2, mod=54, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59474701\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59396207\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  41.52189255\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.5914529\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59093386\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  1.8359617\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59473747\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59473747\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59473747\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59473747\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59473747\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59473747\n",
      "Final training Loss:  0.59473747\n",
      "\n",
      "Running model (trial=2, mod=55, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  25.72326088\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59194934\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  94.32739258\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59302437\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59043252\n",
      "################################  25  ################################\n",
      "Training Loss:  8.11713028\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  8.11743259\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  8.11729813\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  8.11734295\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  8.11735821\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  8.11735821\n",
      "################################  55  ################################\n",
      "Training Loss:  8.11735821\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.11735821\n",
      "################################  65  ################################\n",
      "Training Loss:  8.11735821\n",
      "################################  70  ################################\n",
      "Training Loss:  8.11735821\n",
      "################################  75  ################################\n",
      "Training Loss:  8.11735821\n",
      "################################  80  ################################\n",
      "Training Loss:  8.11735821\n",
      "################################  85  ################################\n",
      "Training Loss:  8.11735821\n",
      "################################  90  ################################\n",
      "Training Loss:  8.11735821\n",
      "################################  95  ################################\n",
      "Training Loss:  8.11735821\n",
      "Final training Loss:  8.11735821\n",
      "\n",
      "Running model (trial=2, mod=56, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59080315\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59551233\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59115154\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.88357687\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58955961\n",
      "################################  25  ################################\n",
      "Training Loss:  0.63298261\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.58921206\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59319305\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59319299\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59319299\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59319299\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59319299\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59319299\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59319299\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59319299\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59319299\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59319299\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59319299\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59319299\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59319299\n",
      "Final training Loss:  0.59319299\n",
      "\n",
      "Running model (trial=2, mod=57, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71444833\n",
      "################################  5  ################################\n",
      "Training Loss:  0.71444827\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59834355\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60117865\n",
      "################################  20  ################################\n",
      "Training Loss:  13.33236885\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59018093\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59139508\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  2.14688468\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58944821\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58944803\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58944803\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58944803\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58944803\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58944803\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58944803\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58944803\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58944803\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58944803\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58944803\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58944803\n",
      "Final training Loss:  0.58944803\n",
      "\n",
      "Running model (trial=2, mod=58, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60988981\n",
      "################################  5  ################################\n",
      "Training Loss:  30.55974197\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60355407\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  9.9424181\n",
      "################################  20  ################################\n",
      "Training Loss:  14.78174782\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59038234\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59038228\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5903824\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5903824\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.5903824\n",
      "################################  50  ################################\n",
      "Training Loss:  0.5903824\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5903824\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5903824\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5903824\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5903824\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5903824\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5903824\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5903824\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5903824\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5903824\n",
      "Final training Loss:  0.5903824\n",
      "\n",
      "Running model (trial=2, mod=59, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70578903\n",
      "################################  5  ################################\n",
      "Training Loss:  0.62466592\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  27.38424873\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59118557\n",
      "################################  20  ################################\n",
      "Training Loss:  45.37215424\n",
      "################################  25  ################################\n",
      "Training Loss:  1.96218574\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  1.96204412\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  1.96203673\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  1.96203673\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.96203673\n",
      "################################  50  ################################\n",
      "Training Loss:  1.96203673\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  1.96203673\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.96203673\n",
      "################################  65  ################################\n",
      "Training Loss:  1.96203673\n",
      "################################  70  ################################\n",
      "Training Loss:  1.96203673\n",
      "################################  75  ################################\n",
      "Training Loss:  1.96203673\n",
      "################################  80  ################################\n",
      "Training Loss:  1.96203673\n",
      "################################  85  ################################\n",
      "Training Loss:  1.96203673\n",
      "################################  90  ################################\n",
      "Training Loss:  1.96203673\n",
      "################################  95  ################################\n",
      "Training Loss:  1.96203673\n",
      "Final training Loss:  1.96203673\n",
      "\n",
      "Running model (trial=2, mod=60, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  43.18785858\n",
      "################################  5  ################################\n",
      "Training Loss:  45.96843338\n",
      "################################  10  ################################\n",
      "Training Loss:  8.80658627\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  8.29528713\n",
      "################################  20  ################################\n",
      "Training Loss:  14.76231194\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  6.11610079\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59009957\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59022689\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59022695\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59022695\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59022695\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59022695\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59022695\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59022695\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59022695\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59022695\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59022695\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59022695\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59022695\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59022695\n",
      "Final training Loss:  0.59022695\n",
      "\n",
      "Running model (trial=2, mod=61, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.20806789\n",
      "################################  5  ################################\n",
      "Training Loss:  55.89395523\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59077638\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59386784\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59790903\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59058821\n",
      "################################  30  ################################\n",
      "Training Loss:  5.58537674\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.68163419\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59063178\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59063178\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59063178\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59063178\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59063178\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59063178\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59063178\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59063178\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59063178\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59063178\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59063178\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59063178\n",
      "Final training Loss:  0.59063178\n",
      "\n",
      "Running model (trial=2, mod=62, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60469496\n",
      "################################  5  ################################\n",
      "Training Loss:  6.78085375\n",
      "################################  10  ################################\n",
      "Training Loss:  11.44042873\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59232134\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  21.84086227\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58942896\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5909071\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.60233992\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.60233861\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60233861\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.60233861\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.60233861\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60233861\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60233861\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60233861\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60233861\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60233861\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60233861\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60233861\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60233861\n",
      "Final training Loss:  0.60233861\n",
      "\n",
      "Running model (trial=2, mod=63, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.80621338\n",
      "################################  5  ################################\n",
      "Training Loss:  5.67079353\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59129739\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59047657\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.79227126\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58919239\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59050041\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58916944\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5899449\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58920193\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58920193\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58920193\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58920193\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58920193\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58920193\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58920193\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58920193\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58920193\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58920193\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58920193\n",
      "Final training Loss:  0.58920193\n",
      "\n",
      "Running model (trial=2, mod=64, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  5  ################################\n",
      "Training Loss:  501.00006104\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  501.00006104\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  501.00006104\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  501.00006104\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  501.00006104\n",
      "################################  35  ################################\n",
      "Training Loss:  501.00006104\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  501.00006104\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  501.00006104\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  55  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  60  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  65  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  70  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  75  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  80  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  85  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  90  ################################\n",
      "Training Loss:  501.00006104\n",
      "################################  95  ################################\n",
      "Training Loss:  501.00006104\n",
      "Final training Loss:  501.00006104\n",
      "\n",
      "Running model (trial=2, mod=65, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6010257\n",
      "################################  5  ################################\n",
      "Training Loss:  4.63118696\n",
      "################################  10  ################################\n",
      "Training Loss:  4.02375603\n",
      "################################  15  ################################\n",
      "Training Loss:  5.99705362\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  4.25495434\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  65.28582764\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59059978\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5897792\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58977914\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5897792\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.5897792\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.5897792\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5897792\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5897792\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5897792\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5897792\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5897792\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5897792\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5897792\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5897792\n",
      "Final training Loss:  0.5897792\n",
      "\n",
      "Running model (trial=2, mod=66, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  107.92926025\n",
      "################################  5  ################################\n",
      "Training Loss:  0.5936271\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59026277\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58970624\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58887869\n",
      "################################  25  ################################\n",
      "Training Loss:  5.31917191\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  17.41506958\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59000349\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59000331\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59000337\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59000337\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59000337\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59000337\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59000337\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59000337\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59000337\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59000337\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59000337\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59000337\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59000337\n",
      "Final training Loss:  0.59000337\n",
      "\n",
      "Running model (trial=2, mod=67, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.55367947\n",
      "################################  5  ################################\n",
      "Training Loss:  0.5911926\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59113109\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  6.58133125\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  27.30178642\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59230435\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59024632\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59191656\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59191644\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59191644\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59191644\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59191644\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59191644\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59191644\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59191644\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59191644\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59191644\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59191644\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59191644\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59191644\n",
      "Final training Loss:  0.59191644\n",
      "\n",
      "Running model (trial=2, mod=68, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  11.39368534\n",
      "################################  5  ################################\n",
      "Training Loss:  39.35881805\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60278445\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  15.03725338\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=69, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60375059\n",
      "################################  5  ################################\n",
      "Training Loss:  30.71753693\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59422183\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59204137\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59066731\n",
      "################################  25  ################################\n",
      "Training Loss:  48.22623444\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59041113\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59688199\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  9.65865231\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59051591\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59051418\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59051418\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59051418\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59051418\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59051418\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59051418\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59051418\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59051418\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59051418\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59051418\n",
      "Final training Loss:  0.59051418\n",
      "\n",
      "Running model (trial=2, mod=70, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69717795\n",
      "################################  5  ################################\n",
      "Training Loss:  0.63838661\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.63838661\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.63574696\n",
      "################################  20  ################################\n",
      "Training Loss:  0.62581313\n",
      "################################  25  ################################\n",
      "Training Loss:  0.62628967\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.62628967\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.62628967\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.62628967\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.62628967\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.62628967\n",
      "################################  55  ################################\n",
      "Training Loss:  0.62628967\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.62628967\n",
      "################################  65  ################################\n",
      "Training Loss:  0.62628967\n",
      "################################  70  ################################\n",
      "Training Loss:  0.62628967\n",
      "################################  75  ################################\n",
      "Training Loss:  0.62628967\n",
      "################################  80  ################################\n",
      "Training Loss:  0.62628967\n",
      "################################  85  ################################\n",
      "Training Loss:  0.62628967\n",
      "################################  90  ################################\n",
      "Training Loss:  0.62628967\n",
      "################################  95  ################################\n",
      "Training Loss:  0.62628967\n",
      "Final training Loss:  0.62628967\n",
      "\n",
      "Running model (trial=2, mod=71, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  18.15269279\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61441529\n",
      "################################  10  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=72, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59520096\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59407091\n",
      "################################  10  ################################\n",
      "Training Loss:  11.80025387\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58952534\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.58913332\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58969581\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59265661\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59265661\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59265661\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59265661\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59265661\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59265661\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59265661\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59265661\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59265661\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59265661\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59265661\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59265661\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59265661\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59265661\n",
      "Final training Loss:  0.59265661\n",
      "\n",
      "Running model (trial=2, mod=73, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62363482\n",
      "################################  5  ################################\n",
      "Training Loss:  5.30146313\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  29.39558792\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  19.83662415\n",
      "################################  20  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=74, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62039477\n",
      "################################  5  ################################\n",
      "Training Loss:  0.66018575\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  7.39520979\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59092689\n",
      "################################  20  ################################\n",
      "Training Loss:  7.93450975\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59609324\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59609324\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59609324\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59609324\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59609324\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59609324\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59609324\n",
      "Final training Loss:  0.59609324\n",
      "\n",
      "Running model (trial=2, mod=75, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62693948\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60176235\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60168618\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59786868\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59724653\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.60322762\n",
      "################################  30  ################################\n",
      "Training Loss:  0.78292847\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59105432\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59351391\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59379041\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59379023\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59379017\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59379029\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59379029\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59379029\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59379029\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59379029\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59379029\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59379029\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59379029\n",
      "Final training Loss:  0.59379029\n",
      "\n",
      "Running model (trial=2, mod=76, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66559458\n",
      "################################  5  ################################\n",
      "Training Loss:  7.70051527\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59184164\n",
      "################################  15  ################################\n",
      "Training Loss:  0.5923692\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59231216\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59278655\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59278655\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59278655\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59278655\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59278655\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59278655\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59278655\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59278655\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59278655\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59278655\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59278655\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59278655\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59278655\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59278655\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59278655\n",
      "Final training Loss:  0.59278655\n",
      "\n",
      "Running model (trial=2, mod=77, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64460611\n",
      "################################  5  ################################\n",
      "Training Loss:  0.6446054\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  8.17959404\n",
      "################################  15  ################################\n",
      "Training Loss:  0.61686724\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59413701\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.90202153\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59048969\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5903644\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5903644\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5903644\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.5903644\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5903644\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5903644\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5903644\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5903644\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5903644\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5903644\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5903644\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5903644\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5903644\n",
      "Final training Loss:  0.5903644\n",
      "\n",
      "Running model (trial=3, mod=78, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5996393\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59674686\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.5937717\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59403718\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59468544\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59381032\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59381032\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59381032\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59381032\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59381032\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59381032\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59381032\n",
      "Final training Loss:  0.59381032\n",
      "\n",
      "Running model (trial=3, mod=79, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59189475\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59014249\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59606701\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  8.61781788\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59178835\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  8.98317719\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5908314\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59082437\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59082437\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59082437\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59082437\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59082437\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59082437\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59082437\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59082437\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59082437\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59082437\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59082437\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59082437\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59082437\n",
      "Final training Loss:  0.59082437\n",
      "\n",
      "Running model (trial=3, mod=80, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.06120777\n",
      "################################  5  ################################\n",
      "Training Loss:  17.77692413\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  5.46332741\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58962685\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59007198\n",
      "################################  25  ################################\n",
      "Training Loss:  8.0170002\n",
      "################################  30  ################################\n",
      "Training Loss:  8.13233948\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58912891\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  4.6421032\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59042126\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.5904212\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5904212\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5904212\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5904212\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5904212\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5904212\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5904212\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5904212\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5904212\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5904212\n",
      "Final training Loss:  0.5904212\n",
      "\n",
      "Running model (trial=3, mod=81, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  31.04522133\n",
      "################################  5  ################################\n",
      "Training Loss:  7.90747595\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59013319\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60662484\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  58.03016281\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59114063\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59102976\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59102976\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59102976\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59102976\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59102976\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59102976\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59102976\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59102976\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59102976\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59102976\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59102976\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59102976\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59102976\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59102976\n",
      "Final training Loss:  0.59102976\n",
      "\n",
      "Running model (trial=3, mod=82, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  70.84642029\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59695399\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59921074\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58997709\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.58992946\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59210241\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59137398\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59137398\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59137398\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59137398\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59137398\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59137398\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59137398\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59137398\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59137398\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59137398\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59137398\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59137398\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59137398\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59137398\n",
      "Final training Loss:  0.59137398\n",
      "\n",
      "Running model (trial=3, mod=83, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12.66001511\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59459662\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  38.27555466\n",
      "################################  15  ################################\n",
      "Training Loss:  42.00056839\n",
      "################################  20  ################################\n",
      "Training Loss:  12.6497221\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.34114265\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58946544\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59054053\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59056604\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59056598\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59056598\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59056598\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59056598\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59056598\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59056598\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59056598\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59056598\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59056598\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59056598\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59056598\n",
      "Final training Loss:  0.59056598\n",
      "\n",
      "Running model (trial=3, mod=84, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67122507\n",
      "################################  5  ################################\n",
      "Training Loss:  40.88319397\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59537774\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  3.31341434\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59022141\n",
      "################################  25  ################################\n",
      "Training Loss:  16.55309868\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59053308\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59020573\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59016371\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59016371\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59016371\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59016371\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59016371\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59016371\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59016371\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59016371\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59016371\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59016371\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59016371\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59016371\n",
      "Final training Loss:  0.59016371\n",
      "\n",
      "Running model (trial=3, mod=85, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63430345\n",
      "################################  5  ################################\n",
      "Training Loss:  0.65993363\n",
      "################################  10  ################################\n",
      "Training Loss:  0.72121114\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58919889\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59213978\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  7.60276079\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=86, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60096455\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59042066\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59122074\n",
      "################################  15  ################################\n",
      "Training Loss:  4.02644396\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59255987\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59075928\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58920568\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58920813\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58919352\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58919352\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58919352\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58919352\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58919352\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58919352\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58919352\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58919352\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58919352\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58919352\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58919352\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58919352\n",
      "Final training Loss:  0.58919352\n",
      "\n",
      "Running model (trial=3, mod=87, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60274965\n",
      "################################  5  ################################\n",
      "Training Loss:  0.5909887\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  23.41229248\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59233963\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58931553\n",
      "################################  25  ################################\n",
      "Training Loss:  18.75285149\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59148097\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59148097\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59148097\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59148097\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59148097\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59148097\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59148097\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59148097\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59148097\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59148097\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59148097\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59148097\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59148097\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59148097\n",
      "Final training Loss:  0.59148097\n",
      "\n",
      "Running model (trial=3, mod=88, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59924561\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59029776\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58930635\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  26.91585922\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58980274\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  8.38300705\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59938288\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59938025\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5993802\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5993802\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.5993802\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.5993802\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5993802\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5993802\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5993802\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5993802\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5993802\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5993802\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5993802\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5993802\n",
      "Final training Loss:  0.5993802\n",
      "\n",
      "Running model (trial=3, mod=89, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60484499\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59552509\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59526557\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59388399\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59583366\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  31.23516846\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5951578\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59137338\n",
      "################################  40  ################################\n",
      "Training Loss:  8.18254662\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59184092\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59095705\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59095675\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59095669\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59095669\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59095669\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59095669\n",
      "Final training Loss:  0.59095669\n",
      "\n",
      "Running model (trial=3, mod=90, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  5  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  500.99996948\n",
      "################################  35  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  55  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  60  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  65  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  70  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  75  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  80  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  85  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  90  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  95  ################################\n",
      "Training Loss:  500.99996948\n",
      "Final training Loss:  500.99996948\n",
      "\n",
      "Running model (trial=3, mod=91, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60102761\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59640092\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59342742\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  40.19535828\n",
      "################################  20  ################################\n",
      "Training Loss:  0.5959596\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59288102\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59158206\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59279865\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59676683\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59193045\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59193045\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59193045\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59193045\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59193045\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59193045\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59193045\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59193045\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59193045\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59193045\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59193045\n",
      "Final training Loss:  0.59193045\n",
      "\n",
      "Running model (trial=3, mod=92, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.80052805\n",
      "################################  5  ################################\n",
      "Training Loss:  2.73302555\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  25.38751602\n",
      "################################  15  ################################\n",
      "Training Loss:  5.44745636\n",
      "################################  20  ################################\n",
      "Training Loss:  48.97855759\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  52.95106888\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=93, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66507024\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59838754\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  34.28677368\n",
      "################################  15  ################################\n",
      "Training Loss:  9.21231079\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  133.39825439\n",
      "################################  25  ################################\n",
      "Training Loss:  17.45874405\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  10.2169466\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59036452\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59036452\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59036452\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59036452\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59036452\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59036452\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59036452\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59036452\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59036452\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59036452\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59036452\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59036452\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59036452\n",
      "Final training Loss:  0.59036452\n",
      "\n",
      "Running model (trial=3, mod=94, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60984528\n",
      "################################  5  ################################\n",
      "Training Loss:  168.74172974\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  88.86896515\n",
      "################################  15  ################################\n",
      "Training Loss:  5.34319019\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  2.3932786\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.39303803\n",
      "################################  30  ################################\n",
      "Training Loss:  2.39320159\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  2.39320159\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  2.39320159\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  2.39320159\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  2.39320159\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  2.39320159\n",
      "################################  60  ################################\n",
      "Training Loss:  2.39320159\n",
      "################################  65  ################################\n",
      "Training Loss:  2.39320159\n",
      "################################  70  ################################\n",
      "Training Loss:  2.39320159\n",
      "################################  75  ################################\n",
      "Training Loss:  2.39320159\n",
      "################################  80  ################################\n",
      "Training Loss:  2.39320159\n",
      "################################  85  ################################\n",
      "Training Loss:  2.39320159\n",
      "################################  90  ################################\n",
      "Training Loss:  2.39320159\n",
      "################################  95  ################################\n",
      "Training Loss:  2.39320159\n",
      "Final training Loss:  2.39320159\n",
      "\n",
      "Running model (trial=3, mod=95, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60983467\n",
      "################################  5  ################################\n",
      "Training Loss:  65.93004608\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  80.32550812\n",
      "################################  15  ################################\n",
      "Training Loss:  13.1688776\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  5.03447247\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59088278\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59084564\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59058452\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59058446\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59058446\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59058446\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59058446\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59058446\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59058446\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59058446\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59058446\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59058446\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59058446\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59058446\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59058446\n",
      "Final training Loss:  0.59058446\n",
      "\n",
      "Running model (trial=3, mod=96, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72889572\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61769903\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.61769903\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60554367\n",
      "################################  20  ################################\n",
      "Training Loss:  31.77388\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.60159355\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60159355\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.60159355\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60159355\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.60159355\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60159355\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60159355\n",
      "Final training Loss:  0.60159355\n",
      "\n",
      "Running model (trial=3, mod=97, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6607216\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61032373\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60746807\n",
      "################################  15  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=98, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60604066\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59178764\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58940393\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59560859\n",
      "################################  20  ################################\n",
      "Training Loss:  7.10586739\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  8.18747425\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=99, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  62.00963974\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60181141\n",
      "################################  10  ################################\n",
      "Training Loss:  14.51480579\n",
      "################################  15  ################################\n",
      "Training Loss:  12.71443367\n",
      "################################  20  ################################\n",
      "Training Loss:  0.60111302\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58929378\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58990723\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58990723\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  13.53212357\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=100, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  27.26548195\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59258968\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59943563\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  7.60875273\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  9.7144022\n",
      "################################  25  ################################\n",
      "Training Loss:  9.59949589\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  5.81689167\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  1.76110148\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  1.76126528\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  1.7612803\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.7612803\n",
      "################################  55  ################################\n",
      "Training Loss:  1.7612803\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.7612803\n",
      "################################  65  ################################\n",
      "Training Loss:  1.7612803\n",
      "################################  70  ################################\n",
      "Training Loss:  1.7612803\n",
      "################################  75  ################################\n",
      "Training Loss:  1.7612803\n",
      "################################  80  ################################\n",
      "Training Loss:  1.7612803\n",
      "################################  85  ################################\n",
      "Training Loss:  1.7612803\n",
      "################################  90  ################################\n",
      "Training Loss:  1.7612803\n",
      "################################  95  ################################\n",
      "Training Loss:  1.7612803\n",
      "Final training Loss:  1.7612803\n",
      "\n",
      "Running model (trial=3, mod=101, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63827264\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60440725\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60440075\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59455973\n",
      "################################  20  ################################\n",
      "Training Loss:  1.62548029\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=102, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6826154\n",
      "################################  5  ################################\n",
      "Training Loss:  272.90097046\n",
      "################################  10  ################################\n",
      "Training Loss:  18.43801689\n",
      "################################  15  ################################\n",
      "Training Loss:  44.54454803\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  4.86790657\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  7.82881832\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59255511\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.60769951\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59191293\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59144109\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.5914408\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5914408\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5914408\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5914408\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5914408\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5914408\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5914408\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5914408\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5914408\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5914408\n",
      "Final training Loss:  0.5914408\n",
      "\n",
      "Running model (trial=3, mod=103, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  37.6815033\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60497463\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  40.4003067\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58963513\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58792138\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58798361\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58801204\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5880121\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5880121\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5880121\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.5880121\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5880121\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5880121\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5880121\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5880121\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5880121\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5880121\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5880121\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5880121\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5880121\n",
      "Final training Loss:  0.5880121\n",
      "\n",
      "Running model (trial=4, mod=104, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60615659\n",
      "################################  5  ################################\n",
      "Training Loss:  3.06384611\n",
      "################################  10  ################################\n",
      "Training Loss:  1.994591\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58948189\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  56.05512238\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  5.71738052\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59001952\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59047651\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59047651\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59047651\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59047651\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59047651\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59047651\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59047651\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59047651\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59047651\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59047651\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59047651\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59047651\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59047651\n",
      "Final training Loss:  0.59047651\n",
      "\n",
      "Running model (trial=4, mod=105, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  64.05865479\n",
      "################################  5  ################################\n",
      "Training Loss:  4.50486612\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  24.1413002\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59204936\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59030372\n",
      "################################  25  ################################\n",
      "Training Loss:  6.1009202\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  8.74196529\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.62039697\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59177142\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5917713\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.5917713\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5917713\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5917713\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.5917713\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5917713\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5917713\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5917713\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5917713\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5917713\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5917713\n",
      "Final training Loss:  0.5917713\n",
      "\n",
      "Running model (trial=4, mod=106, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66221404\n",
      "################################  5  ################################\n",
      "Training Loss:  0.58944607\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59068632\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59296167\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  1.15205264\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  1.08907974\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  1.08915401\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.08915401\n",
      "################################  45  ################################\n",
      "Training Loss:  1.08915401\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  1.08915401\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  60  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  65  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  70  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  75  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  80  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  85  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  90  ################################\n",
      "Training Loss:  1.08915401\n",
      "################################  95  ################################\n",
      "Training Loss:  1.08915401\n",
      "Final training Loss:  1.08915401\n",
      "\n",
      "Running model (trial=4, mod=107, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  33.54057312\n",
      "################################  5  ################################\n",
      "Training Loss:  25.73132324\n",
      "################################  10  ################################\n",
      "Training Loss:  4.61719942\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.60320836\n",
      "################################  20  ################################\n",
      "Training Loss:  8.38557529\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  24.01133919\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59060073\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  8.4543314\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  8.4540329\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  8.45407772\n",
      "################################  50  ################################\n",
      "Training Loss:  8.45407772\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  8.45407772\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.45407772\n",
      "################################  65  ################################\n",
      "Training Loss:  8.45407772\n",
      "################################  70  ################################\n",
      "Training Loss:  8.45407772\n",
      "################################  75  ################################\n",
      "Training Loss:  8.45407772\n",
      "################################  80  ################################\n",
      "Training Loss:  8.45407772\n",
      "################################  85  ################################\n",
      "Training Loss:  8.45407772\n",
      "################################  90  ################################\n",
      "Training Loss:  8.45407772\n",
      "################################  95  ################################\n",
      "Training Loss:  8.45407772\n",
      "Final training Loss:  8.45407772\n",
      "\n",
      "Running model (trial=4, mod=108, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.27750683\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59062171\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  179.00498962\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  106.98156738\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59525365\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59479851\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59479851\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59479851\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59479851\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59479851\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59479851\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59479851\n",
      "Final training Loss:  0.59479851\n",
      "\n",
      "Running model (trial=4, mod=109, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5936271\n",
      "################################  5  ################################\n",
      "Training Loss:  0.62168634\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  30.94786644\n",
      "################################  15  ################################\n",
      "Training Loss:  10.08684349\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59027863\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  8.59766483\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59209317\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59209311\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59209311\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59209311\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59209311\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59209311\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59209311\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59209311\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59209311\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59209311\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59209311\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59209311\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59209311\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59209311\n",
      "Final training Loss:  0.59209311\n",
      "\n",
      "Running model (trial=4, mod=110, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6949566\n",
      "################################  5  ################################\n",
      "Training Loss:  0.6119898\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  60.22910309\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59120595\n",
      "################################  20  ################################\n",
      "Training Loss:  11.93114758\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59084952\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59189528\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  5.61730576\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  5.61713409\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  5.61713409\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  5.61713409\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  5.61713409\n",
      "################################  60  ################################\n",
      "Training Loss:  5.61713409\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  5.61713409\n",
      "################################  70  ################################\n",
      "Training Loss:  5.61713409\n",
      "################################  75  ################################\n",
      "Training Loss:  5.61713409\n",
      "################################  80  ################################\n",
      "Training Loss:  5.61713409\n",
      "################################  85  ################################\n",
      "Training Loss:  5.61713409\n",
      "################################  90  ################################\n",
      "Training Loss:  5.61713409\n",
      "################################  95  ################################\n",
      "Training Loss:  5.61713409\n",
      "Final training Loss:  5.61713409\n",
      "\n",
      "Running model (trial=4, mod=111, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.74551296\n",
      "################################  5  ################################\n",
      "Training Loss:  6.76127863\n",
      "################################  10  ################################\n",
      "Training Loss:  18.03269196\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=112, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59279966\n",
      "################################  5  ################################\n",
      "Training Loss:  3.71605039\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58977872\n",
      "################################  15  ################################\n",
      "Training Loss:  10.63647366\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59265029\n",
      "################################  25  ################################\n",
      "Training Loss:  6.02962971\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  3.48097372\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5899722\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59041202\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59041202\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59041202\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59041202\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59041202\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59041202\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59041202\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59041202\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59041202\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59041202\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59041202\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59041202\n",
      "Final training Loss:  0.59041202\n",
      "\n",
      "Running model (trial=4, mod=113, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59851032\n",
      "################################  5  ################################\n",
      "Training Loss:  33.54199982\n",
      "################################  10  ################################\n",
      "Training Loss:  86.17554474\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59210068\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59402299\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59073412\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  1.19753766\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  35.98674011\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59286338\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59286338\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59286338\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59286338\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59286338\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59286338\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59286338\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59286338\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59286338\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59286338\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59286338\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59286338\n",
      "Final training Loss:  0.59286338\n",
      "\n",
      "Running model (trial=4, mod=114, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.44056451\n",
      "################################  5  ################################\n",
      "Training Loss:  0.58957511\n",
      "################################  10  ################################\n",
      "Training Loss:  5.93897438\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  1.61958241\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  8.13003922\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58945048\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59075886\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59075892\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59075892\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59075892\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59075892\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59075892\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59075892\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59075892\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59075892\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59075892\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59075892\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59075892\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59075892\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59075892\n",
      "Final training Loss:  0.59075892\n",
      "\n",
      "Running model (trial=4, mod=115, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60572284\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60128468\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59156042\n",
      "################################  15  ################################\n",
      "Training Loss:  4.58649158\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59110135\n",
      "################################  25  ################################\n",
      "Training Loss:  2.56802845\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60471237\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5895617\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59328574\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59250093\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59244961\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59244967\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59244967\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59244967\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59244967\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59244967\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59244967\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59244967\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59244967\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59244967\n",
      "Final training Loss:  0.59244967\n",
      "\n",
      "Running model (trial=4, mod=116, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  5  ################################\n",
      "Training Loss:  501.00003052\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  501.00003052\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  501.00003052\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  501.00003052\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  501.00003052\n",
      "################################  35  ################################\n",
      "Training Loss:  501.00003052\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  501.00003052\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  501.00003052\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  55  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  60  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  65  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  70  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  75  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  80  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  85  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  90  ################################\n",
      "Training Loss:  501.00003052\n",
      "################################  95  ################################\n",
      "Training Loss:  501.00003052\n",
      "Final training Loss:  501.00003052\n",
      "\n",
      "Running model (trial=4, mod=117, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6166085\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59679067\n",
      "################################  10  ################################\n",
      "Training Loss:  0.5965001\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59811044\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58950371\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59058022\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59089148\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59098089\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59098089\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59098089\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59098089\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59098089\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59098089\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59098089\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59098089\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59098089\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59098089\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59098089\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59098089\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59098089\n",
      "Final training Loss:  0.59098089\n",
      "\n",
      "Running model (trial=4, mod=118, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60064167\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59371054\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59160978\n",
      "################################  15  ################################\n",
      "Training Loss:  6.83425426\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59118754\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58977669\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.97582185\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  3.00774097\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  1.11984682\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  1.11967075\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.11967075\n",
      "################################  55  ################################\n",
      "Training Loss:  1.11967075\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.11967075\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  1.11967075\n",
      "################################  70  ################################\n",
      "Training Loss:  1.11967075\n",
      "################################  75  ################################\n",
      "Training Loss:  1.11967075\n",
      "################################  80  ################################\n",
      "Training Loss:  1.11967075\n",
      "################################  85  ################################\n",
      "Training Loss:  1.11967075\n",
      "################################  90  ################################\n",
      "Training Loss:  1.11967075\n",
      "################################  95  ################################\n",
      "Training Loss:  1.11967075\n",
      "Final training Loss:  1.11967075\n",
      "\n",
      "Running model (trial=4, mod=119, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  15.21137047\n",
      "################################  5  ################################\n",
      "Training Loss:  0.58976775\n",
      "################################  10  ################################\n",
      "Training Loss:  5.75121355\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59072113\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  2.12594795\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59230983\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59227222\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59227222\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59227222\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59227222\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59227222\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59227222\n",
      "Final training Loss:  0.59227222\n",
      "\n",
      "Running model (trial=4, mod=120, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62164235\n",
      "################################  5  ################################\n",
      "Training Loss:  4.07487059\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58949959\n",
      "################################  15  ################################\n",
      "Training Loss:  16.75848389\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59280676\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  6.02601147\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58911043\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.58911043\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58911043\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58911043\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58911043\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58911043\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58911043\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58911043\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58911043\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58911043\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58911043\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58911043\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58911043\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58911043\n",
      "Final training Loss:  0.58911043\n",
      "\n",
      "Running model (trial=4, mod=121, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68391323\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61900413\n",
      "################################  10  ################################\n",
      "Training Loss:  nan\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=122, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10.66486168\n",
      "################################  5  ################################\n",
      "Training Loss:  0.62089294\n",
      "################################  10  ################################\n",
      "Training Loss:  0.67098975\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.61189461\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  39.32104111\n",
      "################################  25  ################################\n",
      "Training Loss:  18.07380295\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  18.0737896\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  18.07373238\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  18.07384872\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  18.07384872\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  18.07384872\n",
      "################################  55  ################################\n",
      "Training Loss:  18.07384872\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  18.07384872\n",
      "################################  65  ################################\n",
      "Training Loss:  18.07384872\n",
      "################################  70  ################################\n",
      "Training Loss:  18.07384872\n",
      "################################  75  ################################\n",
      "Training Loss:  18.07384872\n",
      "################################  80  ################################\n",
      "Training Loss:  18.07384872\n",
      "################################  85  ################################\n",
      "Training Loss:  18.07384872\n",
      "################################  90  ################################\n",
      "Training Loss:  18.07384872\n",
      "################################  95  ################################\n",
      "Training Loss:  18.07384872\n",
      "Final training Loss:  18.07384872\n",
      "\n",
      "Running model (trial=4, mod=123, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65291041\n",
      "################################  5  ################################\n",
      "Training Loss:  10.08848095\n",
      "################################  10  ################################\n",
      "Training Loss:  12.87218285\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59121829\n",
      "################################  20  ################################\n",
      "Training Loss:  11.0675869\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.62477124\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  16.63738823\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59084463\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59097171\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59096456\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59096456\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59096456\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59096456\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59096456\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59096456\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59096456\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59096456\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59096456\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59096456\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59096456\n",
      "Final training Loss:  0.59096456\n",
      "\n",
      "Running model (trial=4, mod=124, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71158844\n",
      "################################  5  ################################\n",
      "Training Loss:  85.5276947\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.60242951\n",
      "################################  15  ################################\n",
      "Training Loss:  21.20254517\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  12.31084156\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  9.69367695\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  9.69367695\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  9.69367695\n",
      "################################  45  ################################\n",
      "Training Loss:  9.69367695\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  9.69367695\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  60  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  65  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  70  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  75  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  80  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  85  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  90  ################################\n",
      "Training Loss:  9.69367695\n",
      "################################  95  ################################\n",
      "Training Loss:  9.69367695\n",
      "Final training Loss:  9.69367695\n",
      "\n",
      "Running model (trial=4, mod=125, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61950487\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60097533\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.60281569\n",
      "################################  15  ################################\n",
      "Training Loss:  71.87238312\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  23.10986519\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=126, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67110574\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=127, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59664124\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59022546\n",
      "################################  10  ################################\n",
      "Training Loss:  10.32774258\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  3.1718154\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  37.8833313\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  5.68931723\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59242034\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59750044\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59750044\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59750044\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59750044\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59750044\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59750044\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59750044\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59750044\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59750044\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59750044\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59750044\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59750044\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59750044\n",
      "Final training Loss:  0.59750044\n",
      "\n",
      "Running model (trial=4, mod=128, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61698091\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61697882\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.61697882\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59165263\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59165251\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59165257\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59165257\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59165257\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59165257\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59165257\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59165257\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59165257\n",
      "Final training Loss:  0.59165257\n",
      "\n",
      "Running model (trial=4, mod=129, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.78156269\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59236073\n",
      "################################  10  ################################\n",
      "Training Loss:  0.91896296\n",
      "################################  15  ################################\n",
      "Training Loss:  5.24846458\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  39.07077408\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  21.60227966\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59161866\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59175766\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59175748\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59175742\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59175748\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59175742\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59175742\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59175742\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59175742\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59175742\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59175742\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59175742\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59175742\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59175742\n",
      "Final training Loss:  0.59175742\n",
      "\n",
      "Running model (trial=5, mod=130, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60180557\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59809667\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59324533\n",
      "################################  15  ################################\n",
      "Training Loss:  0.5907256\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  3.44390321\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1.46686363\n",
      "################################  30  ################################\n",
      "Training Loss:  5.1284585\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  6.15845442\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  6.33260107\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  6.29807472\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  6.29856586\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  6.2984848\n",
      "################################  60  ################################\n",
      "Training Loss:  6.2984848\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  6.2984848\n",
      "################################  70  ################################\n",
      "Training Loss:  6.2984848\n",
      "################################  75  ################################\n",
      "Training Loss:  6.2984848\n",
      "################################  80  ################################\n",
      "Training Loss:  6.2984848\n",
      "################################  85  ################################\n",
      "Training Loss:  6.2984848\n",
      "################################  90  ################################\n",
      "Training Loss:  6.2984848\n",
      "################################  95  ################################\n",
      "Training Loss:  6.2984848\n",
      "Final training Loss:  6.2984848\n",
      "\n",
      "Running model (trial=5, mod=131, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59126592\n",
      "################################  5  ################################\n",
      "Training Loss:  0.5923053\n",
      "################################  10  ################################\n",
      "Training Loss:  26.15487099\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  16.95435143\n",
      "################################  20  ################################\n",
      "Training Loss:  0.5904991\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  8.10966778\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59086484\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5908609\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5908609\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.5908609\n",
      "################################  50  ################################\n",
      "Training Loss:  0.5908609\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5908609\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5908609\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5908609\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5908609\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5908609\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5908609\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5908609\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5908609\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5908609\n",
      "Final training Loss:  0.5908609\n",
      "\n",
      "Running model (trial=5, mod=132, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59296858\n",
      "################################  5  ################################\n",
      "Training Loss:  0.5959543\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  6.85843372\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59020823\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59023994\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  10.34941292\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  11.16828632\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  11.16830063\n",
      "################################  40  ################################\n",
      "Training Loss:  11.16830063\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  11.16830063\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  11.16830063\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  11.16830063\n",
      "################################  60  ################################\n",
      "Training Loss:  11.16830063\n",
      "################################  65  ################################\n",
      "Training Loss:  11.16830063\n",
      "################################  70  ################################\n",
      "Training Loss:  11.16830063\n",
      "################################  75  ################################\n",
      "Training Loss:  11.16830063\n",
      "################################  80  ################################\n",
      "Training Loss:  11.16830063\n",
      "################################  85  ################################\n",
      "Training Loss:  11.16830063\n",
      "################################  90  ################################\n",
      "Training Loss:  11.16830063\n",
      "################################  95  ################################\n",
      "Training Loss:  11.16830063\n",
      "Final training Loss:  11.16830063\n",
      "\n",
      "Running model (trial=5, mod=133, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  62.34423447\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59042263\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59120065\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59078908\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  7.53176975\n",
      "################################  25  ################################\n",
      "Training Loss:  0.5894053\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  5.34225368\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.58951479\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58951485\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58951485\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58951485\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58951485\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58951485\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58951485\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58951485\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58951485\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58951485\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58951485\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58951485\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58951485\n",
      "Final training Loss:  0.58951485\n",
      "\n",
      "Running model (trial=5, mod=134, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63331002\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59594035\n",
      "################################  10  ################################\n",
      "Training Loss:  11.13579178\n",
      "################################  15  ################################\n",
      "Training Loss:  2.23659372\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59265882\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59055996\n",
      "################################  30  ################################\n",
      "Training Loss:  36.82234192\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59056294\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58957589\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  9.43050575\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  11.43968582\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60035646\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59983146\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59983146\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59983146\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59983146\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59983146\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59983146\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59983146\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59983146\n",
      "Final training Loss:  0.59983146\n",
      "\n",
      "Running model (trial=5, mod=135, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60860968\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59046578\n",
      "################################  10  ################################\n",
      "Training Loss:  3.91319752\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  41.26029587\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58982587\n",
      "################################  25  ################################\n",
      "Training Loss:  0.96090209\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59050018\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5905\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59050006\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59050006\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59050006\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59050006\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59050006\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59050006\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59050006\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59050006\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59050006\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59050006\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59050006\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59050006\n",
      "Final training Loss:  0.59050006\n",
      "\n",
      "Running model (trial=5, mod=136, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  22.2460804\n",
      "################################  5  ################################\n",
      "Training Loss:  4.32463551\n",
      "################################  10  ################################\n",
      "Training Loss:  5.21786404\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  9.52696228\n",
      "################################  20  ################################\n",
      "Training Loss:  20.56698036\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  8.6555109\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59079325\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59107769\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59107757\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59107757\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59107757\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59107757\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59107757\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59107757\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59107757\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59107757\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59107757\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59107757\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59107757\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59107757\n",
      "Final training Loss:  0.59107757\n",
      "\n",
      "Running model (trial=5, mod=137, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59831429\n",
      "################################  5  ################################\n",
      "Training Loss:  19.24427986\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.62093908\n",
      "################################  15  ################################\n",
      "Training Loss:  11.86900902\n",
      "################################  20  ################################\n",
      "Training Loss:  7.69258022\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  14.02234936\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=138, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59120023\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59463131\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  14.35707664\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59059298\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  52.38413239\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59294254\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59291625\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59291625\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59291625\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59291625\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59291625\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59291625\n",
      "Final training Loss:  0.59291625\n",
      "\n",
      "Running model (trial=5, mod=139, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60238981\n",
      "################################  5  ################################\n",
      "Training Loss:  14.46182251\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  21.27782059\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  6.8934207\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59064686\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59405231\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=140, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60290784\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59177154\n",
      "################################  10  ################################\n",
      "Training Loss:  36.16773987\n",
      "################################  15  ################################\n",
      "Training Loss:  6.16589832\n",
      "################################  20  ################################\n",
      "Training Loss:  88.69262695\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59081602\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  5.92135382\n",
      "################################  35  ################################\n",
      "Training Loss:  1.87447131\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59091854\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5909186\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59091842\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59091842\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59091842\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59091842\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59091842\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59091842\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59091842\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59091842\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59091842\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59091842\n",
      "Final training Loss:  0.59091842\n",
      "\n",
      "Running model (trial=5, mod=141, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99087524\n",
      "################################  5  ################################\n",
      "Training Loss:  500.99084473\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  500.99084473\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  500.99084473\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  500.99084473\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  500.99084473\n",
      "################################  35  ################################\n",
      "Training Loss:  500.99084473\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  500.99084473\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  500.99084473\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  55  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  60  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  65  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  70  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  75  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  80  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  85  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  90  ################################\n",
      "Training Loss:  500.99084473\n",
      "################################  95  ################################\n",
      "Training Loss:  500.99084473\n",
      "Final training Loss:  500.99084473\n",
      "\n",
      "Running model (trial=5, mod=142, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  5  ################################\n",
      "Training Loss:  501.00015259\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  501.00015259\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  501.00015259\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  501.00015259\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  501.00015259\n",
      "################################  35  ################################\n",
      "Training Loss:  501.00015259\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  501.00015259\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  501.00015259\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  55  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  60  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  65  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  70  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  75  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  80  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  85  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  90  ################################\n",
      "Training Loss:  501.00015259\n",
      "################################  95  ################################\n",
      "Training Loss:  501.00015259\n",
      "Final training Loss:  501.00015259\n",
      "\n",
      "Running model (trial=5, mod=143, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59572941\n",
      "################################  5  ################################\n",
      "Training Loss:  43.25680542\n",
      "################################  10  ################################\n",
      "Training Loss:  43.0527153\n",
      "################################  15  ################################\n",
      "Training Loss:  11.94879532\n",
      "################################  20  ################################\n",
      "Training Loss:  6.90798521\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59122258\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.58968168\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59011412\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58915287\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58915281\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58915281\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58915281\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58915281\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58915281\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58915281\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58915281\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58915281\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58915281\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58915281\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58915281\n",
      "Final training Loss:  0.58915281\n",
      "\n",
      "Running model (trial=5, mod=144, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66688573\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59629995\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59213859\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59066689\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59038246\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  15.03667259\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  9.68891335\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.69159418\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.69179225\n",
      "################################  45  ################################\n",
      "Training Loss:  0.69140792\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.69140983\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.69140983\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.69140983\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.69140983\n",
      "################################  70  ################################\n",
      "Training Loss:  0.69140983\n",
      "################################  75  ################################\n",
      "Training Loss:  0.69140983\n",
      "################################  80  ################################\n",
      "Training Loss:  0.69140983\n",
      "################################  85  ################################\n",
      "Training Loss:  0.69140983\n",
      "################################  90  ################################\n",
      "Training Loss:  0.69140983\n",
      "################################  95  ################################\n",
      "Training Loss:  0.69140983\n",
      "Final training Loss:  0.69140983\n",
      "\n",
      "Running model (trial=5, mod=145, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63020223\n",
      "################################  5  ################################\n",
      "Training Loss:  9.07973003\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59124315\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59072584\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58965057\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58947605\n",
      "################################  30  ################################\n",
      "Training Loss:  15.4069128\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  105.16570282\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  40.65734863\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58995527\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58990991\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58990985\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58990985\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58990985\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58990985\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58990985\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58990985\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58990985\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58990985\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58990985\n",
      "Final training Loss:  0.58990985\n",
      "\n",
      "Running model (trial=5, mod=146, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62637776\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60074621\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.60289413\n",
      "################################  15  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=147, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60393614\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60081571\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60081506\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  40.2298317\n",
      "################################  20  ################################\n",
      "Training Loss:  11.56272316\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59579813\n",
      "################################  30  ################################\n",
      "Training Loss:  13.1180439\n",
      "################################  35  ################################\n",
      "Training Loss:  35.22748184\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59048289\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59048283\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59048283\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59048283\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59048283\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59048283\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59048283\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59048283\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59048283\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59048283\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59048283\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59048283\n",
      "Final training Loss:  0.59048283\n",
      "\n",
      "Running model (trial=5, mod=148, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65935284\n",
      "################################  5  ################################\n",
      "Training Loss:  33.9294548\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60819894\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60770166\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  142.14805603\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.60831565\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60831571\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.60831571\n",
      "################################  40  ################################\n",
      "Training Loss:  0.60831571\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60831571\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.60831571\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60831571\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60831571\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60831571\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60831571\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60831571\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60831571\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60831571\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60831571\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60831571\n",
      "Final training Loss:  0.60831571\n",
      "\n",
      "Running model (trial=5, mod=149, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  18.7748909\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61756098\n",
      "################################  10  ################################\n",
      "Training Loss:  0.61581439\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  35.87207794\n",
      "################################  20  ################################\n",
      "Training Loss:  40.45272827\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  120.15018463\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60274637\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59131789\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59127063\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59127063\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59127063\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59127063\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59127063\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59127063\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59127063\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59127063\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59127063\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59127063\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59127063\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59127063\n",
      "Final training Loss:  0.59127063\n",
      "\n",
      "Running model (trial=5, mod=150, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  19.77036858\n",
      "################################  5  ################################\n",
      "Training Loss:  40.52082443\n",
      "################################  10  ################################\n",
      "Training Loss:  25.30555725\n",
      "################################  15  ################################\n",
      "Training Loss:  58.69636917\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  8.29310131\n",
      "################################  25  ################################\n",
      "Training Loss:  9.22960854\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  7.05323219\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.86559296\n",
      "################################  40  ################################\n",
      "Training Loss:  0.61486757\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.61486757\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.61486757\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.61486757\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.61486757\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.61486757\n",
      "################################  70  ################################\n",
      "Training Loss:  0.61486757\n",
      "################################  75  ################################\n",
      "Training Loss:  0.61486757\n",
      "################################  80  ################################\n",
      "Training Loss:  0.61486757\n",
      "################################  85  ################################\n",
      "Training Loss:  0.61486757\n",
      "################################  90  ################################\n",
      "Training Loss:  0.61486757\n",
      "################################  95  ################################\n",
      "Training Loss:  0.61486757\n",
      "Final training Loss:  0.61486757\n",
      "\n",
      "Running model (trial=5, mod=151, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64273101\n",
      "################################  5  ################################\n",
      "Training Loss:  0.5914728\n",
      "################################  10  ################################\n",
      "Training Loss:  3.80060911\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  13.18571377\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  13.87408733\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=152, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60198301\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60198271\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60763574\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  25.54427719\n",
      "################################  20  ################################\n",
      "Training Loss:  0.61990124\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  34.71719742\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  34.71640778\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  34.71621323\n",
      "################################  40  ################################\n",
      "Training Loss:  34.71621323\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  34.71621323\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  34.71621323\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  34.71621323\n",
      "################################  60  ################################\n",
      "Training Loss:  34.71621323\n",
      "################################  65  ################################\n",
      "Training Loss:  34.71621323\n",
      "################################  70  ################################\n",
      "Training Loss:  34.71621323\n",
      "################################  75  ################################\n",
      "Training Loss:  34.71621323\n",
      "################################  80  ################################\n",
      "Training Loss:  34.71621323\n",
      "################################  85  ################################\n",
      "Training Loss:  34.71621323\n",
      "################################  90  ################################\n",
      "Training Loss:  34.71621323\n",
      "################################  95  ################################\n",
      "Training Loss:  34.71621323\n",
      "Final training Loss:  34.71621323\n",
      "\n",
      "Running model (trial=5, mod=153, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63998032\n",
      "################################  5  ################################\n",
      "Training Loss:  0.6051622\n",
      "################################  10  ################################\n",
      "Training Loss:  10.00343704\n",
      "################################  15  ################################\n",
      "Training Loss:  53.65739441\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59148145\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=154, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63740599\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60353208\n",
      "################################  10  ################################\n",
      "Training Loss:  0.593054\n",
      "################################  15  ################################\n",
      "Training Loss:  0.5906508\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59084797\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59015089\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59463006\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=155, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64870203\n",
      "################################  5  ################################\n",
      "Training Loss:  0.6517278\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  3.542238\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  7.73869991\n",
      "################################  20  ################################\n",
      "Training Loss:  6.83966589\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58954597\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58989054\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58989054\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58989054\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58989054\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58989054\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58989054\n",
      "Final training Loss:  0.58989054\n",
      "\n",
      "Running model (trial=6, mod=156, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59964126\n",
      "################################  5  ################################\n",
      "Training Loss:  1.53329813\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  76.786026\n",
      "################################  15  ################################\n",
      "Training Loss:  5.24077368\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58943349\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59106636\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.5912773\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59127724\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59127724\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59127724\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59127724\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59127724\n",
      "Final training Loss:  0.59127724\n",
      "\n",
      "Running model (trial=6, mod=157, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  164.2804718\n",
      "################################  5  ################################\n",
      "Training Loss:  145.49440002\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59209126\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.10937214\n",
      "################################  20  ################################\n",
      "Training Loss:  14.52229118\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.14659095\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59035665\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59045142\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59045142\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59045136\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59045136\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59045136\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59045136\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59045136\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59045136\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59045136\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59045136\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59045136\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59045136\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59045136\n",
      "Final training Loss:  0.59045136\n",
      "\n",
      "Running model (trial=6, mod=158, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59202546\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59004354\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  25.10323906\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  12.09197998\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59064877\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58910137\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58965659\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58965635\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58965623\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58965623\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58965623\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.58965623\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58965623\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58965623\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58965623\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58965623\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58965623\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58965623\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58965623\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58965623\n",
      "Final training Loss:  0.58965623\n",
      "\n",
      "Running model (trial=6, mod=159, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.6687808\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59183949\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.61822456\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58951288\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59212476\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  6.86815119\n",
      "################################  30  ################################\n",
      "Training Loss:  4.83264256\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59821832\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59821802\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59821802\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59821802\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59821802\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59821802\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59821802\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59821802\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59821802\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59821802\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59821802\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59821802\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59821802\n",
      "Final training Loss:  0.59821802\n",
      "\n",
      "Running model (trial=6, mod=160, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  16.24141884\n",
      "################################  5  ################################\n",
      "Training Loss:  6.84327316\n",
      "################################  10  ################################\n",
      "Training Loss:  1.06728113\n",
      "################################  15  ################################\n",
      "Training Loss:  0.590599\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59080899\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59997731\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58901143\n",
      "################################  35  ################################\n",
      "Training Loss:  8.31504726\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  6.24269342\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59111929\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59111905\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59111905\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59111911\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59111905\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59111905\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59111905\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59111905\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59111905\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59111905\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59111905\n",
      "Final training Loss:  0.59111905\n",
      "\n",
      "Running model (trial=6, mod=161, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60482734\n",
      "################################  5  ################################\n",
      "Training Loss:  10.46036243\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  101.27375793\n",
      "################################  15  ################################\n",
      "Training Loss:  9.20915699\n",
      "################################  20  ################################\n",
      "Training Loss:  22.56116295\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59031612\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59438628\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  8.76313496\n",
      "################################  40  ################################\n",
      "Training Loss:  8.76310444\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  8.76310444\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  8.76310444\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  8.76310444\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.76310444\n",
      "################################  65  ################################\n",
      "Training Loss:  8.76310444\n",
      "################################  70  ################################\n",
      "Training Loss:  8.76310444\n",
      "################################  75  ################################\n",
      "Training Loss:  8.76310444\n",
      "################################  80  ################################\n",
      "Training Loss:  8.76310444\n",
      "################################  85  ################################\n",
      "Training Loss:  8.76310444\n",
      "################################  90  ################################\n",
      "Training Loss:  8.76310444\n",
      "################################  95  ################################\n",
      "Training Loss:  8.76310444\n",
      "Final training Loss:  8.76310444\n",
      "\n",
      "Running model (trial=6, mod=162, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  115.51096344\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60227036\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59461617\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59283876\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58993566\n",
      "################################  25  ################################\n",
      "Training Loss:  18.708498\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  8.84986877\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  21.36812973\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  12.35251904\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59023422\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59023422\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59023422\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59023422\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59023422\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59023422\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59023422\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59023422\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59023422\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59023422\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59023422\n",
      "Final training Loss:  0.59023422\n",
      "\n",
      "Running model (trial=6, mod=163, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73187131\n",
      "################################  5  ################################\n",
      "Training Loss:  0.58931166\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59054708\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58924234\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58923143\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  72.00673676\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58987713\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58987713\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58987713\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58987713\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58987713\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58987713\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58987713\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58987713\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58987713\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58987713\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58987713\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58987713\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58987713\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58987713\n",
      "Final training Loss:  0.58987713\n",
      "\n",
      "Running model (trial=6, mod=164, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63510424\n",
      "################################  5  ################################\n",
      "Training Loss:  10.49586391\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58948803\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59727156\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  43.20265961\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59027261\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5923363\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59233636\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59233636\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59233636\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59233636\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59233636\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59233636\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59233636\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59233636\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59233636\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59233636\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59233636\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59233636\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59233636\n",
      "Final training Loss:  0.59233636\n",
      "\n",
      "Running model (trial=6, mod=165, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  31.88196564\n",
      "################################  5  ################################\n",
      "Training Loss:  49.08180618\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59338349\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  8.89936924\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59099108\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59008783\n",
      "################################  30  ################################\n",
      "Training Loss:  6.85567808\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  3.73162341\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59352714\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59352702\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59352702\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59352702\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59352702\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59352702\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59352702\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59352702\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59352702\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59352702\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59352702\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59352702\n",
      "Final training Loss:  0.59352702\n",
      "\n",
      "Running model (trial=6, mod=166, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.56807423\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59571093\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59224737\n",
      "################################  15  ################################\n",
      "Training Loss:  8.64422798\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  8.79420757\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  6.5872159\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59163404\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59163409\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59163409\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59163409\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59163409\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59163409\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59163409\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59163409\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59163409\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59163409\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59163409\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59163409\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59163409\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59163409\n",
      "Final training Loss:  0.59163409\n",
      "\n",
      "Running model (trial=6, mod=167, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64074922\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59345478\n",
      "################################  10  ################################\n",
      "Training Loss:  8.72334385\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  10.99399185\n",
      "################################  20  ################################\n",
      "Training Loss:  5.54062271\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59356958\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59141803\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59689385\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59689385\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59689385\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59689385\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59689385\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59689385\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59689385\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59689385\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59689385\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59689385\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59689385\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59689385\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59689385\n",
      "Final training Loss:  0.59689385\n",
      "\n",
      "Running model (trial=6, mod=168, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0\n",
      "################################  5  ################################\n",
      "Training Loss:  501.0\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  501.0\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  501.0\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  501.0\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  501.0\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  501.0\n",
      "################################  35  ################################\n",
      "Training Loss:  501.0\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  501.0\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  501.0\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  501.0\n",
      "################################  55  ################################\n",
      "Training Loss:  501.0\n",
      "################################  60  ################################\n",
      "Training Loss:  501.0\n",
      "################################  65  ################################\n",
      "Training Loss:  501.0\n",
      "################################  70  ################################\n",
      "Training Loss:  501.0\n",
      "################################  75  ################################\n",
      "Training Loss:  501.0\n",
      "################################  80  ################################\n",
      "Training Loss:  501.0\n",
      "################################  85  ################################\n",
      "Training Loss:  501.0\n",
      "################################  90  ################################\n",
      "Training Loss:  501.0\n",
      "################################  95  ################################\n",
      "Training Loss:  501.0\n",
      "Final training Loss:  501.0\n",
      "\n",
      "Running model (trial=6, mod=169, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5940972\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59077936\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59599829\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58969104\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58989352\n",
      "################################  25  ################################\n",
      "Training Loss:  1.47121263\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59354997\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59306997\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59348232\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59348226\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59348226\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59348226\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59348226\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59348226\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59348226\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59348226\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59348226\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59348226\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59348226\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59348226\n",
      "Final training Loss:  0.59348226\n",
      "\n",
      "Running model (trial=6, mod=170, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59632909\n",
      "################################  5  ################################\n",
      "Training Loss:  0.71898043\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59283328\n",
      "################################  15  ################################\n",
      "Training Loss:  10.35026073\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  6.3239665\n",
      "################################  25  ################################\n",
      "Training Loss:  30.47100067\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58930743\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58930743\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58930743\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58930743\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58930743\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58930743\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58930743\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58930743\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58930743\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58930743\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58930743\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58930743\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58930743\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58930743\n",
      "Final training Loss:  0.58930743\n",
      "\n",
      "Running model (trial=6, mod=171, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60512924\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59236842\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59012365\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.5907144\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  31.3386879\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  6.41974163\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.5910731\n",
      "################################  35  ################################\n",
      "Training Loss:  0.6032846\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59188545\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59188545\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59188545\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59188545\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59188545\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59188545\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59188545\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59188545\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59188545\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59188545\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59188545\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59188545\n",
      "Final training Loss:  0.59188545\n",
      "\n",
      "Running model (trial=6, mod=172, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  43.85026932\n",
      "################################  5  ################################\n",
      "Training Loss:  22.12903214\n",
      "################################  10  ################################\n",
      "Training Loss:  50.90673065\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  16.58043289\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59502327\n",
      "################################  25  ################################\n",
      "Training Loss:  4.77200079\n",
      "################################  30  ################################\n",
      "Training Loss:  19.5211544\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  69.36331177\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  5.88792181\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59207916\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59207916\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59207916\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59207916\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59207916\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59207916\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59207916\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59207916\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59207916\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59207916\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59207916\n",
      "Final training Loss:  0.59207916\n",
      "\n",
      "Running model (trial=6, mod=173, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68667603\n",
      "################################  5  ################################\n",
      "Training Loss:  20.35661125\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  11.61267853\n",
      "################################  15  ################################\n",
      "Training Loss:  0.818838\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59181917\n",
      "################################  25  ################################\n",
      "Training Loss:  10.83996582\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59222561\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5955168\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59551585\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59551585\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59551585\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59551585\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59551585\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59551585\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59551585\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59551585\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59551585\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59551585\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59551585\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59551585\n",
      "Final training Loss:  0.59551585\n",
      "\n",
      "Running model (trial=6, mod=174, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62309498\n",
      "################################  5  ################################\n",
      "Training Loss:  82.81544495\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  14.89112091\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60675645\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  62.08409882\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=175, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67226839\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61348575\n",
      "################################  10  ################################\n",
      "Training Loss:  0.62052995\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.61453456\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  17.4586792\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  17.38181496\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  17.38181496\n",
      "################################  40  ################################\n",
      "Training Loss:  17.38181496\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  17.38181496\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  17.38181496\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  60  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  65  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  70  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  75  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  80  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  85  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  90  ################################\n",
      "Training Loss:  17.38181496\n",
      "################################  95  ################################\n",
      "Training Loss:  17.38181496\n",
      "Final training Loss:  17.38181496\n",
      "\n",
      "Running model (trial=6, mod=176, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  22.6432991\n",
      "################################  5  ################################\n",
      "Training Loss:  1.07944524\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58940119\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59281546\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  5.36111641\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59138018\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59219044\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.592179\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.592179\n",
      "################################  45  ################################\n",
      "Training Loss:  0.592179\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.592179\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.592179\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.592179\n",
      "################################  65  ################################\n",
      "Training Loss:  0.592179\n",
      "################################  70  ################################\n",
      "Training Loss:  0.592179\n",
      "################################  75  ################################\n",
      "Training Loss:  0.592179\n",
      "################################  80  ################################\n",
      "Training Loss:  0.592179\n",
      "################################  85  ################################\n",
      "Training Loss:  0.592179\n",
      "################################  90  ################################\n",
      "Training Loss:  0.592179\n",
      "################################  95  ################################\n",
      "Training Loss:  0.592179\n",
      "Final training Loss:  0.592179\n",
      "\n",
      "Running model (trial=6, mod=177, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  15.15440941\n",
      "################################  5  ################################\n",
      "Training Loss:  0.62007838\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=178, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59006363\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=179, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.21687698\n",
      "################################  5  ################################\n",
      "Training Loss:  52.91847229\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  28.18992043\n",
      "################################  15  ################################\n",
      "Training Loss:  57.4512825\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59298372\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  8.7035532\n",
      "################################  30  ################################\n",
      "Training Loss:  8.70336723\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  8.70350838\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  8.7035017\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  8.7035017\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  8.7035017\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  8.7035017\n",
      "################################  60  ################################\n",
      "Training Loss:  8.7035017\n",
      "################################  65  ################################\n",
      "Training Loss:  8.7035017\n",
      "################################  70  ################################\n",
      "Training Loss:  8.7035017\n",
      "################################  75  ################################\n",
      "Training Loss:  8.7035017\n",
      "################################  80  ################################\n",
      "Training Loss:  8.7035017\n",
      "################################  85  ################################\n",
      "Training Loss:  8.7035017\n",
      "################################  90  ################################\n",
      "Training Loss:  8.7035017\n",
      "################################  95  ################################\n",
      "Training Loss:  8.7035017\n",
      "Final training Loss:  8.7035017\n",
      "\n",
      "Running model (trial=6, mod=180, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63776439\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59283996\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59049064\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  8.21134758\n",
      "################################  20  ################################\n",
      "Training Loss:  7.79199076\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  12.64282608\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58913952\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58990902\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58980578\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58980566\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58980566\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58980566\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58980566\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58980566\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58980566\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58980566\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58980566\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58980566\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58980566\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58980566\n",
      "Final training Loss:  0.58980566\n",
      "\n",
      "Running model (trial=6, mod=181, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  14.15288353\n",
      "################################  5  ################################\n",
      "Training Loss:  346.64334106\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.6477825\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58861971\n",
      "################################  20  ################################\n",
      "Training Loss:  9.03682804\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59150875\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59094626\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.60449713\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.60428256\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.60428256\n",
      "################################  50  ################################\n",
      "Training Loss:  0.60428256\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60428256\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60428256\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60428256\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60428256\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60428256\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60428256\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60428256\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60428256\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60428256\n",
      "Final training Loss:  0.60428256\n",
      "\n",
      "Running model (trial=7, mod=182, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59642041\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59062141\n",
      "################################  10  ################################\n",
      "Training Loss:  11.95420742\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  6.21349335\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59038115\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58999121\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59067518\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59095681\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59095669\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59095669\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59095669\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59095669\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59095669\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59095669\n",
      "Final training Loss:  0.59095669\n",
      "\n",
      "Running model (trial=7, mod=183, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59450948\n",
      "################################  5  ################################\n",
      "Training Loss:  0.5904721\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59152794\n",
      "################################  15  ################################\n",
      "Training Loss:  42.00890732\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  6.19228363\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59410149\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59010905\n",
      "################################  35  ################################\n",
      "Training Loss:  8.25460815\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59333032\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59359026\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.5935902\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5935902\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.5935902\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5935902\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5935902\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5935902\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5935902\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5935902\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5935902\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5935902\n",
      "Final training Loss:  0.5935902\n",
      "\n",
      "Running model (trial=7, mod=184, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59197664\n",
      "################################  5  ################################\n",
      "Training Loss:  0.58940214\n",
      "################################  10  ################################\n",
      "Training Loss:  0.5892756\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59478766\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59142381\n",
      "################################  25  ################################\n",
      "Training Loss:  0.6015563\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=185, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59523815\n",
      "################################  5  ################################\n",
      "Training Loss:  5.31921291\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58921808\n",
      "################################  15  ################################\n",
      "Training Loss:  1.55382311\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  9.16130066\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59069657\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  4.49374247\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  1.14371109\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.14365149\n",
      "################################  45  ################################\n",
      "Training Loss:  1.14365149\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  1.14365149\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  1.14365149\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.14365149\n",
      "################################  65  ################################\n",
      "Training Loss:  1.14365149\n",
      "################################  70  ################################\n",
      "Training Loss:  1.14365149\n",
      "################################  75  ################################\n",
      "Training Loss:  1.14365149\n",
      "################################  80  ################################\n",
      "Training Loss:  1.14365149\n",
      "################################  85  ################################\n",
      "Training Loss:  1.14365149\n",
      "################################  90  ################################\n",
      "Training Loss:  1.14365149\n",
      "################################  95  ################################\n",
      "Training Loss:  1.14365149\n",
      "Final training Loss:  1.14365149\n",
      "\n",
      "Running model (trial=7, mod=186, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.15506172\n",
      "################################  5  ################################\n",
      "Training Loss:  74.266716\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.79596114\n",
      "################################  15  ################################\n",
      "Training Loss:  125.35138702\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.61224174\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59106725\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59097725\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59095013\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59095013\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59095019\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59095013\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59095013\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59095013\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59095013\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59095013\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59095013\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59095013\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59095013\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59095013\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59095013\n",
      "Final training Loss:  0.59095013\n",
      "\n",
      "Running model (trial=7, mod=187, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12.17181301\n",
      "################################  5  ################################\n",
      "Training Loss:  6.14978647\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59887111\n",
      "################################  15  ################################\n",
      "Training Loss:  0.5904305\n",
      "################################  20  ################################\n",
      "Training Loss:  13.64950466\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59088653\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59170538\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59170491\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59170491\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59170491\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59170491\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59170491\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59170491\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59170491\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59170491\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59170491\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59170491\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59170491\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59170491\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59170491\n",
      "Final training Loss:  0.59170491\n",
      "\n",
      "Running model (trial=7, mod=188, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64874202\n",
      "################################  5  ################################\n",
      "Training Loss:  89.30068207\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59265852\n",
      "################################  15  ################################\n",
      "Training Loss:  11.07583904\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  25.07833481\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59532076\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59218436\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59217966\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59217966\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59217966\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59217966\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59217966\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59217966\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59217966\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59217966\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59217966\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59217966\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59217966\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59217966\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59217966\n",
      "Final training Loss:  0.59217966\n",
      "\n",
      "Running model (trial=7, mod=189, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  202.94822693\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59472752\n",
      "################################  10  ################################\n",
      "Training Loss:  9.26499844\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59224236\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.5896467\n",
      "################################  25  ################################\n",
      "Training Loss:  7.82863712\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59043729\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=190, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.43374217\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59058529\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59397852\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58934087\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  56.69963074\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  17.62113762\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  5.97722721\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.58955425\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58955425\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58955425\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58955425\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58955425\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58955425\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58955425\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58955425\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58955425\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58955425\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58955425\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58955425\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58955425\n",
      "Final training Loss:  0.58955425\n",
      "\n",
      "Running model (trial=7, mod=191, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59880137\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59125274\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59051383\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.5901075\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.60737562\n",
      "################################  25  ################################\n",
      "Training Loss:  6.8913126\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58994323\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58994323\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58994323\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58994323\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.58994323\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58994323\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58994323\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58994323\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58994323\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58994323\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58994323\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58994323\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58994323\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58994323\n",
      "Final training Loss:  0.58994323\n",
      "\n",
      "Running model (trial=7, mod=192, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60742664\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59258062\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58869785\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59856743\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  1.71135092\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.5916515\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59051132\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59230715\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59230715\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59230715\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59230715\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59230715\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59230715\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59230715\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59230715\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59230715\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59230715\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59230715\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59230715\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59230715\n",
      "Final training Loss:  0.59230715\n",
      "\n",
      "Running model (trial=7, mod=193, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72282314\n",
      "################################  5  ################################\n",
      "Training Loss:  3.71320868\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59081441\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60804641\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59427667\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.67195779\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58979464\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5937255\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5924862\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59255415\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59255415\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59255415\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59255397\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59255397\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59255397\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59255397\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59255397\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59255397\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59255397\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59255397\n",
      "Final training Loss:  0.59255397\n",
      "\n",
      "Running model (trial=7, mod=194, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  5  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  500.99996948\n",
      "################################  35  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  500.99996948\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  55  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  60  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  65  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  70  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  75  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  80  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  85  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  90  ################################\n",
      "Training Loss:  500.99996948\n",
      "################################  95  ################################\n",
      "Training Loss:  500.99996948\n",
      "Final training Loss:  500.99996948\n",
      "\n",
      "Running model (trial=7, mod=195, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5940271\n",
      "################################  5  ################################\n",
      "Training Loss:  0.92461342\n",
      "################################  10  ################################\n",
      "Training Loss:  7.63461351\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.73435277\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58853412\n",
      "################################  25  ################################\n",
      "Training Loss:  6.90555191\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  8.50620937\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59082681\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59082663\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59082663\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59082663\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59082663\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59082663\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59082663\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59082663\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59082663\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59082663\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59082663\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59082663\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59082663\n",
      "Final training Loss:  0.59082663\n",
      "\n",
      "Running model (trial=7, mod=196, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59447032\n",
      "################################  5  ################################\n",
      "Training Loss:  8.49491692\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59111804\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  16.82553673\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.9133091\n",
      "################################  25  ################################\n",
      "Training Loss:  0.5939036\n",
      "################################  30  ################################\n",
      "Training Loss:  0.62764186\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59111518\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59111518\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59111518\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59111518\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59111518\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59111518\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59111518\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59111518\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59111518\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59111518\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59111518\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59111518\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59111518\n",
      "Final training Loss:  0.59111518\n",
      "\n",
      "Running model (trial=7, mod=197, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.99641132\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60191137\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59246266\n",
      "################################  15  ################################\n",
      "Training Loss:  0.5889864\n",
      "################################  20  ################################\n",
      "Training Loss:  82.74312592\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  17.56116867\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59029901\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59181917\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59177518\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59177518\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59177518\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59177518\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59177518\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59177518\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59177518\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59177518\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59177518\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59177518\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59177518\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59177518\n",
      "Final training Loss:  0.59177518\n",
      "\n",
      "Running model (trial=7, mod=198, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69507974\n",
      "################################  5  ################################\n",
      "Training Loss:  28.75787544\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  549.87524414\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  16.21029091\n",
      "################################  20  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=199, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  13.61119652\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60618973\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60562098\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60393763\n",
      "################################  20  ################################\n",
      "Training Loss:  53.56879044\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.60389423\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60263848\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.60263848\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.60263848\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60263848\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.60263848\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60263848\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60263848\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60263848\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60263848\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60263848\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60263848\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60263848\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60263848\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60263848\n",
      "Final training Loss:  0.60263848\n",
      "\n",
      "Running model (trial=7, mod=200, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66806954\n",
      "################################  5  ################################\n",
      "Training Loss:  13.57107735\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  15.15098858\n",
      "################################  15  ################################\n",
      "Training Loss:  0.63384211\n",
      "################################  20  ################################\n",
      "Training Loss:  47.05670547\n",
      "################################  25  ################################\n",
      "Training Loss:  0.60529095\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60491842\n",
      "################################  35  ################################\n",
      "Training Loss:  15.29598904\n",
      "################################  40  ################################\n",
      "Training Loss:  15.61665535\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.60004497\n",
      "################################  50  ################################\n",
      "Training Loss:  18.40878868\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  18.40860748\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  18.40860748\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  18.40860748\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  18.40860748\n",
      "################################  75  ################################\n",
      "Training Loss:  18.40860748\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  18.40860748\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  18.40860748\n",
      "################################  90  ################################\n",
      "Training Loss:  18.40860748\n",
      "################################  95  ################################\n",
      "Training Loss:  18.40860748\n",
      "Final training Loss:  18.40860748\n",
      "\n",
      "Running model (trial=7, mod=201, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70516372\n",
      "################################  5  ################################\n",
      "Training Loss:  22.96137238\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.61054093\n",
      "################################  15  ################################\n",
      "Training Loss:  48.95249176\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  50.11891174\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59611666\n",
      "################################  30  ################################\n",
      "Training Loss:  11.16777039\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  11.16726398\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  11.16727924\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  11.16714478\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  11.16712952\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  11.16712952\n",
      "################################  60  ################################\n",
      "Training Loss:  11.16712952\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  11.16712952\n",
      "################################  70  ################################\n",
      "Training Loss:  11.16712952\n",
      "################################  75  ################################\n",
      "Training Loss:  11.16712952\n",
      "################################  80  ################################\n",
      "Training Loss:  11.16712952\n",
      "################################  85  ################################\n",
      "Training Loss:  11.16712952\n",
      "################################  90  ################################\n",
      "Training Loss:  11.16712952\n",
      "################################  95  ################################\n",
      "Training Loss:  11.16712952\n",
      "Final training Loss:  11.16712952\n",
      "\n",
      "Running model (trial=7, mod=202, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60690045\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60192287\n",
      "################################  10  ################################\n",
      "Training Loss:  8.51667118\n",
      "################################  15  ################################\n",
      "Training Loss:  7.06221724\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=203, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60419744\n",
      "################################  5  ################################\n",
      "Training Loss:  43.00570297\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  10.74618626\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=204, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6189726\n",
      "################################  5  ################################\n",
      "Training Loss:  9.31966591\n",
      "################################  10  ################################\n",
      "Training Loss:  0.96884352\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59454656\n",
      "################################  20  ################################\n",
      "Training Loss:  18.80077362\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58986753\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58918041\n",
      "################################  35  ################################\n",
      "Training Loss:  14.35063839\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58981019\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.6079914\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.6079914\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.6079914\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.6079914\n",
      "################################  65  ################################\n",
      "Training Loss:  0.6079914\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.6079914\n",
      "################################  75  ################################\n",
      "Training Loss:  0.6079914\n",
      "################################  80  ################################\n",
      "Training Loss:  0.6079914\n",
      "################################  85  ################################\n",
      "Training Loss:  0.6079914\n",
      "################################  90  ################################\n",
      "Training Loss:  0.6079914\n",
      "################################  95  ################################\n",
      "Training Loss:  0.6079914\n",
      "Final training Loss:  0.6079914\n",
      "\n",
      "Running model (trial=7, mod=205, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.6380949\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60368061\n",
      "################################  10  ################################\n",
      "Training Loss:  4.3641181\n",
      "################################  15  ################################\n",
      "Training Loss:  18.16509056\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  8.34331703\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  17.13972473\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59836328\n",
      "################################  35  ################################\n",
      "Training Loss:  23.17332649\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58958632\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.58958149\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58958149\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58958149\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58958149\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58958149\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58958149\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58958149\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58958149\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58958149\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58958149\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58958149\n",
      "Final training Loss:  0.58958149\n",
      "\n",
      "Running model (trial=7, mod=206, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60246563\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60574937\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59511536\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58956319\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  15.9662323\n",
      "################################  25  ################################\n",
      "Training Loss:  48.54021072\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59053206\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59804314\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5942899\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5942899\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.5942899\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5942899\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5942899\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5942899\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5942899\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5942899\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5942899\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5942899\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5942899\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5942899\n",
      "Final training Loss:  0.5942899\n",
      "\n",
      "Running model (trial=7, mod=207, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.619317\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59517735\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59065318\n",
      "################################  15  ################################\n",
      "Training Loss:  4.76545525\n",
      "################################  20  ################################\n",
      "Training Loss:  3.33187437\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59035313\n",
      "################################  30  ################################\n",
      "Training Loss:  2.80442119\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  129.65415955\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.61029303\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60997778\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.60997778\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60997778\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60997778\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60997778\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60997778\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60997778\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60997778\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60997778\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60997778\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60997778\n",
      "Final training Loss:  0.60997778\n",
      "\n",
      "Running model (trial=8, mod=208, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59861577\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59548301\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  4.87133312\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58956379\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58989722\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  21.95624161\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  9.05256939\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59560263\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59560263\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59560263\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59560269\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59560263\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59560263\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59560263\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59560263\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59560263\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59560263\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59560263\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59560263\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59560263\n",
      "Final training Loss:  0.59560263\n",
      "\n",
      "Running model (trial=8, mod=209, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59913892\n",
      "################################  5  ################################\n",
      "Training Loss:  3.54498577\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59053695\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  34.8278656\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.60756588\n",
      "################################  25  ################################\n",
      "Training Loss:  10.09210205\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59142619\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59142619\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59142619\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59142619\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59142619\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59142619\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59142619\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59142619\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59142619\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59142619\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59142619\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59142619\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59142619\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59142619\n",
      "Final training Loss:  0.59142619\n",
      "\n",
      "Running model (trial=8, mod=210, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.19322431\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60446793\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  7.56119871\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59100235\n",
      "################################  20  ################################\n",
      "Training Loss:  6.93693733\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59259522\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59252363\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59252346\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59252346\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59252346\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59252346\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59252346\n",
      "Final training Loss:  0.59252346\n",
      "\n",
      "Running model (trial=8, mod=211, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66655922\n",
      "################################  5  ################################\n",
      "Training Loss:  2.23520041\n",
      "################################  10  ################################\n",
      "Training Loss:  52.6206398\n",
      "################################  15  ################################\n",
      "Training Loss:  0.590819\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58976817\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59607762\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5928176\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58957016\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.590334\n",
      "################################  45  ################################\n",
      "Training Loss:  0.590334\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59033394\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59033394\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59033394\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59033394\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59033394\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59033394\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59033394\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59033394\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59033394\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59033394\n",
      "Final training Loss:  0.59033394\n",
      "\n",
      "Running model (trial=8, mod=212, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10.34050846\n",
      "################################  5  ################################\n",
      "Training Loss:  6.78515196\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59417087\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59103757\n",
      "################################  20  ################################\n",
      "Training Loss:  8.23854542\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59116775\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59116751\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59116751\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59116751\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59116751\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59116751\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59116751\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59116751\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59116751\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59116751\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59116751\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59116751\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59116751\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59116751\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59116751\n",
      "Final training Loss:  0.59116751\n",
      "\n",
      "Running model (trial=8, mod=213, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59224629\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59214962\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59047651\n",
      "################################  15  ################################\n",
      "Training Loss:  36.23469162\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58994013\n",
      "################################  25  ################################\n",
      "Training Loss:  4.53022242\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.81768453\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59090412\n",
      "################################  40  ################################\n",
      "Training Loss:  10.11145973\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59063953\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59063941\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59063941\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59063941\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59063941\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59063941\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59063941\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59063941\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59063941\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59063941\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59063941\n",
      "Final training Loss:  0.59063941\n",
      "\n",
      "Running model (trial=8, mod=214, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68067348\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60287756\n",
      "################################  10  ################################\n",
      "Training Loss:  1.33440125\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.60554183\n",
      "################################  20  ################################\n",
      "Training Loss:  18.44750595\n",
      "################################  25  ################################\n",
      "Training Loss:  84.378685\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  16.68539619\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59091753\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59047568\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  1.11134303\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59143543\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59143543\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59143543\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59143543\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59143543\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59143543\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59143543\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59143543\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59143543\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59143543\n",
      "Final training Loss:  0.59143543\n",
      "\n",
      "Running model (trial=8, mod=215, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  336.81277466\n",
      "################################  5  ################################\n",
      "Training Loss:  6.65147734\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58998716\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.5897041\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  51.68117905\n",
      "################################  25  ################################\n",
      "Training Loss:  78.46061707\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58963335\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58963335\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58963335\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58963335\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.58963335\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58963335\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58963335\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58963335\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58963335\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58963335\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58963335\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58963335\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58963335\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58963335\n",
      "Final training Loss:  0.58963335\n",
      "\n",
      "Running model (trial=8, mod=216, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.65573364\n",
      "################################  5  ################################\n",
      "Training Loss:  30.2494545\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59131253\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59465891\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.59163922\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58996081\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59225249\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59066582\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59066576\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59066576\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59066576\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59066576\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59066576\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59066576\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59066576\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59066576\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59066576\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59066576\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59066576\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59066576\n",
      "Final training Loss:  0.59066576\n",
      "\n",
      "Running model (trial=8, mod=217, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.42009497\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59203362\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.5905844\n",
      "################################  15  ################################\n",
      "Training Loss:  13.31113148\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58948612\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  4.50160646\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59124386\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59124386\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59124386\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59124386\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59124386\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59124386\n",
      "Final training Loss:  0.59124386\n",
      "\n",
      "Running model (trial=8, mod=218, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60361463\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59786808\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59279007\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59115726\n",
      "################################  20  ################################\n",
      "Training Loss:  0.589607\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  3.45569444\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  2.70025063\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59199113\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59199107\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59199107\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59199107\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59199107\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59199107\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59199107\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59199107\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59199107\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59199107\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59199107\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59199107\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59199107\n",
      "Final training Loss:  0.59199107\n",
      "\n",
      "Running model (trial=8, mod=219, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60042024\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59556276\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59016347\n",
      "################################  15  ################################\n",
      "Training Loss:  42.12143326\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  22.02403259\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  2.00566888\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59031618\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59031528\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59031516\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59031516\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59031516\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59031516\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59031516\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59031516\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59031516\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59031516\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59031516\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59031516\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59031516\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59031516\n",
      "Final training Loss:  0.59031516\n",
      "\n",
      "Running model (trial=8, mod=220, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  5  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  500.99990845\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  500.99990845\n",
      "################################  35  ################################\n",
      "Training Loss:  500.99990845\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  500.99990845\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  500.99990845\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  55  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  60  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  65  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  70  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  75  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  80  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  85  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  90  ################################\n",
      "Training Loss:  500.99990845\n",
      "################################  95  ################################\n",
      "Training Loss:  500.99990845\n",
      "Final training Loss:  500.99990845\n",
      "\n",
      "Running model (trial=8, mod=221, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59834349\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59047848\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59028274\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58959556\n",
      "################################  20  ################################\n",
      "Training Loss:  19.9808712\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.61872637\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58936006\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59284741\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59284741\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59284741\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59284741\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59284741\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59284741\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59284741\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59284741\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59284741\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59284741\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59284741\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59284741\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59284741\n",
      "Final training Loss:  0.59284741\n",
      "\n",
      "Running model (trial=8, mod=222, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60334015\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60328728\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59745407\n",
      "################################  15  ################################\n",
      "Training Loss:  8.54641724\n",
      "################################  20  ################################\n",
      "Training Loss:  9.27293968\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59034204\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58965957\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58918279\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58977991\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58996171\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  1.86089516\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  1.86059344\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.86051619\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.86048639\n",
      "################################  70  ################################\n",
      "Training Loss:  1.86048639\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  1.86048639\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  1.86048639\n",
      "################################  85  ################################\n",
      "Training Loss:  1.86048639\n",
      "################################  90  ################################\n",
      "Training Loss:  1.86048639\n",
      "################################  95  ################################\n",
      "Training Loss:  1.86048639\n",
      "Final training Loss:  1.86048639\n",
      "\n",
      "Running model (trial=8, mod=223, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60651302\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60295534\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59042048\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59042048\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  19.87247276\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.5945518\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  1.12655389\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  4.64965391\n",
      "################################  40  ################################\n",
      "Training Loss:  4.6879015\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  4.68787527\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  4.68787527\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  4.68787527\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  4.68787527\n",
      "################################  65  ################################\n",
      "Training Loss:  4.68787527\n",
      "################################  70  ################################\n",
      "Training Loss:  4.68787527\n",
      "################################  75  ################################\n",
      "Training Loss:  4.68787527\n",
      "################################  80  ################################\n",
      "Training Loss:  4.68787527\n",
      "################################  85  ################################\n",
      "Training Loss:  4.68787527\n",
      "################################  90  ################################\n",
      "Training Loss:  4.68787527\n",
      "################################  95  ################################\n",
      "Training Loss:  4.68787527\n",
      "Final training Loss:  4.68787527\n",
      "\n",
      "Running model (trial=8, mod=224, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  23.50561142\n",
      "################################  5  ################################\n",
      "Training Loss:  19.97981834\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  9.64922047\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59187073\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59603578\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59489417\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59489399\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59489393\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59489393\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59489393\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59489393\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59489393\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59489393\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59489393\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59489393\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59489393\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59489393\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59489393\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59489393\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59489393\n",
      "Final training Loss:  0.59489393\n",
      "\n",
      "Running model (trial=8, mod=225, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5904792\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=226, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  14.24550247\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61098194\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  153.71894836\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59981054\n",
      "################################  20  ################################\n",
      "Training Loss:  0.61562705\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  3.03827095\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59884739\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59884137\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59884137\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59884137\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59884137\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59884137\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59884137\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59884137\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59884137\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59884137\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59884137\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59884137\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59884137\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59884137\n",
      "Final training Loss:  0.59884137\n",
      "\n",
      "Running model (trial=8, mod=227, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  14.90940762\n",
      "################################  5  ################################\n",
      "Training Loss:  3.60284734\n",
      "################################  10  ################################\n",
      "Training Loss:  0.60751456\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60744119\n",
      "################################  20  ################################\n",
      "Training Loss:  0.60831642\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  1.70666707\n",
      "################################  30  ################################\n",
      "Training Loss:  0.60430735\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.60430735\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.60430735\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60430735\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.60430735\n",
      "################################  55  ################################\n",
      "Training Loss:  0.60430735\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60430735\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60430735\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60430735\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60430735\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60430735\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60430735\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60430735\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60430735\n",
      "Final training Loss:  0.60430735\n",
      "\n",
      "Running model (trial=8, mod=228, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62777859\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59713203\n",
      "################################  10  ################################\n",
      "Training Loss:  0.58938289\n",
      "################################  15  ################################\n",
      "Training Loss:  42.39844131\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59435803\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59500694\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59021509\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59021509\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59021509\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59021509\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59021509\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59021509\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59021509\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59021509\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59021509\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59021509\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59021509\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59021509\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59021509\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59021509\n",
      "Final training Loss:  0.59021509\n",
      "\n",
      "Running model (trial=8, mod=229, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59339011\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59651232\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.5919944\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58986032\n",
      "################################  20  ################################\n",
      "Training Loss:  9.15735149\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=230, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71896964\n",
      "################################  5  ################################\n",
      "Training Loss:  25.90484047\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  9.18801975\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59291983\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59074754\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.86025238\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59195745\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58998942\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.58998948\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58998948\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58998948\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58998948\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58998948\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58998948\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58998948\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58998948\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58998948\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58998948\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58998948\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58998948\n",
      "Final training Loss:  0.58998948\n",
      "\n",
      "Running model (trial=8, mod=231, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.60977173\n",
      "################################  5  ################################\n",
      "Training Loss:  6.12164164\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59501749\n",
      "################################  15  ################################\n",
      "Training Loss:  7.35671282\n",
      "################################  20  ################################\n",
      "Training Loss:  4.88706541\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  11.46782017\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5905081\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  3.06000447\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  7.3778615\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59030277\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59029812\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59029812\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59029812\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59029812\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59029812\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59029812\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59029812\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59029812\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59029812\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59029812\n",
      "Final training Loss:  0.59029812\n",
      "\n",
      "Running model (trial=8, mod=232, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61292416\n",
      "################################  5  ################################\n",
      "Training Loss:  9.53571892\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59024471\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58942789\n",
      "################################  20  ################################\n",
      "Training Loss:  0.590671\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59328502\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  6.80186272\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59258628\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59098601\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59098601\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59098601\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59098601\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59098601\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59098601\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59098601\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59098601\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59098601\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59098601\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59098601\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59098601\n",
      "Final training Loss:  0.59098601\n",
      "\n",
      "Running model (trial=8, mod=233, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  689.91156006\n",
      "################################  5  ################################\n",
      "Training Loss:  6.95467329\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59026527\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  11.17826843\n",
      "################################  20  ################################\n",
      "Training Loss:  74.51660156\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.61468464\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.61468399\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.61468399\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.61468399\n",
      "################################  50  ################################\n",
      "Training Loss:  0.61468399\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  60  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  65  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  70  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  75  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  80  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  85  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  90  ################################\n",
      "Training Loss:  0.61468399\n",
      "################################  95  ################################\n",
      "Training Loss:  0.61468399\n",
      "Final training Loss:  0.61468399\n",
      "\n",
      "Running model (trial=9, mod=234, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.595815\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59127623\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59323728\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59293145\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59081215\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58965021\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59065604\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59065616\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59065616\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59065616\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59065616\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59065616\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59065616\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59065616\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59065616\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59065616\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59065616\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59065616\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59065616\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59065616\n",
      "Final training Loss:  0.59065616\n",
      "\n",
      "Running model (trial=9, mod=235, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.24823165\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59318447\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  15.96412945\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  16.6309948\n",
      "################################  20  ################################\n",
      "Training Loss:  1.54358745\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59274751\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59257376\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59257376\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.59257376\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59257376\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59257376\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59257376\n",
      "Final training Loss:  0.59257376\n",
      "\n",
      "Running model (trial=9, mod=236, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  18.65599632\n",
      "################################  5  ################################\n",
      "Training Loss:  6.42946959\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59019512\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59938252\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  6.06960344\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  6.25808191\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58947623\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58947623\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58947623\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58947623\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58947623\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58947623\n",
      "Final training Loss:  0.58947623\n",
      "\n",
      "Running model (trial=9, mod=237, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61238694\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59168071\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59310293\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58937269\n",
      "################################  20  ################################\n",
      "Training Loss:  3.7951827\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59205824\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  1.15287006\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  3.6630044\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  3.66303396\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  3.66301203\n",
      "################################  50  ################################\n",
      "Training Loss:  3.66301203\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  3.66301203\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.66301203\n",
      "################################  65  ################################\n",
      "Training Loss:  3.66301203\n",
      "################################  70  ################################\n",
      "Training Loss:  3.66301203\n",
      "################################  75  ################################\n",
      "Training Loss:  3.66301203\n",
      "################################  80  ################################\n",
      "Training Loss:  3.66301203\n",
      "################################  85  ################################\n",
      "Training Loss:  3.66301203\n",
      "################################  90  ################################\n",
      "Training Loss:  3.66301203\n",
      "################################  95  ################################\n",
      "Training Loss:  3.66301203\n",
      "Final training Loss:  3.66301203\n",
      "\n",
      "Running model (trial=9, mod=238, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6036216\n",
      "################################  5  ################################\n",
      "Training Loss:  3.9372685\n",
      "################################  10  ################################\n",
      "Training Loss:  5.02236414\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59666711\n",
      "################################  20  ################################\n",
      "Training Loss:  6.66147804\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  7.41975212\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58994788\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  18.48886299\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.60626739\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.60625291\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.60625291\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.60625291\n",
      "################################  60  ################################\n",
      "Training Loss:  0.60625291\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.60625291\n",
      "################################  70  ################################\n",
      "Training Loss:  0.60625291\n",
      "################################  75  ################################\n",
      "Training Loss:  0.60625291\n",
      "################################  80  ################################\n",
      "Training Loss:  0.60625291\n",
      "################################  85  ################################\n",
      "Training Loss:  0.60625291\n",
      "################################  90  ################################\n",
      "Training Loss:  0.60625291\n",
      "################################  95  ################################\n",
      "Training Loss:  0.60625291\n",
      "Final training Loss:  0.60625291\n",
      "\n",
      "Running model (trial=9, mod=239, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.9684267\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59982115\n",
      "################################  10  ################################\n",
      "Training Loss:  61.00735092\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59338993\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59175688\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59161955\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  13.5301981\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  4.04311752\n",
      "################################  40  ################################\n",
      "Training Loss:  4.04325199\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  4.04325914\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  4.04325914\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  4.04325914\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  4.04325914\n",
      "################################  65  ################################\n",
      "Training Loss:  4.04325914\n",
      "################################  70  ################################\n",
      "Training Loss:  4.04325914\n",
      "################################  75  ################################\n",
      "Training Loss:  4.04325914\n",
      "################################  80  ################################\n",
      "Training Loss:  4.04325914\n",
      "################################  85  ################################\n",
      "Training Loss:  4.04325914\n",
      "################################  90  ################################\n",
      "Training Loss:  4.04325914\n",
      "################################  95  ################################\n",
      "Training Loss:  4.04325914\n",
      "Final training Loss:  4.04325914\n",
      "\n",
      "Running model (trial=9, mod=240, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67937374\n",
      "################################  5  ################################\n",
      "Training Loss:  0.58967555\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59012747\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58975822\n",
      "################################  20  ################################\n",
      "Training Loss:  0.58997363\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58914828\n",
      "################################  30  ################################\n",
      "Training Loss:  58.01174164\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59112072\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58926326\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58926326\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58926326\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58926326\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58926326\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58926326\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58926326\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58926326\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58926326\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58926326\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58926326\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58926326\n",
      "Final training Loss:  0.58926326\n",
      "\n",
      "Running model (trial=9, mod=241, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59597838\n",
      "################################  5  ################################\n",
      "Training Loss:  0.6022383\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  5.81962442\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=242, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  38.92109299\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59305274\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.59506834\n",
      "################################  15  ################################\n",
      "Training Loss:  1.16529405\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.66652757\n",
      "################################  25  ################################\n",
      "Training Loss:  13.11633968\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  52.49279404\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59184331\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59184337\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59184337\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59184337\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59184337\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59184337\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59184337\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59184337\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59184337\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59184337\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59184337\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59184337\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59184337\n",
      "Final training Loss:  0.59184337\n",
      "\n",
      "Running model (trial=9, mod=243, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  13.30101585\n",
      "################################  5  ################################\n",
      "Training Loss:  2.25802898\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  7.24254704\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  69.89160919\n",
      "################################  20  ################################\n",
      "Training Loss:  1.73765004\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59510821\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.59496427\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59496367\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59496367\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59496367\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59496367\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59496367\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59496367\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59496367\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59496367\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59496367\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59496367\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59496367\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59496367\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59496367\n",
      "Final training Loss:  0.59496367\n",
      "\n",
      "Running model (trial=9, mod=244, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.6515379\n",
      "################################  5  ################################\n",
      "Training Loss:  6.41628647\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59007955\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58951962\n",
      "################################  20  ################################\n",
      "Training Loss:  148.22595215\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59813702\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  8.51698208\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59108329\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59105533\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59105533\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59105521\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59105521\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59105521\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59105521\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59105521\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59105521\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59105521\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59105521\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59105521\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59105521\n",
      "Final training Loss:  0.59105521\n",
      "\n",
      "Running model (trial=9, mod=245, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60093993\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59050435\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59200287\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  35.28503418\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59810656\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58915281\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  8.81115532\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59027249\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59027249\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59027249\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59027249\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59027249\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59027249\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59027249\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59027249\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59027249\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59027249\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59027249\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59027249\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59027249\n",
      "Final training Loss:  0.59027249\n",
      "\n",
      "Running model (trial=9, mod=246, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  5  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  500.99993896\n",
      "################################  35  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  500.99993896\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  55  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  60  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  65  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  70  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  75  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  80  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  85  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  90  ################################\n",
      "Training Loss:  500.99993896\n",
      "################################  95  ################################\n",
      "Training Loss:  500.99993896\n",
      "Final training Loss:  500.99993896\n",
      "\n",
      "Running model (trial=9, mod=247, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72686058\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59451795\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59359616\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59187388\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59060138\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.58871043\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5884487\n",
      "################################  35  ################################\n",
      "Training Loss:  9.46382141\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.5902946\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59210223\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59040827\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59040827\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.59040827\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59040827\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59040827\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59040827\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59040827\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59040827\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59040827\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59040827\n",
      "Final training Loss:  0.59040827\n",
      "\n",
      "Running model (trial=9, mod=248, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63427895\n",
      "################################  5  ################################\n",
      "Training Loss:  18.83978271\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  89.33447266\n",
      "################################  15  ################################\n",
      "Training Loss:  0.60551637\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=249, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59607345\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59542888\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  28.86162186\n",
      "################################  15  ################################\n",
      "Training Loss:  7.35038471\n",
      "################################  20  ################################\n",
      "Training Loss:  71.39975739\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  7.44348335\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58999437\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58999115\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58999109\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58999109\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.58999109\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58999109\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58999109\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58999109\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58999109\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58999109\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58999109\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58999109\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58999109\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58999109\n",
      "Final training Loss:  0.58999109\n",
      "\n",
      "Running model (trial=9, mod=250, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  37.21171951\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61568266\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59907538\n",
      "################################  15  ################################\n",
      "Training Loss:  8.15021706\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  123.76659393\n",
      "################################  25  ################################\n",
      "Training Loss:  8.0976963\n",
      "################################  30  ################################\n",
      "Training Loss:  8.95791721\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  69.97963715\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59104365\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5910421\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.5910421\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5910421\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5910421\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5910421\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5910421\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5910421\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5910421\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5910421\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5910421\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5910421\n",
      "Final training Loss:  0.5910421\n",
      "\n",
      "Running model (trial=9, mod=251, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61696362\n",
      "################################  5  ################################\n",
      "Training Loss:  90.71082306\n",
      "################################  10  ################################\n",
      "Training Loss:  0.63569969\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  45.50611115\n",
      "################################  20  ################################\n",
      "Training Loss:  17.86699677\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59051698\n",
      "################################  30  ################################\n",
      "Training Loss:  0.5965541\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.5895288\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58952874\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.5895288\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.5895288\n",
      "################################  55  ################################\n",
      "Training Loss:  0.5895288\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.5895288\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.5895288\n",
      "################################  70  ################################\n",
      "Training Loss:  0.5895288\n",
      "################################  75  ################################\n",
      "Training Loss:  0.5895288\n",
      "################################  80  ################################\n",
      "Training Loss:  0.5895288\n",
      "################################  85  ################################\n",
      "Training Loss:  0.5895288\n",
      "################################  90  ################################\n",
      "Training Loss:  0.5895288\n",
      "################################  95  ################################\n",
      "Training Loss:  0.5895288\n",
      "Final training Loss:  0.5895288\n",
      "\n",
      "Running model (trial=9, mod=252, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  49.8188324\n",
      "################################  5  ################################\n",
      "Training Loss:  10.20733547\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59031332\n",
      "################################  15  ################################\n",
      "Training Loss:  20.57398987\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59213495\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  12.80326653\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  32.52926254\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59028894\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59028888\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59028888\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59028888\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59028888\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59028888\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59028888\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59028888\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59028888\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59028888\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59028888\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59028888\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59028888\n",
      "Final training Loss:  0.59028888\n",
      "\n",
      "Running model (trial=9, mod=253, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.75075227\n",
      "################################  5  ################################\n",
      "Training Loss:  0.61063647\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  82.26106262\n",
      "################################  15  ################################\n",
      "Training Loss:  0.61080593\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  17.24379158\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  1.0889678\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.0889678\n",
      "################################  35  ################################\n",
      "Training Loss:  1.08905721\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  1.08905721\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  1.08905721\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  1.08905721\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  1.08905721\n",
      "################################  60  ################################\n",
      "Training Loss:  1.08905721\n",
      "################################  65  ################################\n",
      "Training Loss:  1.08905721\n",
      "################################  70  ################################\n",
      "Training Loss:  1.08905721\n",
      "################################  75  ################################\n",
      "Training Loss:  1.08905721\n",
      "################################  80  ################################\n",
      "Training Loss:  1.08905721\n",
      "################################  85  ################################\n",
      "Training Loss:  1.08905721\n",
      "################################  90  ################################\n",
      "Training Loss:  1.08905721\n",
      "################################  95  ################################\n",
      "Training Loss:  1.08905721\n",
      "Final training Loss:  1.08905721\n",
      "\n",
      "Running model (trial=9, mod=254, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60218602\n",
      "################################  5  ################################\n",
      "Training Loss:  28.93278122\n",
      "################################  10  ################################\n",
      "Training Loss:  29.10097885\n",
      "################################  15  ################################\n",
      "Training Loss:  0.58974236\n",
      "################################  20  ################################\n",
      "Training Loss:  9.21243382\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  68.24357605\n",
      "################################  30  ################################\n",
      "Training Loss:  0.62790483\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  6.31739092\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.59131646\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59131646\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59131646\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59131646\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59131646\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59131646\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.59131646\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59131646\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59131646\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59131646\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59131646\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59131646\n",
      "Final training Loss:  0.59131646\n",
      "\n",
      "Running model (trial=9, mod=255, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59506541\n",
      "################################  5  ################################\n",
      "Training Loss:  0.81891835\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59310728\n",
      "################################  15  ################################\n",
      "Training Loss:  37.2224884\n",
      "################################  20  ################################\n",
      "Training Loss:  30.25351715\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  4.36816645\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  11.12893009\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58877933\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58877897\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58877891\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58877891\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.58877891\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58877891\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58877891\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58877891\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58877891\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58877891\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58877891\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58877891\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58877891\n",
      "Final training Loss:  0.58877891\n",
      "\n",
      "Running model (trial=9, mod=256, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59502965\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59857869\n",
      "################################  10  ################################\n",
      "Training Loss:  0.59263253\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59031153\n",
      "################################  20  ################################\n",
      "Training Loss:  29.78606224\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  9.11649132\n",
      "################################  30  ################################\n",
      "Training Loss:  0.58974653\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.58974391\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.58974391\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58974397\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.58974397\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.58974397\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58974397\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58974397\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58974397\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58974397\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58974397\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58974397\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58974397\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58974397\n",
      "Final training Loss:  0.58974397\n",
      "\n",
      "Running model (trial=9, mod=257, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.26302195\n",
      "################################  5  ################################\n",
      "Training Loss:  0.60243171\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.60243165\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59012181\n",
      "################################  20  ################################\n",
      "Training Loss:  5.19851828\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59005141\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59005636\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.59005636\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59005636\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59005636\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59005636\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59005636\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59005636\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59005636\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59005636\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59005636\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59005636\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59005636\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59005636\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59005636\n",
      "Final training Loss:  0.59005636\n",
      "\n",
      "Running model (trial=9, mod=258, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  14.58008671\n",
      "################################  5  ################################\n",
      "Training Loss:  27.51070976\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  11.61536121\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59128493\n",
      "################################  20  ################################\n",
      "Training Loss:  0.59194958\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  19.35071754\n",
      "################################  30  ################################\n",
      "Training Loss:  9.30454636\n",
      "################################  35  ################################\n",
      "Training Loss:  7.16106176\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  4.40523434\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.58855218\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.58855218\n",
      "################################  55  ################################\n",
      "Training Loss:  0.58855218\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.58855218\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.58855218\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.58855218\n",
      "################################  75  ################################\n",
      "Training Loss:  0.58855218\n",
      "################################  80  ################################\n",
      "Training Loss:  0.58855218\n",
      "################################  85  ################################\n",
      "Training Loss:  0.58855218\n",
      "################################  90  ################################\n",
      "Training Loss:  0.58855218\n",
      "################################  95  ################################\n",
      "Training Loss:  0.58855218\n",
      "Final training Loss:  0.58855218\n",
      "\n",
      "Running model (trial=9, mod=259, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12ceff670>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x143047ee0>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x143047820>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62812883\n",
      "################################  5  ################################\n",
      "Training Loss:  0.59180117\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  36.20545578\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59054232\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.63228804\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.59221673\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.59210449\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.59210455\n",
      "################################  40  ################################\n",
      "Training Loss:  0.59210455\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.59210455\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.59210455\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.59210455\n",
      "################################  60  ################################\n",
      "Training Loss:  0.59210455\n",
      "################################  65  ################################\n",
      "Training Loss:  0.59210455\n",
      "################################  70  ################################\n",
      "Training Loss:  0.59210455\n",
      "################################  75  ################################\n",
      "Training Loss:  0.59210455\n",
      "################################  80  ################################\n",
      "Training Loss:  0.59210455\n",
      "################################  85  ################################\n",
      "Training Loss:  0.59210455\n",
      "################################  90  ################################\n",
      "Training Loss:  0.59210455\n",
      "################################  95  ################################\n",
      "Training Loss:  0.59210455\n",
      "Final training Loss:  0.59210455\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "x_train_ = x_train.detach()\n",
    "x_sorted, indices = torch.sort(x_train_, dim=0)\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": x_sorted,\n",
    "    \"x_train\": x_sorted,\n",
    "    \"y_train\": c2.ksi(x_sorted),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"analytical solution\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: invalid value encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: no_penalty_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "\n",
    "loss_array -= c2.DIST_R_Q\n",
    "# make data frame\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type})\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 410.378125 262.19625\" width=\"410.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T09:09:40.296405</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 410.378125 262.19625 \nL 410.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 68.378125 224.64 \nL 403.178125 224.64 \nL 403.178125 7.2 \nL 68.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m90bde0d728\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.381387\" xlink:href=\"#m90bde0d728\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(192.581387 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"369.893739\" xlink:href=\"#m90bde0d728\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(361.093739 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m6267863ea1\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"83.596307\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"113.269859\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"134.32358\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"150.654114\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"163.997132\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"175.278493\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"185.050852\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"193.670684\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"252.108659\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"281.782212\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"302.835932\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"319.166466\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"332.509484\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"343.790845\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"353.563205\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"362.183037\" xlink:href=\"#m6267863ea1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Neurons -->\n     <g transform=\"translate(215.016406 252.916563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"199.707031\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"238.570312\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"299.751953\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"363.130859\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_19\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9aa1175add\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"68.378125\" xlink:href=\"#m9aa1175add\" y=\"111.099573\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(37.878125 114.898791)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m29c4c7f3b3\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#m29c4c7f3b3\" y=\"200.813763\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#m29c4c7f3b3\" y=\"177.215844\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{6\\times10^{-3}}$ -->\n      <g transform=\"translate(20.878125 181.015063)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 4488 3438 \nL 3059 2003 \nL 4488 575 \nL 4116 197 \nL 2681 1631 \nL 1247 197 \nL 878 575 \nL 2303 2003 \nL 878 3438 \nL 1247 3816 \nL 2681 2381 \nL 4116 3816 \nL 4488 3438 \nz\n\" id=\"DejaVuSans-d7\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-36\"/>\n       <use transform=\"translate(83.105469 0.765625)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#m29c4c7f3b3\" y=\"157.264088\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#m29c4c7f3b3\" y=\"139.981091\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#m29c4c7f3b3\" y=\"124.736407\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"68.378125\" xlink:href=\"#m29c4c7f3b3\" y=\"21.385382\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{2\\times10^{-2}}$ -->\n      <g transform=\"translate(20.878125 25.184601)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-32\"/>\n       <use transform=\"translate(83.105469 0.765625)\" xlink:href=\"#DejaVuSans-d7\"/>\n       <use transform=\"translate(186.376953 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(250 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(314.580078 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(373.232422 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_7\">\n     <!-- $\\Delta E$ -->\n     <g transform=\"translate(14.798437 122.52)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 906 525 \nL 3472 525 \nL 2188 4044 \nz\nM 50 0 \nL 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 50 0 \nz\n\" id=\"DejaVuSans-394\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-394\"/>\n      <use transform=\"translate(68.408203 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 83.596307 189.022476 \nL 83.596307 156.333281 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 134.32358 191.281548 \nL 134.32358 163.992587 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 185.050852 206.374484 \nL 185.050852 183.598988 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 235.778125 202.922556 \nL 235.778125 154.958239 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 286.505398 165.917236 \nL 286.505398 105.775337 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 337.23267 210.69686 \nL 337.23267 163.52835 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 387.959943 213.220052 \nL 387.959943 192.379163 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 83.596307 196.026983 \nL 83.596307 162.188937 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 134.32358 213.518964 \nL 134.32358 185.454414 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 185.050852 214.756364 \nL 185.050852 194.898622 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 235.778125 192.762933 \nL 235.778125 129.413141 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 286.505398 177.982445 \nL 286.505398 100.881611 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 337.23267 59.318191 \nL 337.23267 17.083636 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 387.959943 114.643752 \nL 387.959943 25.45583 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_26\">\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 83.596307 172.328581 \nL 134.32358 177.014776 \nL 185.050852 195.098367 \nL 235.778125 177.449422 \nL 286.505398 133.431289 \nL 337.23267 184.710556 \nL 387.959943 202.397151 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path clip-path=\"url(#p9b77a4dcd6)\" d=\"M 83.596307 178.71847 \nL 134.32358 199.102248 \nL 185.050852 204.214292 \nL 235.778125 157.733573 \nL 286.505398 135.841598 \nL 337.23267 35.532016 \nL 387.959943 62.729068 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_28\"/>\n   <g id=\"line2d_29\"/>\n   <g id=\"line2d_30\"/>\n   <g id=\"line2d_31\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 68.378125 224.64 \nL 68.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 403.178125 224.64 \nL 403.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 68.378125 224.64 \nL 403.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 68.378125 7.2 \nL 403.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 75.378125 59.234375 \nL 142.79375 59.234375 \nQ 144.79375 59.234375 144.79375 57.234375 \nL 144.79375 14.2 \nQ 144.79375 12.2 142.79375 12.2 \nL 75.378125 12.2 \nQ 73.378125 12.2 73.378125 14.2 \nL 73.378125 57.234375 \nQ 73.378125 59.234375 75.378125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_8\">\n     <!-- model -->\n     <g transform=\"translate(93.516406 23.798438)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_32\">\n     <path d=\"M 77.378125 34.976563 \nL 97.378125 34.976563 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_33\"/>\n    <g id=\"text_9\">\n     <!-- FFNN -->\n     <g transform=\"translate(105.378125 38.476563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_34\">\n     <path d=\"M 77.378125 49.654688 \nL 97.378125 49.654688 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_35\"/>\n    <g id=\"text_10\">\n     <!-- ResNet -->\n     <g transform=\"translate(105.378125 53.154688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p9b77a4dcd6\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"68.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\", bbox_inches='tight')\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "0 160\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 1]), len(d_results_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 393.378125 262.19625\" width=\"393.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T09:09:41.608863</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 393.378125 262.19625 \nL 393.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \nL 386.178125 7.2 \nL 51.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m1dcb284706\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.596307\" xlink:href=\"#m1dcb284706\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(57.796307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.81113\" xlink:href=\"#m1dcb284706\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(260.01113 239.238437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m9603fdc903\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"57.343464\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"127.469034\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"163.077297\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"188.341761\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"207.938402\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"223.950024\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"237.487657\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"249.214489\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"259.558287\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"329.683857\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"365.29212\" xlink:href=\"#m9603fdc903\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Hidden layers -->\n     <g transform=\"translate(184.296875 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 2753 \nL 3553 2753 \nL 3553 4666 \nL 4184 4666 \nL 4184 0 \nL 3553 0 \nL 3553 2222 \nL 1259 2222 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-48\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"75.195312\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"102.978516\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"166.455078\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"229.931641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"291.455078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"354.833984\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"386.621094\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"414.404297\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"475.683594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"534.863281\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"596.386719\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"637.5\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m6701672a33\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m6701672a33\" y=\"173.140844\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(20.878125 176.940063)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m6701672a33\" y=\"76.642834\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(20.878125 80.442052)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_16\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"mb02744a23e\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"223.597603\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"211.541263\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"202.18964\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"194.548807\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"188.088575\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"182.492467\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"177.556351\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"144.092048\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"127.099592\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"115.043253\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"105.691629\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"98.050796\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"91.590564\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"85.994457\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"81.05834\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"47.594038\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"30.601582\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"18.545242\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mb02744a23e\" y=\"9.193619\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $\\Delta E$ -->\n     <g transform=\"translate(14.798438 122.52)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 906 525 \nL 3472 525 \nL 2188 4044 \nz\nM 50 0 \nL 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 50 0 \nz\n\" id=\"DejaVuSans-394\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-394\"/>\n      <use transform=\"translate(68.408203 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 66.596307 214.756364 \nL 66.596307 212.023967 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 127.469034 203.914653 \nL 127.469034 196.776781 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 188.341761 201.679748 \nL 188.341761 190.256204 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 249.214489 192.807457 \nL 249.214489 175.640418 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 310.087216 201.983329 \nL 310.087216 82.433338 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 370.959943 17.084553 \nL 370.959943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 66.596307 202.85631 \nL 66.596307 166.643598 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 127.469034 206.379757 \nL 127.469034 200.196007 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 188.341761 194.283067 \nL 188.341761 177.721344 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 249.214489 206.05587 \nL 249.214489 190.14763 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 310.087216 207.057716 \nL 310.087216 194.985049 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 370.959943 188.175234 \nL 370.959943 158.80591 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_35\">\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 66.596307 213.367901 \nL 127.469034 200.339036 \nL 188.341761 195.505712 \nL 249.214489 183.831593 \nL 310.087216 109.127537 \nL 370.959943 17.084107 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#pb02b8cc5c3)\" d=\"M 66.596307 181.513815 \nL 127.469034 203.290706 \nL 188.341761 185.730542 \nL 249.214489 197.659717 \nL 310.087216 200.60263 \nL 370.959943 170.915768 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\"/>\n   <g id=\"line2d_38\"/>\n   <g id=\"line2d_39\"/>\n   <g id=\"line2d_40\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 224.64 \nL 51.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.178125 224.64 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 7.2 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 58.378125 59.234375 \nL 125.79375 59.234375 \nQ 127.79375 59.234375 127.79375 57.234375 \nL 127.79375 14.2 \nQ 127.79375 12.2 125.79375 12.2 \nL 58.378125 12.2 \nQ 56.378125 12.2 56.378125 14.2 \nL 56.378125 57.234375 \nQ 56.378125 59.234375 58.378125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_7\">\n     <!-- model -->\n     <g transform=\"translate(76.516406 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_41\">\n     <path d=\"M 60.378125 34.976562 \nL 80.378125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_42\"/>\n    <g id=\"text_8\">\n     <!-- FFNN -->\n     <g transform=\"translate(88.378125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_43\">\n     <path d=\"M 60.378125 49.654687 \nL 80.378125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_44\"/>\n    <g id=\"text_9\">\n     <!-- ResNet -->\n     <g transform=\"translate(88.378125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pb02b8cc5c3\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"51.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Hidden layers\")\n",
    "fig_layers.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\", bbox_inches='tight')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "0 140\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 1]), len(d_results_layer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 404.936323 262.19625\" width=\"404.936323pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T09:09:42.338730</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 404.936323 262.19625 \nL 404.936323 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \nL 386.178125 7.2 \nL 51.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 186.35851 \nC 67.391916 186.35851 68.155046 186.042411 68.717627 185.47983 \nC 69.280208 184.917249 69.596307 184.154119 69.596307 183.35851 \nC 69.596307 182.5629 69.280208 181.79977 68.717627 181.237189 \nC 68.155046 180.674609 67.391916 180.35851 66.596307 180.35851 \nC 65.800698 180.35851 65.037567 180.674609 64.474986 181.237189 \nC 63.912406 181.79977 63.596307 182.5629 63.596307 183.35851 \nC 63.596307 184.154119 63.912406 184.917249 64.474986 185.47983 \nC 65.037567 186.042411 65.800698 186.35851 66.596307 186.35851 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 175.54835 \nC 67.752451 175.54835 68.515581 175.232251 69.078162 174.66967 \nC 69.640743 174.10709 69.956842 173.343959 69.956842 172.54835 \nC 69.956842 171.752741 69.640743 170.989611 69.078162 170.42703 \nC 68.515581 169.864449 67.752451 169.54835 66.956842 169.54835 \nC 66.161233 169.54835 65.398102 169.864449 64.835521 170.42703 \nC 64.272941 170.989611 63.956842 171.752741 63.956842 172.54835 \nC 63.956842 173.343959 64.272941 174.10709 64.835521 174.66967 \nC 65.398102 175.232251 66.161233 175.54835 66.956842 175.54835 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 176.901302 \nC 68.906163 176.901302 69.669293 176.585203 70.231874 176.022623 \nC 70.794455 175.460042 71.110554 174.696912 71.110554 173.901302 \nC 71.110554 173.105693 70.794455 172.342563 70.231874 171.779982 \nC 69.669293 171.217401 68.906163 170.901302 68.110554 170.901302 \nC 67.314944 170.901302 66.551814 171.217401 65.989233 171.779982 \nC 65.426653 172.342563 65.110554 173.105693 65.110554 173.901302 \nC 65.110554 174.696912 65.426653 175.460042 65.989233 176.022623 \nC 66.551814 176.585203 67.314944 176.901302 68.110554 176.901302 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 181.120739 \nC 72.944155 181.120739 73.707285 180.80464 74.269866 180.24206 \nC 74.832447 179.679479 75.148546 178.916349 75.148546 178.120739 \nC 75.148546 177.32513 74.832447 176.562 74.269866 175.999419 \nC 73.707285 175.436838 72.944155 175.120739 72.148546 175.120739 \nC 71.352936 175.120739 70.589806 175.436838 70.027225 175.999419 \nC 69.464645 176.562 69.148546 177.32513 69.148546 178.120739 \nC 69.148546 178.916349 69.464645 179.679479 70.027225 180.24206 \nC 70.589806 180.80464 71.352936 181.120739 72.148546 181.120739 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 177.178879 \nC 87.94241 177.178879 88.705541 176.86278 89.268121 176.3002 \nC 89.830702 175.737619 90.146801 174.974489 90.146801 174.178879 \nC 90.146801 173.38327 89.830702 172.62014 89.268121 172.057559 \nC 88.705541 171.494978 87.94241 171.178879 87.146801 171.178879 \nC 86.351192 171.178879 85.588061 171.494978 85.025481 172.057559 \nC 84.4629 172.62014 84.146801 173.38327 84.146801 174.178879 \nC 84.146801 174.974489 84.4629 175.737619 85.025481 176.3002 \nC 85.588061 176.86278 86.351192 177.178879 87.146801 177.178879 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 196.434589 \nC 145.628008 196.434589 146.391139 196.118491 146.953719 195.55591 \nC 147.5163 194.993329 147.832399 194.230199 147.832399 193.434589 \nC 147.832399 192.63898 147.5163 191.87585 146.953719 191.313269 \nC 146.391139 190.750688 145.628008 190.434589 144.832399 190.434589 \nC 144.03679 190.434589 143.273659 190.750688 142.711079 191.313269 \nC 142.148498 191.87585 141.832399 192.63898 141.832399 193.434589 \nC 141.832399 194.230199 142.148498 194.993329 142.711079 195.55591 \nC 143.273659 196.118491 144.03679 196.434589 144.832399 196.434589 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 164.72636 \nC 76.693719 164.72636 77.456849 164.410261 78.01943 163.84768 \nC 78.582011 163.2851 78.898109 162.521969 78.898109 161.72636 \nC 78.898109 160.930751 78.582011 160.16762 78.01943 159.60504 \nC 77.456849 159.042459 76.693719 158.72636 75.898109 158.72636 \nC 75.1025 158.72636 74.33937 159.042459 73.776789 159.60504 \nC 73.214208 160.16762 72.898109 160.930751 72.898109 161.72636 \nC 72.898109 162.521969 73.214208 163.2851 73.776789 163.84768 \nC 74.33937 164.410261 75.1025 164.72636 75.898109 164.72636 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 186.817829 \nC 87.077126 186.817829 87.840257 186.50173 88.402837 185.939149 \nC 88.965418 185.376569 89.281517 184.613438 89.281517 183.817829 \nC 89.281517 183.02222 88.965418 182.259089 88.402837 181.696509 \nC 87.840257 181.133928 87.077126 180.817829 86.281517 180.817829 \nC 85.485908 180.817829 84.722778 181.133928 84.160197 181.696509 \nC 83.597616 182.259089 83.281517 183.02222 83.281517 183.817829 \nC 83.281517 184.613438 83.597616 185.376569 84.160197 185.939149 \nC 84.722778 186.50173 85.485908 186.817829 86.281517 186.817829 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.084335 \nC 107.843942 20.084335 108.607072 19.768237 109.169653 19.205656 \nC 109.732233 18.643075 110.048332 17.879945 110.048332 17.084335 \nC 110.048332 16.288726 109.732233 15.525596 109.169653 14.963015 \nC 108.607072 14.400434 107.843942 14.084335 107.048332 14.084335 \nC 106.252723 14.084335 105.489593 14.400434 104.927012 14.963015 \nC 104.364431 15.525596 104.048332 16.288726 104.048332 17.084335 \nC 104.048332 17.879945 104.364431 18.643075 104.927012 19.205656 \nC 105.489593 19.768237 106.252723 20.084335 107.048332 20.084335 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 174.759348 \nL 66.596307 173.259348 \nL 68.096307 174.759348 \nL 69.596307 173.259348 \nL 68.096307 171.759348 \nL 69.596307 170.259348 \nL 68.096307 168.759348 \nL 66.596307 170.259348 \nL 65.096307 168.759348 \nL 63.596307 170.259348 \nL 65.096307 171.759348 \nL 63.596307 173.259348 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.648546 170.538494 \nL 72.148546 169.038494 \nL 73.648546 170.538494 \nL 75.148546 169.038494 \nL 73.648546 167.538494 \nL 75.148546 166.038494 \nL 73.648546 164.538494 \nL 72.148546 166.038494 \nL 70.648546 164.538494 \nL 69.148546 166.038494 \nL 70.648546 167.538494 \nL 69.148546 169.038494 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 85.646801 147.181791 \nL 87.146801 145.681791 \nL 88.646801 147.181791 \nL 90.146801 145.681791 \nL 88.646801 144.181791 \nL 90.146801 142.681791 \nL 88.646801 141.181791 \nL 87.146801 142.681791 \nL 85.646801 141.181791 \nL 84.146801 142.681791 \nL 85.646801 144.181791 \nL 84.146801 145.681791 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.312628 197.868229 \nL 66.812628 196.368229 \nL 68.312628 197.868229 \nL 69.812628 196.368229 \nL 68.312628 194.868229 \nL 69.812628 193.368229 \nL 68.312628 191.868229 \nL 66.812628 193.368229 \nL 65.312628 191.868229 \nL 63.812628 193.368229 \nL 65.312628 194.868229 \nL 63.812628 196.368229 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 188.111831 \nL 68.110554 186.611831 \nL 69.610554 188.111831 \nL 71.110554 186.611831 \nL 69.610554 185.111831 \nL 71.110554 183.611831 \nL 69.610554 182.111831 \nL 68.110554 183.611831 \nL 66.610554 182.111831 \nL 65.110554 183.611831 \nL 66.610554 185.111831 \nL 65.110554 186.611831 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 69.206406 184.445168 \nL 70.706406 182.945168 \nL 72.206406 184.445168 \nL 73.706406 182.945168 \nL 72.206406 181.445168 \nL 73.706406 179.945168 \nL 72.206406 178.445168 \nL 70.706406 179.945168 \nL 69.206406 178.445168 \nL 67.706406 179.945168 \nL 69.206406 181.445168 \nL 67.706406 182.945168 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 74.398109 188.316401 \nL 75.898109 186.816401 \nL 77.398109 188.316401 \nL 78.898109 186.816401 \nL 77.398109 185.316401 \nL 78.898109 183.816401 \nL 77.398109 182.316401 \nL 75.898109 183.816401 \nL 74.398109 182.316401 \nL 72.898109 183.816401 \nL 74.398109 185.316401 \nL 72.898109 186.816401 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 84.781517 195.884262 \nL 86.281517 194.384262 \nL 87.781517 195.884262 \nL 89.281517 194.384262 \nL 87.781517 192.884262 \nL 89.281517 191.384262 \nL 87.781517 189.884262 \nL 86.281517 191.384262 \nL 84.781517 189.884262 \nL 83.281517 191.384262 \nL 84.781517 192.884262 \nL 83.281517 194.384262 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 164.872129 \nL 107.048332 163.372129 \nL 108.548332 164.872129 \nL 110.048332 163.372129 \nL 108.548332 161.872129 \nL 110.048332 160.372129 \nL 108.548332 158.872129 \nL 107.048332 160.372129 \nL 105.548332 158.872129 \nL 104.048332 160.372129 \nL 105.548332 161.872129 \nL 104.048332 163.372129 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 192.822138 \nC 67.391916 192.822138 68.155046 192.506039 68.717627 191.943458 \nC 69.280208 191.380877 69.596307 190.617747 69.596307 189.822138 \nC 69.596307 189.026528 69.280208 188.263398 68.717627 187.700817 \nC 68.155046 187.138237 67.391916 186.822138 66.596307 186.822138 \nC 65.800698 186.822138 65.037567 187.138237 64.474986 187.700817 \nC 63.912406 188.263398 63.596307 189.026528 63.596307 189.822138 \nC 63.596307 190.617747 63.912406 191.380877 64.474986 191.943458 \nC 65.037567 192.506039 65.800698 192.822138 66.596307 192.822138 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 161.146128 \nC 67.752451 161.146128 68.515581 160.830029 69.078162 160.267449 \nC 69.640743 159.704868 69.956842 158.941738 69.956842 158.146128 \nC 69.956842 157.350519 69.640743 156.587389 69.078162 156.024808 \nC 68.515581 155.462227 67.752451 155.146128 66.956842 155.146128 \nC 66.161233 155.146128 65.398102 155.462227 64.835521 156.024808 \nC 64.272941 156.587389 63.956842 157.350519 63.956842 158.146128 \nC 63.956842 158.941738 64.272941 159.704868 64.835521 160.267449 \nC 65.398102 160.830029 66.161233 161.146128 66.956842 161.146128 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 187.314483 \nC 68.906163 187.314483 69.669293 186.998384 70.231874 186.435803 \nC 70.794455 185.873222 71.110554 185.110092 71.110554 184.314483 \nC 71.110554 183.518873 70.794455 182.755743 70.231874 182.193162 \nC 69.669293 181.630582 68.906163 181.314483 68.110554 181.314483 \nC 67.314944 181.314483 66.551814 181.630582 65.989233 182.193162 \nC 65.426653 182.755743 65.110554 183.518873 65.110554 184.314483 \nC 65.110554 185.110092 65.426653 185.873222 65.989233 186.435803 \nC 66.551814 186.998384 67.314944 187.314483 68.110554 187.314483 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 162.748024 \nC 72.944155 162.748024 73.707285 162.431925 74.269866 161.869344 \nC 74.832447 161.306764 75.148546 160.543633 75.148546 159.748024 \nC 75.148546 158.952415 74.832447 158.189284 74.269866 157.626704 \nC 73.707285 157.064123 72.944155 156.748024 72.148546 156.748024 \nC 71.352936 156.748024 70.589806 157.064123 70.027225 157.626704 \nC 69.464645 158.189284 69.148546 158.952415 69.148546 159.748024 \nC 69.148546 160.543633 69.464645 161.306764 70.027225 161.869344 \nC 70.589806 162.431925 71.352936 162.748024 72.148546 162.748024 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 176.920244 \nC 87.94241 176.920244 88.705541 176.604145 89.268121 176.041564 \nC 89.830702 175.478983 90.146801 174.715853 90.146801 173.920244 \nC 90.146801 173.124635 89.830702 172.361504 89.268121 171.798923 \nC 88.705541 171.236343 87.94241 170.920244 87.146801 170.920244 \nC 86.351192 170.920244 85.588061 171.236343 85.025481 171.798923 \nC 84.4629 172.361504 84.146801 173.124635 84.146801 173.920244 \nC 84.146801 174.715853 84.4629 175.478983 85.025481 176.041564 \nC 85.588061 176.604145 86.351192 176.920244 87.146801 176.920244 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 148.955648 \nC 145.628008 148.955648 146.391139 148.639549 146.953719 148.076968 \nC 147.5163 147.514387 147.832399 146.751257 147.832399 145.955648 \nC 147.832399 145.160038 147.5163 144.396908 146.953719 143.834327 \nC 146.391139 143.271747 145.628008 142.955648 144.832399 142.955648 \nC 144.03679 142.955648 143.273659 143.271747 142.711079 143.834327 \nC 142.148498 144.396908 141.832399 145.160038 141.832399 145.955648 \nC 141.832399 146.751257 142.148498 147.514387 142.711079 148.076968 \nC 143.273659 148.639549 144.03679 148.955648 144.832399 148.955648 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 176.528364 \nC 371.755552 176.528364 372.518683 176.212265 373.081264 175.649684 \nC 373.643844 175.087103 373.959943 174.323973 373.959943 173.528364 \nC 373.959943 172.732754 373.643844 171.969624 373.081264 171.407043 \nC 372.518683 170.844463 371.755552 170.528364 370.959943 170.528364 \nC 370.164334 170.528364 369.401204 170.844463 368.838623 171.407043 \nC 368.276042 171.969624 367.959943 172.732754 367.959943 173.528364 \nC 367.959943 174.323973 368.276042 175.087103 368.838623 175.649684 \nC 369.401204 176.212265 370.164334 176.528364 370.959943 176.528364 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 190.485211 \nC 68.906163 190.485211 69.669293 190.169112 70.231874 189.606531 \nC 70.794455 189.043951 71.110554 188.28082 71.110554 187.485211 \nC 71.110554 186.689602 70.794455 185.926471 70.231874 185.363891 \nC 69.669293 184.80131 68.906163 184.485211 68.110554 184.485211 \nC 67.314944 184.485211 66.551814 184.80131 65.989233 185.363891 \nC 65.426653 185.926471 65.110554 186.689602 65.110554 187.485211 \nC 65.110554 188.28082 65.426653 189.043951 65.989233 189.606531 \nC 66.551814 190.169112 67.314944 190.485211 68.110554 190.485211 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.706406 192.155749 \nC 71.502015 192.155749 72.265145 191.83965 72.827726 191.27707 \nC 73.390307 190.714489 73.706406 189.951359 73.706406 189.155749 \nC 73.706406 188.36014 73.390307 187.59701 72.827726 187.034429 \nC 72.265145 186.471848 71.502015 186.155749 70.706406 186.155749 \nC 69.910796 186.155749 69.147666 186.471848 68.585085 187.034429 \nC 68.022505 187.59701 67.706406 188.36014 67.706406 189.155749 \nC 67.706406 189.951359 68.022505 190.714489 68.585085 191.27707 \nC 69.147666 191.83965 69.910796 192.155749 70.706406 192.155749 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 183.579091 \nC 76.693719 183.579091 77.456849 183.262992 78.01943 182.700411 \nC 78.582011 182.137831 78.898109 181.3747 78.898109 180.579091 \nC 78.898109 179.783482 78.582011 179.020351 78.01943 178.457771 \nC 77.456849 177.89519 76.693719 177.579091 75.898109 177.579091 \nC 75.1025 177.579091 74.33937 177.89519 73.776789 178.457771 \nC 73.214208 179.020351 72.898109 179.783482 72.898109 180.579091 \nC 72.898109 181.3747 73.214208 182.137831 73.776789 182.700411 \nC 74.33937 183.262992 75.1025 183.579091 75.898109 183.579091 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 198.444346 \nC 87.077126 198.444346 87.840257 198.128247 88.402837 197.565666 \nC 88.965418 197.003085 89.281517 196.239955 89.281517 195.444346 \nC 89.281517 194.648736 88.965418 193.885606 88.402837 193.323025 \nC 87.840257 192.760445 87.077126 192.444346 86.281517 192.444346 \nC 85.485908 192.444346 84.722778 192.760445 84.160197 193.323025 \nC 83.597616 193.885606 83.281517 194.648736 83.281517 195.444346 \nC 83.281517 196.239955 83.597616 197.003085 84.160197 197.565666 \nC 84.722778 198.128247 85.485908 198.444346 86.281517 198.444346 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.085841 \nC 107.843942 20.085841 108.607072 19.769742 109.169653 19.207162 \nC 109.732233 18.644581 110.048332 17.881451 110.048332 17.085841 \nC 110.048332 16.290232 109.732233 15.527102 109.169653 14.964521 \nC 108.607072 14.40194 107.843942 14.085841 107.048332 14.085841 \nC 106.252723 14.085841 105.489593 14.40194 104.927012 14.964521 \nC 104.364431 15.527102 104.048332 16.290232 104.048332 17.085841 \nC 104.048332 17.881451 104.364431 18.644581 104.927012 19.207162 \nC 105.489593 19.769742 106.252723 20.085841 107.048332 20.085841 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 158.927786 \nL 66.596307 157.427786 \nL 68.096307 158.927786 \nL 69.596307 157.427786 \nL 68.096307 155.927786 \nL 69.596307 154.427786 \nL 68.096307 152.927786 \nL 66.596307 154.427786 \nL 65.096307 152.927786 \nL 63.596307 154.427786 \nL 65.096307 155.927786 \nL 63.596307 157.427786 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.456842 172.177452 \nL 66.956842 170.677452 \nL 68.456842 172.177452 \nL 69.956842 170.677452 \nL 68.456842 169.177452 \nL 69.956842 167.677452 \nL 68.456842 166.177452 \nL 66.956842 167.677452 \nL 65.456842 166.177452 \nL 63.956842 167.677452 \nL 65.456842 169.177452 \nL 63.956842 170.677452 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 187.072132 \nL 68.110554 185.572132 \nL 69.610554 187.072132 \nL 71.110554 185.572132 \nL 69.610554 184.072132 \nL 71.110554 182.572132 \nL 69.610554 181.072132 \nL 68.110554 182.572132 \nL 66.610554 181.072132 \nL 65.110554 182.572132 \nL 66.610554 184.072132 \nL 65.110554 185.572132 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.648546 146.490895 \nL 72.148546 144.990895 \nL 73.648546 146.490895 \nL 75.148546 144.990895 \nL 73.648546 143.490895 \nL 75.148546 141.990895 \nL 73.648546 140.490895 \nL 72.148546 141.990895 \nL 70.648546 140.490895 \nL 69.148546 141.990895 \nL 70.648546 143.490895 \nL 69.148546 144.990895 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 143.332399 130.05743 \nL 144.832399 128.55743 \nL 146.332399 130.05743 \nL 147.832399 128.55743 \nL 146.332399 127.05743 \nL 147.832399 125.55743 \nL 146.332399 124.05743 \nL 144.832399 125.55743 \nL 143.332399 124.05743 \nL 141.832399 125.55743 \nL 143.332399 127.05743 \nL 141.832399 128.55743 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.312628 194.464488 \nL 66.812628 192.964488 \nL 68.312628 194.464488 \nL 69.812628 192.964488 \nL 68.312628 191.464488 \nL 69.812628 189.964488 \nL 68.312628 188.464488 \nL 66.812628 189.964488 \nL 65.312628 188.464488 \nL 63.812628 189.964488 \nL 65.312628 191.464488 \nL 63.812628 192.964488 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 192.3481 \nL 68.110554 190.8481 \nL 69.610554 192.3481 \nL 71.110554 190.8481 \nL 69.610554 189.3481 \nL 71.110554 187.8481 \nL 69.610554 186.3481 \nL 68.110554 187.8481 \nL 66.610554 186.3481 \nL 65.110554 187.8481 \nL 66.610554 189.3481 \nL 65.110554 190.8481 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 69.206406 149.847457 \nL 70.706406 148.347457 \nL 72.206406 149.847457 \nL 73.706406 148.347457 \nL 72.206406 146.847457 \nL 73.706406 145.347457 \nL 72.206406 143.847457 \nL 70.706406 145.347457 \nL 69.206406 143.847457 \nL 67.706406 145.347457 \nL 69.206406 146.847457 \nL 67.706406 148.347457 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 190.620101 \nL 107.048332 189.120101 \nL 108.548332 190.620101 \nL 110.048332 189.120101 \nL 108.548332 187.620101 \nL 110.048332 186.120101 \nL 108.548332 184.620101 \nL 107.048332 186.120101 \nL 105.548332 184.620101 \nL 104.048332 186.120101 \nL 105.548332 187.620101 \nL 104.048332 189.120101 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 168.178699 \nC 67.391916 168.178699 68.155046 167.8626 68.717627 167.30002 \nC 69.280208 166.737439 69.596307 165.974309 69.596307 165.178699 \nC 69.596307 164.38309 69.280208 163.61996 68.717627 163.057379 \nC 68.155046 162.494798 67.391916 162.178699 66.596307 162.178699 \nC 65.800698 162.178699 65.037567 162.494798 64.474986 163.057379 \nC 63.912406 163.61996 63.596307 164.38309 63.596307 165.178699 \nC 63.596307 165.974309 63.912406 166.737439 64.474986 167.30002 \nC 65.037567 167.8626 65.800698 168.178699 66.596307 168.178699 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 197.527892 \nC 67.752451 197.527892 68.515581 197.211793 69.078162 196.649212 \nC 69.640743 196.086631 69.956842 195.323501 69.956842 194.527892 \nC 69.956842 193.732282 69.640743 192.969152 69.078162 192.406571 \nC 68.515581 191.84399 67.752451 191.527892 66.956842 191.527892 \nC 66.161233 191.527892 65.398102 191.84399 64.835521 192.406571 \nC 64.272941 192.969152 63.956842 193.732282 63.956842 194.527892 \nC 63.956842 195.323501 64.272941 196.086631 64.835521 196.649212 \nC 65.398102 197.211793 66.161233 197.527892 66.956842 197.527892 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 163.459599 \nC 68.906163 163.459599 69.669293 163.1435 70.231874 162.58092 \nC 70.794455 162.018339 71.110554 161.255209 71.110554 160.459599 \nC 71.110554 159.66399 70.794455 158.90086 70.231874 158.338279 \nC 69.669293 157.775698 68.906163 157.459599 68.110554 157.459599 \nC 67.314944 157.459599 66.551814 157.775698 65.989233 158.338279 \nC 65.426653 158.90086 65.110554 159.66399 65.110554 160.459599 \nC 65.110554 161.255209 65.426653 162.018339 65.989233 162.58092 \nC 66.551814 163.1435 67.314944 163.459599 68.110554 163.459599 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 181.543124 \nC 72.944155 181.543124 73.707285 181.227025 74.269866 180.664444 \nC 74.832447 180.101863 75.148546 179.338733 75.148546 178.543124 \nC 75.148546 177.747514 74.832447 176.984384 74.269866 176.421803 \nC 73.707285 175.859223 72.944155 175.543124 72.148546 175.543124 \nC 71.352936 175.543124 70.589806 175.859223 70.027225 176.421803 \nC 69.464645 176.984384 69.148546 177.747514 69.148546 178.543124 \nC 69.148546 179.338733 69.464645 180.101863 70.027225 180.664444 \nC 70.589806 181.227025 71.352936 181.543124 72.148546 181.543124 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 170.666187 \nC 87.94241 170.666187 88.705541 170.350088 89.268121 169.787507 \nC 89.830702 169.224927 90.146801 168.461796 90.146801 167.666187 \nC 90.146801 166.870578 89.830702 166.107447 89.268121 165.544867 \nC 88.705541 164.982286 87.94241 164.666187 87.146801 164.666187 \nC 86.351192 164.666187 85.588061 164.982286 85.025481 165.544867 \nC 84.4629 166.107447 84.146801 166.870578 84.146801 167.666187 \nC 84.146801 168.461796 84.4629 169.224927 85.025481 169.787507 \nC 85.588061 170.350088 86.351192 170.666187 87.146801 170.666187 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 197.270651 \nC 145.628008 197.270651 146.391139 196.954552 146.953719 196.391972 \nC 147.5163 195.829391 147.832399 195.066261 147.832399 194.270651 \nC 147.832399 193.475042 147.5163 192.711912 146.953719 192.149331 \nC 146.391139 191.58675 145.628008 191.270651 144.832399 191.270651 \nC 144.03679 191.270651 143.273659 191.58675 142.711079 192.149331 \nC 142.148498 192.711912 141.832399 193.475042 141.832399 194.270651 \nC 141.832399 195.066261 142.148498 195.829391 142.711079 196.391972 \nC 143.273659 196.954552 144.03679 197.270651 144.832399 197.270651 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 188.692175 \nC 371.755552 188.692175 372.518683 188.376076 373.081264 187.813495 \nC 373.643844 187.250914 373.959943 186.487784 373.959943 185.692175 \nC 373.959943 184.896565 373.643844 184.133435 373.081264 183.570854 \nC 372.518683 183.008273 371.755552 182.692175 370.959943 182.692175 \nC 370.164334 182.692175 369.401204 183.008273 368.838623 183.570854 \nC 368.276042 184.133435 367.959943 184.896565 367.959943 185.692175 \nC 367.959943 186.487784 368.276042 187.250914 368.838623 187.813495 \nC 369.401204 188.376076 370.164334 188.692175 370.959943 188.692175 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.812628 196.85211 \nC 67.608237 196.85211 68.371367 196.536012 68.933948 195.973431 \nC 69.496529 195.41085 69.812628 194.64772 69.812628 193.85211 \nC 69.812628 193.056501 69.496529 192.293371 68.933948 191.73079 \nC 68.371367 191.168209 67.608237 190.85211 66.812628 190.85211 \nC 66.017019 190.85211 65.253888 191.168209 64.691307 191.73079 \nC 64.128727 192.293371 63.812628 193.056501 63.812628 193.85211 \nC 63.812628 194.64772 64.128727 195.41085 64.691307 195.973431 \nC 65.253888 196.536012 66.017019 196.85211 66.812628 196.85211 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 189.954014 \nC 68.906163 189.954014 69.669293 189.637915 70.231874 189.075335 \nC 70.794455 188.512754 71.110554 187.749623 71.110554 186.954014 \nC 71.110554 186.158405 70.794455 185.395275 70.231874 184.832694 \nC 69.669293 184.270113 68.906163 183.954014 68.110554 183.954014 \nC 67.314944 183.954014 66.551814 184.270113 65.989233 184.832694 \nC 65.426653 185.395275 65.110554 186.158405 65.110554 186.954014 \nC 65.110554 187.749623 65.426653 188.512754 65.989233 189.075335 \nC 66.551814 189.637915 67.314944 189.954014 68.110554 189.954014 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.706406 194.845435 \nC 71.502015 194.845435 72.265145 194.529336 72.827726 193.966756 \nC 73.390307 193.404175 73.706406 192.641044 73.706406 191.845435 \nC 73.706406 191.049826 73.390307 190.286696 72.827726 189.724115 \nC 72.265145 189.161534 71.502015 188.845435 70.706406 188.845435 \nC 69.910796 188.845435 69.147666 189.161534 68.585085 189.724115 \nC 68.022505 190.286696 67.706406 191.049826 67.706406 191.845435 \nC 67.706406 192.641044 68.022505 193.404175 68.585085 193.966756 \nC 69.147666 194.529336 69.910796 194.845435 70.706406 194.845435 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 140.568256 \nC 76.693719 140.568256 77.456849 140.252157 78.01943 139.689576 \nC 78.582011 139.126996 78.898109 138.363865 78.898109 137.568256 \nC 78.898109 136.772647 78.582011 136.009516 78.01943 135.446936 \nC 77.456849 134.884355 76.693719 134.568256 75.898109 134.568256 \nC 75.1025 134.568256 74.33937 134.884355 73.776789 135.446936 \nC 73.214208 136.009516 72.898109 136.772647 72.898109 137.568256 \nC 72.898109 138.363865 73.214208 139.126996 73.776789 139.689576 \nC 74.33937 140.252157 75.1025 140.568256 75.898109 140.568256 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 199.443228 \nC 87.077126 199.443228 87.840257 199.127129 88.402837 198.564549 \nC 88.965418 198.001968 89.281517 197.238838 89.281517 196.443228 \nC 89.281517 195.647619 88.965418 194.884489 88.402837 194.321908 \nC 87.840257 193.759327 87.077126 193.443228 86.281517 193.443228 \nC 85.485908 193.443228 84.722778 193.759327 84.160197 194.321908 \nC 83.597616 194.884489 83.281517 195.647619 83.281517 196.443228 \nC 83.281517 197.238838 83.597616 198.001968 84.160197 198.564549 \nC 84.722778 199.127129 85.485908 199.443228 86.281517 199.443228 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.085287 \nC 107.843942 20.085287 108.607072 19.769188 109.169653 19.206608 \nC 109.732233 18.644027 110.048332 17.880897 110.048332 17.085287 \nC 110.048332 16.289678 109.732233 15.526548 109.169653 14.963967 \nC 108.607072 14.401386 107.843942 14.085287 107.048332 14.085287 \nC 106.252723 14.085287 105.489593 14.401386 104.927012 14.963967 \nC 104.364431 15.526548 104.048332 16.289678 104.048332 17.085287 \nC 104.048332 17.880897 104.364431 18.644027 104.927012 19.206608 \nC 105.489593 19.769188 106.252723 20.085287 107.048332 20.085287 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 193.902221 \nL 66.596307 192.402221 \nL 68.096307 193.902221 \nL 69.596307 192.402221 \nL 68.096307 190.902221 \nL 69.596307 189.402221 \nL 68.096307 187.902221 \nL 66.596307 189.402221 \nL 65.096307 187.902221 \nL 63.596307 189.402221 \nL 65.096307 190.902221 \nL 63.596307 192.402221 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.456842 192.233756 \nL 66.956842 190.733756 \nL 68.456842 192.233756 \nL 69.956842 190.733756 \nL 68.456842 189.233756 \nL 69.956842 187.733756 \nL 68.456842 186.233756 \nL 66.956842 187.733756 \nL 65.456842 186.233756 \nL 63.956842 187.733756 \nL 65.456842 189.233756 \nL 63.956842 190.733756 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 178.349255 \nL 68.110554 176.849255 \nL 69.610554 178.349255 \nL 71.110554 176.849255 \nL 69.610554 175.349255 \nL 71.110554 173.849255 \nL 69.610554 172.349255 \nL 68.110554 173.849255 \nL 66.610554 172.349255 \nL 65.110554 173.849255 \nL 66.610554 175.349255 \nL 65.110554 176.849255 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 85.646801 187.586463 \nL 87.146801 186.086463 \nL 88.646801 187.586463 \nL 90.146801 186.086463 \nL 88.646801 184.586463 \nL 90.146801 183.086463 \nL 88.646801 181.586463 \nL 87.146801 183.086463 \nL 85.646801 181.586463 \nL 84.146801 183.086463 \nL 85.646801 184.586463 \nL 84.146801 186.086463 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 143.332399 124.278786 \nL 144.832399 122.778786 \nL 146.332399 124.278786 \nL 147.832399 122.778786 \nL 146.332399 121.278786 \nL 147.832399 119.778786 \nL 146.332399 118.278786 \nL 144.832399 119.778786 \nL 143.332399 118.278786 \nL 141.832399 119.778786 \nL 143.332399 121.278786 \nL 141.832399 122.778786 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.312628 173.533998 \nL 66.812628 172.033998 \nL 68.312628 173.533998 \nL 69.812628 172.033998 \nL 68.312628 170.533998 \nL 69.812628 169.033998 \nL 68.312628 167.533998 \nL 66.812628 169.033998 \nL 65.312628 167.533998 \nL 63.812628 169.033998 \nL 65.312628 170.533998 \nL 63.812628 172.033998 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 69.206406 158.25078 \nL 70.706406 156.75078 \nL 72.206406 158.25078 \nL 73.706406 156.75078 \nL 72.206406 155.25078 \nL 73.706406 153.75078 \nL 72.206406 152.25078 \nL 70.706406 153.75078 \nL 69.206406 152.25078 \nL 67.706406 153.75078 \nL 69.206406 155.25078 \nL 67.706406 156.75078 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 74.398109 167.684609 \nL 75.898109 166.184609 \nL 77.398109 167.684609 \nL 78.898109 166.184609 \nL 77.398109 164.684609 \nL 78.898109 163.184609 \nL 77.398109 161.684609 \nL 75.898109 163.184609 \nL 74.398109 161.684609 \nL 72.898109 163.184609 \nL 74.398109 164.684609 \nL 72.898109 166.184609 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 84.781517 172.85518 \nL 86.281517 171.35518 \nL 87.781517 172.85518 \nL 89.281517 171.35518 \nL 87.781517 169.85518 \nL 89.281517 168.35518 \nL 87.781517 166.85518 \nL 86.281517 168.35518 \nL 84.781517 166.85518 \nL 83.281517 168.35518 \nL 84.781517 169.85518 \nL 83.281517 171.35518 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 189.506281 \nL 107.048332 188.006281 \nL 108.548332 189.506281 \nL 110.048332 188.006281 \nL 108.548332 186.506281 \nL 110.048332 185.006281 \nL 108.548332 183.506281 \nL 107.048332 185.006281 \nL 105.548332 183.506281 \nL 104.048332 185.006281 \nL 105.548332 186.506281 \nL 104.048332 188.006281 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 167.729028 \nC 67.391916 167.729028 68.155046 167.41293 68.717627 166.850349 \nC 69.280208 166.287768 69.596307 165.524638 69.596307 164.729028 \nC 69.596307 163.933419 69.280208 163.170289 68.717627 162.607708 \nC 68.155046 162.045127 67.391916 161.729028 66.596307 161.729028 \nC 65.800698 161.729028 65.037567 162.045127 64.474986 162.607708 \nC 63.912406 163.170289 63.596307 163.933419 63.596307 164.729028 \nC 63.596307 165.524638 63.912406 166.287768 64.474986 166.850349 \nC 65.037567 167.41293 65.800698 167.729028 66.596307 167.729028 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 185.315233 \nC 67.752451 185.315233 68.515581 184.999134 69.078162 184.436553 \nC 69.640743 183.873972 69.956842 183.110842 69.956842 182.315233 \nC 69.956842 181.519623 69.640743 180.756493 69.078162 180.193912 \nC 68.515581 179.631332 67.752451 179.315233 66.956842 179.315233 \nC 66.161233 179.315233 65.398102 179.631332 64.835521 180.193912 \nC 64.272941 180.756493 63.956842 181.519623 63.956842 182.315233 \nC 63.956842 183.110842 64.272941 183.873972 64.835521 184.436553 \nC 65.398102 184.999134 66.161233 185.315233 66.956842 185.315233 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 188.694624 \nC 68.906163 188.694624 69.669293 188.378525 70.231874 187.815944 \nC 70.794455 187.253363 71.110554 186.490233 71.110554 185.694624 \nC 71.110554 184.899015 70.794455 184.135884 70.231874 183.573303 \nC 69.669293 183.010723 68.906163 182.694624 68.110554 182.694624 \nC 67.314944 182.694624 66.551814 183.010723 65.989233 183.573303 \nC 65.426653 184.135884 65.110554 184.899015 65.110554 185.694624 \nC 65.110554 186.490233 65.426653 187.253363 65.989233 187.815944 \nC 66.551814 188.378525 67.314944 188.694624 68.110554 188.694624 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 183.489485 \nC 72.944155 183.489485 73.707285 183.173387 74.269866 182.610806 \nC 74.832447 182.048225 75.148546 181.285095 75.148546 180.489485 \nC 75.148546 179.693876 74.832447 178.930746 74.269866 178.368165 \nC 73.707285 177.805584 72.944155 177.489485 72.148546 177.489485 \nC 71.352936 177.489485 70.589806 177.805584 70.027225 178.368165 \nC 69.464645 178.930746 69.148546 179.693876 69.148546 180.489485 \nC 69.148546 181.285095 69.464645 182.048225 70.027225 182.610806 \nC 70.589806 183.173387 71.352936 183.489485 72.148546 183.489485 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 181.491781 \nC 87.94241 181.491781 88.705541 181.175682 89.268121 180.613101 \nC 89.830702 180.05052 90.146801 179.28739 90.146801 178.491781 \nC 90.146801 177.696171 89.830702 176.933041 89.268121 176.37046 \nC 88.705541 175.80788 87.94241 175.491781 87.146801 175.491781 \nC 86.351192 175.491781 85.588061 175.80788 85.025481 176.37046 \nC 84.4629 176.933041 84.146801 177.696171 84.146801 178.491781 \nC 84.146801 179.28739 84.4629 180.05052 85.025481 180.613101 \nC 85.588061 181.175682 86.351192 181.491781 87.146801 181.491781 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 187.133159 \nC 145.628008 187.133159 146.391139 186.817061 146.953719 186.25448 \nC 147.5163 185.691899 147.832399 184.928769 147.832399 184.133159 \nC 147.832399 183.33755 147.5163 182.57442 146.953719 182.011839 \nC 146.391139 181.449258 145.628008 181.133159 144.832399 181.133159 \nC 144.03679 181.133159 143.273659 181.449258 142.711079 182.011839 \nC 142.148498 182.57442 141.832399 183.33755 141.832399 184.133159 \nC 141.832399 184.928769 142.148498 185.691899 142.711079 186.25448 \nC 143.273659 186.817061 144.03679 187.133159 144.832399 187.133159 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 190.710988 \nC 371.755552 190.710988 372.518683 190.394889 373.081264 189.832309 \nC 373.643844 189.269728 373.959943 188.506598 373.959943 187.710988 \nC 373.959943 186.915379 373.643844 186.152249 373.081264 185.589668 \nC 372.518683 185.027087 371.755552 184.710988 370.959943 184.710988 \nC 370.164334 184.710988 369.401204 185.027087 368.838623 185.589668 \nC 368.276042 186.152249 367.959943 186.915379 367.959943 187.710988 \nC 367.959943 188.506598 368.276042 189.269728 368.838623 189.832309 \nC 369.401204 190.394889 370.164334 190.710988 370.959943 190.710988 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 200.766098 \nC 68.906163 200.766098 69.669293 200.449999 70.231874 199.887419 \nC 70.794455 199.324838 71.110554 198.561708 71.110554 197.766098 \nC 71.110554 196.970489 70.794455 196.207359 70.231874 195.644778 \nC 69.669293 195.082197 68.906163 194.766098 68.110554 194.766098 \nC 67.314944 194.766098 66.551814 195.082197 65.989233 195.644778 \nC 65.426653 196.207359 65.110554 196.970489 65.110554 197.766098 \nC 65.110554 198.561708 65.426653 199.324838 65.989233 199.887419 \nC 66.551814 200.449999 67.314944 200.766098 68.110554 200.766098 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.706406 180.865608 \nC 71.502015 180.865608 72.265145 180.549509 72.827726 179.986929 \nC 73.390307 179.424348 73.706406 178.661218 73.706406 177.865608 \nC 73.706406 177.069999 73.390307 176.306869 72.827726 175.744288 \nC 72.265145 175.181707 71.502015 174.865608 70.706406 174.865608 \nC 69.910796 174.865608 69.147666 175.181707 68.585085 175.744288 \nC 68.022505 176.306869 67.706406 177.069999 67.706406 177.865608 \nC 67.706406 178.661218 68.022505 179.424348 68.585085 179.986929 \nC 69.147666 180.549509 69.910796 180.865608 70.706406 180.865608 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 147.9548 \nC 76.693719 147.9548 77.456849 147.638701 78.01943 147.07612 \nC 78.582011 146.51354 78.898109 145.750409 78.898109 144.9548 \nC 78.898109 144.159191 78.582011 143.396061 78.01943 142.83348 \nC 77.456849 142.270899 76.693719 141.9548 75.898109 141.9548 \nC 75.1025 141.9548 74.33937 142.270899 73.776789 142.83348 \nC 73.214208 143.396061 72.898109 144.159191 72.898109 144.9548 \nC 72.898109 145.750409 73.214208 146.51354 73.776789 147.07612 \nC 74.33937 147.638701 75.1025 147.9548 75.898109 147.9548 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 184.453039 \nC 87.077126 184.453039 87.840257 184.13694 88.402837 183.574359 \nC 88.965418 183.011779 89.281517 182.248648 89.281517 181.453039 \nC 89.281517 180.65743 88.965418 179.894299 88.402837 179.331719 \nC 87.840257 178.769138 87.077126 178.453039 86.281517 178.453039 \nC 85.485908 178.453039 84.722778 178.769138 84.160197 179.331719 \nC 83.597616 179.894299 83.281517 180.65743 83.281517 181.453039 \nC 83.281517 182.248648 83.597616 183.011779 84.160197 183.574359 \nC 84.722778 184.13694 85.485908 184.453039 86.281517 184.453039 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.084169 \nC 107.843942 20.084169 108.607072 19.76807 109.169653 19.205489 \nC 109.732233 18.642908 110.048332 17.879778 110.048332 17.084169 \nC 110.048332 16.288559 109.732233 15.525429 109.169653 14.962848 \nC 108.607072 14.400268 107.843942 14.084169 107.048332 14.084169 \nC 106.252723 14.084169 105.489593 14.400268 104.927012 14.962848 \nC 104.364431 15.525429 104.048332 16.288559 104.048332 17.084169 \nC 104.048332 17.879778 104.364431 18.642908 104.927012 19.205489 \nC 105.489593 19.76807 106.252723 20.084169 107.048332 20.084169 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 178.140362 \nL 66.596307 176.640362 \nL 68.096307 178.140362 \nL 69.596307 176.640362 \nL 68.096307 175.140362 \nL 69.596307 173.640362 \nL 68.096307 172.140362 \nL 66.596307 173.640362 \nL 65.096307 172.140362 \nL 63.596307 173.640362 \nL 65.096307 175.140362 \nL 63.596307 176.640362 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 189.222998 \nL 68.110554 187.722998 \nL 69.610554 189.222998 \nL 71.110554 187.722998 \nL 69.610554 186.222998 \nL 71.110554 184.722998 \nL 69.610554 183.222998 \nL 68.110554 184.722998 \nL 66.610554 183.222998 \nL 65.110554 184.722998 \nL 66.610554 186.222998 \nL 65.110554 187.722998 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.648546 197.973575 \nL 72.148546 196.473575 \nL 73.648546 197.973575 \nL 75.148546 196.473575 \nL 73.648546 194.973575 \nL 75.148546 193.473575 \nL 73.648546 191.973575 \nL 72.148546 193.473575 \nL 70.648546 191.973575 \nL 69.148546 193.473575 \nL 70.648546 194.973575 \nL 69.148546 196.473575 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 85.646801 187.999144 \nL 87.146801 186.499144 \nL 88.646801 187.999144 \nL 90.146801 186.499144 \nL 88.646801 184.999144 \nL 90.146801 183.499144 \nL 88.646801 181.999144 \nL 87.146801 183.499144 \nL 85.646801 181.999144 \nL 84.146801 183.499144 \nL 85.646801 184.999144 \nL 84.146801 186.499144 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 143.332399 142.878026 \nL 144.832399 141.378026 \nL 146.332399 142.878026 \nL 147.832399 141.378026 \nL 146.332399 139.878026 \nL 147.832399 138.378026 \nL 146.332399 136.878026 \nL 144.832399 138.378026 \nL 143.332399 136.878026 \nL 141.832399 138.378026 \nL 143.332399 139.878026 \nL 141.832399 141.378026 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 69.206406 189.645214 \nL 70.706406 188.145214 \nL 72.206406 189.645214 \nL 73.706406 188.145214 \nL 72.206406 186.645214 \nL 73.706406 185.145214 \nL 72.206406 183.645214 \nL 70.706406 185.145214 \nL 69.206406 183.645214 \nL 67.706406 185.145214 \nL 69.206406 186.645214 \nL 67.706406 188.145214 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 84.781517 182.603388 \nL 86.281517 181.103388 \nL 87.781517 182.603388 \nL 89.281517 181.103388 \nL 87.781517 179.603388 \nL 89.281517 178.103388 \nL 87.781517 176.603388 \nL 86.281517 178.103388 \nL 84.781517 176.603388 \nL 83.281517 178.103388 \nL 84.781517 179.603388 \nL 83.281517 181.103388 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 217.756364 \nL 107.048332 216.256364 \nL 108.548332 217.756364 \nL 110.048332 216.256364 \nL 108.548332 214.756364 \nL 110.048332 213.256364 \nL 108.548332 211.756364 \nL 107.048332 213.256364 \nL 105.548332 211.756364 \nL 104.048332 213.256364 \nL 105.548332 214.756364 \nL 104.048332 216.256364 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 188.03715 \nC 67.391916 188.03715 68.155046 187.721051 68.717627 187.15847 \nC 69.280208 186.595889 69.596307 185.832759 69.596307 185.03715 \nC 69.596307 184.241541 69.280208 183.47841 68.717627 182.915829 \nC 68.155046 182.353249 67.391916 182.03715 66.596307 182.03715 \nC 65.800698 182.03715 65.037567 182.353249 64.474986 182.915829 \nC 63.912406 183.47841 63.596307 184.241541 63.596307 185.03715 \nC 63.596307 185.832759 63.912406 186.595889 64.474986 187.15847 \nC 65.037567 187.721051 65.800698 188.03715 66.596307 188.03715 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 179.039739 \nC 67.752451 179.039739 68.515581 178.72364 69.078162 178.161059 \nC 69.640743 177.598478 69.956842 176.835348 69.956842 176.039739 \nC 69.956842 175.24413 69.640743 174.480999 69.078162 173.918418 \nC 68.515581 173.355838 67.752451 173.039739 66.956842 173.039739 \nC 66.161233 173.039739 65.398102 173.355838 64.835521 173.918418 \nC 64.272941 174.480999 63.956842 175.24413 63.956842 176.039739 \nC 63.956842 176.835348 64.272941 177.598478 64.835521 178.161059 \nC 65.398102 178.72364 66.161233 179.039739 66.956842 179.039739 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 192.640946 \nC 68.906163 192.640946 69.669293 192.324847 70.231874 191.762266 \nC 70.794455 191.199686 71.110554 190.436555 71.110554 189.640946 \nC 71.110554 188.845337 70.794455 188.082206 70.231874 187.519626 \nC 69.669293 186.957045 68.906163 186.640946 68.110554 186.640946 \nC 67.314944 186.640946 66.551814 186.957045 65.989233 187.519626 \nC 65.426653 188.082206 65.110554 188.845337 65.110554 189.640946 \nC 65.110554 190.436555 65.426653 191.199686 65.989233 191.762266 \nC 66.551814 192.324847 67.314944 192.640946 68.110554 192.640946 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 192.886201 \nC 72.944155 192.886201 73.707285 192.570102 74.269866 192.007522 \nC 74.832447 191.444941 75.148546 190.681811 75.148546 189.886201 \nC 75.148546 189.090592 74.832447 188.327462 74.269866 187.764881 \nC 73.707285 187.2023 72.944155 186.886201 72.148546 186.886201 \nC 71.352936 186.886201 70.589806 187.2023 70.027225 187.764881 \nC 69.464645 188.327462 69.148546 189.090592 69.148546 189.886201 \nC 69.148546 190.681811 69.464645 191.444941 70.027225 192.007522 \nC 70.589806 192.570102 71.352936 192.886201 72.148546 192.886201 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 163.257669 \nC 87.94241 163.257669 88.705541 162.94157 89.268121 162.37899 \nC 89.830702 161.816409 90.146801 161.053279 90.146801 160.257669 \nC 90.146801 159.46206 89.830702 158.69893 89.268121 158.136349 \nC 88.705541 157.573768 87.94241 157.257669 87.146801 157.257669 \nC 86.351192 157.257669 85.588061 157.573768 85.025481 158.136349 \nC 84.4629 158.69893 84.146801 159.46206 84.146801 160.257669 \nC 84.146801 161.053279 84.4629 161.816409 85.025481 162.37899 \nC 85.588061 162.94157 86.351192 163.257669 87.146801 163.257669 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 176.93097 \nC 145.628008 176.93097 146.391139 176.614871 146.953719 176.05229 \nC 147.5163 175.489709 147.832399 174.726579 147.832399 173.93097 \nC 147.832399 173.13536 147.5163 172.37223 146.953719 171.809649 \nC 146.391139 171.247069 145.628008 170.93097 144.832399 170.93097 \nC 144.03679 170.93097 143.273659 171.247069 142.711079 171.809649 \nC 142.148498 172.37223 141.832399 173.13536 141.832399 173.93097 \nC 141.832399 174.726579 142.148498 175.489709 142.711079 176.05229 \nC 143.273659 176.614871 144.03679 176.93097 144.832399 176.93097 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 191.14104 \nC 371.755552 191.14104 372.518683 190.824941 373.081264 190.262361 \nC 373.643844 189.69978 373.959943 188.93665 373.959943 188.14104 \nC 373.959943 187.345431 373.643844 186.582301 373.081264 186.01972 \nC 372.518683 185.457139 371.755552 185.14104 370.959943 185.14104 \nC 370.164334 185.14104 369.401204 185.457139 368.838623 186.01972 \nC 368.276042 186.582301 367.959943 187.345431 367.959943 188.14104 \nC 367.959943 188.93665 368.276042 189.69978 368.838623 190.262361 \nC 369.401204 190.824941 370.164334 191.14104 370.959943 191.14104 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 189.864947 \nC 68.906163 189.864947 69.669293 189.548848 70.231874 188.986268 \nC 70.794455 188.423687 71.110554 187.660556 71.110554 186.864947 \nC 71.110554 186.069338 70.794455 185.306208 70.231874 184.743627 \nC 69.669293 184.181046 68.906163 183.864947 68.110554 183.864947 \nC 67.314944 183.864947 66.551814 184.181046 65.989233 184.743627 \nC 65.426653 185.306208 65.110554 186.069338 65.110554 186.864947 \nC 65.110554 187.660556 65.426653 188.423687 65.989233 188.986268 \nC 66.551814 189.548848 67.314944 189.864947 68.110554 189.864947 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.706406 172.244895 \nC 71.502015 172.244895 72.265145 171.928796 72.827726 171.366215 \nC 73.390307 170.803635 73.706406 170.040504 73.706406 169.244895 \nC 73.706406 168.449286 73.390307 167.686155 72.827726 167.123575 \nC 72.265145 166.560994 71.502015 166.244895 70.706406 166.244895 \nC 69.910796 166.244895 69.147666 166.560994 68.585085 167.123575 \nC 68.022505 167.686155 67.706406 168.449286 67.706406 169.244895 \nC 67.706406 170.040504 68.022505 170.803635 68.585085 171.366215 \nC 69.147666 171.928796 69.910796 172.244895 70.706406 172.244895 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 185.869322 \nC 76.693719 185.869322 77.456849 185.553224 78.01943 184.990643 \nC 78.582011 184.428062 78.898109 183.664932 78.898109 182.869322 \nC 78.898109 182.073713 78.582011 181.310583 78.01943 180.748002 \nC 77.456849 180.185421 76.693719 179.869322 75.898109 179.869322 \nC 75.1025 179.869322 74.33937 180.185421 73.776789 180.748002 \nC 73.214208 181.310583 72.898109 182.073713 72.898109 182.869322 \nC 72.898109 183.664932 73.214208 184.428062 73.776789 184.990643 \nC 74.33937 185.553224 75.1025 185.869322 75.898109 185.869322 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 175.958941 \nC 87.077126 175.958941 87.840257 175.642842 88.402837 175.080262 \nC 88.965418 174.517681 89.281517 173.75455 89.281517 172.958941 \nC 89.281517 172.163332 88.965418 171.400202 88.402837 170.837621 \nC 87.840257 170.27504 87.077126 169.958941 86.281517 169.958941 \nC 85.485908 169.958941 84.722778 170.27504 84.160197 170.837621 \nC 83.597616 171.400202 83.281517 172.163332 83.281517 172.958941 \nC 83.281517 173.75455 83.597616 174.517681 84.160197 175.080262 \nC 84.722778 175.642842 85.485908 175.958941 86.281517 175.958941 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.084513 \nC 107.843942 20.084513 108.607072 19.768414 109.169653 19.205833 \nC 109.732233 18.643253 110.048332 17.880122 110.048332 17.084513 \nC 110.048332 16.288904 109.732233 15.525773 109.169653 14.963193 \nC 108.607072 14.400612 107.843942 14.084513 107.048332 14.084513 \nC 106.252723 14.084513 105.489593 14.400612 104.927012 14.963193 \nC 104.364431 15.525773 104.048332 16.288904 104.048332 17.084513 \nC 104.048332 17.880122 104.364431 18.643253 104.927012 19.205833 \nC 105.489593 19.768414 106.252723 20.084513 107.048332 20.084513 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 184.308132 \nL 66.596307 182.808132 \nL 68.096307 184.308132 \nL 69.596307 182.808132 \nL 68.096307 181.308132 \nL 69.596307 179.808132 \nL 68.096307 178.308132 \nL 66.596307 179.808132 \nL 65.096307 178.308132 \nL 63.596307 179.808132 \nL 65.096307 181.308132 \nL 63.596307 182.808132 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.456842 192.32434 \nL 66.956842 190.82434 \nL 68.456842 192.32434 \nL 69.956842 190.82434 \nL 68.456842 189.32434 \nL 69.956842 187.82434 \nL 68.456842 186.32434 \nL 66.956842 187.82434 \nL 65.456842 186.32434 \nL 63.956842 187.82434 \nL 65.456842 189.32434 \nL 63.956842 190.82434 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 175.629763 \nL 68.110554 174.129763 \nL 69.610554 175.629763 \nL 71.110554 174.129763 \nL 69.610554 172.629763 \nL 71.110554 171.129763 \nL 69.610554 169.629763 \nL 68.110554 171.129763 \nL 66.610554 169.629763 \nL 65.110554 171.129763 \nL 66.610554 172.629763 \nL 65.110554 174.129763 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.648546 201.019351 \nL 72.148546 199.519351 \nL 73.648546 201.019351 \nL 75.148546 199.519351 \nL 73.648546 198.019351 \nL 75.148546 196.519351 \nL 73.648546 195.019351 \nL 72.148546 196.519351 \nL 70.648546 195.019351 \nL 69.148546 196.519351 \nL 70.648546 198.019351 \nL 69.148546 199.519351 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 143.332399 131.700271 \nL 144.832399 130.200271 \nL 146.332399 131.700271 \nL 147.832399 130.200271 \nL 146.332399 128.700271 \nL 147.832399 127.200271 \nL 146.332399 125.700271 \nL 144.832399 127.200271 \nL 143.332399 125.700271 \nL 141.832399 127.200271 \nL 143.332399 128.700271 \nL 141.832399 130.200271 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 369.459943 184.347359 \nL 370.959943 182.847359 \nL 372.459943 184.347359 \nL 373.959943 182.847359 \nL 372.459943 181.347359 \nL 373.959943 179.847359 \nL 372.459943 178.347359 \nL 370.959943 179.847359 \nL 369.459943 178.347359 \nL 367.959943 179.847359 \nL 369.459943 181.347359 \nL 367.959943 182.847359 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.312628 178.466806 \nL 66.812628 176.966806 \nL 68.312628 178.466806 \nL 69.812628 176.966806 \nL 68.312628 175.466806 \nL 69.812628 173.966806 \nL 68.312628 172.466806 \nL 66.812628 173.966806 \nL 65.312628 172.466806 \nL 63.812628 173.966806 \nL 65.312628 175.466806 \nL 63.812628 176.966806 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 74.398109 160.072504 \nL 75.898109 158.572504 \nL 77.398109 160.072504 \nL 78.898109 158.572504 \nL 77.398109 157.072504 \nL 78.898109 155.572504 \nL 77.398109 154.072504 \nL 75.898109 155.572504 \nL 74.398109 154.072504 \nL 72.898109 155.572504 \nL 74.398109 157.072504 \nL 72.898109 158.572504 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 84.781517 179.457482 \nL 86.281517 177.957482 \nL 87.781517 179.457482 \nL 89.281517 177.957482 \nL 87.781517 176.457482 \nL 89.281517 174.957482 \nL 87.781517 173.457482 \nL 86.281517 174.957482 \nL 84.781517 173.457482 \nL 83.281517 174.957482 \nL 84.781517 176.457482 \nL 83.281517 177.957482 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 178.690278 \nL 107.048332 177.190278 \nL 108.548332 178.690278 \nL 110.048332 177.190278 \nL 108.548332 175.690278 \nL 110.048332 174.190278 \nL 108.548332 172.690278 \nL 107.048332 174.190278 \nL 105.548332 172.690278 \nL 104.048332 174.190278 \nL 105.548332 175.690278 \nL 104.048332 177.190278 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 167.664512 \nC 67.391916 167.664512 68.155046 167.348413 68.717627 166.785832 \nC 69.280208 166.223251 69.596307 165.460121 69.596307 164.664512 \nC 69.596307 163.868902 69.280208 163.105772 68.717627 162.543191 \nC 68.155046 161.980611 67.391916 161.664512 66.596307 161.664512 \nC 65.800698 161.664512 65.037567 161.980611 64.474986 162.543191 \nC 63.912406 163.105772 63.596307 163.868902 63.596307 164.664512 \nC 63.596307 165.460121 63.912406 166.223251 64.474986 166.785832 \nC 65.037567 167.348413 65.800698 167.664512 66.596307 167.664512 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 185.017923 \nC 67.752451 185.017923 68.515581 184.701824 69.078162 184.139243 \nC 69.640743 183.576662 69.956842 182.813532 69.956842 182.017923 \nC 69.956842 181.222313 69.640743 180.459183 69.078162 179.896602 \nC 68.515581 179.334021 67.752451 179.017923 66.956842 179.017923 \nC 66.161233 179.017923 65.398102 179.334021 64.835521 179.896602 \nC 64.272941 180.459183 63.956842 181.222313 63.956842 182.017923 \nC 63.956842 182.813532 64.272941 183.576662 64.835521 184.139243 \nC 65.398102 184.701824 66.161233 185.017923 66.956842 185.017923 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 173.830994 \nC 68.906163 173.830994 69.669293 173.514895 70.231874 172.952314 \nC 70.794455 172.389733 71.110554 171.626603 71.110554 170.830994 \nC 71.110554 170.035385 70.794455 169.272254 70.231874 168.709673 \nC 69.669293 168.147093 68.906163 167.830994 68.110554 167.830994 \nC 67.314944 167.830994 66.551814 168.147093 65.989233 168.709673 \nC 65.426653 169.272254 65.110554 170.035385 65.110554 170.830994 \nC 65.110554 171.626603 65.426653 172.389733 65.989233 172.952314 \nC 66.551814 173.514895 67.314944 173.830994 68.110554 173.830994 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 196.687333 \nC 72.944155 196.687333 73.707285 196.371234 74.269866 195.808653 \nC 74.832447 195.246072 75.148546 194.482942 75.148546 193.687333 \nC 75.148546 192.891723 74.832447 192.128593 74.269866 191.566012 \nC 73.707285 191.003432 72.944155 190.687333 72.148546 190.687333 \nC 71.352936 190.687333 70.589806 191.003432 70.027225 191.566012 \nC 69.464645 192.128593 69.148546 192.891723 69.148546 193.687333 \nC 69.148546 194.482942 69.464645 195.246072 70.027225 195.808653 \nC 70.589806 196.371234 71.352936 196.687333 72.148546 196.687333 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 147.029507 \nC 87.94241 147.029507 88.705541 146.713408 89.268121 146.150827 \nC 89.830702 145.588246 90.146801 144.825116 90.146801 144.029507 \nC 90.146801 143.233897 89.830702 142.470767 89.268121 141.908186 \nC 88.705541 141.345606 87.94241 141.029507 87.146801 141.029507 \nC 86.351192 141.029507 85.588061 141.345606 85.025481 141.908186 \nC 84.4629 142.470767 84.146801 143.233897 84.146801 144.029507 \nC 84.146801 144.825116 84.4629 145.588246 85.025481 146.150827 \nC 85.588061 146.713408 86.351192 147.029507 87.146801 147.029507 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 187.523267 \nC 145.628008 187.523267 146.391139 187.207168 146.953719 186.644587 \nC 147.5163 186.082007 147.832399 185.318876 147.832399 184.523267 \nC 147.832399 183.727658 147.5163 182.964527 146.953719 182.401947 \nC 146.391139 181.839366 145.628008 181.523267 144.832399 181.523267 \nC 144.03679 181.523267 143.273659 181.839366 142.711079 182.401947 \nC 142.148498 182.964527 141.832399 183.727658 141.832399 184.523267 \nC 141.832399 185.318876 142.148498 186.082007 142.711079 186.644587 \nC 143.273659 187.207168 144.03679 187.523267 144.832399 187.523267 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 183.447743 \nC 371.755552 183.447743 372.518683 183.131644 373.081264 182.569063 \nC 373.643844 182.006483 373.959943 181.243352 373.959943 180.447743 \nC 373.959943 179.652134 373.643844 178.889003 373.081264 178.326423 \nC 372.518683 177.763842 371.755552 177.447743 370.959943 177.447743 \nC 370.164334 177.447743 369.401204 177.763842 368.838623 178.326423 \nC 368.276042 178.889003 367.959943 179.652134 367.959943 180.447743 \nC 367.959943 181.243352 368.276042 182.006483 368.838623 182.569063 \nC 369.401204 183.131644 370.164334 183.447743 370.959943 183.447743 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 172.155417 \nC 68.906163 172.155417 69.669293 171.839318 70.231874 171.276738 \nC 70.794455 170.714157 71.110554 169.951027 71.110554 169.155417 \nC 71.110554 168.359808 70.794455 167.596678 70.231874 167.034097 \nC 69.669293 166.471516 68.906163 166.155417 68.110554 166.155417 \nC 67.314944 166.155417 66.551814 166.471516 65.989233 167.034097 \nC 65.426653 167.596678 65.110554 168.359808 65.110554 169.155417 \nC 65.110554 169.951027 65.426653 170.714157 65.989233 171.276738 \nC 66.551814 171.839318 67.314944 172.155417 68.110554 172.155417 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 184.474911 \nC 76.693719 184.474911 77.456849 184.158812 78.01943 183.596231 \nC 78.582011 183.033651 78.898109 182.27052 78.898109 181.474911 \nC 78.898109 180.679302 78.582011 179.916171 78.01943 179.353591 \nC 77.456849 178.79101 76.693719 178.474911 75.898109 178.474911 \nC 75.1025 178.474911 74.33937 178.79101 73.776789 179.353591 \nC 73.214208 179.916171 72.898109 180.679302 72.898109 181.474911 \nC 72.898109 182.27052 73.214208 183.033651 73.776789 183.596231 \nC 74.33937 184.158812 75.1025 184.474911 75.898109 184.474911 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 20.367134 \nC 87.077126 20.367134 87.840257 20.051035 88.402837 19.488454 \nC 88.965418 18.925873 89.281517 18.162743 89.281517 17.367134 \nC 89.281517 16.571525 88.965418 15.808394 88.402837 15.245814 \nC 87.840257 14.683233 87.077126 14.367134 86.281517 14.367134 \nC 85.485908 14.367134 84.722778 14.683233 84.160197 15.245814 \nC 83.597616 15.808394 83.281517 16.571525 83.281517 17.367134 \nC 83.281517 18.162743 83.597616 18.925873 84.160197 19.488454 \nC 84.722778 20.051035 85.485908 20.367134 86.281517 20.367134 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.086132 \nC 107.843942 20.086132 108.607072 19.770033 109.169653 19.207452 \nC 109.732233 18.644871 110.048332 17.881741 110.048332 17.086132 \nC 110.048332 16.290522 109.732233 15.527392 109.169653 14.964811 \nC 108.607072 14.402231 107.843942 14.086132 107.048332 14.086132 \nC 106.252723 14.086132 105.489593 14.402231 104.927012 14.964811 \nC 104.364431 15.527392 104.048332 16.290522 104.048332 17.086132 \nC 104.048332 17.881741 104.364431 18.644871 104.927012 19.207452 \nC 105.489593 19.770033 106.252723 20.086132 107.048332 20.086132 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 200.579804 \nL 66.596307 199.079804 \nL 68.096307 200.579804 \nL 69.596307 199.079804 \nL 68.096307 197.579804 \nL 69.596307 196.079804 \nL 68.096307 194.579804 \nL 66.596307 196.079804 \nL 65.096307 194.579804 \nL 63.596307 196.079804 \nL 65.096307 197.579804 \nL 63.596307 199.079804 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.456842 178.89323 \nL 66.956842 177.39323 \nL 68.456842 178.89323 \nL 69.956842 177.39323 \nL 68.456842 175.89323 \nL 69.956842 174.39323 \nL 68.456842 172.89323 \nL 66.956842 174.39323 \nL 65.456842 172.89323 \nL 63.956842 174.39323 \nL 65.456842 175.89323 \nL 63.956842 177.39323 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 192.885653 \nL 68.110554 191.385653 \nL 69.610554 192.885653 \nL 71.110554 191.385653 \nL 69.610554 189.885653 \nL 71.110554 188.385653 \nL 69.610554 186.885653 \nL 68.110554 188.385653 \nL 66.610554 186.885653 \nL 65.110554 188.385653 \nL 66.610554 189.885653 \nL 65.110554 191.385653 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 85.646801 187.797255 \nL 87.146801 186.297255 \nL 88.646801 187.797255 \nL 90.146801 186.297255 \nL 88.646801 184.797255 \nL 90.146801 183.297255 \nL 88.646801 181.797255 \nL 87.146801 183.297255 \nL 85.646801 181.797255 \nL 84.146801 183.297255 \nL 85.646801 184.797255 \nL 84.146801 186.297255 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 143.332399 128.94551 \nL 144.832399 127.44551 \nL 146.332399 128.94551 \nL 147.832399 127.44551 \nL 146.332399 125.94551 \nL 147.832399 124.44551 \nL 146.332399 122.94551 \nL 144.832399 124.44551 \nL 143.332399 122.94551 \nL 141.832399 124.44551 \nL 143.332399 125.94551 \nL 141.832399 127.44551 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 369.459943 181.860423 \nL 370.959943 180.360423 \nL 372.459943 181.860423 \nL 373.959943 180.360423 \nL 372.459943 178.860423 \nL 373.959943 177.360423 \nL 372.459943 175.860423 \nL 370.959943 177.360423 \nL 369.459943 175.860423 \nL 367.959943 177.360423 \nL 369.459943 178.860423 \nL 367.959943 180.360423 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.312628 119.394713 \nL 66.812628 117.894713 \nL 68.312628 119.394713 \nL 69.812628 117.894713 \nL 68.312628 116.394713 \nL 69.812628 114.894713 \nL 68.312628 113.394713 \nL 66.812628 114.894713 \nL 65.312628 113.394713 \nL 63.812628 114.894713 \nL 65.312628 116.394713 \nL 63.812628 117.894713 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 69.206406 161.337408 \nL 70.706406 159.837408 \nL 72.206406 161.337408 \nL 73.706406 159.837408 \nL 72.206406 158.337408 \nL 73.706406 156.837408 \nL 72.206406 155.337408 \nL 70.706406 156.837408 \nL 69.206406 155.337408 \nL 67.706406 156.837408 \nL 69.206406 158.337408 \nL 67.706406 159.837408 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 194.234247 \nL 107.048332 192.734247 \nL 108.548332 194.234247 \nL 110.048332 192.734247 \nL 108.548332 191.234247 \nL 110.048332 189.734247 \nL 108.548332 188.234247 \nL 107.048332 189.734247 \nL 105.548332 188.234247 \nL 104.048332 189.734247 \nL 105.548332 191.234247 \nL 104.048332 192.734247 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 182.807045 \nC 67.391916 182.807045 68.155046 182.490946 68.717627 181.928366 \nC 69.280208 181.365785 69.596307 180.602654 69.596307 179.807045 \nC 69.596307 179.011436 69.280208 178.248306 68.717627 177.685725 \nC 68.155046 177.123144 67.391916 176.807045 66.596307 176.807045 \nC 65.800698 176.807045 65.037567 177.123144 64.474986 177.685725 \nC 63.912406 178.248306 63.596307 179.011436 63.596307 179.807045 \nC 63.596307 180.602654 63.912406 181.365785 64.474986 181.928366 \nC 65.037567 182.490946 65.800698 182.807045 66.596307 182.807045 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 188.718145 \nC 67.752451 188.718145 68.515581 188.402046 69.078162 187.839466 \nC 69.640743 187.276885 69.956842 186.513755 69.956842 185.718145 \nC 69.956842 184.922536 69.640743 184.159406 69.078162 183.596825 \nC 68.515581 183.034244 67.752451 182.718145 66.956842 182.718145 \nC 66.161233 182.718145 65.398102 183.034244 64.835521 183.596825 \nC 64.272941 184.159406 63.956842 184.922536 63.956842 185.718145 \nC 63.956842 186.513755 64.272941 187.276885 64.835521 187.839466 \nC 65.398102 188.402046 66.161233 188.718145 66.956842 188.718145 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 194.901497 \nC 68.906163 194.901497 69.669293 194.585398 70.231874 194.022817 \nC 70.794455 193.460236 71.110554 192.697106 71.110554 191.901497 \nC 71.110554 191.105887 70.794455 190.342757 70.231874 189.780176 \nC 69.669293 189.217596 68.906163 188.901497 68.110554 188.901497 \nC 67.314944 188.901497 66.551814 189.217596 65.989233 189.780176 \nC 65.426653 190.342757 65.110554 191.105887 65.110554 191.901497 \nC 65.110554 192.697106 65.426653 193.460236 65.989233 194.022817 \nC 66.551814 194.585398 67.314944 194.901497 68.110554 194.901497 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 151.866332 \nC 72.944155 151.866332 73.707285 151.550233 74.269866 150.987652 \nC 74.832447 150.425071 75.148546 149.661941 75.148546 148.866332 \nC 75.148546 148.070722 74.832447 147.307592 74.269866 146.745011 \nC 73.707285 146.18243 72.944155 145.866332 72.148546 145.866332 \nC 71.352936 145.866332 70.589806 146.18243 70.027225 146.745011 \nC 69.464645 147.307592 69.148546 148.070722 69.148546 148.866332 \nC 69.148546 149.661941 69.464645 150.425071 70.027225 150.987652 \nC 70.589806 151.550233 71.352936 151.866332 72.148546 151.866332 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 182.951697 \nC 87.94241 182.951697 88.705541 182.635598 89.268121 182.073017 \nC 89.830702 181.510436 90.146801 180.747306 90.146801 179.951697 \nC 90.146801 179.156087 89.830702 178.392957 89.268121 177.830376 \nC 88.705541 177.267796 87.94241 176.951697 87.146801 176.951697 \nC 86.351192 176.951697 85.588061 177.267796 85.025481 177.830376 \nC 84.4629 178.392957 84.146801 179.156087 84.146801 179.951697 \nC 84.146801 180.747306 84.4629 181.510436 85.025481 182.073017 \nC 85.588061 182.635598 86.351192 182.951697 87.146801 182.951697 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 192.254226 \nC 145.628008 192.254226 146.391139 191.938127 146.953719 191.375546 \nC 147.5163 190.812965 147.832399 190.049835 147.832399 189.254226 \nC 147.832399 188.458617 147.5163 187.695486 146.953719 187.132905 \nC 146.391139 186.570325 145.628008 186.254226 144.832399 186.254226 \nC 144.03679 186.254226 143.273659 186.570325 142.711079 187.132905 \nC 142.148498 187.695486 141.832399 188.458617 141.832399 189.254226 \nC 141.832399 190.049835 142.148498 190.812965 142.711079 191.375546 \nC 143.273659 191.938127 144.03679 192.254226 144.832399 192.254226 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 189.665822 \nC 371.755552 189.665822 372.518683 189.349723 373.081264 188.787142 \nC 373.643844 188.224562 373.959943 187.461431 373.959943 186.665822 \nC 373.959943 185.870213 373.643844 185.107082 373.081264 184.544502 \nC 372.518683 183.981921 371.755552 183.665822 370.959943 183.665822 \nC 370.164334 183.665822 369.401204 183.981921 368.838623 184.544502 \nC 368.276042 185.107082 367.959943 185.870213 367.959943 186.665822 \nC 367.959943 187.461431 368.276042 188.224562 368.838623 188.787142 \nC 369.401204 189.349723 370.164334 189.665822 370.959943 189.665822 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.812628 193.208177 \nC 67.608237 193.208177 68.371367 192.892078 68.933948 192.329498 \nC 69.496529 191.766917 69.812628 191.003787 69.812628 190.208177 \nC 69.812628 189.412568 69.496529 188.649438 68.933948 188.086857 \nC 68.371367 187.524276 67.608237 187.208177 66.812628 187.208177 \nC 66.017019 187.208177 65.253888 187.524276 64.691307 188.086857 \nC 64.128727 188.649438 63.812628 189.412568 63.812628 190.208177 \nC 63.812628 191.003787 64.128727 191.766917 64.691307 192.329498 \nC 65.253888 192.892078 66.017019 193.208177 66.812628 193.208177 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 175.217288 \nC 68.906163 175.217288 69.669293 174.901189 70.231874 174.338608 \nC 70.794455 173.776027 71.110554 173.012897 71.110554 172.217288 \nC 71.110554 171.421679 70.794455 170.658548 70.231874 170.095968 \nC 69.669293 169.533387 68.906163 169.217288 68.110554 169.217288 \nC 67.314944 169.217288 66.551814 169.533387 65.989233 170.095968 \nC 65.426653 170.658548 65.110554 171.421679 65.110554 172.217288 \nC 65.110554 173.012897 65.426653 173.776027 65.989233 174.338608 \nC 66.551814 174.901189 67.314944 175.217288 68.110554 175.217288 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.706406 169.411691 \nC 71.502015 169.411691 72.265145 169.095592 72.827726 168.533011 \nC 73.390307 167.97043 73.706406 167.2073 73.706406 166.411691 \nC 73.706406 165.616081 73.390307 164.852951 72.827726 164.29037 \nC 72.265145 163.727789 71.502015 163.411691 70.706406 163.411691 \nC 69.910796 163.411691 69.147666 163.727789 68.585085 164.29037 \nC 68.022505 164.852951 67.706406 165.616081 67.706406 166.411691 \nC 67.706406 167.2073 68.022505 167.97043 68.585085 168.533011 \nC 69.147666 169.095592 69.910796 169.411691 70.706406 169.411691 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 179.730501 \nC 76.693719 179.730501 77.456849 179.414402 78.01943 178.851821 \nC 78.582011 178.289241 78.898109 177.52611 78.898109 176.730501 \nC 78.898109 175.934892 78.582011 175.171761 78.01943 174.609181 \nC 77.456849 174.0466 76.693719 173.730501 75.898109 173.730501 \nC 75.1025 173.730501 74.33937 174.0466 73.776789 174.609181 \nC 73.214208 175.171761 72.898109 175.934892 72.898109 176.730501 \nC 72.898109 177.52611 73.214208 178.289241 73.776789 178.851821 \nC 74.33937 179.414402 75.1025 179.730501 75.898109 179.730501 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 155.494943 \nC 87.077126 155.494943 87.840257 155.178844 88.402837 154.616263 \nC 88.965418 154.053682 89.281517 153.290552 89.281517 152.494943 \nC 89.281517 151.699334 88.965418 150.936203 88.402837 150.373623 \nC 87.840257 149.811042 87.077126 149.494943 86.281517 149.494943 \nC 85.485908 149.494943 84.722778 149.811042 84.160197 150.373623 \nC 83.597616 150.936203 83.281517 151.699334 83.281517 152.494943 \nC 83.281517 153.290552 83.597616 154.053682 84.160197 154.616263 \nC 84.722778 155.178844 85.485908 155.494943 86.281517 155.494943 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.083636 \nC 107.843942 20.083636 108.607072 19.767537 109.169653 19.204957 \nC 109.732233 18.642376 110.048332 17.879246 110.048332 17.083636 \nC 110.048332 16.288027 109.732233 15.524897 109.169653 14.962316 \nC 108.607072 14.399735 107.843942 14.083636 107.048332 14.083636 \nC 106.252723 14.083636 105.489593 14.399735 104.927012 14.962316 \nC 104.364431 15.524897 104.048332 16.288027 104.048332 17.083636 \nC 104.048332 17.879246 104.364431 18.642376 104.927012 19.204957 \nC 105.489593 19.767537 106.252723 20.083636 107.048332 20.083636 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 169.264024 \nL 66.596307 167.764024 \nL 68.096307 169.264024 \nL 69.596307 167.764024 \nL 68.096307 166.264024 \nL 69.596307 164.764024 \nL 68.096307 163.264024 \nL 66.596307 164.764024 \nL 65.096307 163.264024 \nL 63.596307 164.764024 \nL 65.096307 166.264024 \nL 63.596307 167.764024 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.456842 198.871562 \nL 66.956842 197.371562 \nL 68.456842 198.871562 \nL 69.956842 197.371562 \nL 68.456842 195.871562 \nL 69.956842 194.371562 \nL 68.456842 192.871562 \nL 66.956842 194.371562 \nL 65.456842 192.871562 \nL 63.956842 194.371562 \nL 65.456842 195.871562 \nL 63.956842 197.371562 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 184.185489 \nL 68.110554 182.685489 \nL 69.610554 184.185489 \nL 71.110554 182.685489 \nL 69.610554 181.185489 \nL 71.110554 179.685489 \nL 69.610554 178.185489 \nL 68.110554 179.685489 \nL 66.610554 178.185489 \nL 65.110554 179.685489 \nL 66.610554 181.185489 \nL 65.110554 182.685489 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.648546 176.792129 \nL 72.148546 175.292129 \nL 73.648546 176.792129 \nL 75.148546 175.292129 \nL 73.648546 173.792129 \nL 75.148546 172.292129 \nL 73.648546 170.792129 \nL 72.148546 172.292129 \nL 70.648546 170.792129 \nL 69.148546 172.292129 \nL 70.648546 173.792129 \nL 69.148546 175.292129 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 85.646801 160.390193 \nL 87.146801 158.890193 \nL 88.646801 160.390193 \nL 90.146801 158.890193 \nL 88.646801 157.390193 \nL 90.146801 155.890193 \nL 88.646801 154.390193 \nL 87.146801 155.890193 \nL 85.646801 154.390193 \nL 84.146801 155.890193 \nL 85.646801 157.390193 \nL 84.146801 158.890193 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 369.459943 130.475549 \nL 370.959943 128.975549 \nL 372.459943 130.475549 \nL 373.959943 128.975549 \nL 372.459943 127.475549 \nL 373.959943 125.975549 \nL 372.459943 124.475549 \nL 370.959943 125.975549 \nL 369.459943 124.475549 \nL 367.959943 125.975549 \nL 369.459943 127.475549 \nL 367.959943 128.975549 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.312628 176.256261 \nL 66.812628 174.756261 \nL 68.312628 176.256261 \nL 69.812628 174.756261 \nL 68.312628 173.256261 \nL 69.812628 171.756261 \nL 68.312628 170.256261 \nL 66.812628 171.756261 \nL 65.312628 170.256261 \nL 63.812628 171.756261 \nL 65.312628 173.256261 \nL 63.812628 174.756261 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 74.398109 191.313511 \nL 75.898109 189.813511 \nL 77.398109 191.313511 \nL 78.898109 189.813511 \nL 77.398109 188.313511 \nL 78.898109 186.813511 \nL 77.398109 185.313511 \nL 75.898109 186.813511 \nL 74.398109 185.313511 \nL 72.898109 186.813511 \nL 74.398109 188.313511 \nL 72.898109 189.813511 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 84.781517 194.454199 \nL 86.281517 192.954199 \nL 87.781517 194.454199 \nL 89.281517 192.954199 \nL 87.781517 191.454199 \nL 89.281517 189.954199 \nL 87.781517 188.454199 \nL 86.281517 189.954199 \nL 84.781517 188.454199 \nL 83.281517 189.954199 \nL 84.781517 191.454199 \nL 83.281517 192.954199 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 136.604181 \nL 107.048332 135.104181 \nL 108.548332 136.604181 \nL 110.048332 135.104181 \nL 108.548332 133.604181 \nL 110.048332 132.104181 \nL 108.548332 130.604181 \nL 107.048332 132.104181 \nL 105.548332 130.604181 \nL 104.048332 132.104181 \nL 105.548332 133.604181 \nL 104.048332 135.104181 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 184.271123 \nC 67.391916 184.271123 68.155046 183.955024 68.717627 183.392443 \nC 69.280208 182.829863 69.596307 182.066732 69.596307 181.271123 \nC 69.596307 180.475514 69.280208 179.712383 68.717627 179.149803 \nC 68.155046 178.587222 67.391916 178.271123 66.596307 178.271123 \nC 65.800698 178.271123 65.037567 178.587222 64.474986 179.149803 \nC 63.912406 179.712383 63.596307 180.475514 63.596307 181.271123 \nC 63.596307 182.066732 63.912406 182.829863 64.474986 183.392443 \nC 65.037567 183.955024 65.800698 184.271123 66.596307 184.271123 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 168.580617 \nC 67.752451 168.580617 68.515581 168.264518 69.078162 167.701937 \nC 69.640743 167.139357 69.956842 166.376226 69.956842 165.580617 \nC 69.956842 164.785008 69.640743 164.021877 69.078162 163.459297 \nC 68.515581 162.896716 67.752451 162.580617 66.956842 162.580617 \nC 66.161233 162.580617 65.398102 162.896716 64.835521 163.459297 \nC 64.272941 164.021877 63.956842 164.785008 63.956842 165.580617 \nC 63.956842 166.376226 64.272941 167.139357 64.835521 167.701937 \nC 65.398102 168.264518 66.161233 168.580617 66.956842 168.580617 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 201.575291 \nC 72.944155 201.575291 73.707285 201.259192 74.269866 200.696611 \nC 74.832447 200.13403 75.148546 199.3709 75.148546 198.575291 \nC 75.148546 197.779682 74.832447 197.016551 74.269866 196.453971 \nC 73.707285 195.89139 72.944155 195.575291 72.148546 195.575291 \nC 71.352936 195.575291 70.589806 195.89139 70.027225 196.453971 \nC 69.464645 197.016551 69.148546 197.779682 69.148546 198.575291 \nC 69.148546 199.3709 69.464645 200.13403 70.027225 200.696611 \nC 70.589806 201.259192 71.352936 201.575291 72.148546 201.575291 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 184.533153 \nC 87.94241 184.533153 88.705541 184.217054 89.268121 183.654474 \nC 89.830702 183.091893 90.146801 182.328762 90.146801 181.533153 \nC 90.146801 180.737544 89.830702 179.974414 89.268121 179.411833 \nC 88.705541 178.849252 87.94241 178.533153 87.146801 178.533153 \nC 86.351192 178.533153 85.588061 178.849252 85.025481 179.411833 \nC 84.4629 179.974414 84.146801 180.737544 84.146801 181.533153 \nC 84.146801 182.328762 84.4629 183.091893 85.025481 183.654474 \nC 85.588061 184.217054 86.351192 184.533153 87.146801 184.533153 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 179.346313 \nC 145.628008 179.346313 146.391139 179.030214 146.953719 178.467634 \nC 147.5163 177.905053 147.832399 177.141923 147.832399 176.346313 \nC 147.832399 175.550704 147.5163 174.787574 146.953719 174.224993 \nC 146.391139 173.662412 145.628008 173.346313 144.832399 173.346313 \nC 144.03679 173.346313 143.273659 173.662412 142.711079 174.224993 \nC 142.148498 174.787574 141.832399 175.550704 141.832399 176.346313 \nC 141.832399 177.141923 142.148498 177.905053 142.711079 178.467634 \nC 143.273659 179.030214 144.03679 179.346313 144.832399 179.346313 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 176.452738 \nC 371.755552 176.452738 372.518683 176.136639 373.081264 175.574058 \nC 373.643844 175.011477 373.959943 174.248347 373.959943 173.452738 \nC 373.959943 172.657128 373.643844 171.893998 373.081264 171.331417 \nC 372.518683 170.768837 371.755552 170.452738 370.959943 170.452738 \nC 370.164334 170.452738 369.401204 170.768837 368.838623 171.331417 \nC 368.276042 171.893998 367.959943 172.657128 367.959943 173.452738 \nC 367.959943 174.248347 368.276042 175.011477 368.838623 175.574058 \nC 369.401204 176.136639 370.164334 176.452738 370.959943 176.452738 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 195.799445 \nC 68.906163 195.799445 69.669293 195.483346 70.231874 194.920765 \nC 70.794455 194.358184 71.110554 193.595054 71.110554 192.799445 \nC 71.110554 192.003835 70.794455 191.240705 70.231874 190.678124 \nC 69.669293 190.115544 68.906163 189.799445 68.110554 189.799445 \nC 67.314944 189.799445 66.551814 190.115544 65.989233 190.678124 \nC 65.426653 191.240705 65.110554 192.003835 65.110554 192.799445 \nC 65.110554 193.595054 65.426653 194.358184 65.989233 194.920765 \nC 66.551814 195.483346 67.314944 195.799445 68.110554 195.799445 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.706406 192.348641 \nC 71.502015 192.348641 72.265145 192.032542 72.827726 191.469961 \nC 73.390307 190.90738 73.706406 190.14425 73.706406 189.348641 \nC 73.706406 188.553031 73.390307 187.789901 72.827726 187.22732 \nC 72.265145 186.66474 71.502015 186.348641 70.706406 186.348641 \nC 69.910796 186.348641 69.147666 186.66474 68.585085 187.22732 \nC 68.022505 187.789901 67.706406 188.553031 67.706406 189.348641 \nC 67.706406 190.14425 68.022505 190.90738 68.585085 191.469961 \nC 69.147666 192.032542 69.910796 192.348641 70.706406 192.348641 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 175.310649 \nC 76.693719 175.310649 77.456849 174.99455 78.01943 174.431969 \nC 78.582011 173.869388 78.898109 173.106258 78.898109 172.310649 \nC 78.898109 171.515039 78.582011 170.751909 78.01943 170.189328 \nC 77.456849 169.626748 76.693719 169.310649 75.898109 169.310649 \nC 75.1025 169.310649 74.33937 169.626748 73.776789 170.189328 \nC 73.214208 170.751909 72.898109 171.515039 72.898109 172.310649 \nC 72.898109 173.106258 73.214208 173.869388 73.776789 174.431969 \nC 74.33937 174.99455 75.1025 175.310649 75.898109 175.310649 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 174.644137 \nC 87.077126 174.644137 87.840257 174.328038 88.402837 173.765457 \nC 88.965418 173.202876 89.281517 172.439746 89.281517 171.644137 \nC 89.281517 170.848528 88.965418 170.085397 88.402837 169.522817 \nC 87.840257 168.960236 87.077126 168.644137 86.281517 168.644137 \nC 85.485908 168.644137 84.722778 168.960236 84.160197 169.522817 \nC 83.597616 170.085397 83.281517 170.848528 83.281517 171.644137 \nC 83.281517 172.439746 83.597616 173.202876 84.160197 173.765457 \nC 84.722778 174.328038 85.485908 174.644137 86.281517 174.644137 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.087121 \nC 107.843942 20.087121 108.607072 19.771022 109.169653 19.208442 \nC 109.732233 18.645861 110.048332 17.88273 110.048332 17.087121 \nC 110.048332 16.291512 109.732233 15.528382 109.169653 14.965801 \nC 108.607072 14.40322 107.843942 14.087121 107.048332 14.087121 \nC 106.252723 14.087121 105.489593 14.40322 104.927012 14.965801 \nC 104.364431 15.528382 104.048332 16.291512 104.048332 17.087121 \nC 104.048332 17.88273 104.364431 18.645861 104.927012 19.208442 \nC 105.489593 19.771022 106.252723 20.087121 107.048332 20.087121 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 185.318813 \nL 66.596307 183.818813 \nL 68.096307 185.318813 \nL 69.596307 183.818813 \nL 68.096307 182.318813 \nL 69.596307 180.818813 \nL 68.096307 179.318813 \nL 66.596307 180.818813 \nL 65.096307 179.318813 \nL 63.596307 180.818813 \nL 65.096307 182.318813 \nL 63.596307 183.818813 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.456842 182.990365 \nL 66.956842 181.490365 \nL 68.456842 182.990365 \nL 69.956842 181.490365 \nL 68.456842 179.990365 \nL 69.956842 178.490365 \nL 68.456842 176.990365 \nL 66.956842 178.490365 \nL 65.456842 176.990365 \nL 63.956842 178.490365 \nL 65.456842 179.990365 \nL 63.956842 181.490365 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 178.67941 \nL 68.110554 177.17941 \nL 69.610554 178.67941 \nL 71.110554 177.17941 \nL 69.610554 175.67941 \nL 71.110554 174.17941 \nL 69.610554 172.67941 \nL 68.110554 174.17941 \nL 66.610554 172.67941 \nL 65.110554 174.17941 \nL 66.610554 175.67941 \nL 65.110554 177.17941 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 85.646801 140.211702 \nL 87.146801 138.711702 \nL 88.646801 140.211702 \nL 90.146801 138.711702 \nL 88.646801 137.211702 \nL 90.146801 135.711702 \nL 88.646801 134.211702 \nL 87.146801 135.711702 \nL 85.646801 134.211702 \nL 84.146801 135.711702 \nL 85.646801 137.211702 \nL 84.146801 138.711702 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 143.332399 133.838773 \nL 144.832399 132.338773 \nL 146.332399 133.838773 \nL 147.832399 132.338773 \nL 146.332399 130.838773 \nL 147.832399 129.338773 \nL 146.332399 127.838773 \nL 144.832399 129.338773 \nL 143.332399 127.838773 \nL 141.832399 129.338773 \nL 143.332399 130.838773 \nL 141.832399 132.338773 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 369.459943 160.171021 \nL 370.959943 158.671021 \nL 372.459943 160.171021 \nL 373.959943 158.671021 \nL 372.459943 157.171021 \nL 373.959943 155.671021 \nL 372.459943 154.171021 \nL 370.959943 155.671021 \nL 369.459943 154.171021 \nL 367.959943 155.671021 \nL 369.459943 157.171021 \nL 367.959943 158.671021 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 69.206406 162.901245 \nL 70.706406 161.401245 \nL 72.206406 162.901245 \nL 73.706406 161.401245 \nL 72.206406 159.901245 \nL 73.706406 158.401245 \nL 72.206406 156.901245 \nL 70.706406 158.401245 \nL 69.206406 156.901245 \nL 67.706406 158.401245 \nL 69.206406 159.901245 \nL 67.706406 161.401245 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 74.398109 195.602087 \nL 75.898109 194.102087 \nL 77.398109 195.602087 \nL 78.898109 194.102087 \nL 77.398109 192.602087 \nL 78.898109 191.102087 \nL 77.398109 189.602087 \nL 75.898109 191.102087 \nL 74.398109 189.602087 \nL 72.898109 191.102087 \nL 74.398109 192.602087 \nL 72.898109 194.102087 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 84.781517 167.842412 \nL 86.281517 166.342412 \nL 87.781517 167.842412 \nL 89.281517 166.342412 \nL 87.781517 164.842412 \nL 89.281517 163.342412 \nL 87.781517 161.842412 \nL 86.281517 163.342412 \nL 84.781517 161.842412 \nL 83.281517 163.342412 \nL 84.781517 164.842412 \nL 83.281517 166.342412 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 127.077381 \nL 107.048332 125.577381 \nL 108.548332 127.077381 \nL 110.048332 125.577381 \nL 108.548332 124.077381 \nL 110.048332 122.577381 \nL 108.548332 121.077381 \nL 107.048332 122.577381 \nL 105.548332 121.077381 \nL 104.048332 122.577381 \nL 105.548332 124.077381 \nL 104.048332 125.577381 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 160.424433 \nC 67.391916 160.424433 68.155046 160.108334 68.717627 159.545754 \nC 69.280208 158.983173 69.596307 158.220043 69.596307 157.424433 \nC 69.596307 156.628824 69.280208 155.865694 68.717627 155.303113 \nC 68.155046 154.740532 67.391916 154.424433 66.596307 154.424433 \nC 65.800698 154.424433 65.037567 154.740532 64.474986 155.303113 \nC 63.912406 155.865694 63.596307 156.628824 63.596307 157.424433 \nC 63.596307 158.220043 63.912406 158.983173 64.474986 159.545754 \nC 65.037567 160.108334 65.800698 160.424433 66.596307 160.424433 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 180.948335 \nC 67.752451 180.948335 68.515581 180.632236 69.078162 180.069655 \nC 69.640743 179.507075 69.956842 178.743944 69.956842 177.948335 \nC 69.956842 177.152726 69.640743 176.389595 69.078162 175.827015 \nC 68.515581 175.264434 67.752451 174.948335 66.956842 174.948335 \nC 66.161233 174.948335 65.398102 175.264434 64.835521 175.827015 \nC 64.272941 176.389595 63.956842 177.152726 63.956842 177.948335 \nC 63.956842 178.743944 64.272941 179.507075 64.835521 180.069655 \nC 65.398102 180.632236 66.161233 180.948335 66.956842 180.948335 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 174.467631 \nC 68.906163 174.467631 69.669293 174.151532 70.231874 173.588951 \nC 70.794455 173.026371 71.110554 172.26324 71.110554 171.467631 \nC 71.110554 170.672022 70.794455 169.908891 70.231874 169.346311 \nC 69.669293 168.78373 68.906163 168.467631 68.110554 168.467631 \nC 67.314944 168.467631 66.551814 168.78373 65.989233 169.346311 \nC 65.426653 169.908891 65.110554 170.672022 65.110554 171.467631 \nC 65.110554 172.26324 65.426653 173.026371 65.989233 173.588951 \nC 66.551814 174.151532 67.314944 174.467631 68.110554 174.467631 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 189.081168 \nC 72.944155 189.081168 73.707285 188.76507 74.269866 188.202489 \nC 74.832447 187.639908 75.148546 186.876778 75.148546 186.081168 \nC 75.148546 185.285559 74.832447 184.522429 74.269866 183.959848 \nC 73.707285 183.397267 72.944155 183.081168 72.148546 183.081168 \nC 71.352936 183.081168 70.589806 183.397267 70.027225 183.959848 \nC 69.464645 184.522429 69.148546 185.285559 69.148546 186.081168 \nC 69.148546 186.876778 69.464645 187.639908 70.027225 188.202489 \nC 70.589806 188.76507 71.352936 189.081168 72.148546 189.081168 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 182.770656 \nC 87.94241 182.770656 88.705541 182.454557 89.268121 181.891976 \nC 89.830702 181.329396 90.146801 180.566265 90.146801 179.770656 \nC 90.146801 178.975047 89.830702 178.211916 89.268121 177.649336 \nC 88.705541 177.086755 87.94241 176.770656 87.146801 176.770656 \nC 86.351192 176.770656 85.588061 177.086755 85.025481 177.649336 \nC 84.4629 178.211916 84.146801 178.975047 84.146801 179.770656 \nC 84.146801 180.566265 84.4629 181.329396 85.025481 181.891976 \nC 85.588061 182.454557 86.351192 182.770656 87.146801 182.770656 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 186.564783 \nC 145.628008 186.564783 146.391139 186.248684 146.953719 185.686103 \nC 147.5163 185.123522 147.832399 184.360392 147.832399 183.564783 \nC 147.832399 182.769173 147.5163 182.006043 146.953719 181.443462 \nC 146.391139 180.880882 145.628008 180.564783 144.832399 180.564783 \nC 144.03679 180.564783 143.273659 180.880882 142.711079 181.443462 \nC 142.148498 182.006043 141.832399 182.769173 141.832399 183.564783 \nC 141.832399 184.360392 142.148498 185.123522 142.711079 185.686103 \nC 143.273659 186.248684 144.03679 186.564783 144.832399 186.564783 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 180.639839 \nC 371.755552 180.639839 372.518683 180.323741 373.081264 179.76116 \nC 373.643844 179.198579 373.959943 178.435449 373.959943 177.639839 \nC 373.959943 176.84423 373.643844 176.0811 373.081264 175.518519 \nC 372.518683 174.955938 371.755552 174.639839 370.959943 174.639839 \nC 370.164334 174.639839 369.401204 174.955938 368.838623 175.518519 \nC 368.276042 176.0811 367.959943 176.84423 367.959943 177.639839 \nC 367.959943 178.435449 368.276042 179.198579 368.838623 179.76116 \nC 369.401204 180.323741 370.164334 180.639839 370.959943 180.639839 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.812628 195.368266 \nC 67.608237 195.368266 68.371367 195.052167 68.933948 194.489586 \nC 69.496529 193.927006 69.812628 193.163875 69.812628 192.368266 \nC 69.812628 191.572657 69.496529 190.809526 68.933948 190.246946 \nC 68.371367 189.684365 67.608237 189.368266 66.812628 189.368266 \nC 66.017019 189.368266 65.253888 189.684365 64.691307 190.246946 \nC 64.128727 190.809526 63.812628 191.572657 63.812628 192.368266 \nC 63.812628 193.163875 64.128727 193.927006 64.691307 194.489586 \nC 65.253888 195.052167 66.017019 195.368266 66.812628 195.368266 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 186.380605 \nC 68.906163 186.380605 69.669293 186.064506 70.231874 185.501926 \nC 70.794455 184.939345 71.110554 184.176215 71.110554 183.380605 \nC 71.110554 182.584996 70.794455 181.821866 70.231874 181.259285 \nC 69.669293 180.696704 68.906163 180.380605 68.110554 180.380605 \nC 67.314944 180.380605 66.551814 180.696704 65.989233 181.259285 \nC 65.426653 181.821866 65.110554 182.584996 65.110554 183.380605 \nC 65.110554 184.176215 65.426653 184.939345 65.989233 185.501926 \nC 66.551814 186.064506 67.314944 186.380605 68.110554 186.380605 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.706406 182.131912 \nC 71.502015 182.131912 72.265145 181.815813 72.827726 181.253232 \nC 73.390307 180.690652 73.706406 179.927521 73.706406 179.131912 \nC 73.706406 178.336303 73.390307 177.573173 72.827726 177.010592 \nC 72.265145 176.448011 71.502015 176.131912 70.706406 176.131912 \nC 69.910796 176.131912 69.147666 176.448011 68.585085 177.010592 \nC 68.022505 177.573173 67.706406 178.336303 67.706406 179.131912 \nC 67.706406 179.927521 68.022505 180.690652 68.585085 181.253232 \nC 69.147666 181.815813 69.910796 182.131912 70.706406 182.131912 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 177.529684 \nC 76.693719 177.529684 77.456849 177.213585 78.01943 176.651004 \nC 78.582011 176.088423 78.898109 175.325293 78.898109 174.529684 \nC 78.898109 173.734074 78.582011 172.970944 78.01943 172.408363 \nC 77.456849 171.845783 76.693719 171.529684 75.898109 171.529684 \nC 75.1025 171.529684 74.33937 171.845783 73.776789 172.408363 \nC 73.214208 172.970944 72.898109 173.734074 72.898109 174.529684 \nC 72.898109 175.325293 73.214208 176.088423 73.776789 176.651004 \nC 74.33937 177.213585 75.1025 177.529684 75.898109 177.529684 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 189.928689 \nC 87.077126 189.928689 87.840257 189.612591 88.402837 189.05001 \nC 88.965418 188.487429 89.281517 187.724299 89.281517 186.928689 \nC 89.281517 186.13308 88.965418 185.36995 88.402837 184.807369 \nC 87.840257 184.244788 87.077126 183.928689 86.281517 183.928689 \nC 85.485908 183.928689 84.722778 184.244788 84.160197 184.807369 \nC 83.597616 185.36995 83.281517 186.13308 83.281517 186.928689 \nC 83.281517 187.724299 83.597616 188.487429 84.160197 189.05001 \nC 84.722778 189.612591 85.485908 189.928689 86.281517 189.928689 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.085099 \nC 107.843942 20.085099 108.607072 19.769 109.169653 19.206419 \nC 109.732233 18.643839 110.048332 17.880708 110.048332 17.085099 \nC 110.048332 16.28949 109.732233 15.526359 109.169653 14.963779 \nC 108.607072 14.401198 107.843942 14.085099 107.048332 14.085099 \nC 106.252723 14.085099 105.489593 14.401198 104.927012 14.963779 \nC 104.364431 15.526359 104.048332 16.28949 104.048332 17.085099 \nC 104.048332 17.880708 104.364431 18.643839 104.927012 19.206419 \nC 105.489593 19.769 106.252723 20.085099 107.048332 20.085099 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 172.515574 \nL 66.596307 171.015574 \nL 68.096307 172.515574 \nL 69.596307 171.015574 \nL 68.096307 169.515574 \nL 69.596307 168.015574 \nL 68.096307 166.515574 \nL 66.596307 168.015574 \nL 65.096307 166.515574 \nL 63.596307 168.015574 \nL 65.096307 169.515574 \nL 63.596307 171.015574 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.456842 181.860423 \nL 66.956842 180.360423 \nL 68.456842 181.860423 \nL 69.956842 180.360423 \nL 68.456842 178.860423 \nL 69.956842 177.360423 \nL 68.456842 175.860423 \nL 66.956842 177.360423 \nL 65.456842 175.860423 \nL 63.956842 177.360423 \nL 65.456842 178.860423 \nL 63.956842 180.360423 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 179.813454 \nL 68.110554 178.313454 \nL 69.610554 179.813454 \nL 71.110554 178.313454 \nL 69.610554 176.813454 \nL 71.110554 175.313454 \nL 69.610554 173.813454 \nL 68.110554 175.313454 \nL 66.610554 173.813454 \nL 65.110554 175.313454 \nL 66.610554 176.813454 \nL 65.110554 178.313454 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.648546 162.852146 \nL 72.148546 161.352146 \nL 73.648546 162.852146 \nL 75.148546 161.352146 \nL 73.648546 159.852146 \nL 75.148546 158.352146 \nL 73.648546 156.852146 \nL 72.148546 158.352146 \nL 70.648546 156.852146 \nL 69.148546 158.352146 \nL 70.648546 159.852146 \nL 69.148546 161.352146 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 143.332399 149.36553 \nL 144.832399 147.86553 \nL 146.332399 149.36553 \nL 147.832399 147.86553 \nL 146.332399 146.36553 \nL 147.832399 144.86553 \nL 146.332399 143.36553 \nL 144.832399 144.86553 \nL 143.332399 143.36553 \nL 141.832399 144.86553 \nL 143.332399 146.36553 \nL 141.832399 147.86553 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 369.459943 136.296857 \nL 370.959943 134.796857 \nL 372.459943 136.296857 \nL 373.959943 134.796857 \nL 372.459943 133.296857 \nL 373.959943 131.796857 \nL 372.459943 130.296857 \nL 370.959943 131.796857 \nL 369.459943 130.296857 \nL 367.959943 131.796857 \nL 369.459943 133.296857 \nL 367.959943 134.796857 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.312628 190.011819 \nL 66.812628 188.511819 \nL 68.312628 190.011819 \nL 69.812628 188.511819 \nL 68.312628 187.011819 \nL 69.812628 185.511819 \nL 68.312628 184.011819 \nL 66.812628 185.511819 \nL 65.312628 184.011819 \nL 63.812628 185.511819 \nL 65.312628 187.011819 \nL 63.812628 188.511819 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 69.206406 191.900812 \nL 70.706406 190.400812 \nL 72.206406 191.900812 \nL 73.706406 190.400812 \nL 72.206406 188.900812 \nL 73.706406 187.400812 \nL 72.206406 185.900812 \nL 70.706406 187.400812 \nL 69.206406 185.900812 \nL 67.706406 187.400812 \nL 69.206406 188.900812 \nL 67.706406 190.400812 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 74.398109 190.007759 \nL 75.898109 188.507759 \nL 77.398109 190.007759 \nL 78.898109 188.507759 \nL 77.398109 187.007759 \nL 78.898109 185.507759 \nL 77.398109 184.007759 \nL 75.898109 185.507759 \nL 74.398109 184.007759 \nL 72.898109 185.507759 \nL 74.398109 187.007759 \nL 72.898109 188.507759 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 84.781517 183.93752 \nL 86.281517 182.43752 \nL 87.781517 183.93752 \nL 89.281517 182.43752 \nL 87.781517 180.93752 \nL 89.281517 179.43752 \nL 87.781517 177.93752 \nL 86.281517 179.43752 \nL 84.781517 177.93752 \nL 83.281517 179.43752 \nL 84.781517 180.93752 \nL 83.281517 182.43752 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 120.916221 \nL 107.048332 119.416221 \nL 108.548332 120.916221 \nL 110.048332 119.416221 \nL 108.548332 117.916221 \nL 110.048332 116.416221 \nL 108.548332 114.916221 \nL 107.048332 116.416221 \nL 105.548332 114.916221 \nL 104.048332 116.416221 \nL 105.548332 117.916221 \nL 104.048332 119.416221 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.596307 186.543965 \nC 67.391916 186.543965 68.155046 186.227866 68.717627 185.665286 \nC 69.280208 185.102705 69.596307 184.339575 69.596307 183.543965 \nC 69.596307 182.748356 69.280208 181.985226 68.717627 181.422645 \nC 68.155046 180.860064 67.391916 180.543965 66.596307 180.543965 \nC 65.800698 180.543965 65.037567 180.860064 64.474986 181.422645 \nC 63.912406 181.985226 63.596307 182.748356 63.596307 183.543965 \nC 63.596307 184.339575 63.912406 185.102705 64.474986 185.665286 \nC 65.037567 186.227866 65.800698 186.543965 66.596307 186.543965 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.956842 174.055135 \nC 67.752451 174.055135 68.515581 173.739036 69.078162 173.176455 \nC 69.640743 172.613874 69.956842 171.850744 69.956842 171.055135 \nC 69.956842 170.259525 69.640743 169.496395 69.078162 168.933814 \nC 68.515581 168.371234 67.752451 168.055135 66.956842 168.055135 \nC 66.161233 168.055135 65.398102 168.371234 64.835521 168.933814 \nC 64.272941 169.496395 63.956842 170.259525 63.956842 171.055135 \nC 63.956842 171.850744 64.272941 172.613874 64.835521 173.176455 \nC 65.398102 173.739036 66.161233 174.055135 66.956842 174.055135 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 197.307027 \nC 68.906163 197.307027 69.669293 196.990928 70.231874 196.428348 \nC 70.794455 195.865767 71.110554 195.102637 71.110554 194.307027 \nC 71.110554 193.511418 70.794455 192.748288 70.231874 192.185707 \nC 69.669293 191.623126 68.906163 191.307027 68.110554 191.307027 \nC 67.314944 191.307027 66.551814 191.623126 65.989233 192.185707 \nC 65.426653 192.748288 65.110554 193.511418 65.110554 194.307027 \nC 65.110554 195.102637 65.426653 195.865767 65.989233 196.428348 \nC 66.551814 196.990928 67.314944 197.307027 68.110554 197.307027 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 72.148546 173.396138 \nC 72.944155 173.396138 73.707285 173.080039 74.269866 172.517459 \nC 74.832447 171.954878 75.148546 171.191748 75.148546 170.396138 \nC 75.148546 169.600529 74.832447 168.837399 74.269866 168.274818 \nC 73.707285 167.712237 72.944155 167.396138 72.148546 167.396138 \nC 71.352936 167.396138 70.589806 167.712237 70.027225 168.274818 \nC 69.464645 168.837399 69.148546 169.600529 69.148546 170.396138 \nC 69.148546 171.191748 69.464645 171.954878 70.027225 172.517459 \nC 70.589806 173.080039 71.352936 173.396138 72.148546 173.396138 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 87.146801 132.792018 \nC 87.94241 132.792018 88.705541 132.475919 89.268121 131.913338 \nC 89.830702 131.350758 90.146801 130.587627 90.146801 129.792018 \nC 90.146801 128.996409 89.830702 128.233279 89.268121 127.670698 \nC 88.705541 127.108117 87.94241 126.792018 87.146801 126.792018 \nC 86.351192 126.792018 85.588061 127.108117 85.025481 127.670698 \nC 84.4629 128.233279 84.146801 128.996409 84.146801 129.792018 \nC 84.146801 130.587627 84.4629 131.350758 85.025481 131.913338 \nC 85.588061 132.475919 86.351192 132.792018 87.146801 132.792018 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 144.832399 180.27321 \nC 145.628008 180.27321 146.391139 179.957111 146.953719 179.39453 \nC 147.5163 178.83195 147.832399 178.068819 147.832399 177.27321 \nC 147.832399 176.477601 147.5163 175.71447 146.953719 175.15189 \nC 146.391139 174.589309 145.628008 174.27321 144.832399 174.27321 \nC 144.03679 174.27321 143.273659 174.589309 142.711079 175.15189 \nC 142.148498 175.71447 141.832399 176.477601 141.832399 177.27321 \nC 141.832399 178.068819 142.148498 178.83195 142.711079 179.39453 \nC 143.273659 179.957111 144.03679 180.27321 144.832399 180.27321 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 370.959943 199.235458 \nC 371.755552 199.235458 372.518683 198.919359 373.081264 198.356779 \nC 373.643844 197.794198 373.959943 197.031068 373.959943 196.235458 \nC 373.959943 195.439849 373.643844 194.676719 373.081264 194.114138 \nC 372.518683 193.551557 371.755552 193.235458 370.959943 193.235458 \nC 370.164334 193.235458 369.401204 193.551557 368.838623 194.114138 \nC 368.276042 194.676719 367.959943 195.439849 367.959943 196.235458 \nC 367.959943 197.031068 368.276042 197.794198 368.838623 198.356779 \nC 369.401204 198.919359 370.164334 199.235458 370.959943 199.235458 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 68.110554 178.141101 \nC 68.906163 178.141101 69.669293 177.825002 70.231874 177.262421 \nC 70.794455 176.699841 71.110554 175.93671 71.110554 175.141101 \nC 71.110554 174.345492 70.794455 173.582361 70.231874 173.019781 \nC 69.669293 172.4572 68.906163 172.141101 68.110554 172.141101 \nC 67.314944 172.141101 66.551814 172.4572 65.989233 173.019781 \nC 65.426653 173.582361 65.110554 174.345492 65.110554 175.141101 \nC 65.110554 175.93671 65.426653 176.699841 65.989233 177.262421 \nC 66.551814 177.825002 67.314944 178.141101 68.110554 178.141101 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.706406 162.823943 \nC 71.502015 162.823943 72.265145 162.507844 72.827726 161.945264 \nC 73.390307 161.382683 73.706406 160.619553 73.706406 159.823943 \nC 73.706406 159.028334 73.390307 158.265204 72.827726 157.702623 \nC 72.265145 157.140042 71.502015 156.823943 70.706406 156.823943 \nC 69.910796 156.823943 69.147666 157.140042 68.585085 157.702623 \nC 68.022505 158.265204 67.706406 159.028334 67.706406 159.823943 \nC 67.706406 160.619553 68.022505 161.382683 68.585085 161.945264 \nC 69.147666 162.507844 69.910796 162.823943 70.706406 162.823943 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 75.898109 183.555175 \nC 76.693719 183.555175 77.456849 183.239076 78.01943 182.676495 \nC 78.582011 182.113915 78.898109 181.350784 78.898109 180.555175 \nC 78.898109 179.759566 78.582011 178.996436 78.01943 178.433855 \nC 77.456849 177.871274 76.693719 177.555175 75.898109 177.555175 \nC 75.1025 177.555175 74.33937 177.871274 73.776789 178.433855 \nC 73.214208 178.996436 72.898109 179.759566 72.898109 180.555175 \nC 72.898109 181.350784 73.214208 182.113915 73.776789 182.676495 \nC 74.33937 183.239076 75.1025 183.555175 75.898109 183.555175 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 86.281517 190.716676 \nC 87.077126 190.716676 87.840257 190.400577 88.402837 189.837997 \nC 88.965418 189.275416 89.281517 188.512286 89.281517 187.716676 \nC 89.281517 186.921067 88.965418 186.157937 88.402837 185.595356 \nC 87.840257 185.032775 87.077126 184.716676 86.281517 184.716676 \nC 85.485908 184.716676 84.722778 185.032775 84.160197 185.595356 \nC 83.597616 186.157937 83.281517 186.921067 83.281517 187.716676 \nC 83.281517 188.512286 83.597616 189.275416 84.160197 189.837997 \nC 84.722778 190.400577 85.485908 190.716676 86.281517 190.716676 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 107.048332 20.084911 \nC 107.843942 20.084911 108.607072 19.768812 109.169653 19.206231 \nC 109.732233 18.64365 110.048332 17.88052 110.048332 17.084911 \nC 110.048332 16.289302 109.732233 15.526171 109.169653 14.963591 \nC 108.607072 14.40101 107.843942 14.084911 107.048332 14.084911 \nC 106.252723 14.084911 105.489593 14.40101 104.927012 14.963591 \nC 104.364431 15.526171 104.048332 16.289302 104.048332 17.084911 \nC 104.048332 17.88052 104.364431 18.64365 104.927012 19.206231 \nC 105.489593 19.768812 106.252723 20.084911 107.048332 20.084911 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.096307 188.436374 \nL 66.596307 186.936374 \nL 68.096307 188.436374 \nL 69.596307 186.936374 \nL 68.096307 185.436374 \nL 69.596307 183.936374 \nL 68.096307 182.436374 \nL 66.596307 183.936374 \nL 65.096307 182.436374 \nL 63.596307 183.936374 \nL 65.096307 185.436374 \nL 63.596307 186.936374 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 192.105803 \nL 68.110554 190.605803 \nL 69.610554 192.105803 \nL 71.110554 190.605803 \nL 69.610554 189.105803 \nL 71.110554 187.605803 \nL 69.610554 186.105803 \nL 68.110554 187.605803 \nL 66.610554 186.105803 \nL 65.110554 187.605803 \nL 66.610554 189.105803 \nL 65.110554 190.605803 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 70.648546 183.560726 \nL 72.148546 182.060726 \nL 73.648546 183.560726 \nL 75.148546 182.060726 \nL 73.648546 180.560726 \nL 75.148546 179.060726 \nL 73.648546 177.560726 \nL 72.148546 179.060726 \nL 70.648546 177.560726 \nL 69.148546 179.060726 \nL 70.648546 180.560726 \nL 69.148546 182.060726 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 85.646801 196.399052 \nL 87.146801 194.899052 \nL 88.646801 196.399052 \nL 90.146801 194.899052 \nL 88.646801 193.399052 \nL 90.146801 191.899052 \nL 88.646801 190.399052 \nL 87.146801 191.899052 \nL 85.646801 190.399052 \nL 84.146801 191.899052 \nL 85.646801 193.399052 \nL 84.146801 194.899052 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 143.332399 189.369359 \nL 144.832399 187.869359 \nL 146.332399 189.369359 \nL 147.832399 187.869359 \nL 146.332399 186.369359 \nL 147.832399 184.869359 \nL 146.332399 183.369359 \nL 144.832399 184.869359 \nL 143.332399 183.369359 \nL 141.832399 184.869359 \nL 143.332399 186.369359 \nL 141.832399 187.869359 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 369.459943 122.408596 \nL 370.959943 120.908596 \nL 372.459943 122.408596 \nL 373.959943 120.908596 \nL 372.459943 119.408596 \nL 373.959943 117.908596 \nL 372.459943 116.408596 \nL 370.959943 117.908596 \nL 369.459943 116.408596 \nL 367.959943 117.908596 \nL 369.459943 119.408596 \nL 367.959943 120.908596 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 65.312628 181.717081 \nL 66.812628 180.217081 \nL 68.312628 181.717081 \nL 69.812628 180.217081 \nL 68.312628 178.717081 \nL 69.812628 177.217081 \nL 68.312628 175.717081 \nL 66.812628 177.217081 \nL 65.312628 175.717081 \nL 63.812628 177.217081 \nL 65.312628 178.717081 \nL 63.812628 180.217081 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 66.610554 204.643564 \nL 68.110554 203.143564 \nL 69.610554 204.643564 \nL 71.110554 203.143564 \nL 69.610554 201.643564 \nL 71.110554 200.143564 \nL 69.610554 198.643564 \nL 68.110554 200.143564 \nL 66.610554 198.643564 \nL 65.110554 200.143564 \nL 66.610554 201.643564 \nL 65.110554 203.143564 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 69.206406 194.507387 \nL 70.706406 193.007387 \nL 72.206406 194.507387 \nL 73.706406 193.007387 \nL 72.206406 191.507387 \nL 73.706406 190.007387 \nL 72.206406 188.507387 \nL 70.706406 190.007387 \nL 69.206406 188.507387 \nL 67.706406 190.007387 \nL 69.206406 191.507387 \nL 67.706406 193.007387 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 74.398109 191.348732 \nL 75.898109 189.848732 \nL 77.398109 191.348732 \nL 78.898109 189.848732 \nL 77.398109 188.348732 \nL 78.898109 186.848732 \nL 77.398109 185.348732 \nL 75.898109 186.848732 \nL 74.398109 185.348732 \nL 72.898109 186.848732 \nL 74.398109 188.348732 \nL 72.898109 189.848732 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 84.781517 208.647354 \nL 86.281517 207.147354 \nL 87.781517 208.647354 \nL 89.281517 207.147354 \nL 87.781517 205.647354 \nL 89.281517 204.147354 \nL 87.781517 202.647354 \nL 86.281517 204.147354 \nL 84.781517 202.647354 \nL 83.281517 204.147354 \nL 84.781517 205.647354 \nL 83.281517 207.147354 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p00fa8979ae)\" d=\"M 105.548332 176.691089 \nL 107.048332 175.191089 \nL 108.548332 176.691089 \nL 110.048332 175.191089 \nL 108.548332 173.691089 \nL 110.048332 172.191089 \nL 108.548332 170.691089 \nL 107.048332 172.191089 \nL 105.548332 170.691089 \nL 104.048332 172.191089 \nL 105.548332 173.691089 \nL 104.048332 175.191089 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n   </g>\n   <g id=\"PathCollection_2\"/>\n   <g id=\"PathCollection_3\"/>\n   <g id=\"PathCollection_4\"/>\n   <g id=\"PathCollection_5\"/>\n   <g id=\"PathCollection_6\"/>\n   <g id=\"PathCollection_7\"/>\n   <g id=\"PathCollection_8\"/>\n   <g id=\"PathCollection_9\"/>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m4208e8ab7a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.361959\" xlink:href=\"#m4208e8ab7a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(63.180709 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"111.428833\" xlink:href=\"#m4208e8ab7a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2500 -->\n      <g transform=\"translate(98.703833 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"156.495706\" xlink:href=\"#m4208e8ab7a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5000 -->\n      <g transform=\"translate(143.770706 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.562579\" xlink:href=\"#m4208e8ab7a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 7500 -->\n      <g transform=\"translate(188.837579 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.629453\" xlink:href=\"#m4208e8ab7a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10000 -->\n      <g transform=\"translate(230.723203 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"291.696326\" xlink:href=\"#m4208e8ab7a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12500 -->\n      <g transform=\"translate(275.790076 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"336.7632\" xlink:href=\"#m4208e8ab7a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 15000 -->\n      <g transform=\"translate(320.85695 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"381.830073\" xlink:href=\"#m4208e8ab7a\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 17500 -->\n      <g transform=\"translate(365.923823 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- parameters -->\n     <g transform=\"translate(189.776563 252.916563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"165.869141\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"324.560547\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"386.083984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"425.292969\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"486.816406\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"527.929688\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m5e1bbf5060\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m5e1bbf5060\" y=\"156.242053\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(20.878125 160.041272)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m5e1bbf5060\" y=\"70.194062\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(20.878125 73.993281)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m5bdf207d4e\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"216.387018\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"201.234719\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"190.483992\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"182.14508\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"175.331693\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"169.571056\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"164.580965\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"160.179393\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"130.339027\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"115.186728\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"104.436001\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"96.097089\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"89.283702\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"83.523065\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"78.532974\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"74.131403\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"44.291036\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"29.138737\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"18.38801\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5bdf207d4e\" y=\"10.049098\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 125.577813)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 224.64 \nL 51.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.178125 224.64 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 7.2 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 185.070312 219.64 \nL 252.485938 219.64 \nQ 254.485938 219.64 254.485938 217.64 \nL 254.485938 71.85875 \nQ 254.485938 69.85875 252.485938 69.85875 \nL 185.070312 69.85875 \nQ 183.070312 69.85875 183.070312 71.85875 \nL 183.070312 217.64 \nQ 183.070312 219.64 185.070312 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- layers -->\n     <g transform=\"translate(195.070312 81.457188)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"209.765625\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"250.878906\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_10\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m1fda646e72\" style=\"stroke:#edd1cb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#edd1cb;stroke:#edd1cb;\" x=\"197.070312\" xlink:href=\"#m1fda646e72\" y=\"93.510313\"/>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- 1 -->\n     <g transform=\"translate(215.070312 96.135313)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_11\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m5770b19a25\" style=\"stroke:#ebcac5;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ebcac5;stroke:#ebcac5;\" x=\"197.070312\" xlink:href=\"#m5770b19a25\" y=\"108.188438\"/>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- 2 -->\n     <g transform=\"translate(215.070312 110.813438)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_12\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m9a004dabd0\" style=\"stroke:#e5bcbb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#e5bcbb;stroke:#e5bcbb;\" x=\"197.070312\" xlink:href=\"#m9a004dabd0\" y=\"122.866563\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- 4 -->\n     <g transform=\"translate(215.070312 125.491563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-34\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_13\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"ma30cb5aa51\" style=\"stroke:#d89faa;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#d89faa;stroke:#d89faa;\" x=\"197.070312\" xlink:href=\"#ma30cb5aa51\" y=\"137.544688\"/>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- 8 -->\n     <g transform=\"translate(215.070312 140.169688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-38\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_14\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m2d80268de5\" style=\"stroke:#ae6b91;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ae6b91;stroke:#ae6b91;\" x=\"197.070312\" xlink:href=\"#m2d80268de5\" y=\"152.222813\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- 16 -->\n     <g transform=\"translate(215.070312 154.847813)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_15\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"ma4e563e23f\" style=\"stroke:#2d1e3e;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#2d1e3e;stroke:#2d1e3e;\" x=\"197.070312\" xlink:href=\"#ma4e563e23f\" y=\"166.900938\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- 32 -->\n     <g transform=\"translate(215.070312 169.525938)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- model -->\n     <g transform=\"translate(195.070312 184.204063)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_16\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m027ecfedc6\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"197.070312\" xlink:href=\"#m027ecfedc6\" y=\"196.257188\"/>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- FFNN -->\n     <g transform=\"translate(215.070312 198.882188)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_17\">\n     <defs>\n      <path d=\"M -1.5 3 \nL 0 1.5 \nL 1.5 3 \nL 3 1.5 \nL 1.5 0 \nL 3 -1.5 \nL 1.5 -3 \nL 0 -1.5 \nL -1.5 -3 \nL -3 -1.5 \nL -1.5 0 \nL -3 1.5 \nz\n\" id=\"mbe823a1361\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"197.070312\" xlink:href=\"#mbe823a1361\" y=\"210.935313\"/>\n     </g>\n    </g>\n    <g id=\"text_22\">\n     <!-- ResNet -->\n     <g transform=\"translate(215.070312 213.560313)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p00fa8979ae\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"51.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "216 260\n"
     ]
    }
   ],
   "source": [
    "print(len(d_results[d_results.loss < 2]), len(d_results))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [
    {
     "data": {
      "text/plain": "<generator object Module.parameters at 0x1590089e0>"
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_results[\"models\"][0][0].parameters()\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}