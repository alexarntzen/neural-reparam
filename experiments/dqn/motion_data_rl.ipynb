{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DQN experiment 3\n",
    "\n",
    "This notebook implements Experiment 1 in Chapter 3.\n",
    "\n",
    "We find a reparametrization of motion capture data using DQN.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from matplotlib.lines import Line2D\n",
    "import extratorch as etorch\n",
    "\n",
    "import neural_reparam as nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reproducible\n",
    "seed = torch.manual_seed(0)\n",
    "\n",
    "# better plotting\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "matplotlib.rcParams.update({\"font.size\": 12})\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from signatureshape.animation.animation_manager import fetch_animations, unpack\n",
    "from signatureshape.so3.curves import move_origin_to_zero\n",
    "from signatureshape.so3.helpers import crop_curve\n",
    "from signatureshape.so3.dynamic_distance import (\n",
    "    find_optimal_diffeomorphism,\n",
    "    create_shared_parameterization,\n",
    ")\n",
    "from signatureshape.so3.transformations import skew_to_vector, SRVT\n",
    "from signatureshape.so3 import animation_to_SO3\n",
    "\n",
    "\n",
    "print(\"Load data:\")\n",
    "\n",
    "max_frame_count = 360  # 3 sek\n",
    "data = [\n",
    "    fetch_animations(1, file_name=\"39_02.amc\"),  # walk 6.5 steps\n",
    "    fetch_animations(1, file_name=\"35_26.amc\"),  # run/jog 3 steps\n",
    "    fetch_animations(1, file_name=\"16_35.amc\"),  # run/jog 3 steps\n",
    "    fetch_animations(\n",
    "        1, file_name=\"38_04.amc\"\n",
    "    ),  # walk around, frequent turns, cyclic walk along a line\n",
    "    fetch_animations(1, file_name=\"07_01.amc\"),  # walk\n",
    "    fetch_animations(1, file_name=\"07_12.amc\"),  # brisk walk\n",
    "]\n",
    "\n",
    "# walk\n",
    "subject, animation, desc0 = unpack(data[3])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=max_frame_count)\n",
    "c_0 = move_origin_to_zero(curve)\n",
    "print(\"desc 0:\", desc0)\n",
    "\n",
    "# run\n",
    "subject, animation, desc1 = unpack(data[5])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=max_frame_count)  # first 2 seconds\n",
    "c_1 = move_origin_to_zero(curve)\n",
    "print(\"desc 1:\", desc1)\n",
    "\n",
    "# calculate distances\n",
    "I0 = np.linspace(0, 1, c_0.shape[1])\n",
    "I1 = np.linspace(0, 1, c_1.shape[1])\n",
    "q_data_ = skew_to_vector(SRVT(c_0, I0))\n",
    "r_data_ = skew_to_vector(SRVT(c_1, I1))\n",
    "I, q_data, r_data = create_shared_parameterization(q0=q_data_, q1=r_data_, I0=I0, I1=I1)\n",
    "shared_frames = I.shape[0]\n",
    "print(\"shared_frames:\", shared_frames)\n",
    "\n",
    "# data used to create env\n",
    "data = (I[::8], q_data[::8], r_data[::8])\n",
    "print(\"Shorted frames:\", len(data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 4\n",
    "get_env = partial(nr.reparam_env.DiscreteReparamEnv, data=data)\n",
    "nlist = np.array([len(data[0])])\n",
    "\n",
    "#  ######\n",
    "DIR = \"../figures/motion_data_rl/\"\n",
    "SET_NAME = f\"dqn_1_N{nlist[0]}\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "########\n",
    "\n",
    "default_env = get_env(size=nlist[0], depth=depth)\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": [etorch.FFNN],\n",
    "    \"input_dimension\": [2],\n",
    "    \"output_dimension\": [default_env.num_actions],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"n_hidden_layers\": [5],\n",
    "    \"neurons\": [32],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"get_env\": [get_env],\n",
    "    \"epsilon\": [0.05, 0.01, 0.005],\n",
    "    \"DDQN\": [True, False],\n",
    "    \"update_every\": [10, 100, 1000],\n",
    "    \"double_search\": [True, False],\n",
    "    \"optimizer\": [\"ADAM\"],\n",
    "    \"num_epochs\": [200],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "    \"verbose\": [True],\n",
    "}\n",
    "EXTRA_TRAINING_PARAMS = {\n",
    "    \"N\": nlist,  # for easier plotting\n",
    "    \"batch_size\": nlist * 2,\n",
    "    \"initial_steps\": nlist * 20,\n",
    "    \"memory_size\": nlist * 100,\n",
    "    \"env_kwargs\": [dict(size=n, depth=depth) for n in nlist],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter = etorch.create_subdictionary_iterator(MODEL_PARAMS)\n",
    "\n",
    "t_iter_temp_1 = etorch.create_subdictionary_iterator(\n",
    "    EXTRA_TRAINING_PARAMS, product=False\n",
    ")\n",
    "t_iter_temp_2 = etorch.create_subdictionary_iterator(\n",
    "    TRAINING_PARAMS,\n",
    ")\n",
    "\n",
    "training_params_iter = etorch.add_dictionary_iterators(\n",
    "    t_iter_temp_1, t_iter_temp_2, product=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = etorch.k_fold_cv_grid(\n",
    "    model_params=model_params_iter,\n",
    "    fit=nr.rl.fit_dqn_deterministic,\n",
    "    training_params=training_params_iter,\n",
    "    folds=1,\n",
    "    verbose=True,\n",
    "    trials=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all solutions\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"env\": default_env,\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "}\n",
    "etorch.plotting.plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    plot_function=nr.plot_solution_rl,\n",
    "    function_kwargs=plot_kwargs,\n",
    "    **cv_results\n",
    ")\n",
    "PATH_FIGURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine histories and log\n",
    "histories_iter = (df[-1:] for df in chain(*cv_results[\"histories\"]))\n",
    "history_df = pd.concat(histories_iter)\n",
    "history_df.index = range(len(history_df))\n",
    "log_df = pd.concat(\n",
    "    [cv_results[\"model_params\"], cv_results[\"training_params\"], history_df], axis=1\n",
    ")\n",
    "log_df.columns = log_df.columns.str.replace(\"double_search\", \"2-greedy\")\n",
    "log_df.columns = log_df.columns.str.replace(\"Cost\", \"Final cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find DP, greedy solution and plot it\n",
    "from neural_reparam.plotting import plot_value_func\n",
    "\n",
    "n_cost = {\"dp\": [], \"greedy\": []}\n",
    "for n in nlist:\n",
    "    n_env = get_env(size=n, depth=depth)\n",
    "    I1_new, path, A = find_optimal_diffeomorphism(\n",
    "        q0=n_env.q_data,\n",
    "        q1=n_env.r_data,\n",
    "        I0=n_env.t_data,\n",
    "        I1=n_env.t_data,\n",
    "        depth=depth,\n",
    "        return_all=True,\n",
    "    )\n",
    "    n_cost[\"greedy\"].append(-nr.rl.get_value(model=None, env=n_env, double_search=True))\n",
    "    n_cost[\"dp\"].append(\n",
    "        nr.rl.get_path_value(path=path, reward_func=n_env.dp_local_cost)\n",
    "    )\n",
    "    fig = plot_value_func(\n",
    "        value_matrix=A[::-1, ::-1],\n",
    "        t_data=n_env.t_data,\n",
    "        path=path,\n",
    "        path_figures=PATH_FIGURES,\n",
    "        plot_name=f\"dp_solution_{n}\",\n",
    "        x_axis=\"t\",\n",
    "        y_axis=\"$\\\\varphi(t)$\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amke log_df with Method\n",
    "solutions_df1 = log_df[[\"N\", \"Final cost\"]].copy()\n",
    "solutions_df1[\"Method\"] = \"DQN\"\n",
    "solutions_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add greedy and dp  solutions to table\n",
    "temp = pd.DataFrame(n_cost)\n",
    "temp[\"N\"] = nlist\n",
    "temp.columns = temp.columns.str.replace(\"dp\", \"Dynamic programming\")\n",
    "temp.columns = temp.columns.str.replace(\"greedy\", \"Greedy\")\n",
    "solutions_df2 = pd.melt(temp, id_vars=[\"N\"], var_name=\"Method\", value_name=\"Final cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all solutions in histplot\n",
    "fig = sns.displot(\n",
    "    log_df,\n",
    "    x=\"Final cost\",\n",
    "    bins=20,\n",
    "    alpha=1,\n",
    "    log_scale=(False, True),\n",
    ")\n",
    "fig.savefig(os.path.join(PATH_FIGURES, \"end_cost.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot final cost and q_loss\n",
    "dp_distance, greedy_distance = n_cost[\"dp\"][-1], n_cost[\"greedy\"][-1]\n",
    "\n",
    "sns.relplot(\n",
    "    data=log_df,\n",
    "    x=\"Final cost\",\n",
    "    y=\"Q loss\",\n",
    "    hue=\"2-greedy\",\n",
    "    style=\"2-greedy\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "#\n",
    "ylim = plt.ylim()\n",
    "(line1,) = plt.plot([dp_distance, dp_distance], ylim, lw=1, label=\"Minimal cost\")\n",
    "(line2,) = plt.plot(\n",
    "    [greedy_distance, greedy_distance],\n",
    "    ylim,\n",
    "    color=\"grey\",\n",
    "    ls=\"dashed\",\n",
    "    lw=1,\n",
    "    label=\"Greedy strategy cost\",\n",
    ")\n",
    "plt.legend(handles=[line1, line2])\n",
    "# plt.xticks([10,100])\n",
    "# plt.xlim([10,None])\n",
    "plt.savefig(os.path.join(PATH_FIGURES, \"q_loss_and_cost.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better = len(log_df[log_df[\"Final cost\"] < greedy_distance])\n",
    "all = len(log_df)\n",
    "\n",
    "print(f\"Number better than greedy strategy: {better }, total: {all} ({better/all}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot solutions for different N\n",
    "sns.catplot(\n",
    "    x=\"N\", y=\"Final cost\", data=solutions_df1, kind=\"box\", hue=\"Method\", legend=False\n",
    ")\n",
    "\n",
    "sns.pointplot(\n",
    "    x=\"N\",\n",
    "    y=\"Final cost\",\n",
    "    data=solutions_df2,\n",
    "    kind=\"point\",\n",
    "    hue=\"Method\",\n",
    "    markers=[\"o\", \"x\"],\n",
    "    linestyles=[\"-\", \"--\"],\n",
    "    alpha=1,\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig(os.path.join(PATH_FIGURES, \"q_loss_different_n.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best and worst trial\n",
    "print(\"Best index:\", log_df.sort_values(\"Final cost\")[0:1].index[0])\n",
    "print(\"Worst index:\", log_df.sort_values(\"Final cost\")[-2:-1].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and plot histories properly\n",
    "model_num = 19\n",
    "\n",
    "hist_file_name = f\"history_plot_{model_num}_0.csv\"\n",
    "hist = pd.read_csv(os.path.join(PATH_FIGURES, hist_file_name))\n",
    "\n",
    "sns.lineplot(x=\"Episode\", y=\"Q loss\", data=hist)\n",
    "plt.yscale(\"log\")\n",
    "ax2 = plt.twinx()\n",
    "sns.lineplot(x=\"Episode\", y=\"Cost\", data=hist, ax=ax2, color=\"#FF800E\", linestyle=\"--\")\n",
    "ax2.legend(\n",
    "    handles=[\n",
    "        Line2D([], [], label=\"Q loss\"),\n",
    "        Line2D([], [], color=\"#FF800E\", linestyle=\"--\", label=\"Cost\"),\n",
    "    ]\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PATH_FIGURES, \"plot_\" + hist_file_name[:-4]) + \".pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
