{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "from itertools import chain\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from matplotlib.lines import Line2D\n",
    "import extratorch as etorch\n",
    "import neural_reparam as nr\n",
    "\n",
    "import experiments.curves_2 as c\n",
    "from signatureshape.so3.dynamic_distance import (\n",
    "    find_optimal_diffeomorphism,\n",
    ")\n",
    "\n",
    "# make reproducible\n",
    "seed = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make reproducible\n",
    "seed = torch.manual_seed(0)\n",
    "\n",
    "# better plotting\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "matplotlib.rcParams.update({\"font.size\": 12})\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "plt.style.use(\"tableau-colorblind10\")\n",
    "sns.set_style(\"white\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup model parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_env = partial(nr.reparam_env.DiscreteReparamEnv, r_func=c.r, q_func=c.q)\n",
    "lr_scheduler = partial(\n",
    "    torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    mode=\"min\",\n",
    "    factor=0.5,\n",
    "    patience=20,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depth = 4\n",
    "nlist = np.array([32])\n",
    "#  ######\n",
    "DIR = \"../figures/curve_2_rl/\"\n",
    "SET_NAME = f\"dqn_1_N{nlist[0]}\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "########\n",
    "\n",
    "default_env = get_env(size=nlist[0], depth=depth)\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": [etorch.FFNN],\n",
    "    \"input_dimension\": [2],\n",
    "    \"output_dimension\": [default_env.num_actions],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"n_hidden_layers\": [5],\n",
    "    \"neurons\": [32],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"get_env\": [get_env],\n",
    "    \"epsilon\": [0.05, 0.01, 0.005],\n",
    "    \"DDQN\": [True, False],\n",
    "    \"update_every\": [10, 100],\n",
    "    \"double_search\": [True, False],\n",
    "    \"optimizer\": [\"ADAM\"],\n",
    "    \"num_epochs\": [200],\n",
    "    \"learning_rate\": [0.1, 0.01, 0.001],\n",
    "}\n",
    "EXTRA_TRAINING_PARAMS = {\n",
    "    \"N\": nlist,  # for easier plotting\n",
    "    \"batch_size\": nlist * 2,\n",
    "    \"initial_steps\": nlist * 20,\n",
    "    \"memory_size\": nlist * 100,\n",
    "    \"env_kwargs\": [dict(size=n, depth=depth) for n in nlist],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter = etorch.create_subdictionary_iterator(MODEL_PARAMS)\n",
    "\n",
    "t_iter_temp_1 = etorch.create_subdictionary_iterator(\n",
    "    EXTRA_TRAINING_PARAMS, product=False\n",
    ")\n",
    "t_iter_temp_2 = etorch.create_subdictionary_iterator(\n",
    "    TRAINING_PARAMS,\n",
    ")\n",
    "\n",
    "training_params_iter = etorch.add_dictionary_iterators(\n",
    "    t_iter_temp_1, t_iter_temp_2, product=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = etorch.k_fold_cv_grid(\n",
    "    model_params=model_params_iter,\n",
    "    fit=nr.rl.fit_dqn_deterministic,\n",
    "    training_params=training_params_iter,\n",
    "    folds=1,\n",
    "    verbose=True,\n",
    "    trials=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all solutions\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"env\": default_env,\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "}\n",
    "etorch.plotting.plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    plot_function=nr.plot_solution_rl,\n",
    "    function_kwargs=plot_kwargs,\n",
    "    **cv_results\n",
    ")\n",
    "PATH_FIGURES"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot and store results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine histories and log\n",
    "histories_iter = (df[-1:] for df in chain(*cv_results[\"histories\"]))\n",
    "history_df = pd.concat(histories_iter)\n",
    "history_df.index = range(len(history_df))\n",
    "log_df = pd.concat(\n",
    "    [cv_results[\"model_params\"], cv_results[\"training_params\"], history_df], axis=1\n",
    ")\n",
    "log_df.columns = log_df.columns.str.replace(\"double_search\", \"2-greedy\")\n",
    "log_df.columns = log_df.columns.str.replace(\"Cost\", \"Final cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find DP  and greedy solution and plot it\n",
    "\n",
    "n_cost = {\"dp\": [], \"greedy\": []}\n",
    "for n in nlist:\n",
    "    n_env = get_env(size=n, depth=depth)\n",
    "    I1_new, path, A = find_optimal_diffeomorphism(\n",
    "        q0=n_env.q_data,\n",
    "        q1=n_env.r_data,\n",
    "        I0=n_env.t_data,\n",
    "        I1=n_env.t_data,\n",
    "        depth=depth,\n",
    "        return_all=True,\n",
    "    )\n",
    "    n_cost[\"greedy\"].append(-nr.rl.get_value(model=None, env=n_env, double_search=True))\n",
    "    n_cost[\"dp\"].append(\n",
    "        nr.rl.get_path_value(path=path, reward_func=n_env.dp_local_cost)\n",
    "    )\n",
    "    fig = nr.plotting.plot_value_func(\n",
    "        value_matrix=A[::-1, ::-1],\n",
    "        t_data=n_env.t_data,\n",
    "        path=path,\n",
    "        path_figures=PATH_FIGURES,\n",
    "        plot_name=f\"dp_solution_{n}\",\n",
    "        x_axis=\"t\",\n",
    "        y_axis=\"$\\\\varphi(t)$\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amke log_df with Method\n",
    "solutions_df1 = log_df[[\"N\", \"Final cost\"]].copy()\n",
    "solutions_df1[\"Method\"] = \"DQN\"\n",
    "solutions_df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add greedy and dp  solutions to table\n",
    "temp = pd.DataFrame(n_cost)\n",
    "temp[\"N\"] = nlist\n",
    "temp.columns = temp.columns.str.replace(\"dp\", \"Dynamic programming\")\n",
    "temp.columns = temp.columns.str.replace(\"greedy\", \"Greedy\")\n",
    "solutions_df2 = pd.melt(temp, id_vars=[\"N\"], var_name=\"Method\", value_name=\"Final cost\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot all solutions in histplot\n",
    "fig = sns.displot(\n",
    "    log_df,\n",
    "    x=\"Final cost\",\n",
    "    bins=20,\n",
    "    alpha=1,\n",
    "    log_scale=(False, True),\n",
    ")\n",
    "fig.savefig(os.path.join(PATH_FIGURES, \"end_cost.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot final cost and q_loss\n",
    "dp_distance, greedy_distance = n_cost[\"dp\"][-1], n_cost[\"greedy\"][-1]\n",
    "\n",
    "sns.relplot(\n",
    "    data=log_df,\n",
    "    x=\"Final cost\",\n",
    "    y=\"Q loss\",\n",
    "    hue=\"2-greedy\",\n",
    "    style=\"2-greedy\",\n",
    ")\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "#\n",
    "ylim = plt.ylim()\n",
    "(line1,) = plt.plot([dp_distance, dp_distance], ylim, lw=1, label=\"Minimal cost\")\n",
    "(line2,) = plt.plot(\n",
    "    [greedy_distance, greedy_distance],\n",
    "    ylim,\n",
    "    color=\"grey\",\n",
    "    ls=\"dashed\",\n",
    "    lw=1,\n",
    "    label=\"Greedy strategy cost\",\n",
    ")\n",
    "(line3,) = plt.plot(\n",
    "    [c.DIST_R_Q, c.DIST_R_Q], ylim, lw=1, label=\"Continuous problem minimal cost\"\n",
    ")\n",
    "plt.legend(handles=[line1, line2, line3])\n",
    "# plt.xticks([10,100])\n",
    "# plt.xlim([10,None])\n",
    "plt.savefig(os.path.join(PATH_FIGURES, \"q_loss_and_cost.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "better = len(log_df[log_df[\"Final cost\"] < greedy_distance])\n",
    "all = len(log_df)\n",
    "print(f\"Number better than greedy strategy: {better }, total: {all} ({better/all}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot solutions for different N\n",
    "sns.catplot(\n",
    "    x=\"N\", y=\"Final cost\", data=solutions_df1, kind=\"box\", hue=\"Method\", legend=False\n",
    ")\n",
    "\n",
    "sns.pointplot(\n",
    "    x=\"N\",\n",
    "    y=\"Final cost\",\n",
    "    data=solutions_df2,\n",
    "    kind=\"point\",\n",
    "    hue=\"Method\",\n",
    "    markers=[\"o\", \"x\"],\n",
    "    linestyles=[\"-\", \"--\"],\n",
    "    alpha=1,\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.savefig(os.path.join(PATH_FIGURES, \"q_loss_different_n.pdf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the best and worst trial\n",
    "print(\"Best index:\", log_df.sort_values(\"Final cost\")[0:1].index[0])\n",
    "print(\"Worst index:\", log_df.sort_values(\"Final cost\")[-2:-1].index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 39\n",
    "# Read and plot histories properly\n",
    "hist_file_name = f\"history_plot_{index}_{0}.csv\"\n",
    "hist = pd.read_csv(os.path.join(PATH_FIGURES, hist_file_name))\n",
    "\n",
    "sns.lineplot(x=\"Episode\", y=\"Q loss\", data=hist)\n",
    "plt.yscale(\"log\")\n",
    "ax2 = plt.twinx()\n",
    "sns.lineplot(x=\"Episode\", y=\"Cost\", data=hist, ax=ax2, color=\"#FF800E\", linestyle=\"--\")\n",
    "ax2.legend(\n",
    "    handles=[\n",
    "        Line2D([], [], label=\"Q loss\"),\n",
    "        Line2D([], [], color=\"#FF800E\", linestyle=\"--\", label=\"Cost\"),\n",
    "    ]\n",
    ")\n",
    "plt.yscale(\"log\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(PATH_FIGURES, \"plot_\" + hist_file_name[:-4]) + \".pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
