{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Test on piecewise constant curves in so3"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from deep_reparametrization.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from deep_reparametrization.helpers import get_pc_curve_from_data, get_pl_curve_from_data\n",
    "from deep_reparametrization.ResNET import ResNET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "run/jog\n",
      "run/jog\n",
      "(23, 161, 3, 3)\n",
      "(23, 137, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from animation.animation_manager import fetch_animations, unpack\n",
    "from so3.curves import move_origin_to_zero, dynamic_distance\n",
    "from so3.helpers import crop_curve\n",
    "from so3.dynamic_distance import find_optimal_diffeomorphism, create_shared_parameterization\n",
    "from so3.clustering.id_set import crop_curve_based_on_id, get_id_set\n",
    "from so3.transformations import skew_to_vector, SRVT\n",
    "from so3 import animation_to_SO3\n",
    "\n",
    "max_frame_count = 180\n",
    "\n",
    "id_set = get_id_set()\n",
    "print(\"Load data\")\n",
    "\n",
    "data = [fetch_animations(1, file_name=\"39_02.amc\"),  #walk 6.5 steps\n",
    "        fetch_animations(1, file_name=\"35_26.amc\"),  # run/jog 3 steps\n",
    "        fetch_animations(1, file_name=\"16_35.amc\")  # run/jog 3 steps\n",
    "        ]\n",
    "\n",
    "# walk\n",
    "subject, animation, desc0 = unpack(data[2])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=240)  #first 2 seconds\n",
    "c_0 = move_origin_to_zero(curve)\n",
    "print(desc0)\n",
    "\n",
    "# run\n",
    "subject, animation, desc1 = unpack(data[1])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=240)  #first 2 seconds\n",
    "c_1 = move_origin_to_zero(curve)\n",
    "print(desc1)\n",
    "print(c_0.shape)\n",
    "print(c_1.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#calculate distances\n",
    "I0 = np.linspace(0, 1, c_0.shape[1])\n",
    "I1 = np.linspace(0, 1, c_1.shape[1])\n",
    "q_data_ = skew_to_vector(SRVT(c_0, I0))\n",
    "r_data_ = skew_to_vector(SRVT(c_1, I1))\n",
    "I, q_data, r_data = create_shared_parameterization(q0=q_data_, q1=r_data_, I0=I0, I1=I1)\n",
    "shared_frames = I.shape[0]\n",
    "q_func = get_pc_curve_from_data(data=q_data)\n",
    "r_func = get_pc_curve_from_data(data=r_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "I1_new = find_optimal_diffeomorphism(q0=q_data, q1=r_data, I0=I, I1=I, depth=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_so3/\"\n",
    "SET_NAME = \"pc_eks_2\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "FOLDS = 1\n",
    "N = shared_frames  # training points internal\n",
    "\n",
    "loss_func = get_elastic_metric_loss(r=r_func, constrain_cost=1e3, verbose=False)\n",
    "penalty_free_loss_func = get_elastic_metric_loss(r=r_func, constrain_cost=0, verbose=False)\n",
    "lr_scheduler = lambda optimizer: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.7, patience=200,\n",
    "                                                                            verbose=True)\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": [FFNN, ResNET],\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\", \"relu\"],\n",
    "    \"n_hidden_layers\": [2],  #,8,16,64],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"neurons\": [16]\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"ADAM\"],\n",
    "    \"num_epochs\": [2000, ],\n",
    "    \"learning_rate\": [0.01],\n",
    "    \"lr_scheduler\": [lr_scheduler]\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = torch.tensor(q_data)\n",
    "data = TensorDataset(x_train, q_train)\n",
    "\n",
    "model_params_iter = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46582.25\n",
      "################################  100  ################################\n",
      "Training Loss:  123.65589905\n",
      "################################  200  ################################\n",
      "Training Loss:  121.68216705\n",
      "Epoch   250: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  120.83898926\n",
      "################################  400  ################################\n",
      "Training Loss:  120.41929626\n",
      "Epoch   451: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  119.95506287\n",
      "################################  600  ################################\n",
      "Training Loss:  118.80109406\n",
      "Epoch   652: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  117.96164703\n",
      "################################  800  ################################\n",
      "Training Loss:  116.72117615\n",
      "Epoch   853: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  115.18305206\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.26721191\n",
      "Epoch  1054: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.10198975\n",
      "################################  1200  ################################\n",
      "Training Loss:  112.33830261\n",
      "Epoch  1255: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  111.13805389\n",
      "################################  1400  ################################\n",
      "Training Loss:  109.91580963\n",
      "Epoch  1456: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  109.05377197\n",
      "################################  1600  ################################\n",
      "Training Loss:  108.58021545\n",
      "Epoch  1657: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.71488953\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.04137421\n",
      "Epoch  1858: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  106.75548553\n",
      "Final training Loss:  106.75202179\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5083.03222656\n",
      "################################  100  ################################\n",
      "Training Loss:  107.07748413\n",
      "################################  200  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch   250: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.14177704\n",
      "################################  400  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch   451: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  600  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch   652: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  800  ################################\n",
      "Training Loss:  107.16636658\n",
      "Epoch   853: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.16636658\n",
      "Epoch  1054: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.16636658\n",
      "Epoch  1255: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch  1456: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1657: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch  1858: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.16635132\n",
      "Final training Loss:  107.1417923\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  70582.4375\n",
      "################################  100  ################################\n",
      "Training Loss:  115.98453522\n",
      "################################  200  ################################\n",
      "Training Loss:  114.09526062\n",
      "Epoch   232: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  113.76426697\n",
      "################################  400  ################################\n",
      "Training Loss:  113.86727905\n",
      "Epoch   433: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  113.2769165\n",
      "################################  600  ################################\n",
      "Training Loss:  113.24917603\n",
      "Epoch   634: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  113.27575684\n",
      "################################  800  ################################\n",
      "Training Loss:  113.25424957\n",
      "Epoch   835: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  113.07502747\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.90284729\n",
      "Epoch  1036: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  112.86494446\n",
      "################################  1200  ################################\n",
      "Training Loss:  112.48693848\n",
      "Epoch  1237: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  112.52186584\n",
      "################################  1400  ################################\n",
      "Training Loss:  112.53279877\n",
      "Epoch  1438: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  112.58942413\n",
      "################################  1600  ################################\n",
      "Training Loss:  112.41577148\n",
      "Epoch  1639: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  112.22007751\n",
      "################################  1800  ################################\n",
      "Training Loss:  112.16288757\n",
      "Epoch  1840: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.1563797\n",
      "Final training Loss:  112.16228485\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4196.92919922\n",
      "################################  100  ################################\n",
      "Training Loss:  119.55843353\n",
      "################################  200  ################################\n",
      "Training Loss:  108.24123383\n",
      "Epoch   221: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  400  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch   422: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.1417923\n",
      "################################  600  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch   623: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  107.14177704\n",
      "################################  800  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch   824: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1025: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.1417923\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.14177704\n",
      "Epoch  1226: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch  1427: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.1417923\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch  1628: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1829: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.1417923\n",
      "Final training Loss:  107.14178467\n",
      "\n",
      "Running model (trial=1, mod=4, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2536.41772461\n",
      "################################  100  ################################\n",
      "Training Loss:  120.25148773\n",
      "################################  200  ################################\n",
      "Training Loss:  120.53907776\n",
      "Epoch   228: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  121.22454071\n",
      "################################  400  ################################\n",
      "Training Loss:  122.10977936\n",
      "Epoch   429: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  122.8677063\n",
      "################################  600  ################################\n",
      "Training Loss:  123.5331192\n",
      "Epoch   630: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  123.6102829\n",
      "################################  800  ################################\n",
      "Training Loss:  123.39596558\n",
      "Epoch   831: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  121.87421417\n",
      "################################  1000  ################################\n",
      "Training Loss:  120.67552948\n",
      "Epoch  1032: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.01992798\n",
      "################################  1200  ################################\n",
      "Training Loss:  112.97880554\n",
      "Epoch  1233: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  108.58231354\n",
      "################################  1400  ################################\n",
      "Training Loss:  105.03672028\n",
      "Epoch  1434: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  101.64954376\n",
      "################################  1600  ################################\n",
      "Training Loss:  99.19274902\n",
      "################################  1700  ################################\n",
      "Training Loss:  95.84858704\n",
      "################################  1800  ################################\n",
      "Training Loss:  93.11089325\n",
      "################################  1900  ################################\n",
      "Training Loss:  89.95992279\n",
      "Final training Loss:  87.19999695\n",
      "\n",
      "Running model (trial=1, mod=5, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3104.41845703\n",
      "################################  100  ################################\n",
      "Training Loss:  93.40694427\n",
      "################################  200  ################################\n",
      "Training Loss:  107.72669983\n",
      "Epoch   300: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  400  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  500  ################################\n",
      "Epoch   501: reducing learning rate of group 0 to 4.9000e-03.\n",
      "Training Loss:  107.16635895\n",
      "################################  600  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  700  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch   702: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  800  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  900  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch   903: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch  1104: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.16636658\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1305: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.16636658\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch  1506: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.14177704\n",
      "Epoch  1707: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch  1908: reducing learning rate of group 0 to 4.0354e-04.\n",
      "Final training Loss:  107.16635895\n",
      "\n",
      "Running model (trial=1, mod=6, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  100507.34375\n",
      "################################  100  ################################\n",
      "Training Loss:  117.33699036\n",
      "################################  200  ################################\n",
      "Training Loss:  117.76805115\n",
      "Epoch   230: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  117.67098999\n",
      "################################  400  ################################\n",
      "Training Loss:  117.6761322\n",
      "Epoch   431: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  117.59368134\n",
      "################################  600  ################################\n",
      "Training Loss:  117.25227356\n",
      "Epoch   632: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  116.99345398\n",
      "################################  800  ################################\n",
      "Training Loss:  116.56209564\n",
      "Epoch   833: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  116.91344452\n",
      "################################  1000  ################################\n",
      "Training Loss:  117.06521606\n",
      "Epoch  1034: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.03955078\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.80353546\n",
      "Epoch  1235: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.82680511\n",
      "################################  1400  ################################\n",
      "Training Loss:  116.65891266\n",
      "Epoch  1436: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.61109161\n",
      "################################  1600  ################################\n",
      "Training Loss:  116.37921906\n",
      "Epoch  1637: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.34249878\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.26165771\n",
      "Epoch  1838: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.25865173\n",
      "Final training Loss:  116.17245483\n",
      "\n",
      "Running model (trial=1, mod=7, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1426.65283203\n",
      "################################  100  ################################\n",
      "Training Loss:  95.40822601\n",
      "################################  200  ################################\n",
      "Training Loss:  90.48427582\n",
      "Epoch   237: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.6244278\n",
      "################################  400  ################################\n",
      "Training Loss:  107.68196106\n",
      "Epoch   438: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.14336395\n",
      "################################  600  ################################\n",
      "Training Loss:  107.14746094\n",
      "Epoch   639: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  800  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch   840: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch  1041: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.1417923\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1242: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.1417923\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.16636658\n",
      "Epoch  1443: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.16636658\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch  1644: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1845: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.14178467\n",
      "Final training Loss:  107.16635895\n",
      "\n",
      "Running model (trial=2, mod=8, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  21290.29101562\n",
      "################################  100  ################################\n",
      "Training Loss:  117.20191193\n",
      "################################  200  ################################\n",
      "Training Loss:  114.44856262\n",
      "Epoch   233: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  112.77217865\n",
      "################################  400  ################################\n",
      "Training Loss:  110.1510849\n",
      "Epoch   434: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.38547516\n",
      "################################  600  ################################\n",
      "Training Loss:  105.55181885\n",
      "Epoch   635: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  104.27924347\n",
      "################################  800  ################################\n",
      "Training Loss:  101.66693115\n",
      "Epoch   836: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  100.05012512\n",
      "################################  1000  ################################\n",
      "Training Loss:  98.77993774\n",
      "Epoch  1037: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  97.26998901\n",
      "################################  1200  ################################\n",
      "Training Loss:  96.02858734\n",
      "Epoch  1238: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  95.43914795\n",
      "################################  1400  ################################\n",
      "Training Loss:  94.59519196\n",
      "Epoch  1439: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  93.72956085\n",
      "################################  1600  ################################\n",
      "Training Loss:  93.17087555\n",
      "Epoch  1640: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  92.53225708\n",
      "################################  1800  ################################\n",
      "Training Loss:  91.90393829\n",
      "Epoch  1841: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  91.40682983\n",
      "Final training Loss:  90.74848938\n",
      "\n",
      "Running model (trial=2, mod=9, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  11608.53125\n",
      "################################  100  ################################\n",
      "Training Loss:  86.6780777\n",
      "################################  200  ################################\n",
      "Training Loss:  105.49211884\n",
      "Epoch   235: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.07938385\n",
      "################################  400  ################################\n",
      "Training Loss:  107.14177704\n",
      "Epoch   436: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  600  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch   637: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  800  ################################\n",
      "Training Loss:  107.16636658\n",
      "Epoch   838: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.16634369\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch  1039: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1240: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch  1441: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch  1642: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.16634369\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch  1843: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.16635895\n",
      "Final training Loss:  107.14178467\n",
      "\n",
      "Running model (trial=2, mod=10, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  771.55505371\n",
      "################################  100  ################################\n",
      "Training Loss:  121.2165451\n",
      "################################  200  ################################\n",
      "Training Loss:  114.21173096\n",
      "Epoch   210: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  93.01657867\n",
      "################################  400  ################################\n",
      "Training Loss:  116.57913208\n",
      "################################  500  ################################\n",
      "Training Loss:  118.91526794\n",
      "Epoch   579: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  600  ################################\n",
      "Training Loss:  118.12013245\n",
      "################################  700  ################################\n",
      "Training Loss:  116.40287781\n",
      "Epoch   780: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  800  ################################\n",
      "Training Loss:  115.68405914\n",
      "################################  900  ################################\n",
      "Training Loss:  114.29073334\n",
      "Epoch   981: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.96743774\n",
      "################################  1100  ################################\n",
      "Training Loss:  111.80643463\n",
      "Epoch  1182: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1200  ################################\n",
      "Training Loss:  109.47928619\n",
      "################################  1300  ################################\n",
      "Training Loss:  108.33940125\n",
      "Epoch  1383: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.45711517\n",
      "################################  1500  ################################\n",
      "Training Loss:  105.9691925\n",
      "Epoch  1584: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1600  ################################\n",
      "Training Loss:  104.43158722\n",
      "################################  1700  ################################\n",
      "Training Loss:  103.63010406\n",
      "Epoch  1785: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1800  ################################\n",
      "Training Loss:  102.16607666\n",
      "################################  1900  ################################\n",
      "Training Loss:  101.1448288\n",
      "Epoch  1986: reducing learning rate of group 0 to 4.0354e-04.\n",
      "Final training Loss:  99.77440643\n",
      "\n",
      "Running model (trial=2, mod=11, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  94667.8125\n",
      "################################  100  ################################\n",
      "Training Loss:  118.15231323\n",
      "################################  200  ################################\n",
      "Training Loss:  120.34059143\n",
      "Epoch   215: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  117.74707031\n",
      "################################  400  ################################\n",
      "Training Loss:  118.92095947\n",
      "Epoch   416: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  120.36988068\n",
      "################################  600  ################################\n",
      "Training Loss:  124.32201385\n",
      "Epoch   617: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  123.34375763\n",
      "################################  800  ################################\n",
      "Training Loss:  123.17498779\n",
      "Epoch   818: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  123.57430267\n",
      "################################  1000  ################################\n",
      "Training Loss:  124.56958771\n",
      "Epoch  1019: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  124.41123962\n",
      "################################  1200  ################################\n",
      "Training Loss:  124.26521301\n",
      "Epoch  1220: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  124.70832062\n",
      "################################  1400  ################################\n",
      "Training Loss:  124.86641693\n",
      "Epoch  1421: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  124.90825653\n",
      "################################  1600  ################################\n",
      "Training Loss:  124.866745\n",
      "Epoch  1622: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  124.37973785\n",
      "################################  1800  ################################\n",
      "Training Loss:  122.93361664\n",
      "Epoch  1823: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  121.17711639\n",
      "Final training Loss:  119.26873779\n",
      "\n",
      "Running model (trial=3, mod=12, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10969.38671875\n",
      "################################  100  ################################\n",
      "Training Loss:  120.51719666\n",
      "################################  200  ################################\n",
      "Training Loss:  119.19819641\n",
      "Epoch   236: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  118.537323\n",
      "################################  400  ################################\n",
      "Training Loss:  118.04502106\n",
      "Epoch   437: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  116.84148407\n",
      "################################  600  ################################\n",
      "Training Loss:  116.36825562\n",
      "Epoch   638: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  115.50223541\n",
      "################################  800  ################################\n",
      "Training Loss:  113.81671143\n",
      "Epoch   839: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  112.76740265\n",
      "################################  1000  ################################\n",
      "Training Loss:  111.6463089\n",
      "Epoch  1040: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  110.59117889\n",
      "################################  1200  ################################\n",
      "Training Loss:  110.0574646\n",
      "Epoch  1241: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  108.93397522\n",
      "################################  1400  ################################\n",
      "Training Loss:  108.19184113\n",
      "Epoch  1442: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.66759491\n",
      "################################  1600  ################################\n",
      "Training Loss:  106.98049927\n",
      "Epoch  1643: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  106.77027893\n",
      "################################  1800  ################################\n",
      "Training Loss:  106.65037537\n",
      "Epoch  1844: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  105.81634521\n",
      "Final training Loss:  105.19800568\n",
      "\n",
      "Running model (trial=3, mod=13, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  15546.91015625\n",
      "################################  100  ################################\n",
      "Training Loss:  110.07667542\n",
      "################################  200  ################################\n",
      "Training Loss:  107.61274719\n",
      "Epoch   289: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.56534576\n",
      "################################  400  ################################\n",
      "Training Loss:  107.54077911\n",
      "Epoch   490: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.56535339\n",
      "################################  600  ################################\n",
      "Training Loss:  107.54077911\n",
      "Epoch   691: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  107.56534576\n",
      "################################  800  ################################\n",
      "Training Loss:  107.56535339\n",
      "Epoch   892: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.56535339\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.56535339\n",
      "Epoch  1093: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.56535339\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.56533813\n",
      "Epoch  1294: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.54078674\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.56535339\n",
      "Epoch  1495: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.54077911\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.56535339\n",
      "Epoch  1696: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.54077911\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.54077911\n",
      "Epoch  1897: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.54077911\n",
      "Final training Loss:  107.56534576\n",
      "\n",
      "Running model (trial=3, mod=14, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  109827.9921875\n",
      "################################  100  ################################\n",
      "Training Loss:  114.42886353\n",
      "################################  200  ################################\n",
      "Training Loss:  112.02037048\n",
      "################################  300  ################################\n",
      "Training Loss:  111.22544861\n",
      "################################  400  ################################\n",
      "Training Loss:  110.42729187\n",
      "################################  500  ################################\n",
      "Training Loss:  109.2223053\n",
      "################################  600  ################################\n",
      "Training Loss:  106.90890503\n",
      "################################  700  ################################\n",
      "Training Loss:  105.22994995\n",
      "################################  800  ################################\n",
      "Training Loss:  103.54286957\n",
      "################################  900  ################################\n",
      "Training Loss:  100.31129456\n",
      "################################  1000  ################################\n",
      "Training Loss:  97.93109894\n",
      "################################  1100  ################################\n",
      "Training Loss:  95.14211273\n",
      "################################  1200  ################################\n",
      "Training Loss:  93.20982361\n",
      "################################  1300  ################################\n",
      "Training Loss:  90.08454132\n",
      "################################  1400  ################################\n",
      "Training Loss:  86.64430237\n",
      "################################  1500  ################################\n",
      "Training Loss:  82.85734558\n",
      "################################  1600  ################################\n",
      "Training Loss:  77.95159912\n",
      "################################  1700  ################################\n",
      "Training Loss:  71.72827911\n",
      "################################  1800  ################################\n",
      "Training Loss:  64.26371002\n",
      "################################  1900  ################################\n",
      "Training Loss:  57.57164383\n",
      "Final training Loss:  53.61845779\n",
      "\n",
      "Running model (trial=3, mod=15, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5210.90332031\n",
      "################################  100  ################################\n",
      "Training Loss:  101.48937225\n",
      "################################  200  ################################\n",
      "Training Loss:  105.2774353\n",
      "Epoch   223: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  400  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch   424: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  600  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch   625: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  800  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch   826: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1027: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.1417923\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch  1228: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch  1429: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.14177704\n",
      "Epoch  1630: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.1417923\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1831: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.16635895\n",
      "Final training Loss:  107.16635895\n",
      "\n",
      "Running model (trial=4, mod=16, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  836.50415039\n",
      "################################  100  ################################\n",
      "Training Loss:  119.52032471\n",
      "################################  200  ################################\n",
      "Training Loss:  123.94945526\n",
      "Epoch   221: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  95.24311829\n",
      "################################  400  ################################\n",
      "Training Loss:  119.76895905\n",
      "################################  500  ################################\n",
      "Training Loss:  119.64328003\n",
      "Epoch   536: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  600  ################################\n",
      "Training Loss:  116.69844818\n",
      "################################  700  ################################\n",
      "Training Loss:  114.09108734\n",
      "Epoch   737: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  800  ################################\n",
      "Training Loss:  109.63513184\n",
      "################################  900  ################################\n",
      "Training Loss:  116.84714508\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.84385681\n",
      "Epoch  1027: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  111.50905609\n",
      "################################  1200  ################################\n",
      "Training Loss:  108.35244751\n",
      "Epoch  1228: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  120.75281525\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.09104156\n",
      "Epoch  1429: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1500  ################################\n",
      "Training Loss:  112.60453796\n",
      "################################  1600  ################################\n",
      "Training Loss:  112.58453369\n",
      "Epoch  1630: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  113.03649139\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.13250732\n",
      "Epoch  1831: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.95888519\n",
      "Final training Loss:  117.71239471\n",
      "\n",
      "Running model (trial=4, mod=17, k=0):\n",
      "Parameters: ({'model': <class 'deepthermal.FFNN_model.FFNN'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1075.22937012\n",
      "################################  100  ################################\n",
      "Training Loss:  106.5832901\n",
      "################################  200  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch   223: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  400  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch   424: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  600  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch   625: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  800  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch   826: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.14178467\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch  1027: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch  1228: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.16635132\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1429: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.14179993\n",
      "Epoch  1630: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.16636658\n",
      "Epoch  1831: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.16635895\n",
      "Final training Loss:  107.16635132\n",
      "\n",
      "Running model (trial=4, mod=18, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4661.47753906\n",
      "################################  100  ################################\n",
      "Training Loss:  112.29709625\n",
      "################################  200  ################################\n",
      "Training Loss:  110.39251709\n",
      "Epoch   226: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  108.73866272\n",
      "################################  400  ################################\n",
      "Training Loss:  106.94641876\n",
      "Epoch   427: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  105.69760132\n",
      "################################  600  ################################\n",
      "Training Loss:  104.18953705\n",
      "Epoch   628: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  102.04682159\n",
      "################################  800  ################################\n",
      "Training Loss:  100.81851959\n",
      "Epoch   829: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  99.36473846\n",
      "################################  1000  ################################\n",
      "Training Loss:  98.6543045\n",
      "Epoch  1030: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  97.11871338\n",
      "################################  1200  ################################\n",
      "Training Loss:  96.61851501\n",
      "Epoch  1231: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  96.40148926\n",
      "################################  1400  ################################\n",
      "Training Loss:  95.6595993\n",
      "Epoch  1432: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  95.29579926\n",
      "################################  1600  ################################\n",
      "Training Loss:  94.86475372\n",
      "Epoch  1633: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  94.60995483\n",
      "################################  1800  ################################\n",
      "Training Loss:  94.05929565\n",
      "Epoch  1834: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  93.81622314\n",
      "Final training Loss:  93.91508484\n",
      "\n",
      "Running model (trial=4, mod=19, k=0):\n",
      "Parameters: ({'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'input_dimension': 1, 'output_dimension': 1, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 16}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123cffe50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x123eef430>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.01, 'lr_scheduler': <function <lambda> at 0x123eef5e0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  647.43713379\n",
      "################################  100  ################################\n",
      "Training Loss:  77.75543213\n",
      "################################  200  ################################\n",
      "Training Loss:  107.59326172\n",
      "Epoch   263: reducing learning rate of group 0 to 7.0000e-03.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  400  ################################\n",
      "Training Loss:  107.16635132\n",
      "Epoch   464: reducing learning rate of group 0 to 4.9000e-03.\n",
      "################################  500  ################################\n",
      "Training Loss:  107.14177704\n",
      "################################  600  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch   665: reducing learning rate of group 0 to 3.4300e-03.\n",
      "################################  700  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  800  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch   866: reducing learning rate of group 0 to 2.4010e-03.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.1417923\n",
      "################################  1000  ################################\n",
      "Training Loss:  107.14178467\n",
      "Epoch  1067: reducing learning rate of group 0 to 1.6807e-03.\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1268: reducing learning rate of group 0 to 1.1765e-03.\n",
      "################################  1300  ################################\n",
      "Training Loss:  107.16635895\n",
      "################################  1400  ################################\n",
      "Training Loss:  107.16635895\n",
      "Epoch  1469: reducing learning rate of group 0 to 8.2354e-04.\n",
      "################################  1500  ################################\n",
      "Training Loss:  107.14177704\n",
      "################################  1600  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch  1670: reducing learning rate of group 0 to 5.7648e-04.\n",
      "################################  1700  ################################\n",
      "Training Loss:  107.14177704\n",
      "################################  1800  ################################\n",
      "Training Loss:  107.1417923\n",
      "Epoch  1871: reducing learning rate of group 0 to 4.0354e-04.\n",
      "################################  1900  ################################\n",
      "Training Loss:  107.1417923\n",
      "Final training Loss:  107.1417923\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# plotting\n",
    "disc_points = x_train.detach()\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": disc_points,\n",
    "    \"x_train\": disc_points,\n",
    "    \"y_train\": I1_new,\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"DP solution\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}