{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import (\n",
    "    create_subdictionary_iterator,\n",
    "    k_fold_cv_grid,\n",
    "    add_dictionary_iterators,\n",
    ")\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from neural_reparam.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from neural_reparam.ResNet import ResNet\n",
    "from neural_reparam.models import ResCNN, BResCNN, CNN\n",
    "from neural_reparam.reparam_env import (\n",
    "    get_epsilon_greedy,\n",
    "    get_optimal_path,\n",
    "    DiscreteReparamEnv,\n",
    "    plot_solution_rl,\n",
    "    DiscreteReparamReverseEnv,\n",
    ")\n",
    "from neural_reparam.reinforcement_learning import fit_dqn_deterministic\n",
    "\n",
    "import experiments.curves as c1\n",
    "from so3.dynamic_distance import find_optimal_diffeomorphism\n",
    "\n",
    "# make reproducible\n",
    "seed = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "N = 32\n",
    "\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True)\n",
    "q_train = c1.q(x_train.unsqueeze(1).detach())\n",
    "r_train = c1.r(x_train.unsqueeze(1).detach())\n",
    "\n",
    "data = TensorDataset(x_train, q_train, r_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "0  ######\n",
    "DIR = \"../figures/curve_1_rl/\"\n",
    "SET_NAME = f\"dqn_2_{N}\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "\n",
    "FOLDS = 1\n",
    "# loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=1e3, verbose=False)\n",
    "# no_penalty_loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=0, verbose=False)\n",
    "depth = 4\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode=\"min\", factor=0.5, patience=5000, verbose=True\n",
    ")\n",
    "env = DiscreteReparamReverseEnv(depth=depth, data=data)\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": [ResNet],\n",
    "    \"input_dimension\": [2],\n",
    "    \"output_dimension\": [env.num_actions],\n",
    "    \"activation\": [\"relu\"],\n",
    "    \"n_hidden_layers\": [2],\n",
    "}\n",
    "\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"neurons\": [32],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"env\": [env],\n",
    "    \"batch_size\": [200],\n",
    "    \"choose_action\": [get_epsilon_greedy(epsilon=0.01, num_actions=env.num_actions)],\n",
    "    \"C\": [10],\n",
    "    \"memory_size\": [20 * env.num_actions * N],\n",
    "    \"DDQN\": [True],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"ADAM\"],\n",
    "    \"num_epochs\": [500],\n",
    "    \"verbose_interval\": [30],\n",
    "    \"learning_rate\": [0.1],\n",
    "    \"lr_scheduler\": [lr_scheduler],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "# model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter_1)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(\n",
    "    TRAINING_PARAMS_EXPERIMENT, product=False\n",
    ")\n",
    "exp_training_params_iter = add_dictionary_iterators(\n",
    "    training_exp_iter, training_params_iter\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'model': <class 'neural_reparam.ResNet.ResNet'>, 'input_dimension': 2, 'output_dimension': 11, 'activation': 'relu', 'n_hidden_layers': 2, 'neurons': 32}, {'env': <neural_reparam.reparam_env.DiscreteReparamReverseEnv object at 0x128b68f10>, 'batch_size': 200, 'choose_action': <function get_epsilon_greedy.<locals>.epsilon_greedy at 0x128a27f70>, 'C': 10, 'memory_size': 7040, 'DDQN': True, 'optimizer': 'ADAM', 'num_epochs': 500, 'verbose_interval': 30, 'learning_rate': 0.1, 'lr_scheduler': <function <lambda> at 0x12932b670>})\n",
      "################################  0  ################################\n",
      "Training Loss:  tensor(145.9852)\n",
      "################################  30  ################################\n",
      "Training Loss:  tensor(104.2414)\n",
      "################################  60  ################################\n",
      "Training Loss:  tensor(107.3963)\n",
      "################################  90  ################################\n",
      "Training Loss:  tensor(169.9613)\n",
      "################################  120  ################################\n",
      "Training Loss:  tensor(143.7157)\n",
      "################################  150  ################################\n",
      "Training Loss:  tensor(180.3618)\n",
      "################################  180  ################################\n",
      "Training Loss:  tensor(158.5606)\n",
      "################################  210  ################################\n",
      "Training Loss:  tensor(75.7103)\n",
      "################################  240  ################################\n",
      "Training Loss:  tensor(56.4058)\n",
      "################################  270  ################################\n",
      "Training Loss:  tensor(17.6250)\n",
      "################################  300  ################################\n",
      "Training Loss:  tensor(28.7176)\n",
      "################################  330  ################################\n",
      "Training Loss:  tensor(14.4612)\n",
      "################################  360  ################################\n",
      "Training Loss:  tensor(57.7004)\n",
      "################################  390  ################################\n",
      "Training Loss:  tensor(39.2423)\n",
      "################################  420  ################################\n",
      "Training Loss:  tensor(73.0940)\n",
      "################################  450  ################################\n",
      "Training Loss:  tensor(23.8256)\n",
      "################################  480  ################################\n",
      "Training Loss:  tensor(20.5715)\n",
      "Final training Loss:  tensor(30.7291)\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_dqn_deterministic,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find DP solution\n",
    "\n",
    "with torch.no_grad():\n",
    "    I1_new = find_optimal_diffeomorphism(\n",
    "        q0=q_train, q1=r_train, I0=x_train, I1=x_train, depth=depth\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = (cv_results[\"models\"][0][0],)\n",
    "plot_kwargs = {\n",
    "    \"env\": env,\n",
    "    \"x_train\": x_train,\n",
    "    \"y_train\": I1_new,\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"DP solution\",\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_solution_rl,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "# N=16\n",
    "# s = (N,N,2)\n",
    "# test = torch.arange(np.prod(s)).reshape(s) % 5\n",
    "# ind = torch.tensor(np.indices((N, N)).T)\n",
    "# x_train2 = torch.linspace(0,1 , N, requires_grad=True)\n",
    "# x_train2[ind]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
