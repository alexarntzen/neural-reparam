{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Test on piecewise linear curves in $SO(3)^n$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from itertools import chain\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from neural_reparam.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from neural_reparam.interpolation import get_pl_curve_from_data\n",
    "from neural_reparam.ResNet import ResNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "run/jog\n",
      "run/jog\n",
      "(23, 161, 3, 3)\n",
      "(23, 137, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from animation.animation_manager import fetch_animations, unpack\n",
    "from so3.curves import move_origin_to_zero, dynamic_distance\n",
    "from so3.helpers import crop_curve\n",
    "from so3.dynamic_distance import find_optimal_diffeomorphism, create_shared_parameterization\n",
    "from so3.clustering.id_set import crop_curve_based_on_id, get_id_set\n",
    "from so3.transformations import skew_to_vector, SRVT\n",
    "from so3 import animation_to_SO3\n",
    "\n",
    "max_frame_count = 180\n",
    "\n",
    "id_set = get_id_set()\n",
    "print(\"Load data\")\n",
    "\n",
    "data = [fetch_animations(1, file_name=\"39_02.amc\"),  #walk 6.5 steps\n",
    "        fetch_animations(1, file_name=\"35_26.amc\"),  # run/jog 3 steps\n",
    "        fetch_animations(1, file_name=\"16_35.amc\")  # run/jog 3 steps\n",
    "        ]\n",
    "\n",
    "# walk\n",
    "subject, animation, desc0 = unpack(data[2])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=240)  #first 2 seconds\n",
    "c_0 = move_origin_to_zero(curve)\n",
    "print(desc0)\n",
    "\n",
    "# run\n",
    "subject, animation, desc1 = unpack(data[1])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=240)  #first 2 seconds\n",
    "c_1 = move_origin_to_zero(curve)\n",
    "print(desc1)\n",
    "print(c_0.shape)\n",
    "print(c_1.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "outputs": [],
   "source": [
    "#calculate distances\n",
    "I0 = np.linspace(0, 1, c_0.shape[1])\n",
    "I1 = np.linspace(0, 1, c_1.shape[1])\n",
    "q_data_ = skew_to_vector(SRVT(c_0, I0))\n",
    "r_data_ = skew_to_vector(SRVT(c_1, I1))\n",
    "I, q_data, r_data = create_shared_parameterization(q0=q_data_, q1=r_data_, I0=I0, I1=I1)\n",
    "shared_frames = I.shape[0]\n",
    "q_func = get_pl_curve_from_data(data=q_data)\n",
    "r_func = get_pl_curve_from_data(data=r_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-65-545c3fe2a24e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0md_01\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdynamic_distance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc_0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mc_1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdepth\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m10\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0mprint\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0md_01\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git_repos/git_skole/paalel-master/code/so3/curves.py\u001B[0m in \u001B[0;36mdynamic_distance\u001B[0;34m(c0, c1, depth)\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0mq1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mskew_to_vector\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mSRVT\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m     \u001B[0mI1_new\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfind_optimal_diffeomorphism\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mq0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mq1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdepth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m     \u001B[0mc1_new\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreparameterize\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mI1_new\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mc1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mdistance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mc1_new\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mI0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mI1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git_repos/git_skole/paalel-master/code/so3/dynamic_distance.py\u001B[0m in \u001B[0;36mfind_optimal_diffeomorphism\u001B[0;34m(q0, q1, I0, I1, depth)\u001B[0m\n\u001B[1;32m    114\u001B[0m     \u001B[0mlocal_cost_partial\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpartial\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlocal_cost\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mq0\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mq0_new\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mq1\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mq1_new\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mI\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    115\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 116\u001B[0;31m     \u001B[0mpointers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mA\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mdynamic\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlocal_cost_partial\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mM\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdepth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    117\u001B[0m     \u001B[0mpath\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mreconstruct\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mpointers\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mM\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mM\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    118\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git_repos/git_skole/paalel-master/code/so3/dynamic_distance.py\u001B[0m in \u001B[0;36mdynamic\u001B[0;34m(local_cost, M, depth)\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0;32mfor\u001B[0m \u001B[0mpred\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpredecessors\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdepth\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     42\u001B[0m                 \u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ml\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mpred\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 43\u001B[0;31m                 \u001B[0mcost\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mlocal_cost\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0mA\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     44\u001B[0m                 \u001B[0;32mif\u001B[0m \u001B[0mmin_cost\u001B[0m \u001B[0;34m>\u001B[0m \u001B[0mcost\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m                     \u001B[0mmin_cost\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcost\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git_repos/git_skole/paalel-master/code/so3/dynamic_distance.py\u001B[0m in \u001B[0;36mlocal_cost\u001B[0;34m(k, l, i, j, q0, q1, I)\u001B[0m\n\u001B[1;32m     52\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     53\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mlocal_cost\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0ml\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mj\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mq0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mq1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 54\u001B[0;31m     return L2_metric(\n\u001B[0m\u001B[1;32m     55\u001B[0m             \u001B[0mq0\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     56\u001B[0m             \u001B[0msqrt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mI\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mI\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m/\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mI\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m-\u001B[0m\u001B[0mI\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mk\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0mq1\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0ml\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0mj\u001B[0m\u001B[0;34m+\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git_repos/git_skole/paalel-master/code/so3/dynamic_distance.py\u001B[0m in \u001B[0;36mL2_metric\u001B[0;34m(q0, q1, I0, I1)\u001B[0m\n\u001B[1;32m     71\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m     \u001B[0;31m#create array of shared interpolation points\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 73\u001B[0;31m     \u001B[0mI\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0munique\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconcatenate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mI0\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mI1\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     74\u001B[0m     \u001B[0mi\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mj\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     75\u001B[0m     \u001B[0ml2_sum\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0.0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<__array_function__ internals>\u001B[0m in \u001B[0;36munique\u001B[0;34m(*args, **kwargs)\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/numpy/lib/arraysetops.py\u001B[0m in \u001B[0;36munique\u001B[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    137\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 138\u001B[0;31m \u001B[0;34m@\u001B[0m\u001B[0marray_function_dispatch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0m_unique_dispatcher\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    139\u001B[0m def unique(ar, return_index=False, return_inverse=False,\n\u001B[1;32m    140\u001B[0m            return_counts=False, axis=None):\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "d_01 = dynamic_distance(c_0, c_1, depth=10)\n",
    "print(d_01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "I1_new = find_optimal_diffeomorphism(q0=q_data, q1=r_data, I0=I, I1=I, depth=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_so3/\"\n",
    "SET_NAME = \"pl_eks_7\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "FOLDS = 1\n",
    "N = shared_frames  # training points internal\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=5,\n",
    "                                                                        verbose=True)\n",
    "loss_func = get_elastic_metric_loss(r=r_func, constrain_cost=1e4, verbose=False)\n",
    "penalty_free_loss_func = get_elastic_metric_loss(r=r_func, constrain_cost=0, verbose=False)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"n_hidden_layers\": [2],  #,8,16,64],\n",
    "    \"model\": [ResNet],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"model\": [ResNet],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N // 2],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [100],\n",
    "    \"learning_rate\": [1],\n",
    "    \"stop_stochastic\": [90],\n",
    "    \"lr_scheduler\": [lr_scheduler]\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [],
   "source": [
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = torch.tensor(q_data)\n",
    "data = TensorDataset(x_train, q_train)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 68,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  47.35105515\n",
      "################################  5  ################################\n",
      "Training Loss:  52.95888519\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  52.37704849\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  52.51105499\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  52.11737823\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  52.14411163\n",
      "################################  30  ################################\n",
      "Training Loss:  52.13967514\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  52.13964462\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  52.13965988\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  52.13965988\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  55  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  60  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  65  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  70  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  75  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  80  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  85  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  90  ################################\n",
      "Training Loss:  52.13965988\n",
      "################################  95  ################################\n",
      "Training Loss:  52.13965988\n",
      "Final training Loss:  52.13965988\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  119.44451141\n",
      "################################  5  ################################\n",
      "Training Loss:  119.36746979\n",
      "################################  10  ################################\n",
      "Training Loss:  118.20462036\n",
      "################################  15  ################################\n",
      "Training Loss:  49.28183365\n",
      "################################  20  ################################\n",
      "Training Loss:  47.16185379\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  46.72442627\n",
      "################################  30  ################################\n",
      "Training Loss:  42.54571533\n",
      "################################  35  ################################\n",
      "Training Loss:  41.93042374\n",
      "################################  40  ################################\n",
      "Training Loss:  41.06036377\n",
      "################################  45  ################################\n",
      "Training Loss:  39.69587708\n",
      "################################  50  ################################\n",
      "Training Loss:  38.45316696\n",
      "################################  55  ################################\n",
      "Training Loss:  37.80650711\n",
      "################################  60  ################################\n",
      "Training Loss:  37.44530106\n",
      "################################  65  ################################\n",
      "Training Loss:  37.49263763\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.50026703\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.63352966\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  80  ################################\n",
      "Training Loss:  38.17510986\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.71017838\n",
      "################################  90  ################################\n",
      "Training Loss:  37.71017075\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  95  ################################\n",
      "Training Loss:  37.71017075\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Final training Loss:  37.71017075\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  45.04143143\n",
      "################################  5  ################################\n",
      "Training Loss:  43.95684814\n",
      "################################  10  ################################\n",
      "Training Loss:  38.8466568\n",
      "################################  15  ################################\n",
      "Training Loss:  57.05635834\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  3088.93310547\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  41.61294556\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  41.50285721\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  40.31522369\n",
      "################################  40  ################################\n",
      "Training Loss:  40.3594017\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  40.35937881\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  40.35938263\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  40.35938644\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  40.35938644\n",
      "################################  65  ################################\n",
      "Training Loss:  40.35938644\n",
      "################################  70  ################################\n",
      "Training Loss:  40.35938644\n",
      "################################  75  ################################\n",
      "Training Loss:  40.35938644\n",
      "################################  80  ################################\n",
      "Training Loss:  40.35938644\n",
      "################################  85  ################################\n",
      "Training Loss:  40.35938644\n",
      "################################  90  ################################\n",
      "Training Loss:  40.35938644\n",
      "################################  95  ################################\n",
      "Training Loss:  40.35938644\n",
      "Final training Loss:  40.35938644\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.84649658\n",
      "################################  5  ################################\n",
      "Training Loss:  43.09152985\n",
      "################################  10  ################################\n",
      "Training Loss:  43.06491852\n",
      "################################  15  ################################\n",
      "Training Loss:  43.12973404\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  42.48480225\n",
      "################################  25  ################################\n",
      "Training Loss:  38.68530655\n",
      "################################  30  ################################\n",
      "Training Loss:  38.66248703\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.70468521\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  38.57761765\n",
      "################################  45  ################################\n",
      "Training Loss:  37.95838928\n",
      "################################  50  ################################\n",
      "Training Loss:  37.70991898\n",
      "################################  55  ################################\n",
      "Training Loss:  37.51353455\n",
      "################################  60  ################################\n",
      "Training Loss:  37.57151031\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.59147263\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.59147263\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.59147263\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.59148026\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.59148026\n",
      "################################  90  ################################\n",
      "Training Loss:  37.59148026\n",
      "################################  95  ################################\n",
      "Training Loss:  37.59148026\n",
      "Final training Loss:  37.59148026\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  45.42694092\n",
      "################################  5  ################################\n",
      "Training Loss:  39.16889572\n",
      "################################  10  ################################\n",
      "Training Loss:  38.16944885\n",
      "################################  15  ################################\n",
      "Training Loss:  43.93458176\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  40.33180237\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  39.41930389\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  39.21421814\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.32588577\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  39.3177948\n",
      "################################  45  ################################\n",
      "Training Loss:  39.31777954\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  39.31778717\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.31778717\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.31778336\n",
      "################################  65  ################################\n",
      "Training Loss:  39.31778336\n",
      "################################  70  ################################\n",
      "Training Loss:  39.31778336\n",
      "################################  75  ################################\n",
      "Training Loss:  39.31778336\n",
      "################################  80  ################################\n",
      "Training Loss:  39.31778336\n",
      "################################  85  ################################\n",
      "Training Loss:  39.31778336\n",
      "################################  90  ################################\n",
      "Training Loss:  39.31778336\n",
      "################################  95  ################################\n",
      "Training Loss:  39.31778336\n",
      "Final training Loss:  39.31778336\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  39.61411285\n",
      "################################  5  ################################\n",
      "Training Loss:  37.00859833\n",
      "################################  10  ################################\n",
      "Training Loss:  325.84487915\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  nan\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.53187561\n",
      "################################  5  ################################\n",
      "Training Loss:  43.01096725\n",
      "################################  10  ################################\n",
      "Training Loss:  37.02629852\n",
      "################################  15  ################################\n",
      "Training Loss:  37.2687149\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  36.85489655\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  36.93867493\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.47116089\n",
      "################################  35  ################################\n",
      "Training Loss:  36.84083939\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.85518265\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.85516739\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.85516739\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.85516739\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  36.85516739\n",
      "################################  65  ################################\n",
      "Training Loss:  36.85516739\n",
      "################################  70  ################################\n",
      "Training Loss:  36.85516739\n",
      "################################  75  ################################\n",
      "Training Loss:  36.85516739\n",
      "################################  80  ################################\n",
      "Training Loss:  36.85516739\n",
      "################################  85  ################################\n",
      "Training Loss:  36.85516739\n",
      "################################  90  ################################\n",
      "Training Loss:  36.85516739\n",
      "################################  95  ################################\n",
      "Training Loss:  36.85516739\n",
      "Final training Loss:  36.85516739\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.31390762\n",
      "################################  5  ################################\n",
      "Training Loss:  42.98856354\n",
      "################################  10  ################################\n",
      "Training Loss:  38.51570129\n",
      "################################  15  ################################\n",
      "Training Loss:  38.46110916\n",
      "################################  20  ################################\n",
      "Training Loss:  38.43073654\n",
      "################################  25  ################################\n",
      "Training Loss:  37.62171173\n",
      "################################  30  ################################\n",
      "Training Loss:  37.3037529\n",
      "################################  35  ################################\n",
      "Training Loss:  37.36595535\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  52.09260178\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  51.93460846\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  52.09453201\n",
      "################################  55  ################################\n",
      "Training Loss:  51.86820221\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  51.86820221\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  51.86820221\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  51.86820602\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  51.86820602\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  51.86820602\n",
      "################################  85  ################################\n",
      "Training Loss:  51.86820602\n",
      "################################  90  ################################\n",
      "Training Loss:  51.86820602\n",
      "################################  95  ################################\n",
      "Training Loss:  51.86820602\n",
      "Final training Loss:  51.86820602\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  120.60705566\n",
      "################################  5  ################################\n",
      "Training Loss:  120.64355469\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  120.57318115\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  120.96006012\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  120.88845825\n",
      "################################  25  ################################\n",
      "Training Loss:  120.8768158\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  120.98387146\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  120.98223114\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  120.9821167\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  120.98207855\n",
      "################################  55  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  60  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  65  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  70  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  75  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  80  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  85  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  90  ################################\n",
      "Training Loss:  120.98207855\n",
      "################################  95  ################################\n",
      "Training Loss:  120.98207855\n",
      "Final training Loss:  120.98207855\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  45.1727066\n",
      "################################  5  ################################\n",
      "Training Loss:  43.01006317\n",
      "################################  10  ################################\n",
      "Training Loss:  41.45878983\n",
      "################################  15  ################################\n",
      "Training Loss:  39.78905487\n",
      "################################  20  ################################\n",
      "Training Loss:  39.38936615\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9120.39257812\n",
      "################################  5  ################################\n",
      "Training Loss:  114.00023651\n",
      "################################  10  ################################\n",
      "Training Loss:  112.94490814\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  112.77345276\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  112.89801788\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  112.9630127\n",
      "################################  30  ################################\n",
      "Training Loss:  113.05329895\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  113.06929779\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  113.06932068\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  113.06932068\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  113.06932068\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  113.06932068\n",
      "################################  60  ################################\n",
      "Training Loss:  113.06932068\n",
      "################################  65  ################################\n",
      "Training Loss:  113.06932068\n",
      "################################  70  ################################\n",
      "Training Loss:  113.06932068\n",
      "################################  75  ################################\n",
      "Training Loss:  113.06932068\n",
      "################################  80  ################################\n",
      "Training Loss:  113.06932068\n",
      "################################  85  ################################\n",
      "Training Loss:  113.06932068\n",
      "################################  90  ################################\n",
      "Training Loss:  113.06932068\n",
      "################################  95  ################################\n",
      "Training Loss:  113.06932068\n",
      "Final training Loss:  113.06932068\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  98.97943115\n",
      "################################  5  ################################\n",
      "Training Loss:  77.14900208\n",
      "################################  10  ################################\n",
      "Training Loss:  76.09488678\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  76.10190582\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  76.10312653\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  76.094841\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  76.094841\n",
      "################################  35  ################################\n",
      "Training Loss:  76.09719849\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  76.09719849\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  76.09719849\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  76.09719849\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  76.09719849\n",
      "################################  60  ################################\n",
      "Training Loss:  76.09719849\n",
      "################################  65  ################################\n",
      "Training Loss:  76.09719849\n",
      "################################  70  ################################\n",
      "Training Loss:  76.09719849\n",
      "################################  75  ################################\n",
      "Training Loss:  76.09719849\n",
      "################################  80  ################################\n",
      "Training Loss:  76.09719849\n",
      "################################  85  ################################\n",
      "Training Loss:  76.09719849\n",
      "################################  90  ################################\n",
      "Training Loss:  76.09719849\n",
      "################################  95  ################################\n",
      "Training Loss:  76.09719849\n",
      "Final training Loss:  76.09719849\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.15151215\n",
      "################################  5  ################################\n",
      "Training Loss:  51.34716034\n",
      "################################  10  ################################\n",
      "Training Loss:  38.17086411\n",
      "################################  15  ################################\n",
      "Training Loss:  39.38819122\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  38.53564453\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.16900253\n",
      "################################  30  ################################\n",
      "Training Loss:  38.32530975\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  38.13009644\n",
      "################################  40  ################################\n",
      "Training Loss:  38.72909927\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.22176743\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.22177124\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.22184753\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.22184753\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  38.22184753\n",
      "################################  70  ################################\n",
      "Training Loss:  38.22184753\n",
      "################################  75  ################################\n",
      "Training Loss:  38.22184753\n",
      "################################  80  ################################\n",
      "Training Loss:  38.22184753\n",
      "################################  85  ################################\n",
      "Training Loss:  38.22184753\n",
      "################################  90  ################################\n",
      "Training Loss:  38.22184753\n",
      "################################  95  ################################\n",
      "Training Loss:  38.22184753\n",
      "Final training Loss:  38.22184753\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.62162018\n",
      "################################  5  ################################\n",
      "Training Loss:  50.89548111\n",
      "################################  10  ################################\n",
      "Training Loss:  1124.3170166\n",
      "################################  15  ################################\n",
      "Training Loss:  40.82243347\n",
      "################################  20  ################################\n",
      "Training Loss:  106.67824554\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  45.48427582\n",
      "################################  30  ################################\n",
      "Training Loss:  36.96281052\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  41.20426559\n",
      "################################  40  ################################\n",
      "Training Loss:  37.01355362\n",
      "################################  45  ################################\n",
      "Training Loss:  36.15256119\n",
      "################################  50  ################################\n",
      "Training Loss:  36.14172363\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.10395813\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.10360718\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.10327911\n",
      "################################  70  ################################\n",
      "Training Loss:  36.10327911\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.10327911\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.10327911\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  36.10327911\n",
      "################################  90  ################################\n",
      "Training Loss:  36.10327911\n",
      "################################  95  ################################\n",
      "Training Loss:  36.10327911\n",
      "Final training Loss:  36.10327911\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.41365814\n",
      "################################  5  ################################\n",
      "Training Loss:  46.64217758\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.66346359\n",
      "################################  5  ################################\n",
      "Training Loss:  52.83530426\n",
      "################################  10  ################################\n",
      "Training Loss:  50.0435524\n",
      "################################  15  ################################\n",
      "Training Loss:  739.19262695\n",
      "################################  20  ################################\n",
      "Training Loss:  40.11481857\n",
      "################################  25  ################################\n",
      "Training Loss:  38.15684509\n",
      "################################  30  ################################\n",
      "Training Loss:  39.19912338\n",
      "################################  35  ################################\n",
      "Training Loss:  608.41290283\n",
      "################################  40  ################################\n",
      "Training Loss:  37.23107147\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  238.11808777\n",
      "################################  50  ################################\n",
      "Training Loss:  220.24310303\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.10354996\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.6647377\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  37.64016724\n",
      "################################  70  ################################\n",
      "Training Loss:  37.64014435\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.64014435\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.64014435\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  37.64014435\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  37.64014435\n",
      "################################  95  ################################\n",
      "Training Loss:  37.64014435\n",
      "Final training Loss:  37.64014435\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.12804031\n",
      "################################  5  ################################\n",
      "Training Loss:  51.19518661\n",
      "################################  10  ################################\n",
      "Training Loss:  274.28952026\n",
      "################################  15  ################################\n",
      "Training Loss:  46.04128265\n",
      "################################  20  ################################\n",
      "Training Loss:  46.44287872\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  1995.8848877\n",
      "################################  30  ################################\n",
      "Training Loss:  45.36190796\n",
      "################################  35  ################################\n",
      "Training Loss:  308.89672852\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  47.3134079\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  45.67145538\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  45.84992981\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  45.84876633\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  45.84883499\n",
      "################################  65  ################################\n",
      "Training Loss:  45.84883499\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  45.84883499\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  45.84883499\n",
      "################################  80  ################################\n",
      "Training Loss:  45.84883499\n",
      "################################  85  ################################\n",
      "Training Loss:  45.84883499\n",
      "################################  90  ################################\n",
      "Training Loss:  45.84883499\n",
      "################################  95  ################################\n",
      "Training Loss:  45.84883499\n",
      "Final training Loss:  45.84883499\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.07242203\n",
      "################################  5  ################################\n",
      "Training Loss:  4091.49536133\n",
      "################################  10  ################################\n",
      "Training Loss:  42.72418213\n",
      "################################  15  ################################\n",
      "Training Loss:  499.32302856\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  39.60936356\n",
      "################################  25  ################################\n",
      "Training Loss:  39.8129921\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  40.74731064\n",
      "################################  35  ################################\n",
      "Training Loss:  39.42799759\n",
      "################################  40  ################################\n",
      "Training Loss:  40.18890381\n",
      "################################  45  ################################\n",
      "Training Loss:  39.40846634\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  50.34431458\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  39.52164459\n",
      "################################  60  ################################\n",
      "Training Loss:  39.52157211\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.52159882\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.52157211\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.52157211\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  39.52157211\n",
      "################################  85  ################################\n",
      "Training Loss:  39.52157211\n",
      "################################  90  ################################\n",
      "Training Loss:  39.52157211\n",
      "################################  95  ################################\n",
      "Training Loss:  39.52157211\n",
      "Final training Loss:  39.52157211\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.71742249\n",
      "################################  5  ################################\n",
      "Training Loss:  51.09617996\n",
      "################################  10  ################################\n",
      "Training Loss:  50.79453278\n",
      "################################  15  ################################\n",
      "Training Loss:  200.33839417\n",
      "################################  20  ################################\n",
      "Training Loss:  38.36422348\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  547.81738281\n",
      "################################  30  ################################\n",
      "Training Loss:  41.39387512\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  40.6870575\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.36280823\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.31417465\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.31407928\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  37.3139801\n",
      "################################  60  ################################\n",
      "Training Loss:  37.31398773\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.31398773\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.31398773\n",
      "################################  75  ################################\n",
      "Training Loss:  37.31398773\n",
      "################################  80  ################################\n",
      "Training Loss:  37.31398773\n",
      "################################  85  ################################\n",
      "Training Loss:  37.31398773\n",
      "################################  90  ################################\n",
      "Training Loss:  37.31398773\n",
      "################################  95  ################################\n",
      "Training Loss:  37.31398773\n",
      "Final training Loss:  37.31398773\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  121.33010101\n",
      "################################  5  ################################\n",
      "Training Loss:  117.80835724\n",
      "################################  10  ################################\n",
      "Training Loss:  115.40606689\n",
      "################################  15  ################################\n",
      "Training Loss:  115.10030365\n",
      "################################  20  ################################\n",
      "Training Loss:  110.48778534\n",
      "################################  25  ################################\n",
      "Training Loss:  121.68728638\n",
      "################################  30  ################################\n",
      "Training Loss:  103.88379669\n",
      "################################  35  ################################\n",
      "Training Loss:  102.85459137\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  439.9180603\n",
      "################################  45  ################################\n",
      "Training Loss:  118.81384277\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  122.21575165\n",
      "################################  5  ################################\n",
      "Training Loss:  105.66582489\n",
      "################################  10  ################################\n",
      "Training Loss:  100.31173706\n",
      "################################  15  ################################\n",
      "Training Loss:  450.46417236\n",
      "################################  20  ################################\n",
      "Training Loss:  96.54859161\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  97.33466339\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  99.39626312\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  98.51507568\n",
      "################################  40  ################################\n",
      "Training Loss:  98.36503601\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  98.36234283\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  98.36229706\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  98.36234283\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  98.36234283\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  98.36234283\n",
      "################################  70  ################################\n",
      "Training Loss:  98.36234283\n",
      "################################  75  ################################\n",
      "Training Loss:  98.36234283\n",
      "################################  80  ################################\n",
      "Training Loss:  98.36234283\n",
      "################################  85  ################################\n",
      "Training Loss:  98.36234283\n",
      "################################  90  ################################\n",
      "Training Loss:  98.36234283\n",
      "################################  95  ################################\n",
      "Training Loss:  98.36234283\n",
      "Final training Loss:  98.36234283\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.47853851\n",
      "################################  5  ################################\n",
      "Training Loss:  49.30032349\n",
      "################################  10  ################################\n",
      "Training Loss:  241.31536865\n",
      "################################  15  ################################\n",
      "Training Loss:  216.4150238\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  39.1199646\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  37.74396133\n",
      "################################  30  ################################\n",
      "Training Loss:  37.63358688\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.64060593\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.64081192\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.64072037\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.64072037\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  37.64072037\n",
      "################################  60  ################################\n",
      "Training Loss:  37.64072037\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.64072037\n",
      "################################  70  ################################\n",
      "Training Loss:  37.64072037\n",
      "################################  75  ################################\n",
      "Training Loss:  37.64072037\n",
      "################################  80  ################################\n",
      "Training Loss:  37.64072037\n",
      "################################  85  ################################\n",
      "Training Loss:  37.64072037\n",
      "################################  90  ################################\n",
      "Training Loss:  37.64072037\n",
      "################################  95  ################################\n",
      "Training Loss:  37.64072037\n",
      "Final training Loss:  37.64072037\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  121.87291718\n",
      "################################  5  ################################\n",
      "Training Loss:  50.55710983\n",
      "################################  10  ################################\n",
      "Training Loss:  50.06317139\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  50.05759811\n",
      "################################  20  ################################\n",
      "Training Loss:  47.95227814\n",
      "################################  25  ################################\n",
      "Training Loss:  5639.35400391\n",
      "################################  30  ################################\n",
      "Training Loss:  39.64953995\n",
      "################################  35  ################################\n",
      "Training Loss:  182.76205444\n",
      "################################  40  ################################\n",
      "Training Loss:  37.53384781\n",
      "################################  45  ################################\n",
      "Training Loss:  235.89801025\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.37945175\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.37926102\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.38020706\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.38022995\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.38018799\n",
      "################################  75  ################################\n",
      "Training Loss:  38.38018799\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  38.38018799\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  38.38018799\n",
      "################################  90  ################################\n",
      "Training Loss:  38.38018799\n",
      "################################  95  ################################\n",
      "Training Loss:  38.38018799\n",
      "Final training Loss:  38.38018799\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  114.0969162\n",
      "################################  5  ################################\n",
      "Training Loss:  109.92240143\n",
      "################################  10  ################################\n",
      "Training Loss:  109.43221283\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  109.34662628\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  109.50102997\n",
      "################################  25  ################################\n",
      "Training Loss:  109.48335266\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  109.51119232\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  109.47441864\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  109.47440338\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  109.47444153\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  109.47444153\n",
      "################################  55  ################################\n",
      "Training Loss:  109.47444153\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  109.47444153\n",
      "################################  65  ################################\n",
      "Training Loss:  109.47444153\n",
      "################################  70  ################################\n",
      "Training Loss:  109.47444153\n",
      "################################  75  ################################\n",
      "Training Loss:  109.47444153\n",
      "################################  80  ################################\n",
      "Training Loss:  109.47444153\n",
      "################################  85  ################################\n",
      "Training Loss:  109.47444153\n",
      "################################  90  ################################\n",
      "Training Loss:  109.47444153\n",
      "################################  95  ################################\n",
      "Training Loss:  109.47444153\n",
      "Final training Loss:  109.47444153\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  115.88885498\n",
      "################################  5  ################################\n",
      "Training Loss:  114.34374237\n",
      "################################  10  ################################\n",
      "Training Loss:  112.25371552\n",
      "################################  15  ################################\n",
      "Training Loss:  109.83921051\n",
      "################################  20  ################################\n",
      "Training Loss:  110.33598328\n",
      "################################  25  ################################\n",
      "Training Loss:  140.51681519\n",
      "################################  30  ################################\n",
      "Training Loss:  107.17080688\n",
      "################################  35  ################################\n",
      "Training Loss:  295.33761597\n",
      "################################  40  ################################\n",
      "Training Loss:  94.71888733\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  94.78334808\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  94.83161926\n",
      "################################  55  ################################\n",
      "Training Loss:  94.77947998\n",
      "################################  60  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  5  ################################\n",
      "Training Loss:  5040.11376953\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  5040.11376953\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  5040.11376953\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  5040.11376953\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  5040.11376953\n",
      "################################  35  ################################\n",
      "Training Loss:  5040.11376953\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  5040.11376953\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  5040.11376953\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  55  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  60  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  65  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  70  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  75  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  80  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  85  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  90  ################################\n",
      "Training Loss:  5040.11376953\n",
      "################################  95  ################################\n",
      "Training Loss:  5040.11376953\n",
      "Final training Loss:  5040.11376953\n",
      "\n",
      "Running model (trial=1, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  63.65127563\n",
      "################################  5  ################################\n",
      "Training Loss:  61.71681976\n",
      "################################  10  ################################\n",
      "Training Loss:  58.35738754\n",
      "################################  15  ################################\n",
      "Training Loss:  40.98350906\n",
      "################################  20  ################################\n",
      "Training Loss:  37.04066086\n",
      "################################  25  ################################\n",
      "Training Loss:  37.14523697\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  37.04001617\n",
      "################################  35  ################################\n",
      "Training Loss:  37.17976761\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  57.29571533\n",
      "################################  5  ################################\n",
      "Training Loss:  52.6881218\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  52.95450974\n",
      "################################  15  ################################\n",
      "Training Loss:  44.09694672\n",
      "################################  20  ################################\n",
      "Training Loss:  43.28470612\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  43.29848862\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  43.37394333\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  43.35940933\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  43.37683868\n",
      "################################  45  ################################\n",
      "Training Loss:  43.3768158\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  43.3768158\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  43.3768158\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  43.3768158\n",
      "################################  65  ################################\n",
      "Training Loss:  43.3768158\n",
      "################################  70  ################################\n",
      "Training Loss:  43.3768158\n",
      "################################  75  ################################\n",
      "Training Loss:  43.3768158\n",
      "################################  80  ################################\n",
      "Training Loss:  43.3768158\n",
      "################################  85  ################################\n",
      "Training Loss:  43.3768158\n",
      "################################  90  ################################\n",
      "Training Loss:  43.3768158\n",
      "################################  95  ################################\n",
      "Training Loss:  43.3768158\n",
      "Final training Loss:  43.3768158\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  44.8732338\n",
      "################################  5  ################################\n",
      "Training Loss:  40.43712616\n",
      "################################  10  ################################\n",
      "Training Loss:  45.46147537\n",
      "################################  15  ################################\n",
      "Training Loss:  166.16464233\n",
      "################################  20  ################################\n",
      "Training Loss:  37.6273613\n",
      "################################  25  ################################\n",
      "Training Loss:  36.9728508\n",
      "################################  30  ################################\n",
      "Training Loss:  36.83532333\n",
      "################################  35  ################################\n",
      "Training Loss:  36.73666\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  36.75309753\n",
      "################################  45  ################################\n",
      "Training Loss:  36.13861847\n",
      "################################  50  ################################\n",
      "Training Loss:  36.98826599\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.74961472\n",
      "################################  60  ################################\n",
      "Training Loss:  36.76150131\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.84838867\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.78163528\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.67469788\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.67470169\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.67470932\n",
      "################################  90  ################################\n",
      "Training Loss:  36.67469788\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  36.67469788\n",
      "Final training Loss:  36.67469788\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  45.29404449\n",
      "################################  5  ################################\n",
      "Training Loss:  43.20554733\n",
      "################################  10  ################################\n",
      "Training Loss:  38.76843643\n",
      "################################  15  ################################\n",
      "Training Loss:  38.61552811\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  38.75763702\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.67526627\n",
      "################################  30  ################################\n",
      "Training Loss:  38.49332428\n",
      "################################  35  ################################\n",
      "Training Loss:  38.31845093\n",
      "################################  40  ################################\n",
      "Training Loss:  38.2090683\n",
      "################################  45  ################################\n",
      "Training Loss:  38.18060684\n",
      "################################  50  ################################\n",
      "Training Loss:  38.12069321\n",
      "################################  55  ################################\n",
      "Training Loss:  38.16883469\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.1617775\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.17385483\n",
      "################################  70  ################################\n",
      "Training Loss:  38.22385025\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  38.22385025\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  38.22385025\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  38.22384644\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  38.22384644\n",
      "################################  95  ################################\n",
      "Training Loss:  38.22384644\n",
      "Final training Loss:  38.22384644\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  39.64604568\n",
      "################################  5  ################################\n",
      "Training Loss:  37.59832382\n",
      "################################  10  ################################\n",
      "Training Loss:  36.59072495\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  41.28519821\n",
      "################################  20  ################################\n",
      "Training Loss:  nan\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.02048492\n",
      "################################  5  ################################\n",
      "Training Loss:  40.58457184\n",
      "################################  10  ################################\n",
      "Training Loss:  39.20557022\n",
      "################################  15  ################################\n",
      "Training Loss:  37.75774002\n",
      "################################  20  ################################\n",
      "Training Loss:  37.08499908\n",
      "################################  25  ################################\n",
      "Training Loss:  37.12245178\n",
      "################################  30  ################################\n",
      "Training Loss:  36.2758522\n",
      "################################  35  ################################\n",
      "Training Loss:  36.19292068\n",
      "################################  40  ################################\n",
      "Training Loss:  36.02322006\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.01612854\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  35.99851608\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  35.93522263\n",
      "################################  60  ################################\n",
      "Training Loss:  35.8708992\n",
      "################################  65  ################################\n",
      "Training Loss:  35.87752151\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.04140091\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.03464127\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.03464127\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.03464127\n",
      "################################  90  ################################\n",
      "Training Loss:  36.03464127\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  36.03464127\n",
      "Final training Loss:  36.03464127\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  40.69910431\n",
      "################################  5  ################################\n",
      "Training Loss:  37.28038025\n",
      "################################  10  ################################\n",
      "Training Loss:  36.7486496\n",
      "################################  15  ################################\n",
      "Training Loss:  36.56917191\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  36.93761444\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.90573502\n",
      "################################  30  ################################\n",
      "Training Loss:  36.55417252\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.1365509\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.94975662\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.94975662\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.94975281\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.94975281\n",
      "################################  60  ################################\n",
      "Training Loss:  36.94974899\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.94974899\n",
      "################################  70  ################################\n",
      "Training Loss:  36.94974899\n",
      "################################  75  ################################\n",
      "Training Loss:  36.94974899\n",
      "################################  80  ################################\n",
      "Training Loss:  36.94974899\n",
      "################################  85  ################################\n",
      "Training Loss:  36.94974899\n",
      "################################  90  ################################\n",
      "Training Loss:  36.94974899\n",
      "################################  95  ################################\n",
      "Training Loss:  36.94974899\n",
      "Final training Loss:  36.94974899\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.6516571\n",
      "################################  5  ################################\n",
      "Training Loss:  53.6146431\n",
      "################################  10  ################################\n",
      "Training Loss:  52.44150162\n",
      "################################  15  ################################\n",
      "Training Loss:  52.5507431\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  52.22984695\n",
      "################################  25  ################################\n",
      "Training Loss:  52.40813065\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  52.35452271\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  52.33837128\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  52.33835602\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  52.33835602\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  52.33835602\n",
      "################################  55  ################################\n",
      "Training Loss:  52.33835602\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  52.33836365\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  52.33836365\n",
      "################################  70  ################################\n",
      "Training Loss:  52.33836365\n",
      "################################  75  ################################\n",
      "Training Loss:  52.33836365\n",
      "################################  80  ################################\n",
      "Training Loss:  52.33836365\n",
      "################################  85  ################################\n",
      "Training Loss:  52.33836365\n",
      "################################  90  ################################\n",
      "Training Loss:  52.33836365\n",
      "################################  95  ################################\n",
      "Training Loss:  52.33836365\n",
      "Final training Loss:  52.33836365\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  48.48960495\n",
      "################################  5  ################################\n",
      "Training Loss:  38.7208786\n",
      "################################  10  ################################\n",
      "Training Loss:  38.43729782\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  38.41919708\n",
      "################################  20  ################################\n",
      "Training Loss:  38.3654213\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.53084183\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  38.39746857\n",
      "################################  35  ################################\n",
      "Training Loss:  38.30899811\n",
      "################################  40  ################################\n",
      "Training Loss:  38.52026749\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.36684418\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.28483963\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.28475571\n",
      "################################  60  ################################\n",
      "Training Loss:  38.28475571\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.28475571\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.28475571\n",
      "################################  75  ################################\n",
      "Training Loss:  38.28475571\n",
      "################################  80  ################################\n",
      "Training Loss:  38.28475571\n",
      "################################  85  ################################\n",
      "Training Loss:  38.28475571\n",
      "################################  90  ################################\n",
      "Training Loss:  38.28475571\n",
      "################################  95  ################################\n",
      "Training Loss:  38.28475571\n",
      "Final training Loss:  38.28475571\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  116.70063782\n",
      "################################  5  ################################\n",
      "Training Loss:  113.05092621\n",
      "################################  10  ################################\n",
      "Training Loss:  112.6894989\n",
      "################################  15  ################################\n",
      "Training Loss:  113.05500031\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  113.25605011\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  113.25140381\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  113.35831451\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  113.25370026\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  113.25349426\n",
      "################################  45  ################################\n",
      "Training Loss:  113.25354767\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  113.25356293\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  113.25356293\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  113.25356293\n",
      "################################  65  ################################\n",
      "Training Loss:  113.25356293\n",
      "################################  70  ################################\n",
      "Training Loss:  113.25356293\n",
      "################################  75  ################################\n",
      "Training Loss:  113.25356293\n",
      "################################  80  ################################\n",
      "Training Loss:  113.25356293\n",
      "################################  85  ################################\n",
      "Training Loss:  113.25356293\n",
      "################################  90  ################################\n",
      "Training Loss:  113.25356293\n",
      "################################  95  ################################\n",
      "Training Loss:  113.25356293\n",
      "Final training Loss:  113.25356293\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  67.5879364\n",
      "################################  5  ################################\n",
      "Training Loss:  66.36138916\n",
      "################################  10  ################################\n",
      "Training Loss:  49.5325737\n",
      "################################  15  ################################\n",
      "Training Loss:  46.67567825\n",
      "################################  20  ################################\n",
      "Training Loss:  44.64834213\n",
      "################################  25  ################################\n",
      "Training Loss:  43.41595078\n",
      "################################  30  ################################\n",
      "Training Loss:  42.74098969\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  121.75321198\n",
      "################################  5  ################################\n",
      "Training Loss:  119.71851349\n",
      "################################  10  ################################\n",
      "Training Loss:  117.56178284\n",
      "################################  15  ################################\n",
      "Training Loss:  111.59155273\n",
      "################################  20  ################################\n",
      "Training Loss:  157.86491394\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  111.96979523\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  111.53739166\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  111.20835114\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  110.54302216\n",
      "################################  45  ################################\n",
      "Training Loss:  110.47509003\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  110.47509003\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  110.47509766\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  110.47509766\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  110.47509766\n",
      "################################  70  ################################\n",
      "Training Loss:  110.47509766\n",
      "################################  75  ################################\n",
      "Training Loss:  110.47509766\n",
      "################################  80  ################################\n",
      "Training Loss:  110.47509766\n",
      "################################  85  ################################\n",
      "Training Loss:  110.47509766\n",
      "################################  90  ################################\n",
      "Training Loss:  110.47509766\n",
      "################################  95  ################################\n",
      "Training Loss:  110.47509766\n",
      "Final training Loss:  110.47509766\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.27947617\n",
      "################################  5  ################################\n",
      "Training Loss:  273.93515015\n",
      "################################  10  ################################\n",
      "Training Loss:  40.72803879\n",
      "################################  15  ################################\n",
      "Training Loss:  39.93190765\n",
      "################################  20  ################################\n",
      "Training Loss:  48.23052597\n",
      "################################  25  ################################\n",
      "Training Loss:  37.13444901\n",
      "################################  30  ################################\n",
      "Training Loss:  207.12228394\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  122.68097687\n",
      "################################  5  ################################\n",
      "Training Loss:  112.04374695\n",
      "################################  10  ################################\n",
      "Training Loss:  111.54200745\n",
      "################################  15  ################################\n",
      "Training Loss:  111.60227966\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  111.55657959\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  111.50240326\n",
      "################################  30  ################################\n",
      "Training Loss:  111.92848969\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  111.74417877\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  111.67306519\n",
      "################################  45  ################################\n",
      "Training Loss:  111.67318726\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  111.67311859\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  111.67311096\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  111.67311096\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  111.67311096\n",
      "################################  70  ################################\n",
      "Training Loss:  111.67311096\n",
      "################################  75  ################################\n",
      "Training Loss:  111.67311096\n",
      "################################  80  ################################\n",
      "Training Loss:  111.67311096\n",
      "################################  85  ################################\n",
      "Training Loss:  111.67311096\n",
      "################################  90  ################################\n",
      "Training Loss:  111.67311096\n",
      "################################  95  ################################\n",
      "Training Loss:  111.67311096\n",
      "Final training Loss:  111.67311096\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.4031868\n",
      "################################  5  ################################\n",
      "Training Loss:  47.66905975\n",
      "################################  10  ################################\n",
      "Training Loss:  5008.68896484\n",
      "################################  15  ################################\n",
      "Training Loss:  689.95904541\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  39.58641815\n",
      "################################  25  ################################\n",
      "Training Loss:  38.34589005\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.35229492\n",
      "################################  35  ################################\n",
      "Training Loss:  37.44264221\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.46139526\n",
      "################################  45  ################################\n",
      "Training Loss:  37.65306854\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.65297318\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.6529808\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.65299606\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.65299606\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.65299606\n",
      "################################  75  ################################\n",
      "Training Loss:  37.65299606\n",
      "################################  80  ################################\n",
      "Training Loss:  37.65299606\n",
      "################################  85  ################################\n",
      "Training Loss:  37.65299606\n",
      "################################  90  ################################\n",
      "Training Loss:  37.65299606\n",
      "################################  95  ################################\n",
      "Training Loss:  37.65299606\n",
      "Final training Loss:  37.65299606\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.8438797\n",
      "################################  5  ################################\n",
      "Training Loss:  47.85604095\n",
      "################################  10  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.6531868\n",
      "################################  5  ################################\n",
      "Training Loss:  51.30802155\n",
      "################################  10  ################################\n",
      "Training Loss:  50.2846489\n",
      "################################  15  ################################\n",
      "Training Loss:  37.29413986\n",
      "################################  20  ################################\n",
      "Training Loss:  38.33156967\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  37.23704147\n",
      "################################  30  ################################\n",
      "Training Loss:  36.95750809\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.96798706\n",
      "################################  40  ################################\n",
      "Training Loss:  36.69944\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  43.10005569\n",
      "################################  50  ################################\n",
      "Training Loss:  43.00060272\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  42.75582504\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  42.75553894\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  42.75555038\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  42.75583649\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  42.75583649\n",
      "################################  80  ################################\n",
      "Training Loss:  42.75583649\n",
      "################################  85  ################################\n",
      "Training Loss:  42.75583649\n",
      "################################  90  ################################\n",
      "Training Loss:  42.75583649\n",
      "################################  95  ################################\n",
      "Training Loss:  42.75583649\n",
      "Final training Loss:  42.75583649\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.76485062\n",
      "################################  5  ################################\n",
      "Training Loss:  50.51778793\n",
      "################################  10  ################################\n",
      "Training Loss:  50.0014267\n",
      "################################  15  ################################\n",
      "Training Loss:  45.39487839\n",
      "################################  20  ################################\n",
      "Training Loss:  38.64952469\n",
      "################################  25  ################################\n",
      "Training Loss:  218.13475037\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  40.07268524\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.06749725\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.85083008\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.76405716\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  38.76422119\n",
      "################################  55  ################################\n",
      "Training Loss:  38.76445389\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.764328\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.764328\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.764328\n",
      "################################  75  ################################\n",
      "Training Loss:  38.764328\n",
      "################################  80  ################################\n",
      "Training Loss:  38.764328\n",
      "################################  85  ################################\n",
      "Training Loss:  38.764328\n",
      "################################  90  ################################\n",
      "Training Loss:  38.764328\n",
      "################################  95  ################################\n",
      "Training Loss:  38.764328\n",
      "Final training Loss:  38.764328\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.17928696\n",
      "################################  5  ################################\n",
      "Training Loss:  49.49197388\n",
      "################################  10  ################################\n",
      "Training Loss:  40.50263214\n",
      "################################  15  ################################\n",
      "Training Loss:  937.80889893\n",
      "################################  20  ################################\n",
      "Training Loss:  226.35188293\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  37.15484238\n",
      "################################  30  ################################\n",
      "Training Loss:  40.15270615\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.1550827\n",
      "################################  40  ################################\n",
      "Training Loss:  37.09495163\n",
      "################################  45  ################################\n",
      "Training Loss:  37.58239365\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.10982895\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.10991287\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.10977936\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.10978317\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  37.10978317\n",
      "################################  75  ################################\n",
      "Training Loss:  37.10978317\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.10978317\n",
      "################################  85  ################################\n",
      "Training Loss:  37.10978317\n",
      "################################  90  ################################\n",
      "Training Loss:  37.10978317\n",
      "################################  95  ################################\n",
      "Training Loss:  37.10978317\n",
      "Final training Loss:  37.10978317\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  106.9309082\n",
      "################################  5  ################################\n",
      "Training Loss:  51.29870605\n",
      "################################  10  ################################\n",
      "Training Loss:  50.5618248\n",
      "################################  15  ################################\n",
      "Training Loss:  456.02883911\n",
      "################################  20  ################################\n",
      "Training Loss:  45.19435883\n",
      "################################  25  ################################\n",
      "Training Loss:  37.99290466\n",
      "################################  30  ################################\n",
      "Training Loss:  38.42879868\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.0411644\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  1875.67382812\n",
      "################################  45  ################################\n",
      "Training Loss:  37.86408615\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.45248795\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.02179718\n",
      "################################  60  ################################\n",
      "Training Loss:  38.02162933\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.02147293\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.02147293\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  38.02147293\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  38.02147293\n",
      "################################  85  ################################\n",
      "Training Loss:  38.02147293\n",
      "################################  90  ################################\n",
      "Training Loss:  38.02147293\n",
      "################################  95  ################################\n",
      "Training Loss:  38.02147293\n",
      "Final training Loss:  38.02147293\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  119.05606842\n",
      "################################  5  ################################\n",
      "Training Loss:  95.81476593\n",
      "################################  10  ################################\n",
      "Training Loss:  95.3093338\n",
      "################################  15  ################################\n",
      "Training Loss:  95.16139221\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  96.96338654\n",
      "################################  25  ################################\n",
      "Training Loss:  97.9905014\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  95.05752563\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.08971786\n",
      "################################  5  ################################\n",
      "Training Loss:  48.30755997\n",
      "################################  10  ################################\n",
      "Training Loss:  279.04013062\n",
      "################################  15  ################################\n",
      "Training Loss:  751.99090576\n",
      "################################  20  ################################\n",
      "Training Loss:  35.64612198\n",
      "################################  25  ################################\n",
      "Training Loss:  40.08879852\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  42.03035736\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  35.47985458\n",
      "################################  40  ################################\n",
      "Training Loss:  35.22481918\n",
      "################################  45  ################################\n",
      "Training Loss:  35.53489304\n",
      "################################  50  ################################\n",
      "Training Loss:  34.91047668\n",
      "################################  55  ################################\n",
      "Training Loss:  51.28827667\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  35.21760941\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  35.04457474\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  35.04442978\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  35.04443359\n",
      "################################  80  ################################\n",
      "Training Loss:  35.04442978\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  35.04442978\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  35.04442978\n",
      "################################  95  ################################\n",
      "Training Loss:  35.04442978\n",
      "Final training Loss:  35.04442978\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.43323135\n",
      "################################  5  ################################\n",
      "Training Loss:  48.80657959\n",
      "################################  10  ################################\n",
      "Training Loss:  667.53826904\n",
      "################################  15  ################################\n",
      "Training Loss:  204.18858337\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  47.4432869\n",
      "################################  25  ################################\n",
      "Training Loss:  42.11528778\n",
      "################################  30  ################################\n",
      "Training Loss:  51.19344711\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.88285446\n",
      "################################  40  ################################\n",
      "Training Loss:  39.37332535\n",
      "################################  45  ################################\n",
      "Training Loss:  39.26868439\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  39.36326218\n",
      "################################  55  ################################\n",
      "Training Loss:  39.46855545\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.46856308\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.46848297\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.46846771\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.46846771\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  39.46846771\n",
      "################################  85  ################################\n",
      "Training Loss:  39.46846771\n",
      "################################  90  ################################\n",
      "Training Loss:  39.46846771\n",
      "################################  95  ################################\n",
      "Training Loss:  39.46846771\n",
      "Final training Loss:  39.46846771\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  83.2661438\n",
      "################################  5  ################################\n",
      "Training Loss:  82.22070312\n",
      "################################  10  ################################\n",
      "Training Loss:  80.17457581\n",
      "################################  15  ################################\n",
      "Training Loss:  79.7339325\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  81.01676178\n",
      "################################  25  ################################\n",
      "Training Loss:  76.46653748\n",
      "################################  30  ################################\n",
      "Training Loss:  66.98071289\n",
      "################################  35  ################################\n",
      "Training Loss:  97.69374084\n",
      "################################  40  ################################\n",
      "Training Loss:  46.25856781\n",
      "################################  45  ################################\n",
      "Training Loss:  45.51301193\n",
      "################################  50  ################################\n",
      "Training Loss:  129.18296814\n",
      "################################  55  ################################\n",
      "Training Loss:  43.65890503\n",
      "################################  60  ################################\n",
      "Training Loss:  43.34451294\n",
      "################################  65  ################################\n",
      "Training Loss:  41.89881516\n",
      "################################  70  ################################\n",
      "Training Loss:  38.31102753\n",
      "################################  75  ################################\n",
      "Training Loss:  37.58066177\n",
      "################################  80  ################################\n",
      "Training Loss:  38.51016998\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  85  ################################\n",
      "Training Loss:  37.15390778\n",
      "################################  90  ################################\n",
      "Training Loss:  36.493927\n",
      "################################  95  ################################\n",
      "Training Loss:  36.493927\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  36.493927\n",
      "\n",
      "Running model (trial=2, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  101.66605377\n",
      "################################  5  ################################\n",
      "Training Loss:  101.71495056\n",
      "################################  10  ################################\n",
      "Training Loss:  101.41838074\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  101.3160553\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  101.43580627\n",
      "################################  25  ################################\n",
      "Training Loss:  101.33314514\n",
      "################################  30  ################################\n",
      "Training Loss:  101.39958954\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  101.38069916\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  101.3807373\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  101.3807373\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  101.3807373\n",
      "################################  55  ################################\n",
      "Training Loss:  101.3807373\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  101.3807373\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  101.3807373\n",
      "################################  70  ################################\n",
      "Training Loss:  101.3807373\n",
      "################################  75  ################################\n",
      "Training Loss:  101.3807373\n",
      "################################  80  ################################\n",
      "Training Loss:  101.3807373\n",
      "################################  85  ################################\n",
      "Training Loss:  101.3807373\n",
      "################################  90  ################################\n",
      "Training Loss:  101.3807373\n",
      "################################  95  ################################\n",
      "Training Loss:  101.3807373\n",
      "Final training Loss:  101.3807373\n",
      "\n",
      "Running model (trial=2, mod=53, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  101.36038971\n",
      "################################  5  ################################\n",
      "Training Loss:  98.44445038\n",
      "################################  10  ################################\n",
      "Training Loss:  61.77332306\n",
      "################################  15  ################################\n",
      "Training Loss:  54.75518799\n",
      "################################  20  ################################\n",
      "Training Loss:  52.54302216\n",
      "################################  25  ################################\n",
      "Training Loss:  52.26539612\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  52.19298553\n",
      "################################  35  ################################\n",
      "Training Loss:  52.22977066\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  52.36142349\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  52.23109055\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.210392\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.21038437\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  52.21038437\n",
      "################################  65  ################################\n",
      "Training Loss:  52.21038818\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  52.21038818\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  52.21038818\n",
      "################################  80  ################################\n",
      "Training Loss:  52.21038818\n",
      "################################  85  ################################\n",
      "Training Loss:  52.21038818\n",
      "################################  90  ################################\n",
      "Training Loss:  52.21038818\n",
      "################################  95  ################################\n",
      "Training Loss:  52.21038818\n",
      "Final training Loss:  52.21038818\n",
      "\n",
      "Running model (trial=2, mod=54, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  58.67054749\n",
      "################################  5  ################################\n",
      "Training Loss:  53.03728104\n",
      "################################  10  ################################\n",
      "Training Loss:  52.50075912\n",
      "################################  15  ################################\n",
      "Training Loss:  52.50303268\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  52.33860397\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  52.20690155\n",
      "################################  30  ################################\n",
      "Training Loss:  52.15021515\n",
      "################################  35  ################################\n",
      "Training Loss:  52.11334991\n",
      "################################  40  ################################\n",
      "Training Loss:  52.3175087\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  52.44075012\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.11989594\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.11988449\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  52.11988449\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  52.11988449\n",
      "################################  70  ################################\n",
      "Training Loss:  52.11988449\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  52.11988449\n",
      "################################  80  ################################\n",
      "Training Loss:  52.11988449\n",
      "################################  85  ################################\n",
      "Training Loss:  52.11988449\n",
      "################################  90  ################################\n",
      "Training Loss:  52.11988449\n",
      "################################  95  ################################\n",
      "Training Loss:  52.11988449\n",
      "Final training Loss:  52.11988449\n",
      "\n",
      "Running model (trial=2, mod=55, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.99139786\n",
      "################################  5  ################################\n",
      "Training Loss:  40.4786644\n",
      "################################  10  ################################\n",
      "Training Loss:  40.06975937\n",
      "################################  15  ################################\n",
      "Training Loss:  39.41989517\n",
      "################################  20  ################################\n",
      "Training Loss:  40.38015366\n",
      "################################  25  ################################\n",
      "Training Loss:  38.39367676\n",
      "################################  30  ################################\n",
      "Training Loss:  37.33608627\n",
      "################################  35  ################################\n",
      "Training Loss:  37.1386528\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.95928192\n",
      "################################  45  ################################\n",
      "Training Loss:  37.17088318\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.19487762\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=56, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  43.14763641\n",
      "################################  5  ################################\n",
      "Training Loss:  38.65485764\n",
      "################################  10  ################################\n",
      "Training Loss:  38.73980331\n",
      "################################  15  ################################\n",
      "Training Loss:  38.36826324\n",
      "################################  20  ################################\n",
      "Training Loss:  37.26325607\n",
      "################################  25  ################################\n",
      "Training Loss:  36.99016953\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  36.72813034\n",
      "################################  35  ################################\n",
      "Training Loss:  36.72853851\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.54930496\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.60620117\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  36.57384872\n",
      "################################  55  ################################\n",
      "Training Loss:  36.57386398\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.57384491\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.5738678\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.5738678\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.5738678\n",
      "################################  80  ################################\n",
      "Training Loss:  36.5738678\n",
      "################################  85  ################################\n",
      "Training Loss:  36.5738678\n",
      "################################  90  ################################\n",
      "Training Loss:  36.5738678\n",
      "################################  95  ################################\n",
      "Training Loss:  36.5738678\n",
      "Final training Loss:  36.5738678\n",
      "\n",
      "Running model (trial=2, mod=57, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  121.71557617\n",
      "################################  5  ################################\n",
      "Training Loss:  40.89120102\n",
      "################################  10  ################################\n",
      "Training Loss:  38.20570374\n",
      "################################  15  ################################\n",
      "Training Loss:  759.73706055\n",
      "################################  20  ################################\n",
      "Training Loss:  37.12590027\n",
      "################################  25  ################################\n",
      "Training Loss:  37.00399017\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.33589554\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.84147644\n",
      "################################  40  ################################\n",
      "Training Loss:  36.88755798\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.70909882\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=58, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.51226044\n",
      "################################  5  ################################\n",
      "Training Loss:  37.56296158\n",
      "################################  10  ################################\n",
      "Training Loss:  36.94728851\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  36.52215958\n",
      "################################  20  ################################\n",
      "Training Loss:  223.03967285\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2018.03527832\n",
      "################################  30  ################################\n",
      "Training Loss:  45.24532318\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  41.58958435\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  41.7449913\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  41.34844971\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  41.34842682\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  41.34842682\n",
      "################################  60  ################################\n",
      "Training Loss:  41.34843063\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  41.34842682\n",
      "################################  70  ################################\n",
      "Training Loss:  41.34842682\n",
      "################################  75  ################################\n",
      "Training Loss:  41.34842682\n",
      "################################  80  ################################\n",
      "Training Loss:  41.34842682\n",
      "################################  85  ################################\n",
      "Training Loss:  41.34842682\n",
      "################################  90  ################################\n",
      "Training Loss:  41.34842682\n",
      "################################  95  ################################\n",
      "Training Loss:  41.34842682\n",
      "Final training Loss:  41.34842682\n",
      "\n",
      "Running model (trial=2, mod=59, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  110.11354065\n",
      "################################  5  ################################\n",
      "Training Loss:  102.20707703\n",
      "################################  10  ################################\n",
      "Training Loss:  102.2014389\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  102.20256805\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  102.20063782\n",
      "################################  25  ################################\n",
      "Training Loss:  102.21583557\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  102.21583557\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  102.21583557\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  102.21583557\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  102.21583557\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  102.21583557\n",
      "################################  55  ################################\n",
      "Training Loss:  102.21583557\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  102.21583557\n",
      "################################  65  ################################\n",
      "Training Loss:  102.21583557\n",
      "################################  70  ################################\n",
      "Training Loss:  102.21583557\n",
      "################################  75  ################################\n",
      "Training Loss:  102.21583557\n",
      "################################  80  ################################\n",
      "Training Loss:  102.21583557\n",
      "################################  85  ################################\n",
      "Training Loss:  102.21583557\n",
      "################################  90  ################################\n",
      "Training Loss:  102.21583557\n",
      "################################  95  ################################\n",
      "Training Loss:  102.21583557\n",
      "Final training Loss:  102.21583557\n",
      "\n",
      "Running model (trial=2, mod=60, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  48.86505127\n",
      "################################  5  ################################\n",
      "Training Loss:  45.4152832\n",
      "################################  10  ################################\n",
      "Training Loss:  42.12665176\n",
      "################################  15  ################################\n",
      "Training Loss:  39.0538826\n",
      "################################  20  ################################\n",
      "Training Loss:  37.67083359\n",
      "################################  25  ################################\n",
      "Training Loss:  37.42081833\n",
      "################################  30  ################################\n",
      "Training Loss:  36.54796982\n",
      "################################  35  ################################\n",
      "Training Loss:  36.91530609\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.67304993\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.36852646\n",
      "################################  50  ################################\n",
      "Training Loss:  36.50326538\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.76298904\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.46253967\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.4444809\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.44451141\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.44452286\n",
      "################################  80  ################################\n",
      "Training Loss:  36.44451523\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  36.44451523\n",
      "################################  90  ################################\n",
      "Training Loss:  36.44451523\n",
      "################################  95  ################################\n",
      "Training Loss:  36.44451523\n",
      "Final training Loss:  36.44451523\n",
      "\n",
      "Running model (trial=2, mod=61, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  48.95180893\n",
      "################################  5  ################################\n",
      "Training Loss:  41.81929779\n",
      "################################  10  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=62, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  102.26776886\n",
      "################################  5  ################################\n",
      "Training Loss:  101.65276337\n",
      "################################  10  ################################\n",
      "Training Loss:  101.67871857\n",
      "################################  15  ################################\n",
      "Training Loss:  102.57243347\n",
      "################################  20  ################################\n",
      "Training Loss:  101.47215271\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  101.36376953\n",
      "################################  30  ################################\n",
      "Training Loss:  101.32107544\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  101.32221985\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  101.4127655\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  101.50073242\n",
      "################################  50  ################################\n",
      "Training Loss:  101.37615204\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  101.37615204\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  101.37615967\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  101.37615967\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  101.37615967\n",
      "################################  75  ################################\n",
      "Training Loss:  101.37615967\n",
      "################################  80  ################################\n",
      "Training Loss:  101.37615967\n",
      "################################  85  ################################\n",
      "Training Loss:  101.37615967\n",
      "################################  90  ################################\n",
      "Training Loss:  101.37615967\n",
      "################################  95  ################################\n",
      "Training Loss:  101.37615967\n",
      "Final training Loss:  101.37615967\n",
      "\n",
      "Running model (trial=2, mod=63, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  91.85650635\n",
      "################################  5  ################################\n",
      "Training Loss:  56.67222595\n",
      "################################  10  ################################\n",
      "Training Loss:  49.12305069\n",
      "################################  15  ################################\n",
      "Training Loss:  43.64677811\n",
      "################################  20  ################################\n",
      "Training Loss:  42.74009323\n",
      "################################  25  ################################\n",
      "Training Loss:  40.85199738\n",
      "################################  30  ################################\n",
      "Training Loss:  38.82970047\n",
      "################################  35  ################################\n",
      "Training Loss:  37.75486374\n",
      "################################  40  ################################\n",
      "Training Loss:  38.00374222\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  38.00039673\n",
      "################################  50  ################################\n",
      "Training Loss:  38.40561676\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.97934723\n",
      "################################  60  ################################\n",
      "Training Loss:  36.94695663\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.01567459\n",
      "################################  70  ################################\n",
      "Training Loss:  37.01534271\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.01481247\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.01523209\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  37.01531982\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  37.0151329\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.0151329\n",
      "Final training Loss:  37.0151329\n",
      "\n",
      "Running model (trial=2, mod=64, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=65, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.97994232\n",
      "################################  5  ################################\n",
      "Training Loss:  50.2207756\n",
      "################################  10  ################################\n",
      "Training Loss:  47.30271912\n",
      "################################  15  ################################\n",
      "Training Loss:  654.78839111\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  3727.44970703\n",
      "################################  25  ################################\n",
      "Training Loss:  40.63126755\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  39.08255768\n",
      "################################  35  ################################\n",
      "Training Loss:  38.20837402\n",
      "################################  40  ################################\n",
      "Training Loss:  37.46641541\n",
      "################################  45  ################################\n",
      "Training Loss:  37.16053772\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.37706375\n",
      "################################  55  ################################\n",
      "Training Loss:  37.37652206\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.37608719\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.37644958\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.37644958\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.37644958\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.37644958\n",
      "################################  85  ################################\n",
      "Training Loss:  37.37644958\n",
      "################################  90  ################################\n",
      "Training Loss:  37.37644958\n",
      "################################  95  ################################\n",
      "Training Loss:  37.37644958\n",
      "Final training Loss:  37.37644958\n",
      "\n",
      "Running model (trial=2, mod=66, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.62331009\n",
      "################################  5  ################################\n",
      "Training Loss:  51.64188766\n",
      "################################  10  ################################\n",
      "Training Loss:  152.13414001\n",
      "################################  15  ################################\n",
      "Training Loss:  37.5077858\n",
      "################################  20  ################################\n",
      "Training Loss:  37.8848381\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  231.8974762\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  210.41746521\n",
      "################################  35  ################################\n",
      "Training Loss:  37.79381943\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  40.13668823\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.48639679\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.48641968\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.48622513\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  38.48622513\n",
      "################################  65  ################################\n",
      "Training Loss:  38.48622513\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.48622513\n",
      "################################  75  ################################\n",
      "Training Loss:  38.48622513\n",
      "################################  80  ################################\n",
      "Training Loss:  38.48622513\n",
      "################################  85  ################################\n",
      "Training Loss:  38.48622513\n",
      "################################  90  ################################\n",
      "Training Loss:  38.48622513\n",
      "################################  95  ################################\n",
      "Training Loss:  38.48622513\n",
      "Final training Loss:  38.48622513\n",
      "\n",
      "Running model (trial=2, mod=67, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.21362686\n",
      "################################  5  ################################\n",
      "Training Loss:  48.90305328\n",
      "################################  10  ################################\n",
      "Training Loss:  47.07386398\n",
      "################################  15  ################################\n",
      "Training Loss:  46.30544662\n",
      "################################  20  ################################\n",
      "Training Loss:  4600.26123047\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  51.37599182\n",
      "################################  30  ################################\n",
      "Training Loss:  44.34607697\n",
      "################################  35  ################################\n",
      "Training Loss:  311.12191772\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  42.5242157\n",
      "################################  45  ################################\n",
      "Training Loss:  38.9837265\n",
      "################################  50  ################################\n",
      "Training Loss:  37.63731384\n",
      "################################  55  ################################\n",
      "Training Loss:  37.57254028\n",
      "################################  60  ################################\n",
      "Training Loss:  37.69013596\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.58610535\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.55857468\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.5587883\n",
      "################################  80  ################################\n",
      "Training Loss:  37.55878448\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  37.55878448\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  37.55878448\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  37.55878448\n",
      "Final training Loss:  37.55878448\n",
      "\n",
      "Running model (trial=2, mod=68, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.42537689\n",
      "################################  5  ################################\n",
      "Training Loss:  247.22579956\n",
      "################################  10  ################################\n",
      "Training Loss:  39.66294861\n",
      "################################  15  ################################\n",
      "Training Loss:  36.58673859\n",
      "################################  20  ################################\n",
      "Training Loss:  36.16572952\n",
      "################################  25  ################################\n",
      "Training Loss:  147.81999207\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  45.08950424\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.22821426\n",
      "################################  40  ################################\n",
      "Training Loss:  36.21048737\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  35.99110413\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.00930405\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.01268005\n",
      "################################  60  ################################\n",
      "Training Loss:  36.01273727\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.01273727\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.01273727\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.01273727\n",
      "################################  80  ################################\n",
      "Training Loss:  36.01273727\n",
      "################################  85  ################################\n",
      "Training Loss:  36.01273727\n",
      "################################  90  ################################\n",
      "Training Loss:  36.01273727\n",
      "################################  95  ################################\n",
      "Training Loss:  36.01273727\n",
      "Final training Loss:  36.01273727\n",
      "\n",
      "Running model (trial=2, mod=69, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.11004257\n",
      "################################  5  ################################\n",
      "Training Loss:  49.87963486\n",
      "################################  10  ################################\n",
      "Training Loss:  45.12720871\n",
      "################################  15  ################################\n",
      "Training Loss:  39.05069733\n",
      "################################  20  ################################\n",
      "Training Loss:  37.82164764\n",
      "################################  25  ################################\n",
      "Training Loss:  39.03893661\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  591.25085449\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.94363785\n",
      "################################  40  ################################\n",
      "Training Loss:  37.83509064\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  70.64024353\n",
      "################################  50  ################################\n",
      "Training Loss:  37.92328644\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.90504074\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.90518188\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.90518188\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.90524292\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.90524292\n",
      "################################  80  ################################\n",
      "Training Loss:  37.90524292\n",
      "################################  85  ################################\n",
      "Training Loss:  37.90524292\n",
      "################################  90  ################################\n",
      "Training Loss:  37.90524292\n",
      "################################  95  ################################\n",
      "Training Loss:  37.90524292\n",
      "Final training Loss:  37.90524292\n",
      "\n",
      "Running model (trial=2, mod=70, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.95010376\n",
      "################################  5  ################################\n",
      "Training Loss:  50.09745789\n",
      "################################  10  ################################\n",
      "Training Loss:  49.39483643\n",
      "################################  15  ################################\n",
      "Training Loss:  171.2293396\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  39.00927734\n",
      "################################  25  ################################\n",
      "Training Loss:  39.8053894\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.78172684\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.64861679\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.73883438\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.73878098\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.73888016\n",
      "################################  55  ################################\n",
      "Training Loss:  38.73888016\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.73888016\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.73888016\n",
      "################################  70  ################################\n",
      "Training Loss:  38.73888016\n",
      "################################  75  ################################\n",
      "Training Loss:  38.73888016\n",
      "################################  80  ################################\n",
      "Training Loss:  38.73888016\n",
      "################################  85  ################################\n",
      "Training Loss:  38.73888016\n",
      "################################  90  ################################\n",
      "Training Loss:  38.73888016\n",
      "################################  95  ################################\n",
      "Training Loss:  38.73888016\n",
      "Final training Loss:  38.73888016\n",
      "\n",
      "Running model (trial=2, mod=71, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.48229599\n",
      "################################  5  ################################\n",
      "Training Loss:  50.21228409\n",
      "################################  10  ################################\n",
      "Training Loss:  347.33834839\n",
      "################################  15  ################################\n",
      "Training Loss:  38.91083908\n",
      "################################  20  ################################\n",
      "Training Loss:  38.46456146\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=72, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.47887802\n",
      "################################  5  ################################\n",
      "Training Loss:  38.68389893\n",
      "################################  10  ################################\n",
      "Training Loss:  38.62366486\n",
      "################################  15  ################################\n",
      "Training Loss:  97.6186142\n",
      "################################  20  ################################\n",
      "Training Loss:  36.89037323\n",
      "################################  25  ################################\n",
      "Training Loss:  48.10768509\n",
      "################################  30  ################################\n",
      "Training Loss:  102.16924286\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  35.27152634\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.0171051\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  61.65836716\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  35.25179672\n",
      "################################  55  ################################\n",
      "Training Loss:  35.25179672\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  35.25165558\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  35.25166702\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  35.25166702\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  35.25166702\n",
      "################################  80  ################################\n",
      "Training Loss:  35.25166702\n",
      "################################  85  ################################\n",
      "Training Loss:  35.25166702\n",
      "################################  90  ################################\n",
      "Training Loss:  35.25166702\n",
      "################################  95  ################################\n",
      "Training Loss:  35.25166702\n",
      "Final training Loss:  35.25166702\n",
      "\n",
      "Running model (trial=2, mod=73, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  122.21528625\n",
      "################################  5  ################################\n",
      "Training Loss:  46.78506851\n",
      "################################  10  ################################\n",
      "Training Loss:  1088.25524902\n",
      "################################  15  ################################\n",
      "Training Loss:  213.3664856\n",
      "################################  20  ################################\n",
      "Training Loss:  4301.31787109\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  181.07640076\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  41.12430573\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.70804977\n",
      "################################  40  ################################\n",
      "Training Loss:  37.70794296\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.70785141\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.70785904\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.70785904\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.70785904\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.70785904\n",
      "################################  70  ################################\n",
      "Training Loss:  37.70785904\n",
      "################################  75  ################################\n",
      "Training Loss:  37.70785904\n",
      "################################  80  ################################\n",
      "Training Loss:  37.70785904\n",
      "################################  85  ################################\n",
      "Training Loss:  37.70785904\n",
      "################################  90  ################################\n",
      "Training Loss:  37.70785904\n",
      "################################  95  ################################\n",
      "Training Loss:  37.70785904\n",
      "Final training Loss:  37.70785904\n",
      "\n",
      "Running model (trial=2, mod=74, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  107.81891632\n",
      "################################  5  ################################\n",
      "Training Loss:  107.85071564\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  107.80830383\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  107.799263\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  107.88370514\n",
      "################################  25  ################################\n",
      "Training Loss:  107.88910675\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  107.81649017\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  107.81651306\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  107.81650543\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  107.81650543\n",
      "################################  55  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  60  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  65  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  70  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  75  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  80  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  85  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  90  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  95  ################################\n",
      "Training Loss:  107.81650543\n",
      "Final training Loss:  107.81650543\n",
      "\n",
      "Running model (trial=2, mod=75, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  115.70503235\n",
      "################################  5  ################################\n",
      "Training Loss:  188.67999268\n",
      "################################  10  ################################\n",
      "Training Loss:  229.89482117\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  94.59564209\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  104.02120972\n",
      "################################  25  ################################\n",
      "Training Loss:  99.0362854\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  116.65512848\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  98.92424011\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  98.92419434\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  98.92340088\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  98.92337036\n",
      "################################  55  ################################\n",
      "Training Loss:  98.92337036\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  98.92337036\n",
      "################################  65  ################################\n",
      "Training Loss:  98.92337036\n",
      "################################  70  ################################\n",
      "Training Loss:  98.92337036\n",
      "################################  75  ################################\n",
      "Training Loss:  98.92337036\n",
      "################################  80  ################################\n",
      "Training Loss:  98.92337036\n",
      "################################  85  ################################\n",
      "Training Loss:  98.92337036\n",
      "################################  90  ################################\n",
      "Training Loss:  98.92337036\n",
      "################################  95  ################################\n",
      "Training Loss:  98.92337036\n",
      "Final training Loss:  98.92337036\n",
      "\n",
      "Running model (trial=2, mod=76, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  121.51970673\n",
      "################################  5  ################################\n",
      "Training Loss:  118.48538208\n",
      "################################  10  ################################\n",
      "Training Loss:  118.61405945\n",
      "################################  15  ################################\n",
      "Training Loss:  115.36383057\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  114.39390564\n",
      "################################  25  ################################\n",
      "Training Loss:  113.59442139\n",
      "################################  30  ################################\n",
      "Training Loss:  114.16043091\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  113.9278717\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  113.7521286\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  113.72418976\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  113.72415161\n",
      "################################  55  ################################\n",
      "Training Loss:  113.72414398\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  113.7240448\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  113.7240448\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  113.7240448\n",
      "################################  75  ################################\n",
      "Training Loss:  113.7240448\n",
      "################################  80  ################################\n",
      "Training Loss:  113.7240448\n",
      "################################  85  ################################\n",
      "Training Loss:  113.7240448\n",
      "################################  90  ################################\n",
      "Training Loss:  113.7240448\n",
      "################################  95  ################################\n",
      "Training Loss:  113.7240448\n",
      "Final training Loss:  113.7240448\n",
      "\n",
      "Running model (trial=2, mod=77, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  43.82305908\n",
      "################################  5  ################################\n",
      "Training Loss:  37.61853409\n",
      "################################  10  ################################\n",
      "Training Loss:  39.00887299\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  39.67088699\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  38.5958252\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.06142807\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.06189346\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  38.07260895\n",
      "################################  40  ################################\n",
      "Training Loss:  38.0725975\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.07259369\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.07259369\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.07259369\n",
      "################################  60  ################################\n",
      "Training Loss:  38.07259369\n",
      "################################  65  ################################\n",
      "Training Loss:  38.07259369\n",
      "################################  70  ################################\n",
      "Training Loss:  38.07259369\n",
      "################################  75  ################################\n",
      "Training Loss:  38.07259369\n",
      "################################  80  ################################\n",
      "Training Loss:  38.07259369\n",
      "################################  85  ################################\n",
      "Training Loss:  38.07259369\n",
      "################################  90  ################################\n",
      "Training Loss:  38.07259369\n",
      "################################  95  ################################\n",
      "Training Loss:  38.07259369\n",
      "Final training Loss:  38.07259369\n",
      "\n",
      "Running model (trial=3, mod=78, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  101.65668488\n",
      "################################  5  ################################\n",
      "Training Loss:  101.86006165\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  101.79386139\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  101.64473724\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  101.72153473\n",
      "################################  25  ################################\n",
      "Training Loss:  101.68598175\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  101.68598175\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  101.68598175\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  101.68598175\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  101.68598175\n",
      "################################  55  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  60  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  65  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  70  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  75  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  80  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  85  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  90  ################################\n",
      "Training Loss:  101.68598175\n",
      "################################  95  ################################\n",
      "Training Loss:  101.68598175\n",
      "Final training Loss:  101.68598175\n",
      "\n",
      "Running model (trial=3, mod=79, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=80, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.70852661\n",
      "################################  5  ################################\n",
      "Training Loss:  42.47542953\n",
      "################################  10  ################################\n",
      "Training Loss:  40.38870621\n",
      "################################  15  ################################\n",
      "Training Loss:  37.00390244\n",
      "################################  20  ################################\n",
      "Training Loss:  37.46944809\n",
      "################################  25  ################################\n",
      "Training Loss:  36.84398651\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  37.37686539\n",
      "################################  35  ################################\n",
      "Training Loss:  37.29148102\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.76255035\n",
      "################################  45  ################################\n",
      "Training Loss:  37.07421494\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.24328613\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.25101089\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.94692993\n",
      "################################  65  ################################\n",
      "Training Loss:  36.9469223\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.94691467\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.94694901\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.94694901\n",
      "################################  85  ################################\n",
      "Training Loss:  36.94694901\n",
      "################################  90  ################################\n",
      "Training Loss:  36.94694901\n",
      "################################  95  ################################\n",
      "Training Loss:  36.94694901\n",
      "Final training Loss:  36.94694901\n",
      "\n",
      "Running model (trial=3, mod=81, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.07387543\n",
      "################################  5  ################################\n",
      "Training Loss:  123.24408722\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  122.9540863\n",
      "################################  15  ################################\n",
      "Training Loss:  122.38809967\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  122.73760986\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  122.62470245\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  122.6359024\n",
      "################################  35  ################################\n",
      "Training Loss:  122.69703674\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  122.6473999\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  122.6473999\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  122.6473999\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  122.6473999\n",
      "################################  60  ################################\n",
      "Training Loss:  122.6473999\n",
      "################################  65  ################################\n",
      "Training Loss:  122.6473999\n",
      "################################  70  ################################\n",
      "Training Loss:  122.6473999\n",
      "################################  75  ################################\n",
      "Training Loss:  122.6473999\n",
      "################################  80  ################################\n",
      "Training Loss:  122.6473999\n",
      "################################  85  ################################\n",
      "Training Loss:  122.6473999\n",
      "################################  90  ################################\n",
      "Training Loss:  122.6473999\n",
      "################################  95  ################################\n",
      "Training Loss:  122.6473999\n",
      "Final training Loss:  122.6473999\n",
      "\n",
      "Running model (trial=3, mod=82, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  42.3377037\n",
      "################################  5  ################################\n",
      "Training Loss:  37.73071289\n",
      "################################  10  ################################\n",
      "Training Loss:  36.66547012\n",
      "################################  15  ################################\n",
      "Training Loss:  36.69355774\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  36.98484421\n",
      "################################  25  ################################\n",
      "Training Loss:  36.70645905\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  36.45285416\n",
      "################################  35  ################################\n",
      "Training Loss:  36.34449387\n",
      "################################  40  ################################\n",
      "Training Loss:  36.20079803\n",
      "################################  45  ################################\n",
      "Training Loss:  36.26436615\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.73433685\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.70032501\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.5851593\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.5851593\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.58515549\n",
      "################################  75  ################################\n",
      "Training Loss:  36.58515549\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.58515549\n",
      "################################  85  ################################\n",
      "Training Loss:  36.58515549\n",
      "################################  90  ################################\n",
      "Training Loss:  36.58515549\n",
      "################################  95  ################################\n",
      "Training Loss:  36.58515549\n",
      "Final training Loss:  36.58515549\n",
      "\n",
      "Running model (trial=3, mod=83, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.26259995\n",
      "################################  5  ################################\n",
      "Training Loss:  40.90855408\n",
      "################################  10  ################################\n",
      "Training Loss:  40.92131805\n",
      "################################  15  ################################\n",
      "Training Loss:  39.6861496\n",
      "################################  20  ################################\n",
      "Training Loss:  39.84367371\n",
      "################################  25  ################################\n",
      "Training Loss:  38.32632446\n",
      "################################  30  ################################\n",
      "Training Loss:  38.086689\n",
      "################################  35  ################################\n",
      "Training Loss:  37.74478912\n",
      "################################  40  ################################\n",
      "Training Loss:  38.00543213\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.73865509\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.4761467\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.21128464\n",
      "################################  60  ################################\n",
      "Training Loss:  37.2652092\n",
      "################################  65  ################################\n",
      "Training Loss:  37.24164581\n",
      "################################  70  ################################\n",
      "Training Loss:  37.28466034\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.28097916\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.26928329\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  37.26912308\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  37.26911545\n",
      "################################  95  ################################\n",
      "Training Loss:  37.26909256\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  37.26909256\n",
      "\n",
      "Running model (trial=3, mod=84, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.76343155\n",
      "################################  5  ################################\n",
      "Training Loss:  38.78326035\n",
      "################################  10  ################################\n",
      "Training Loss:  617.85351562\n",
      "################################  15  ################################\n",
      "Training Loss:  37.38999176\n",
      "################################  20  ################################\n",
      "Training Loss:  36.78892517\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  39.85300064\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.87107086\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.04466248\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.58662796\n",
      "################################  45  ################################\n",
      "Training Loss:  38.54005432\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.54005814\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.54005814\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.54005814\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.54005814\n",
      "################################  70  ################################\n",
      "Training Loss:  38.54005814\n",
      "################################  75  ################################\n",
      "Training Loss:  38.54005814\n",
      "################################  80  ################################\n",
      "Training Loss:  38.54005814\n",
      "################################  85  ################################\n",
      "Training Loss:  38.54005814\n",
      "################################  90  ################################\n",
      "Training Loss:  38.54005814\n",
      "################################  95  ################################\n",
      "Training Loss:  38.54005814\n",
      "Final training Loss:  38.54005814\n",
      "\n",
      "Running model (trial=3, mod=85, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.08979797\n",
      "################################  5  ################################\n",
      "Training Loss:  53.25200272\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  53.05361938\n",
      "################################  15  ################################\n",
      "Training Loss:  52.51079559\n",
      "################################  20  ################################\n",
      "Training Loss:  43.37893295\n",
      "################################  25  ################################\n",
      "Training Loss:  40.1973381\n",
      "################################  30  ################################\n",
      "Training Loss:  37.05079651\n",
      "################################  35  ################################\n",
      "Training Loss:  36.73085403\n",
      "################################  40  ################################\n",
      "Training Loss:  36.84264374\n",
      "################################  45  ################################\n",
      "Training Loss:  36.35057449\n",
      "################################  50  ################################\n",
      "Training Loss:  36.04522324\n",
      "################################  55  ################################\n",
      "Training Loss:  36.94996262\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.64717102\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.86422729\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.07411575\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.07404327\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  37.07404327\n",
      "################################  85  ################################\n",
      "Training Loss:  37.07404327\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  37.07404327\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  37.07404327\n",
      "Final training Loss:  37.07404327\n",
      "\n",
      "Running model (trial=3, mod=86, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  91.5900116\n",
      "################################  5  ################################\n",
      "Training Loss:  53.12417221\n",
      "################################  10  ################################\n",
      "Training Loss:  53.02478027\n",
      "################################  15  ################################\n",
      "Training Loss:  41.07540512\n",
      "################################  20  ################################\n",
      "Training Loss:  38.04169083\n",
      "################################  25  ################################\n",
      "Training Loss:  37.53847122\n",
      "################################  30  ################################\n",
      "Training Loss:  37.45495605\n",
      "################################  35  ################################\n",
      "Training Loss:  36.85979462\n",
      "################################  40  ################################\n",
      "Training Loss:  36.71875\n",
      "################################  45  ################################\n",
      "Training Loss:  37.22173309\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.22566986\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  37.2194519\n",
      "################################  60  ################################\n",
      "Training Loss:  37.56331635\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.25370407\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.24626541\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.24343872\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.24343872\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  37.24343491\n",
      "################################  90  ################################\n",
      "Training Loss:  37.24341583\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  37.24341583\n",
      "Final training Loss:  37.24341583\n",
      "\n",
      "Running model (trial=3, mod=87, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.56196976\n",
      "################################  5  ################################\n",
      "Training Loss:  52.59695435\n",
      "################################  10  ################################\n",
      "Training Loss:  42.96458435\n",
      "################################  15  ################################\n",
      "Training Loss:  42.79208755\n",
      "################################  20  ################################\n",
      "Training Loss:  37.94665146\n",
      "################################  25  ################################\n",
      "Training Loss:  36.47584152\n",
      "################################  30  ################################\n",
      "Training Loss:  37.02647781\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.73158646\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.16413116\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.81777573\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.85703278\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.85704041\n",
      "################################  60  ################################\n",
      "Training Loss:  36.85705948\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.85719681\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.85719681\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.85719681\n",
      "################################  80  ################################\n",
      "Training Loss:  36.85719681\n",
      "################################  85  ################################\n",
      "Training Loss:  36.85719681\n",
      "################################  90  ################################\n",
      "Training Loss:  36.85719681\n",
      "################################  95  ################################\n",
      "Training Loss:  36.85719681\n",
      "Final training Loss:  36.85719681\n",
      "\n",
      "Running model (trial=3, mod=88, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=89, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  115.5947113\n",
      "################################  5  ################################\n",
      "Training Loss:  116.12013245\n",
      "################################  10  ################################\n",
      "Training Loss:  114.63591003\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  115.58889008\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  115.95829773\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  116.39900208\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  116.40657043\n",
      "################################  35  ################################\n",
      "Training Loss:  116.40657043\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  116.40657043\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  116.40653992\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  116.40653992\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  116.40653992\n",
      "################################  60  ################################\n",
      "Training Loss:  116.40653992\n",
      "################################  65  ################################\n",
      "Training Loss:  116.40653992\n",
      "################################  70  ################################\n",
      "Training Loss:  116.40653992\n",
      "################################  75  ################################\n",
      "Training Loss:  116.40653992\n",
      "################################  80  ################################\n",
      "Training Loss:  116.40653992\n",
      "################################  85  ################################\n",
      "Training Loss:  116.40653992\n",
      "################################  90  ################################\n",
      "Training Loss:  116.40653992\n",
      "################################  95  ################################\n",
      "Training Loss:  116.40653992\n",
      "Final training Loss:  116.40653992\n",
      "\n",
      "Running model (trial=3, mod=90, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  132.67483521\n",
      "################################  5  ################################\n",
      "Training Loss:  63.05743408\n",
      "################################  10  ################################\n",
      "Training Loss:  63.0518074\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  63.02997208\n",
      "################################  20  ################################\n",
      "Training Loss:  63.02997208\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  63.07193756\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  63.07146835\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  63.07146835\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  63.07146835\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  63.07146835\n",
      "################################  50  ################################\n",
      "Training Loss:  63.07146835\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  63.07146835\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  63.07146835\n",
      "################################  65  ################################\n",
      "Training Loss:  63.07146835\n",
      "################################  70  ################################\n",
      "Training Loss:  63.07146835\n",
      "################################  75  ################################\n",
      "Training Loss:  63.07146835\n",
      "################################  80  ################################\n",
      "Training Loss:  63.07146835\n",
      "################################  85  ################################\n",
      "Training Loss:  63.07146835\n",
      "################################  90  ################################\n",
      "Training Loss:  63.07146835\n",
      "################################  95  ################################\n",
      "Training Loss:  63.07146835\n",
      "Final training Loss:  63.07146835\n",
      "\n",
      "Running model (trial=3, mod=91, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.33897781\n",
      "################################  5  ################################\n",
      "Training Loss:  47.76953125\n",
      "################################  10  ################################\n",
      "Training Loss:  582.48400879\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  1496.64355469\n",
      "################################  20  ################################\n",
      "Training Loss:  44.66838455\n",
      "################################  25  ################################\n",
      "Training Loss:  39.3233757\n",
      "################################  30  ################################\n",
      "Training Loss:  38.58674622\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  44.38445282\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.50144196\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.91972733\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.91978836\n",
      "################################  55  ################################\n",
      "Training Loss:  37.91973877\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.91973877\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.91973877\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.91973877\n",
      "################################  75  ################################\n",
      "Training Loss:  37.91973877\n",
      "################################  80  ################################\n",
      "Training Loss:  37.91973877\n",
      "################################  85  ################################\n",
      "Training Loss:  37.91973877\n",
      "################################  90  ################################\n",
      "Training Loss:  37.91973877\n",
      "################################  95  ################################\n",
      "Training Loss:  37.91973877\n",
      "Final training Loss:  37.91973877\n",
      "\n",
      "Running model (trial=3, mod=92, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.67766953\n",
      "################################  5  ################################\n",
      "Training Loss:  51.65655518\n",
      "################################  10  ################################\n",
      "Training Loss:  47.37875748\n",
      "################################  15  ################################\n",
      "Training Loss:  2023.28234863\n",
      "################################  20  ################################\n",
      "Training Loss:  236.19174194\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  37.10139084\n",
      "################################  30  ################################\n",
      "Training Loss:  2524.62939453\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  37.66435242\n",
      "################################  40  ################################\n",
      "Training Loss:  36.41195679\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.57384872\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.4327507\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.43283463\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.43283463\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.43283463\n",
      "################################  70  ################################\n",
      "Training Loss:  36.43283463\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.43283463\n",
      "################################  80  ################################\n",
      "Training Loss:  36.43283463\n",
      "################################  85  ################################\n",
      "Training Loss:  36.43283463\n",
      "################################  90  ################################\n",
      "Training Loss:  36.43283463\n",
      "################################  95  ################################\n",
      "Training Loss:  36.43283463\n",
      "Final training Loss:  36.43283463\n",
      "\n",
      "Running model (trial=3, mod=93, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.87349701\n",
      "################################  5  ################################\n",
      "Training Loss:  48.21738052\n",
      "################################  10  ################################\n",
      "Training Loss:  3158.09619141\n",
      "################################  15  ################################\n",
      "Training Loss:  38.55152893\n",
      "################################  20  ################################\n",
      "Training Loss:  181.76144409\n",
      "################################  25  ################################\n",
      "Training Loss:  37.12310791\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.91057205\n",
      "################################  35  ################################\n",
      "Training Loss:  37.81113815\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  35.12092972\n",
      "################################  45  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=94, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.4053154\n",
      "################################  5  ################################\n",
      "Training Loss:  293.18121338\n",
      "################################  10  ################################\n",
      "Training Loss:  38.18222427\n",
      "################################  15  ################################\n",
      "Training Loss:  38.094944\n",
      "################################  20  ################################\n",
      "Training Loss:  224.60765076\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  681.35571289\n",
      "################################  30  ################################\n",
      "Training Loss:  1418.44543457\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.54343414\n",
      "################################  40  ################################\n",
      "Training Loss:  37.68428421\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.48205566\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.48665237\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.48651886\n",
      "################################  60  ################################\n",
      "Training Loss:  37.48651886\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.48651886\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.48651886\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.48651886\n",
      "################################  80  ################################\n",
      "Training Loss:  37.48651886\n",
      "################################  85  ################################\n",
      "Training Loss:  37.48651886\n",
      "################################  90  ################################\n",
      "Training Loss:  37.48651886\n",
      "################################  95  ################################\n",
      "Training Loss:  37.48651886\n",
      "Final training Loss:  37.48651886\n",
      "\n",
      "Running model (trial=3, mod=95, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.22758865\n",
      "################################  5  ################################\n",
      "Training Loss:  51.00032425\n",
      "################################  10  ################################\n",
      "Training Loss:  43.09664154\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  48.74993896\n",
      "################################  20  ################################\n",
      "Training Loss:  40.57979965\n",
      "################################  25  ################################\n",
      "Training Loss:  38.58373642\n",
      "################################  30  ################################\n",
      "Training Loss:  233.24504089\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  41.8979187\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.95840073\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.95303345\n",
      "################################  50  ################################\n",
      "Training Loss:  38.95307159\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.95324707\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.95323563\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.95323563\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.95323563\n",
      "################################  75  ################################\n",
      "Training Loss:  38.95323563\n",
      "################################  80  ################################\n",
      "Training Loss:  38.95323563\n",
      "################################  85  ################################\n",
      "Training Loss:  38.95323563\n",
      "################################  90  ################################\n",
      "Training Loss:  38.95323563\n",
      "################################  95  ################################\n",
      "Training Loss:  38.95323563\n",
      "Final training Loss:  38.95323563\n",
      "\n",
      "Running model (trial=3, mod=96, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  56.76604843\n",
      "################################  5  ################################\n",
      "Training Loss:  50.09972\n",
      "################################  10  ################################\n",
      "Training Loss:  50.08724594\n",
      "################################  15  ################################\n",
      "Training Loss:  559.38226318\n",
      "################################  20  ################################\n",
      "Training Loss:  1134.21533203\n",
      "################################  25  ################################\n",
      "Training Loss:  220.18486023\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.73212814\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=97, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.21323395\n",
      "################################  5  ################################\n",
      "Training Loss:  50.71161652\n",
      "################################  10  ################################\n",
      "Training Loss:  50.73999023\n",
      "################################  15  ################################\n",
      "Training Loss:  270.6930542\n",
      "################################  20  ################################\n",
      "Training Loss:  54.41830444\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  43.69977188\n",
      "################################  30  ################################\n",
      "Training Loss:  293.46078491\n",
      "################################  35  ################################\n",
      "Training Loss:  36.95310211\n",
      "################################  40  ################################\n",
      "Training Loss:  70.46884155\n",
      "################################  45  ################################\n",
      "Training Loss:  64.08646393\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.24933624\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.90948868\n",
      "################################  60  ################################\n",
      "Training Loss:  36.914608\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.80229568\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.80237961\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.80215836\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.80215836\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  36.80215836\n",
      "################################  90  ################################\n",
      "Training Loss:  36.80215836\n",
      "################################  95  ################################\n",
      "Training Loss:  36.80215836\n",
      "Final training Loss:  36.80215836\n",
      "\n",
      "Running model (trial=3, mod=98, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.42792892\n",
      "################################  5  ################################\n",
      "Training Loss:  51.86873627\n",
      "################################  10  ################################\n",
      "Training Loss:  38.76574707\n",
      "################################  15  ################################\n",
      "Training Loss:  1184.98010254\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  37.10757065\n",
      "################################  25  ################################\n",
      "Training Loss:  39.24355698\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  39.29461288\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  57.23934555\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.89028168\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.8902626\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  37.89028549\n",
      "################################  55  ################################\n",
      "Training Loss:  37.89028549\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.89028549\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.89028549\n",
      "################################  70  ################################\n",
      "Training Loss:  37.89028549\n",
      "################################  75  ################################\n",
      "Training Loss:  37.89028549\n",
      "################################  80  ################################\n",
      "Training Loss:  37.89028549\n",
      "################################  85  ################################\n",
      "Training Loss:  37.89028549\n",
      "################################  90  ################################\n",
      "Training Loss:  37.89028549\n",
      "################################  95  ################################\n",
      "Training Loss:  37.89028549\n",
      "Final training Loss:  37.89028549\n",
      "\n",
      "Running model (trial=3, mod=99, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.87123108\n",
      "################################  5  ################################\n",
      "Training Loss:  55.37392426\n",
      "################################  10  ################################\n",
      "Training Loss:  41.31816483\n",
      "################################  15  ################################\n",
      "Training Loss:  38.54261398\n",
      "################################  20  ################################\n",
      "Training Loss:  40.06245804\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  37.62514877\n",
      "################################  30  ################################\n",
      "Training Loss:  39.8519783\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.91539001\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.5461235\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.54611588\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.40270615\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.40269852\n",
      "################################  60  ################################\n",
      "Training Loss:  38.40269852\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.40269852\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.40269852\n",
      "################################  75  ################################\n",
      "Training Loss:  38.40269852\n",
      "################################  80  ################################\n",
      "Training Loss:  38.40269852\n",
      "################################  85  ################################\n",
      "Training Loss:  38.40269852\n",
      "################################  90  ################################\n",
      "Training Loss:  38.40269852\n",
      "################################  95  ################################\n",
      "Training Loss:  38.40269852\n",
      "Final training Loss:  38.40269852\n",
      "\n",
      "Running model (trial=3, mod=100, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.65258789\n",
      "################################  5  ################################\n",
      "Training Loss:  55.28302765\n",
      "################################  10  ################################\n",
      "Training Loss:  51.83354568\n",
      "################################  15  ################################\n",
      "Training Loss:  49.1595192\n",
      "################################  20  ################################\n",
      "Training Loss:  47.67831039\n",
      "################################  25  ################################\n",
      "Training Loss:  266.02050781\n",
      "################################  30  ################################\n",
      "Training Loss:  193.89237976\n",
      "################################  35  ################################\n",
      "Training Loss:  40.31138611\n",
      "################################  40  ################################\n",
      "Training Loss:  38.23672104\n",
      "################################  45  ################################\n",
      "Training Loss:  38.44051743\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.15019989\n",
      "################################  55  ################################\n",
      "Training Loss:  37.9281044\n",
      "################################  60  ################################\n",
      "Training Loss:  37.90422821\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.05055237\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.7806778\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.2410202\n",
      "################################  80  ################################\n",
      "Training Loss:  38.24091721\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  38.24091721\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  38.24091721\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  38.2408638\n",
      "Epoch   100: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  38.2408638\n",
      "\n",
      "Running model (trial=3, mod=101, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  124.50444794\n",
      "################################  5  ################################\n",
      "Training Loss:  121.7616272\n",
      "################################  10  ################################\n",
      "Training Loss:  118.09233856\n",
      "################################  15  ################################\n",
      "Training Loss:  117.24330902\n",
      "################################  20  ################################\n",
      "Training Loss:  115.49832153\n",
      "################################  25  ################################\n",
      "Training Loss:  112.73143768\n",
      "################################  30  ################################\n",
      "Training Loss:  103.74515533\n",
      "################################  35  ################################\n",
      "Training Loss:  100.75847626\n",
      "################################  40  ################################\n",
      "Training Loss:  101.28647614\n",
      "################################  45  ################################\n",
      "Training Loss:  485.10092163\n",
      "################################  50  ################################\n",
      "Training Loss:  98.7197113\n",
      "################################  55  ################################\n",
      "Training Loss:  94.41412354\n",
      "################################  60  ################################\n",
      "Training Loss:  94.96264648\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  65  ################################\n",
      "Training Loss:  686.70654297\n",
      "################################  70  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=102, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.03750992\n",
      "################################  5  ################################\n",
      "Training Loss:  39.04024887\n",
      "################################  10  ################################\n",
      "Training Loss:  38.46760941\n",
      "################################  15  ################################\n",
      "Training Loss:  37.24422073\n",
      "################################  20  ################################\n",
      "Training Loss:  559.12652588\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  37.02579117\n",
      "################################  30  ################################\n",
      "Training Loss:  36.98996353\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  213.21496582\n",
      "################################  40  ################################\n",
      "Training Loss:  155.67552185\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.69817734\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.69229889\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.69229889\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.69226074\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.69226074\n",
      "################################  70  ################################\n",
      "Training Loss:  36.69226074\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.69226074\n",
      "################################  80  ################################\n",
      "Training Loss:  36.69226074\n",
      "################################  85  ################################\n",
      "Training Loss:  36.69226074\n",
      "################################  90  ################################\n",
      "Training Loss:  36.69226074\n",
      "################################  95  ################################\n",
      "Training Loss:  36.69226074\n",
      "Final training Loss:  36.69226074\n",
      "\n",
      "Running model (trial=3, mod=103, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.74816895\n",
      "################################  5  ################################\n",
      "Training Loss:  51.85719299\n",
      "################################  10  ################################\n",
      "Training Loss:  50.37087631\n",
      "################################  15  ################################\n",
      "Training Loss:  118.81052399\n",
      "################################  20  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=104, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=105, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  114.12241364\n",
      "################################  5  ################################\n",
      "Training Loss:  112.63477325\n",
      "################################  10  ################################\n",
      "Training Loss:  113.55628204\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  113.55101013\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  113.55095673\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  113.55092621\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  113.55088806\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  113.55088806\n",
      "################################  40  ################################\n",
      "Training Loss:  113.55088806\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  113.55088806\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  113.55088806\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  113.55088806\n",
      "################################  60  ################################\n",
      "Training Loss:  113.55088806\n",
      "################################  65  ################################\n",
      "Training Loss:  113.55088806\n",
      "################################  70  ################################\n",
      "Training Loss:  113.55088806\n",
      "################################  75  ################################\n",
      "Training Loss:  113.55088806\n",
      "################################  80  ################################\n",
      "Training Loss:  113.55088806\n",
      "################################  85  ################################\n",
      "Training Loss:  113.55088806\n",
      "################################  90  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=106, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  57.09976578\n",
      "################################  5  ################################\n",
      "Training Loss:  42.69682693\n",
      "################################  10  ################################\n",
      "Training Loss:  42.28507233\n",
      "################################  15  ################################\n",
      "Training Loss:  40.97838974\n",
      "################################  20  ################################\n",
      "Training Loss:  37.88138199\n",
      "################################  25  ################################\n",
      "Training Loss:  36.7457962\n",
      "################################  30  ################################\n",
      "Training Loss:  36.70041656\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.17491913\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.1755867\n",
      "################################  45  ################################\n",
      "Training Loss:  36.2135582\n",
      "################################  50  ################################\n",
      "Training Loss:  36.1147995\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  36.17531204\n",
      "################################  60  ################################\n",
      "Training Loss:  36.02918625\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.0461235\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.00371552\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.00374222\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.00374222\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  36.00374222\n",
      "################################  90  ################################\n",
      "Training Loss:  36.00374222\n",
      "################################  95  ################################\n",
      "Training Loss:  36.00374222\n",
      "Final training Loss:  36.00374222\n",
      "\n",
      "Running model (trial=4, mod=107, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  44.42936707\n",
      "################################  5  ################################\n",
      "Training Loss:  41.05261993\n",
      "################################  10  ################################\n",
      "Training Loss:  39.8592186\n",
      "################################  15  ################################\n",
      "Training Loss:  36.70993042\n",
      "################################  20  ################################\n",
      "Training Loss:  37.51276016\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  36.95034409\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  36.68915176\n",
      "################################  35  ################################\n",
      "Training Loss:  36.54501343\n",
      "################################  40  ################################\n",
      "Training Loss:  36.73163223\n",
      "################################  45  ################################\n",
      "Training Loss:  36.46533585\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.61619568\n",
      "################################  55  ################################\n",
      "Training Loss:  36.40478897\n",
      "################################  60  ################################\n",
      "Training Loss:  36.57273483\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.76702499\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.82019806\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.82019043\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.82019043\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  36.82019043\n",
      "################################  90  ################################\n",
      "Training Loss:  36.82019043\n",
      "################################  95  ################################\n",
      "Training Loss:  36.82019043\n",
      "Final training Loss:  36.82019043\n",
      "\n",
      "Running model (trial=4, mod=108, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.88268661\n",
      "################################  5  ################################\n",
      "Training Loss:  41.68182373\n",
      "################################  10  ################################\n",
      "Training Loss:  41.65965271\n",
      "################################  15  ################################\n",
      "Training Loss:  37.58439636\n",
      "################################  20  ################################\n",
      "Training Loss:  37.34250641\n",
      "################################  25  ################################\n",
      "Training Loss:  36.96366119\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  37.26290894\n",
      "################################  35  ################################\n",
      "Training Loss:  36.90761185\n",
      "################################  40  ################################\n",
      "Training Loss:  36.76694107\n",
      "################################  45  ################################\n",
      "Training Loss:  37.20415878\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.15087891\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.03531647\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.05420685\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.05629349\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  37.05633163\n",
      "################################  75  ################################\n",
      "Training Loss:  37.05633163\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.05633163\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  37.05633163\n",
      "################################  90  ################################\n",
      "Training Loss:  37.05633163\n",
      "################################  95  ################################\n",
      "Training Loss:  37.05633163\n",
      "Final training Loss:  37.05633163\n",
      "\n",
      "Running model (trial=4, mod=109, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  41.09508514\n",
      "################################  5  ################################\n",
      "Training Loss:  65.60996246\n",
      "################################  10  ################################\n",
      "Training Loss:  37.60248184\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  40.0477829\n",
      "################################  20  ################################\n",
      "Training Loss:  39.50571442\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  39.38260269\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.80434036\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.7938118\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.79382706\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.79383469\n",
      "################################  50  ################################\n",
      "Training Loss:  38.79383469\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.79383469\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.79383469\n",
      "################################  65  ################################\n",
      "Training Loss:  38.79383469\n",
      "################################  70  ################################\n",
      "Training Loss:  38.79383469\n",
      "################################  75  ################################\n",
      "Training Loss:  38.79383469\n",
      "################################  80  ################################\n",
      "Training Loss:  38.79383469\n",
      "################################  85  ################################\n",
      "Training Loss:  38.79383469\n",
      "################################  90  ################################\n",
      "Training Loss:  38.79383469\n",
      "################################  95  ################################\n",
      "Training Loss:  38.79383469\n",
      "Final training Loss:  38.79383469\n",
      "\n",
      "Running model (trial=4, mod=110, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.11549377\n",
      "################################  5  ################################\n",
      "Training Loss:  40.6183548\n",
      "################################  10  ################################\n",
      "Training Loss:  40.19781876\n",
      "################################  15  ################################\n",
      "Training Loss:  392.91952515\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  43.00684357\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  40.62746811\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  42.21895599\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  40.89709091\n",
      "################################  40  ################################\n",
      "Training Loss:  40.84241104\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  40.12902451\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  40.1289711\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  40.12190247\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  40.12190247\n",
      "################################  65  ################################\n",
      "Training Loss:  40.12190247\n",
      "################################  70  ################################\n",
      "Training Loss:  40.12190247\n",
      "################################  75  ################################\n",
      "Training Loss:  40.12190247\n",
      "################################  80  ################################\n",
      "Training Loss:  40.12190247\n",
      "################################  85  ################################\n",
      "Training Loss:  40.12190247\n",
      "################################  90  ################################\n",
      "Training Loss:  40.12190247\n",
      "################################  95  ################################\n",
      "Training Loss:  40.12190247\n",
      "Final training Loss:  40.12190247\n",
      "\n",
      "Running model (trial=4, mod=111, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  64.03981018\n",
      "################################  5  ################################\n",
      "Training Loss:  62.89473724\n",
      "################################  10  ################################\n",
      "Training Loss:  60.6664505\n",
      "################################  15  ################################\n",
      "Training Loss:  53.08081818\n",
      "################################  20  ################################\n",
      "Training Loss:  52.22787857\n",
      "################################  25  ################################\n",
      "Training Loss:  52.50399399\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  52.19920731\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  52.42205429\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  52.41781616\n",
      "################################  45  ################################\n",
      "Training Loss:  52.45637894\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.52392197\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.52392197\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  52.52392197\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  52.52392197\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  52.52392197\n",
      "################################  75  ################################\n",
      "Training Loss:  52.52392197\n",
      "################################  80  ################################\n",
      "Training Loss:  52.52392197\n",
      "################################  85  ################################\n",
      "Training Loss:  52.52392197\n",
      "################################  90  ################################\n",
      "Training Loss:  52.52392197\n",
      "################################  95  ################################\n",
      "Training Loss:  52.52392197\n",
      "Final training Loss:  52.52392197\n",
      "\n",
      "Running model (trial=4, mod=112, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.25222397\n",
      "################################  5  ################################\n",
      "Training Loss:  42.58889008\n",
      "################################  10  ################################\n",
      "Training Loss:  40.39957809\n",
      "################################  15  ################################\n",
      "Training Loss:  39.69291306\n",
      "################################  20  ################################\n",
      "Training Loss:  38.57797241\n",
      "################################  25  ################################\n",
      "Training Loss:  37.79613113\n",
      "################################  30  ################################\n",
      "Training Loss:  37.73981857\n",
      "################################  35  ################################\n",
      "Training Loss:  37.64495087\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=113, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.63601303\n",
      "################################  5  ################################\n",
      "Training Loss:  37.83532715\n",
      "################################  10  ################################\n",
      "Training Loss:  37.26893997\n",
      "################################  15  ################################\n",
      "Training Loss:  37.24409485\n",
      "################################  20  ################################\n",
      "Training Loss:  37.13368988\n",
      "################################  25  ################################\n",
      "Training Loss:  36.61613846\n",
      "################################  30  ################################\n",
      "Training Loss:  37.47560883\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  45.84369659\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  44.31693649\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  43.78177643\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  43.73064423\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  43.73055267\n",
      "################################  60  ################################\n",
      "Training Loss:  43.73055267\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  43.73056412\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  43.73056412\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  43.7305603\n",
      "################################  80  ################################\n",
      "Training Loss:  43.7305603\n",
      "################################  85  ################################\n",
      "Training Loss:  43.7305603\n",
      "################################  90  ################################\n",
      "Training Loss:  43.7305603\n",
      "################################  95  ################################\n",
      "Training Loss:  43.7305603\n",
      "Final training Loss:  43.7305603\n",
      "\n",
      "Running model (trial=4, mod=114, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  47.71118927\n",
      "################################  5  ################################\n",
      "Training Loss:  40.2567482\n",
      "################################  10  ################################\n",
      "Training Loss:  39.89278412\n",
      "################################  15  ################################\n",
      "Training Loss:  39.81949234\n",
      "################################  20  ################################\n",
      "Training Loss:  37.48644638\n",
      "################################  25  ################################\n",
      "Training Loss:  37.42733383\n",
      "################################  30  ################################\n",
      "Training Loss:  37.5152626\n",
      "################################  35  ################################\n",
      "Training Loss:  37.3891983\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.44281006\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.89420319\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.60364914\n",
      "################################  55  ################################\n",
      "Training Loss:  37.43388367\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.43476486\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.49861145\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.49862289\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.49858093\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.49858093\n",
      "################################  85  ################################\n",
      "Training Loss:  37.49858093\n",
      "################################  90  ################################\n",
      "Training Loss:  37.49858093\n",
      "################################  95  ################################\n",
      "Training Loss:  37.49858093\n",
      "Final training Loss:  37.49858093\n",
      "\n",
      "Running model (trial=4, mod=115, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  55.36120224\n",
      "################################  5  ################################\n",
      "Training Loss:  51.40065002\n",
      "################################  10  ################################\n",
      "Training Loss:  50.3040657\n",
      "################################  15  ################################\n",
      "Training Loss:  45.40414429\n",
      "################################  20  ################################\n",
      "Training Loss:  44.02524567\n",
      "################################  25  ################################\n",
      "Training Loss:  41.48551178\n",
      "################################  30  ################################\n",
      "Training Loss:  39.46959305\n",
      "################################  35  ################################\n",
      "Training Loss:  37.742939\n",
      "################################  40  ################################\n",
      "Training Loss:  37.24213028\n",
      "################################  45  ################################\n",
      "Training Loss:  37.14053345\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  50  ################################\n",
      "Training Loss:  40.49430084\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  39.83338928\n",
      "################################  60  ################################\n",
      "Training Loss:  39.28144073\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.25632095\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.13800049\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.13803101\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  39.13805008\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  39.13805008\n",
      "################################  90  ################################\n",
      "Training Loss:  39.13805008\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  39.13805008\n",
      "Final training Loss:  39.13805008\n",
      "\n",
      "Running model (trial=4, mod=116, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  649938.3125\n",
      "################################  5  ################################\n",
      "Training Loss:  113.65723419\n",
      "################################  10  ################################\n",
      "Training Loss:  112.83766937\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  112.77431488\n",
      "################################  20  ################################\n",
      "Training Loss:  112.61534882\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  112.69933319\n",
      "################################  30  ################################\n",
      "Training Loss:  112.69262695\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  112.69210815\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  112.69210815\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  112.69210815\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  112.69210815\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  112.69210815\n",
      "################################  60  ################################\n",
      "Training Loss:  112.69210815\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  112.69210815\n",
      "################################  70  ################################\n",
      "Training Loss:  112.69210815\n",
      "################################  75  ################################\n",
      "Training Loss:  112.69210815\n",
      "################################  80  ################################\n",
      "Training Loss:  112.69210815\n",
      "################################  85  ################################\n",
      "Training Loss:  112.69210815\n",
      "################################  90  ################################\n",
      "Training Loss:  112.69210815\n",
      "################################  95  ################################\n",
      "Training Loss:  112.69210815\n",
      "Final training Loss:  112.69210815\n",
      "\n",
      "Running model (trial=4, mod=117, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.84993362\n",
      "################################  5  ################################\n",
      "Training Loss:  47.68196869\n",
      "################################  10  ################################\n",
      "Training Loss:  47.92063522\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  114.23229218\n",
      "################################  20  ################################\n",
      "Training Loss:  47.11274719\n",
      "################################  25  ################################\n",
      "Training Loss:  239.70361328\n",
      "################################  30  ################################\n",
      "Training Loss:  39.88719559\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.58707809\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  39.34840012\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.88174057\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.85163879\n",
      "################################  55  ################################\n",
      "Training Loss:  37.85163498\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.85163498\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.85163498\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.85163498\n",
      "################################  75  ################################\n",
      "Training Loss:  37.85163498\n",
      "################################  80  ################################\n",
      "Training Loss:  37.85163498\n",
      "################################  85  ################################\n",
      "Training Loss:  37.85163498\n",
      "################################  90  ################################\n",
      "Training Loss:  37.85163498\n",
      "################################  95  ################################\n",
      "Training Loss:  37.85163498\n",
      "Final training Loss:  37.85163498\n",
      "\n",
      "Running model (trial=4, mod=118, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.02174759\n",
      "################################  5  ################################\n",
      "Training Loss:  47.57263947\n",
      "################################  10  ################################\n",
      "Training Loss:  37.35188675\n",
      "################################  15  ################################\n",
      "Training Loss:  37.37009048\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  37.48608398\n",
      "################################  25  ################################\n",
      "Training Loss:  36.9029007\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.74705124\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  36.95935822\n",
      "################################  40  ################################\n",
      "Training Loss:  36.75128174\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.72942352\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.72990799\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.72834778\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.72834778\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  36.72834778\n",
      "################################  70  ################################\n",
      "Training Loss:  36.72834778\n",
      "################################  75  ################################\n",
      "Training Loss:  36.72834778\n",
      "################################  80  ################################\n",
      "Training Loss:  36.72834778\n",
      "################################  85  ################################\n",
      "Training Loss:  36.72834778\n",
      "################################  90  ################################\n",
      "Training Loss:  36.72834778\n",
      "################################  95  ################################\n",
      "Training Loss:  36.72834778\n",
      "Final training Loss:  36.72834778\n",
      "\n",
      "Running model (trial=4, mod=119, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  122.12944031\n",
      "################################  5  ################################\n",
      "Training Loss:  51.72681808\n",
      "################################  10  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=120, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.10030365\n",
      "################################  5  ################################\n",
      "Training Loss:  49.9438591\n",
      "################################  10  ################################\n",
      "Training Loss:  43.46389008\n",
      "################################  15  ################################\n",
      "Training Loss:  39.23476791\n",
      "################################  20  ################################\n",
      "Training Loss:  37.25379944\n",
      "################################  25  ################################\n",
      "Training Loss:  36.00132751\n",
      "################################  30  ################################\n",
      "Training Loss:  36.01995087\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  35.85844803\n",
      "################################  40  ################################\n",
      "Training Loss:  35.28910065\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.70715332\n",
      "################################  50  ################################\n",
      "Training Loss:  nan\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=121, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.83755493\n",
      "################################  5  ################################\n",
      "Training Loss:  233.85092163\n",
      "################################  10  ################################\n",
      "Training Loss:  40.15740967\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  40.32987213\n",
      "################################  20  ################################\n",
      "Training Loss:  38.33486938\n",
      "################################  25  ################################\n",
      "Training Loss:  98.62410736\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  90.07282257\n",
      "################################  35  ################################\n",
      "Training Loss:  89.97309113\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  89.41674805\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  89.3705368\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  89.37051392\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  89.37052155\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  89.37052155\n",
      "################################  65  ################################\n",
      "Training Loss:  89.37052155\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  89.37052155\n",
      "################################  75  ################################\n",
      "Training Loss:  89.37052155\n",
      "################################  80  ################################\n",
      "Training Loss:  89.37052155\n",
      "################################  85  ################################\n",
      "Training Loss:  89.37052155\n",
      "################################  90  ################################\n",
      "Training Loss:  89.37052155\n",
      "################################  95  ################################\n",
      "Training Loss:  89.37052155\n",
      "Final training Loss:  89.37052155\n",
      "\n",
      "Running model (trial=4, mod=122, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.79714966\n",
      "################################  5  ################################\n",
      "Training Loss:  40.78276443\n",
      "################################  10  ################################\n",
      "Training Loss:  489.12506104\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  36.62641144\n",
      "################################  20  ################################\n",
      "Training Loss:  38.07111359\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  37.25218582\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.02056122\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.99971771\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.99951172\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  36.99951172\n",
      "################################  50  ################################\n",
      "Training Loss:  36.99951172\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.99951172\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.99951172\n",
      "################################  65  ################################\n",
      "Training Loss:  36.99951172\n",
      "################################  70  ################################\n",
      "Training Loss:  36.99951172\n",
      "################################  75  ################################\n",
      "Training Loss:  36.99951172\n",
      "################################  80  ################################\n",
      "Training Loss:  36.99951172\n",
      "################################  85  ################################\n",
      "Training Loss:  36.99951172\n",
      "################################  90  ################################\n",
      "Training Loss:  36.99951172\n",
      "################################  95  ################################\n",
      "Training Loss:  36.99951172\n",
      "Final training Loss:  36.99951172\n",
      "\n",
      "Running model (trial=4, mod=123, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.78819275\n",
      "################################  5  ################################\n",
      "Training Loss:  45.00968552\n",
      "################################  10  ################################\n",
      "Training Loss:  130.05575562\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  86.48937988\n",
      "################################  20  ################################\n",
      "Training Loss:  40.86269379\n",
      "################################  25  ################################\n",
      "Training Loss:  848.70825195\n",
      "################################  30  ################################\n",
      "Training Loss:  39.98505402\n",
      "################################  35  ################################\n",
      "Training Loss:  671.86657715\n",
      "################################  40  ################################\n",
      "Training Loss:  39.28914261\n",
      "################################  45  ################################\n",
      "Training Loss:  42.65018463\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  81.1133194\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.91182709\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.41333389\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  39.41332626\n",
      "################################  70  ################################\n",
      "Training Loss:  39.41339493\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.41339874\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  39.41339874\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  39.41339874\n",
      "################################  90  ################################\n",
      "Training Loss:  39.41339874\n",
      "################################  95  ################################\n",
      "Training Loss:  39.41339874\n",
      "Final training Loss:  39.41339874\n",
      "\n",
      "Running model (trial=4, mod=124, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.91597366\n",
      "################################  5  ################################\n",
      "Training Loss:  51.68995285\n",
      "################################  10  ################################\n",
      "Training Loss:  49.72541046\n",
      "################################  15  ################################\n",
      "Training Loss:  39.14416122\n",
      "################################  20  ################################\n",
      "Training Loss:  40.6836319\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.79764938\n",
      "################################  30  ################################\n",
      "Training Loss:  38.41833878\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.40275955\n",
      "################################  40  ################################\n",
      "Training Loss:  37.91734314\n",
      "################################  45  ################################\n",
      "Training Loss:  37.99414825\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.11725616\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.09746552\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.09746552\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.09771729\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  38.09771729\n",
      "################################  75  ################################\n",
      "Training Loss:  38.09771729\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  38.09771729\n",
      "################################  85  ################################\n",
      "Training Loss:  38.09771729\n",
      "################################  90  ################################\n",
      "Training Loss:  38.09771729\n",
      "################################  95  ################################\n",
      "Training Loss:  38.09771729\n",
      "Final training Loss:  38.09771729\n",
      "\n",
      "Running model (trial=4, mod=125, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.81908417\n",
      "################################  5  ################################\n",
      "Training Loss:  51.21815109\n",
      "################################  10  ################################\n",
      "Training Loss:  39.7640152\n",
      "################################  15  ################################\n",
      "Training Loss:  38.15655136\n",
      "################################  20  ################################\n",
      "Training Loss:  217.20149231\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  259.38528442\n",
      "################################  30  ################################\n",
      "Training Loss:  37.84191895\n",
      "################################  35  ################################\n",
      "Training Loss:  202.13044739\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.8277626\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.40524673\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.40371323\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  38.40370941\n",
      "################################  60  ################################\n",
      "Training Loss:  38.4037323\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.4037323\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.4037323\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  38.4037323\n",
      "################################  80  ################################\n",
      "Training Loss:  38.4037323\n",
      "################################  85  ################################\n",
      "Training Loss:  38.4037323\n",
      "################################  90  ################################\n",
      "Training Loss:  38.4037323\n",
      "################################  95  ################################\n",
      "Training Loss:  38.4037323\n",
      "Final training Loss:  38.4037323\n",
      "\n",
      "Running model (trial=4, mod=126, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.18806839\n",
      "################################  5  ################################\n",
      "Training Loss:  43.66039658\n",
      "################################  10  ################################\n",
      "Training Loss:  40.74537277\n",
      "################################  15  ################################\n",
      "Training Loss:  41.75376129\n",
      "################################  20  ################################\n",
      "Training Loss:  253.65652466\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  37.74433517\n",
      "################################  30  ################################\n",
      "Training Loss:  36.62878036\n",
      "################################  35  ################################\n",
      "Training Loss:  36.3548584\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.45680237\n",
      "################################  45  ################################\n",
      "Training Loss:  36.4622345\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.18902206\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.18087387\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.18065262\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.18105698\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.18105698\n",
      "################################  75  ################################\n",
      "Training Loss:  36.18105698\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.18105698\n",
      "################################  85  ################################\n",
      "Training Loss:  36.18105698\n",
      "################################  90  ################################\n",
      "Training Loss:  36.18105698\n",
      "################################  95  ################################\n",
      "Training Loss:  36.18105698\n",
      "Final training Loss:  36.18105698\n",
      "\n",
      "Running model (trial=4, mod=127, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.01898575\n",
      "################################  5  ################################\n",
      "Training Loss:  39.46800613\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  39.43366623\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  39.35005569\n",
      "################################  20  ################################\n",
      "Training Loss:  38.97381592\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  39.02806854\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  39.00553131\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  39.00553131\n",
      "################################  40  ################################\n",
      "Training Loss:  39.00557709\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  39.00552368\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  39.00552368\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.00552368\n",
      "################################  60  ################################\n",
      "Training Loss:  39.00552368\n",
      "################################  65  ################################\n",
      "Training Loss:  39.00552368\n",
      "################################  70  ################################\n",
      "Training Loss:  39.00552368\n",
      "################################  75  ################################\n",
      "Training Loss:  39.00552368\n",
      "################################  80  ################################\n",
      "Training Loss:  39.00552368\n",
      "################################  85  ################################\n",
      "Training Loss:  39.00552368\n",
      "################################  90  ################################\n",
      "Training Loss:  39.00552368\n",
      "################################  95  ################################\n",
      "Training Loss:  39.00552368\n",
      "Final training Loss:  39.00552368\n",
      "\n",
      "Running model (trial=4, mod=128, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.39315796\n",
      "################################  5  ################################\n",
      "Training Loss:  230.26055908\n",
      "################################  10  ################################\n",
      "Training Loss:  38.99505615\n",
      "################################  15  ################################\n",
      "Training Loss:  39.674263\n",
      "################################  20  ################################\n",
      "Training Loss:  37.97613907\n",
      "################################  25  ################################\n",
      "Training Loss:  41.01910782\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.96582794\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=129, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  39.06718826\n",
      "################################  5  ################################\n",
      "Training Loss:  556.13427734\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  36.9002533\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=130, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  101.77400208\n",
      "################################  5  ################################\n",
      "Training Loss:  101.35419464\n",
      "################################  10  ################################\n",
      "Training Loss:  101.30493927\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  101.35425568\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  101.37243652\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  101.33818817\n",
      "################################  35  ################################\n",
      "Training Loss:  101.33818817\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  101.33818817\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  101.33818817\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  101.33818817\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  60  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  65  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  70  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  75  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  80  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  85  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  90  ################################\n",
      "Training Loss:  101.33818817\n",
      "################################  95  ################################\n",
      "Training Loss:  101.33818817\n",
      "Final training Loss:  101.33818817\n",
      "\n",
      "Running model (trial=5, mod=131, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  101.31904602\n",
      "################################  5  ################################\n",
      "Training Loss:  101.3345108\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  101.32964325\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  101.34046936\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  101.34257507\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  101.34253693\n",
      "################################  35  ################################\n",
      "Training Loss:  101.34253693\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  101.34253693\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  101.34253693\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  55  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  60  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  65  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  70  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  75  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  80  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  85  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  90  ################################\n",
      "Training Loss:  101.34253693\n",
      "################################  95  ################################\n",
      "Training Loss:  101.34253693\n",
      "Final training Loss:  101.34253693\n",
      "\n",
      "Running model (trial=5, mod=132, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  47.00217056\n",
      "################################  5  ################################\n",
      "Training Loss:  42.06978989\n",
      "################################  10  ################################\n",
      "Training Loss:  37.03821182\n",
      "################################  15  ################################\n",
      "Training Loss:  36.8979187\n",
      "################################  20  ################################\n",
      "Training Loss:  36.25987244\n",
      "################################  25  ################################\n",
      "Training Loss:  36.5052948\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  36.19907761\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.65309143\n",
      "################################  40  ################################\n",
      "Training Loss:  36.6437912\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.25928879\n",
      "################################  50  ################################\n",
      "Training Loss:  36.27105331\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.14942551\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.14269638\n",
      "################################  65  ################################\n",
      "Training Loss:  36.14271927\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.14270401\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.14268494\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.14269257\n",
      "################################  85  ################################\n",
      "Training Loss:  36.14269257\n",
      "################################  90  ################################\n",
      "Training Loss:  36.14269257\n",
      "################################  95  ################################\n",
      "Training Loss:  36.14269257\n",
      "Final training Loss:  36.14269257\n",
      "\n",
      "Running model (trial=5, mod=133, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.41137695\n",
      "################################  5  ################################\n",
      "Training Loss:  40.23040771\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=134, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.89577484\n",
      "################################  5  ################################\n",
      "Training Loss:  41.0595932\n",
      "################################  10  ################################\n",
      "Training Loss:  39.57975388\n",
      "################################  15  ################################\n",
      "Training Loss:  37.62265778\n",
      "################################  20  ################################\n",
      "Training Loss:  37.94532394\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  37.70274734\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=135, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  40.66801834\n",
      "################################  5  ################################\n",
      "Training Loss:  36.73280716\n",
      "################################  10  ################################\n",
      "Training Loss:  36.50603867\n",
      "################################  15  ################################\n",
      "Training Loss:  36.67768097\n",
      "################################  20  ################################\n",
      "Training Loss:  36.52593613\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  36.48270798\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  36.69229507\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  36.30942154\n",
      "################################  40  ################################\n",
      "Training Loss:  36.42856216\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.40502167\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.40501022\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.40504837\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.4050293\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  36.4050293\n",
      "################################  70  ################################\n",
      "Training Loss:  36.4050293\n",
      "################################  75  ################################\n",
      "Training Loss:  36.4050293\n",
      "################################  80  ################################\n",
      "Training Loss:  36.4050293\n",
      "################################  85  ################################\n",
      "Training Loss:  36.4050293\n",
      "################################  90  ################################\n",
      "Training Loss:  36.4050293\n",
      "################################  95  ################################\n",
      "Training Loss:  36.4050293\n",
      "Final training Loss:  36.4050293\n",
      "\n",
      "Running model (trial=5, mod=136, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.14812851\n",
      "################################  5  ################################\n",
      "Training Loss:  40.91227722\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  41.58997345\n",
      "################################  15  ################################\n",
      "Training Loss:  40.50751495\n",
      "################################  20  ################################\n",
      "Training Loss:  39.69028854\n",
      "################################  25  ################################\n",
      "Training Loss:  39.05743408\n",
      "################################  30  ################################\n",
      "Training Loss:  42.61079788\n",
      "################################  35  ################################\n",
      "Training Loss:  38.14947128\n",
      "################################  40  ################################\n",
      "Training Loss:  39.04119492\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.86655807\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  2540.94873047\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  4852.62158203\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  54.81243134\n",
      "################################  65  ################################\n",
      "Training Loss:  54.81244659\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  54.71567154\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  54.71567154\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  54.71567154\n",
      "################################  85  ################################\n",
      "Training Loss:  54.71567154\n",
      "################################  90  ################################\n",
      "Training Loss:  54.71567154\n",
      "################################  95  ################################\n",
      "Training Loss:  54.71567154\n",
      "Final training Loss:  54.71567154\n",
      "\n",
      "Running model (trial=5, mod=137, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=138, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.64925766\n",
      "################################  5  ################################\n",
      "Training Loss:  51.64410782\n",
      "################################  10  ################################\n",
      "Training Loss:  50.34722519\n",
      "################################  15  ################################\n",
      "Training Loss:  50.36822891\n",
      "################################  20  ################################\n",
      "Training Loss:  50.89603806\n",
      "################################  25  ################################\n",
      "Training Loss:  49.81689835\n",
      "################################  30  ################################\n",
      "Training Loss:  38.80963135\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.90692139\n",
      "################################  40  ################################\n",
      "Training Loss:  38.08587646\n",
      "################################  45  ################################\n",
      "Training Loss:  38.00898361\n",
      "################################  50  ################################\n",
      "Training Loss:  37.91166306\n",
      "################################  55  ################################\n",
      "Training Loss:  37.73207855\n",
      "################################  60  ################################\n",
      "Training Loss:  37.48962402\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.84674454\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.6440239\n",
      "################################  75  ################################\n",
      "Training Loss:  37.2907486\n",
      "################################  80  ################################\n",
      "Training Loss:  36.61263657\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  36.64230347\n",
      "################################  90  ################################\n",
      "Training Loss:  36.85447693\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  95  ################################\n",
      "Training Loss:  36.85447693\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Final training Loss:  36.85447693\n",
      "\n",
      "Running model (trial=5, mod=139, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=140, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  58.74079132\n",
      "################################  5  ################################\n",
      "Training Loss:  44.69817734\n",
      "################################  10  ################################\n",
      "Training Loss:  43.79594421\n",
      "################################  15  ################################\n",
      "Training Loss:  39.57278061\n",
      "################################  20  ################################\n",
      "Training Loss:  37.90768433\n",
      "################################  25  ################################\n",
      "Training Loss:  37.38980103\n",
      "################################  30  ################################\n",
      "Training Loss:  38.69065475\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.89017105\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.82061005\n",
      "################################  45  ################################\n",
      "Training Loss:  203.35929871\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.55846405\n",
      "################################  55  ################################\n",
      "Training Loss:  36.49516678\n",
      "################################  60  ################################\n",
      "Training Loss:  36.52526474\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.51985931\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.51984024\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.51984787\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.51984787\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  36.51984787\n",
      "################################  90  ################################\n",
      "Training Loss:  36.51984787\n",
      "################################  95  ################################\n",
      "Training Loss:  36.51984787\n",
      "Final training Loss:  36.51984787\n",
      "\n",
      "Running model (trial=5, mod=141, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  49.3899765\n",
      "################################  5  ################################\n",
      "Training Loss:  41.7395401\n",
      "################################  10  ################################\n",
      "Training Loss:  38.45910263\n",
      "################################  15  ################################\n",
      "Training Loss:  37.32995224\n",
      "################################  20  ################################\n",
      "Training Loss:  37.21203613\n",
      "################################  25  ################################\n",
      "Training Loss:  37.20643997\n",
      "################################  30  ################################\n",
      "Training Loss:  36.94224548\n",
      "################################  35  ################################\n",
      "Training Loss:  36.95829391\n",
      "################################  40  ################################\n",
      "Training Loss:  36.5107193\n",
      "################################  45  ################################\n",
      "Training Loss:  36.38419342\n",
      "################################  50  ################################\n",
      "Training Loss:  36.49702835\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.50761795\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.57366562\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.51383591\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.51300049\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.51325607\n",
      "################################  80  ################################\n",
      "Training Loss:  36.51282883\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  36.51282883\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  36.51282883\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  36.51282883\n",
      "Final training Loss:  36.51282883\n",
      "\n",
      "Running model (trial=5, mod=142, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  70.61824036\n",
      "################################  5  ################################\n",
      "Training Loss:  66.96183014\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  66.96204376\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  66.95631409\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  66.956604\n",
      "################################  25  ################################\n",
      "Training Loss:  66.9541626\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  66.9541626\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  66.9541626\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  66.9541626\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  66.9541626\n",
      "################################  55  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  60  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  65  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  70  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  75  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  80  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  85  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  90  ################################\n",
      "Training Loss:  66.9541626\n",
      "################################  95  ################################\n",
      "Training Loss:  66.9541626\n",
      "Final training Loss:  66.9541626\n",
      "\n",
      "Running model (trial=5, mod=143, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.41453552\n",
      "################################  5  ################################\n",
      "Training Loss:  51.72001266\n",
      "################################  10  ################################\n",
      "Training Loss:  227.73928833\n",
      "################################  15  ################################\n",
      "Training Loss:  37.91400528\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  37.49399567\n",
      "################################  25  ################################\n",
      "Training Loss:  38.01204681\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.93875122\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.74671936\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.63656998\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.6365242\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  37.63652802\n",
      "################################  55  ################################\n",
      "Training Loss:  37.63652802\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.63652802\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.63652802\n",
      "################################  70  ################################\n",
      "Training Loss:  37.63652802\n",
      "################################  75  ################################\n",
      "Training Loss:  37.63652802\n",
      "################################  80  ################################\n",
      "Training Loss:  37.63652802\n",
      "################################  85  ################################\n",
      "Training Loss:  37.63652802\n",
      "################################  90  ################################\n",
      "Training Loss:  37.63652802\n",
      "################################  95  ################################\n",
      "Training Loss:  37.63652802\n",
      "Final training Loss:  37.63652802\n",
      "\n",
      "Running model (trial=5, mod=144, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.49328232\n",
      "################################  5  ################################\n",
      "Training Loss:  48.27757645\n",
      "################################  10  ################################\n",
      "Training Loss:  37.87468338\n",
      "################################  15  ################################\n",
      "Training Loss:  47.00499344\n",
      "################################  20  ################################\n",
      "Training Loss:  35.2302475\n",
      "################################  25  ################################\n",
      "Training Loss:  43.79268646\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  83.60385895\n",
      "################################  35  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=145, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.04112625\n",
      "################################  5  ################################\n",
      "Training Loss:  50.49343872\n",
      "################################  10  ################################\n",
      "Training Loss:  49.09542084\n",
      "################################  15  ################################\n",
      "Training Loss:  49.26728821\n",
      "################################  20  ################################\n",
      "Training Loss:  45.7355957\n",
      "################################  25  ################################\n",
      "Training Loss:  45.14711761\n",
      "################################  30  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=146, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.9899826\n",
      "################################  5  ################################\n",
      "Training Loss:  49.16276169\n",
      "################################  10  ################################\n",
      "Training Loss:  44.97890091\n",
      "################################  15  ################################\n",
      "Training Loss:  39.02577591\n",
      "################################  20  ################################\n",
      "Training Loss:  210.20704651\n",
      "################################  25  ################################\n",
      "Training Loss:  216.25447083\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.40513229\n",
      "################################  35  ################################\n",
      "Training Loss:  37.3393364\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  37.54784775\n",
      "################################  45  ################################\n",
      "Training Loss:  38.55135345\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.95915985\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.95909882\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.95909882\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.9590683\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  37.9590683\n",
      "################################  75  ################################\n",
      "Training Loss:  37.9590683\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.9590683\n",
      "################################  85  ################################\n",
      "Training Loss:  37.9590683\n",
      "################################  90  ################################\n",
      "Training Loss:  37.9590683\n",
      "################################  95  ################################\n",
      "Training Loss:  37.9590683\n",
      "Final training Loss:  37.9590683\n",
      "\n",
      "Running model (trial=5, mod=147, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.96392822\n",
      "################################  5  ################################\n",
      "Training Loss:  50.34315872\n",
      "################################  10  ################################\n",
      "Training Loss:  49.63528824\n",
      "################################  15  ################################\n",
      "Training Loss:  49.66371918\n",
      "################################  20  ################################\n",
      "Training Loss:  47.17922211\n",
      "################################  25  ################################\n",
      "Training Loss:  309.318573\n",
      "################################  30  ################################\n",
      "Training Loss:  45.24980164\n",
      "################################  35  ################################\n",
      "Training Loss:  347.74816895\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  46.71563339\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  47.35995102\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  46.53376389\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  46.53248596\n",
      "################################  60  ################################\n",
      "Training Loss:  46.53274918\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  46.53287888\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  46.53287888\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  46.53287888\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  46.53287888\n",
      "################################  85  ################################\n",
      "Training Loss:  46.53287888\n",
      "################################  90  ################################\n",
      "Training Loss:  46.53287888\n",
      "################################  95  ################################\n",
      "Training Loss:  46.53287888\n",
      "Final training Loss:  46.53287888\n",
      "\n",
      "Running model (trial=5, mod=148, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.1230278\n",
      "################################  5  ################################\n",
      "Training Loss:  50.6690712\n",
      "################################  10  ################################\n",
      "Training Loss:  45.48515701\n",
      "################################  15  ################################\n",
      "Training Loss:  44.32731247\n",
      "################################  20  ################################\n",
      "Training Loss:  39.4417572\n",
      "################################  25  ################################\n",
      "Training Loss:  99.18115234\n",
      "################################  30  ################################\n",
      "Training Loss:  36.44750977\n",
      "################################  35  ################################\n",
      "Training Loss:  35.76564026\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=149, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.08427429\n",
      "################################  5  ################################\n",
      "Training Loss:  50.87082291\n",
      "################################  10  ################################\n",
      "Training Loss:  44.85669327\n",
      "################################  15  ################################\n",
      "Training Loss:  153.25817871\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  200.79255676\n",
      "################################  25  ################################\n",
      "Training Loss:  49.07388687\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.74642563\n",
      "################################  35  ################################\n",
      "Training Loss:  37.9225235\n",
      "################################  40  ################################\n",
      "Training Loss:  38.04767609\n",
      "################################  45  ################################\n",
      "Training Loss:  107.00997162\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.92867279\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.00455093\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.00404739\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.00403214\n",
      "################################  70  ################################\n",
      "Training Loss:  38.00403214\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  38.00403214\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  38.00403214\n",
      "################################  85  ################################\n",
      "Training Loss:  38.00403214\n",
      "################################  90  ################################\n",
      "Training Loss:  38.00403214\n",
      "################################  95  ################################\n",
      "Training Loss:  38.00403214\n",
      "Final training Loss:  38.00403214\n",
      "\n",
      "Running model (trial=5, mod=150, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.6448288\n",
      "################################  5  ################################\n",
      "Training Loss:  207.79705811\n",
      "################################  10  ################################\n",
      "Training Loss:  828.72924805\n",
      "################################  15  ################################\n",
      "Training Loss:  35.37516785\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  35.68411255\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  159.8616333\n",
      "################################  30  ################################\n",
      "Training Loss:  35.29850388\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  35.24415207\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  35.24359131\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  35.24298096\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  35.24298096\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  35.24298096\n",
      "################################  60  ################################\n",
      "Training Loss:  35.24298096\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  35.24298096\n",
      "################################  70  ################################\n",
      "Training Loss:  35.24298096\n",
      "################################  75  ################################\n",
      "Training Loss:  35.24298096\n",
      "################################  80  ################################\n",
      "Training Loss:  35.24298096\n",
      "################################  85  ################################\n",
      "Training Loss:  35.24298096\n",
      "################################  90  ################################\n",
      "Training Loss:  35.24298096\n",
      "################################  95  ################################\n",
      "Training Loss:  35.24298096\n",
      "Final training Loss:  35.24298096\n",
      "\n",
      "Running model (trial=5, mod=151, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.06951523\n",
      "################################  5  ################################\n",
      "Training Loss:  49.12991333\n",
      "################################  10  ################################\n",
      "Training Loss:  44.41652298\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  43.24227142\n",
      "################################  20  ################################\n",
      "Training Loss:  96.97660065\n",
      "################################  25  ################################\n",
      "Training Loss:  889.02423096\n",
      "################################  30  ################################\n",
      "Training Loss:  92.51602173\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=152, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.83403397\n",
      "################################  5  ################################\n",
      "Training Loss:  49.75603104\n",
      "################################  10  ################################\n",
      "Training Loss:  41.34331512\n",
      "################################  15  ################################\n",
      "Training Loss:  116.47138977\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  113.62451172\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  113.8211441\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  113.47886658\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  113.46891785\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  113.46879578\n",
      "################################  45  ################################\n",
      "Training Loss:  113.46880341\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  113.46880341\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  113.46880341\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  113.46880341\n",
      "################################  65  ################################\n",
      "Training Loss:  113.46880341\n",
      "################################  70  ################################\n",
      "Training Loss:  113.46880341\n",
      "################################  75  ################################\n",
      "Training Loss:  113.46880341\n",
      "################################  80  ################################\n",
      "Training Loss:  113.46880341\n",
      "################################  85  ################################\n",
      "Training Loss:  113.46880341\n",
      "################################  90  ################################\n",
      "Training Loss:  113.46880341\n",
      "################################  95  ################################\n",
      "Training Loss:  113.46880341\n",
      "Final training Loss:  113.46880341\n",
      "\n",
      "Running model (trial=5, mod=153, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.8368721\n",
      "################################  5  ################################\n",
      "Training Loss:  50.14567566\n",
      "################################  10  ################################\n",
      "Training Loss:  47.91558838\n",
      "################################  15  ################################\n",
      "Training Loss:  38.00032806\n",
      "################################  20  ################################\n",
      "Training Loss:  64.17269897\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=5, mod=154, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.32162857\n",
      "################################  5  ################################\n",
      "Training Loss:  40.87399292\n",
      "################################  10  ################################\n",
      "Training Loss:  36.46061325\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  37.16185379\n",
      "################################  20  ################################\n",
      "Training Loss:  404.16519165\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  454.42202759\n",
      "################################  30  ################################\n",
      "Training Loss:  154.63192749\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.50442505\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.41087723\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.4108963\n",
      "################################  50  ################################\n",
      "Training Loss:  36.41090775\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.41090775\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.41090775\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.41090775\n",
      "################################  70  ################################\n",
      "Training Loss:  36.41090775\n",
      "################################  75  ################################\n",
      "Training Loss:  36.41090775\n",
      "################################  80  ################################\n",
      "Training Loss:  36.41090775\n",
      "################################  85  ################################\n",
      "Training Loss:  36.41090775\n",
      "################################  90  ################################\n",
      "Training Loss:  36.41090775\n",
      "################################  95  ################################\n",
      "Training Loss:  36.41090775\n",
      "Final training Loss:  36.41090775\n",
      "\n",
      "Running model (trial=5, mod=155, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1463.0065918\n",
      "################################  5  ################################\n",
      "Training Loss:  38.80614853\n",
      "################################  10  ################################\n",
      "Training Loss:  36.77105331\n",
      "################################  15  ################################\n",
      "Training Loss:  91.67007446\n",
      "################################  20  ################################\n",
      "Training Loss:  35.55290985\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  35.50201797\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  35.8086319\n",
      "################################  35  ################################\n",
      "Training Loss:  38.35535812\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  35.66553116\n",
      "################################  45  ################################\n",
      "Training Loss:  35.39337158\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  35.39342117\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  35.39348602\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  35.39367676\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  35.39367676\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  35.39367676\n",
      "################################  75  ################################\n",
      "Training Loss:  35.39367676\n",
      "################################  80  ################################\n",
      "Training Loss:  35.39367676\n",
      "################################  85  ################################\n",
      "Training Loss:  35.39367676\n",
      "################################  90  ################################\n",
      "Training Loss:  35.39367676\n",
      "################################  95  ################################\n",
      "Training Loss:  35.39367676\n",
      "Final training Loss:  35.39367676\n",
      "\n",
      "Running model (trial=6, mod=156, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  102.25527191\n",
      "################################  5  ################################\n",
      "Training Loss:  101.67890167\n",
      "################################  10  ################################\n",
      "Training Loss:  101.64365387\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  101.68260956\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  101.68402863\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  101.68306732\n",
      "################################  30  ################################\n",
      "Training Loss:  101.68306732\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  101.68306732\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  101.68306732\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  101.68306732\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  101.68306732\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  101.68306732\n",
      "################################  60  ################################\n",
      "Training Loss:  101.68306732\n",
      "################################  65  ################################\n",
      "Training Loss:  101.68306732\n",
      "################################  70  ################################\n",
      "Training Loss:  101.68306732\n",
      "################################  75  ################################\n",
      "Training Loss:  101.68306732\n",
      "################################  80  ################################\n",
      "Training Loss:  101.68306732\n",
      "################################  85  ################################\n",
      "Training Loss:  101.68306732\n",
      "################################  90  ################################\n",
      "Training Loss:  101.68306732\n",
      "################################  95  ################################\n",
      "Training Loss:  101.68306732\n",
      "Final training Loss:  101.68306732\n",
      "\n",
      "Running model (trial=6, mod=157, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.6055336\n",
      "################################  5  ################################\n",
      "Training Loss:  39.58057404\n",
      "################################  10  ################################\n",
      "Training Loss:  37.50751495\n",
      "################################  15  ################################\n",
      "Training Loss:  37.62496185\n",
      "################################  20  ################################\n",
      "Training Loss:  37.52487946\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  36.97910309\n",
      "################################  30  ################################\n",
      "Training Loss:  37.10511398\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.78791809\n",
      "################################  40  ################################\n",
      "Training Loss:  36.98694229\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.76383972\n",
      "################################  50  ################################\n",
      "Training Loss:  36.90970993\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.65010071\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.65007782\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.65009689\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.65009689\n",
      "################################  75  ################################\n",
      "Training Loss:  36.65009689\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.65009689\n",
      "################################  85  ################################\n",
      "Training Loss:  36.65009689\n",
      "################################  90  ################################\n",
      "Training Loss:  36.65009689\n",
      "################################  95  ################################\n",
      "Training Loss:  36.65009689\n",
      "Final training Loss:  36.65009689\n",
      "\n",
      "Running model (trial=6, mod=158, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  120.72200775\n",
      "################################  5  ################################\n",
      "Training Loss:  119.45165253\n",
      "################################  10  ################################\n",
      "Training Loss:  116.43250275\n",
      "################################  15  ################################\n",
      "Training Loss:  115.60949707\n",
      "################################  20  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=159, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  86.35939789\n",
      "################################  5  ################################\n",
      "Training Loss:  37.30241776\n",
      "################################  10  ################################\n",
      "Training Loss:  37.25482941\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  37.04949188\n",
      "################################  20  ################################\n",
      "Training Loss:  36.74624252\n",
      "################################  25  ################################\n",
      "Training Loss:  36.50989914\n",
      "################################  30  ################################\n",
      "Training Loss:  37.09607697\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.56734848\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.88459015\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.68840027\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.68837357\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  36.68837357\n",
      "################################  60  ################################\n",
      "Training Loss:  36.68837357\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.68837357\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.68837357\n",
      "################################  75  ################################\n",
      "Training Loss:  36.68837357\n",
      "################################  80  ################################\n",
      "Training Loss:  36.68837357\n",
      "################################  85  ################################\n",
      "Training Loss:  36.68837357\n",
      "################################  90  ################################\n",
      "Training Loss:  36.68837357\n",
      "################################  95  ################################\n",
      "Training Loss:  36.68837357\n",
      "Final training Loss:  36.68837357\n",
      "\n",
      "Running model (trial=6, mod=160, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.46066666\n",
      "################################  5  ################################\n",
      "Training Loss:  47.59989929\n",
      "################################  10  ################################\n",
      "Training Loss:  45.1272316\n",
      "################################  15  ################################\n",
      "Training Loss:  43.55715942\n",
      "################################  20  ################################\n",
      "Training Loss:  41.02866745\n",
      "################################  25  ################################\n",
      "Training Loss:  39.31552124\n",
      "################################  30  ################################\n",
      "Training Loss:  37.14305115\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.79090118\n",
      "################################  40  ################################\n",
      "Training Loss:  36.84194183\n",
      "################################  45  ################################\n",
      "Training Loss:  36.89095688\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.93501663\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.26798248\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.21873093\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.18022156\n",
      "################################  70  ################################\n",
      "Training Loss:  37.18017197\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.18017197\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.18017197\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  37.18017197\n",
      "################################  90  ################################\n",
      "Training Loss:  37.18017197\n",
      "################################  95  ################################\n",
      "Training Loss:  37.18017197\n",
      "Final training Loss:  37.18017197\n",
      "\n",
      "Running model (trial=6, mod=161, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  42.1960144\n",
      "################################  5  ################################\n",
      "Training Loss:  37.52465439\n",
      "################################  10  ################################\n",
      "Training Loss:  36.31999969\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=162, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  47.35829163\n",
      "################################  5  ################################\n",
      "Training Loss:  39.26622391\n",
      "################################  10  ################################\n",
      "Training Loss:  39.37856293\n",
      "################################  15  ################################\n",
      "Training Loss:  39.14994431\n",
      "################################  20  ################################\n",
      "Training Loss:  38.90363693\n",
      "################################  25  ################################\n",
      "Training Loss:  38.46992111\n",
      "################################  30  ################################\n",
      "Training Loss:  38.28979874\n",
      "################################  35  ################################\n",
      "Training Loss:  37.92952347\n",
      "################################  40  ################################\n",
      "Training Loss:  36.83633423\n",
      "################################  45  ################################\n",
      "Training Loss:  36.7522316\n",
      "################################  50  ################################\n",
      "Training Loss:  37.64531708\n",
      "################################  55  ################################\n",
      "Training Loss:  36.31167603\n",
      "################################  60  ################################\n",
      "Training Loss:  35.95763779\n",
      "################################  65  ################################\n",
      "Training Loss:  39.93408585\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  70  ################################\n",
      "Training Loss:  145.15029907\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  75  ################################\n",
      "Training Loss:  47.82074356\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  45.35019302\n",
      "################################  85  ################################\n",
      "Training Loss:  45.52342987\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  90  ################################\n",
      "Training Loss:  45.46516418\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  95  ################################\n",
      "Training Loss:  45.46516418\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Final training Loss:  45.46516418\n",
      "\n",
      "Running model (trial=6, mod=163, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  119.11201477\n",
      "################################  5  ################################\n",
      "Training Loss:  118.71038818\n",
      "################################  10  ################################\n",
      "Training Loss:  118.98875427\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  119.6760025\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  119.38709259\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  118.69458771\n",
      "################################  30  ################################\n",
      "Training Loss:  118.36612701\n",
      "################################  35  ################################\n",
      "Training Loss:  116.61781311\n",
      "################################  40  ################################\n",
      "Training Loss:  116.25479126\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  116.25479889\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  116.25479889\n",
      "################################  55  ################################\n",
      "Training Loss:  116.25479889\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  116.25479889\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  116.25479889\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  116.25479889\n",
      "################################  75  ################################\n",
      "Training Loss:  116.25479889\n",
      "################################  80  ################################\n",
      "Training Loss:  116.25479889\n",
      "################################  85  ################################\n",
      "Training Loss:  116.25479889\n",
      "################################  90  ################################\n",
      "Training Loss:  116.25479889\n",
      "################################  95  ################################\n",
      "Training Loss:  116.25479889\n",
      "Final training Loss:  116.25479889\n",
      "\n",
      "Running model (trial=6, mod=164, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  64.32585144\n",
      "################################  5  ################################\n",
      "Training Loss:  50.47709274\n",
      "################################  10  ################################\n",
      "Training Loss:  37.97487259\n",
      "################################  15  ################################\n",
      "Training Loss:  37.00608444\n",
      "################################  20  ################################\n",
      "Training Loss:  36.86917496\n",
      "################################  25  ################################\n",
      "Training Loss:  37.01999283\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  36.95853424\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.66940308\n",
      "################################  40  ################################\n",
      "Training Loss:  36.91490936\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.70707703\n",
      "################################  50  ################################\n",
      "Training Loss:  37.19925308\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.82293701\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.82279205\n",
      "################################  65  ################################\n",
      "Training Loss:  36.82278824\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.82282257\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.82282257\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.82282257\n",
      "################################  85  ################################\n",
      "Training Loss:  36.82282257\n",
      "################################  90  ################################\n",
      "Training Loss:  36.82282257\n",
      "################################  95  ################################\n",
      "Training Loss:  36.82282257\n",
      "Final training Loss:  36.82282257\n",
      "\n",
      "Running model (trial=6, mod=165, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  63.28096008\n",
      "################################  5  ################################\n",
      "Training Loss:  53.87961578\n",
      "################################  10  ################################\n",
      "Training Loss:  52.37818527\n",
      "################################  15  ################################\n",
      "Training Loss:  38.8756218\n",
      "################################  20  ################################\n",
      "Training Loss:  37.99573517\n",
      "################################  25  ################################\n",
      "Training Loss:  36.01174545\n",
      "################################  30  ################################\n",
      "Training Loss:  35.62538147\n",
      "################################  35  ################################\n",
      "Training Loss:  36.09196091\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  39.57950211\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  39.1131134\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  39.12033844\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.09160233\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  39.07597733\n",
      "################################  65  ################################\n",
      "Training Loss:  39.07594299\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.07594681\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.07594681\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  39.07594681\n",
      "################################  85  ################################\n",
      "Training Loss:  39.07594681\n",
      "################################  90  ################################\n",
      "Training Loss:  39.07594681\n",
      "################################  95  ################################\n",
      "Training Loss:  39.07594681\n",
      "Final training Loss:  39.07594681\n",
      "\n",
      "Running model (trial=6, mod=166, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  61.47950745\n",
      "################################  5  ################################\n",
      "Training Loss:  45.19771957\n",
      "################################  10  ################################\n",
      "Training Loss:  45.04956436\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  43.46827698\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  43.21323395\n",
      "################################  25  ################################\n",
      "Training Loss:  42.67515945\n",
      "################################  30  ################################\n",
      "Training Loss:  42.46504593\n",
      "################################  35  ################################\n",
      "Training Loss:  42.18725204\n",
      "################################  40  ################################\n",
      "Training Loss:  42.23070908\n",
      "################################  45  ################################\n",
      "Training Loss:  42.15048218\n",
      "################################  50  ################################\n",
      "Training Loss:  40.48996353\n",
      "################################  55  ################################\n",
      "Training Loss:  40.48335266\n",
      "################################  60  ################################\n",
      "Training Loss:  40.36649323\n",
      "################################  65  ################################\n",
      "Training Loss:  40.29581451\n",
      "################################  70  ################################\n",
      "Training Loss:  39.77649689\n",
      "################################  75  ################################\n",
      "Training Loss:  39.66580582\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  39.76083374\n",
      "################################  85  ################################\n",
      "Training Loss:  39.28888702\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  90  ################################\n",
      "Training Loss:  39.21128845\n",
      "################################  95  ################################\n",
      "Training Loss:  39.21128845\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Final training Loss:  39.21124268\n",
      "\n",
      "Running model (trial=6, mod=167, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.39348984\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=168, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  119.43399811\n",
      "################################  5  ################################\n",
      "Training Loss:  119.52561951\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  119.76623535\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  119.76623535\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  119.76568604\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  119.76564789\n",
      "################################  35  ################################\n",
      "Training Loss:  119.76564789\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  119.76564789\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  119.76564789\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  55  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  60  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  65  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  70  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  75  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  80  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  85  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  90  ################################\n",
      "Training Loss:  119.76564789\n",
      "################################  95  ################################\n",
      "Training Loss:  119.76564789\n",
      "Final training Loss:  119.76564789\n",
      "\n",
      "Running model (trial=6, mod=169, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.12608719\n",
      "################################  5  ################################\n",
      "Training Loss:  52.11478806\n",
      "################################  10  ################################\n",
      "Training Loss:  51.99352646\n",
      "################################  15  ################################\n",
      "Training Loss:  41.42858124\n",
      "################################  20  ################################\n",
      "Training Loss:  37.6748848\n",
      "################################  25  ################################\n",
      "Training Loss:  36.4180336\n",
      "################################  30  ################################\n",
      "Training Loss:  36.38008499\n",
      "################################  35  ################################\n",
      "Training Loss:  36.62498856\n",
      "################################  40  ################################\n",
      "Training Loss:  34.92565155\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  35.51423645\n",
      "################################  50  ################################\n",
      "Training Loss:  35.55487823\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  34.34035492\n",
      "################################  60  ################################\n",
      "Training Loss:  35.21127701\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  35.21642685\n",
      "################################  70  ################################\n",
      "Training Loss:  34.73828888\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  34.75154495\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  34.75154495\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  34.75149155\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  34.75149155\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  34.75149155\n",
      "Final training Loss:  34.75149155\n",
      "\n",
      "Running model (trial=6, mod=170, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  120.51034546\n",
      "################################  5  ################################\n",
      "Training Loss:  116.33914948\n",
      "################################  10  ################################\n",
      "Training Loss:  113.21430969\n",
      "################################  15  ################################\n",
      "Training Loss:  103.6009903\n",
      "################################  20  ################################\n",
      "Training Loss:  105.47283936\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  222.227005\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  107.28569031\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  105.95594025\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  104.76256561\n",
      "################################  45  ################################\n",
      "Training Loss:  104.74949646\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  104.74950409\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  104.74950409\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  104.74950409\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  104.74950409\n",
      "################################  70  ################################\n",
      "Training Loss:  104.74950409\n",
      "################################  75  ################################\n",
      "Training Loss:  104.74950409\n",
      "################################  80  ################################\n",
      "Training Loss:  104.74950409\n",
      "################################  85  ################################\n",
      "Training Loss:  104.74950409\n",
      "################################  90  ################################\n",
      "Training Loss:  104.74950409\n",
      "################################  95  ################################\n",
      "Training Loss:  104.74950409\n",
      "Final training Loss:  104.74950409\n",
      "\n",
      "Running model (trial=6, mod=171, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.4672966\n",
      "################################  5  ################################\n",
      "Training Loss:  49.63378525\n",
      "################################  10  ################################\n",
      "Training Loss:  270.65948486\n",
      "################################  15  ################################\n",
      "Training Loss:  39.41034317\n",
      "################################  20  ################################\n",
      "Training Loss:  37.87717438\n",
      "################################  25  ################################\n",
      "Training Loss:  37.80329895\n",
      "################################  30  ################################\n",
      "Training Loss:  37.7928772\n",
      "################################  35  ################################\n",
      "Training Loss:  37.97335052\n",
      "################################  40  ################################\n",
      "Training Loss:  37.22026062\n",
      "################################  45  ################################\n",
      "Training Loss:  36.34202576\n",
      "################################  50  ################################\n",
      "Training Loss:  102.46942902\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  55  ################################\n",
      "Training Loss:  35.85158539\n",
      "################################  60  ################################\n",
      "Training Loss:  35.64387131\n",
      "################################  65  ################################\n",
      "Training Loss:  35.28305435\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.89678192\n",
      "################################  75  ################################\n",
      "Training Loss:  35.98899078\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  80  ################################\n",
      "Training Loss:  35.88766098\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  85  ################################\n",
      "Training Loss:  35.47393799\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  90  ################################\n",
      "Training Loss:  35.46380234\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  95  ################################\n",
      "Training Loss:  35.46380234\n",
      "Final training Loss:  35.46380234\n",
      "\n",
      "Running model (trial=6, mod=172, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.86132812\n",
      "################################  5  ################################\n",
      "Training Loss:  49.26924896\n",
      "################################  10  ################################\n",
      "Training Loss:  43.26799011\n",
      "################################  15  ################################\n",
      "Training Loss:  1651.44873047\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  40.99171066\n",
      "################################  25  ################################\n",
      "Training Loss:  40.09099579\n",
      "################################  30  ################################\n",
      "Training Loss:  41.73304367\n",
      "################################  35  ################################\n",
      "Training Loss:  37.51636124\n",
      "################################  40  ################################\n",
      "Training Loss:  37.46669769\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  38.30649185\n",
      "################################  50  ################################\n",
      "Training Loss:  37.80663681\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  47.76508713\n",
      "################################  60  ################################\n",
      "Training Loss:  37.25091553\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.25090027\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.25090027\n",
      "################################  75  ################################\n",
      "Training Loss:  37.25090027\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.25090027\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  37.25090027\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  37.25090027\n",
      "################################  95  ################################\n",
      "Training Loss:  37.25090027\n",
      "Final training Loss:  37.25090027\n",
      "\n",
      "Running model (trial=6, mod=173, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.10177231\n",
      "################################  5  ################################\n",
      "Training Loss:  49.56111908\n",
      "################################  10  ################################\n",
      "Training Loss:  49.58317947\n",
      "################################  15  ################################\n",
      "Training Loss:  194.38076782\n",
      "################################  20  ################################\n",
      "Training Loss:  1413.08886719\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  269.64050293\n",
      "################################  30  ################################\n",
      "Training Loss:  39.40856934\n",
      "################################  35  ################################\n",
      "Training Loss:  38.79235458\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  41.98530197\n",
      "################################  45  ################################\n",
      "Training Loss:  39.89566422\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  39.52272797\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.5363121\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.53663635\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.53659821\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  39.53659821\n",
      "################################  75  ################################\n",
      "Training Loss:  39.53659821\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  39.53659821\n",
      "################################  85  ################################\n",
      "Training Loss:  39.53659821\n",
      "################################  90  ################################\n",
      "Training Loss:  39.53659821\n",
      "################################  95  ################################\n",
      "Training Loss:  39.53659821\n",
      "Final training Loss:  39.53659821\n",
      "\n",
      "Running model (trial=6, mod=174, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.57384872\n",
      "################################  5  ################################\n",
      "Training Loss:  51.13386536\n",
      "################################  10  ################################\n",
      "Training Loss:  44.86836624\n",
      "################################  15  ################################\n",
      "Training Loss:  3010.88378906\n",
      "################################  20  ################################\n",
      "Training Loss:  44.00909805\n",
      "################################  25  ################################\n",
      "Training Loss:  39.57392883\n",
      "################################  30  ################################\n",
      "Training Loss:  39.76573944\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  38.64926147\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=175, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.07653809\n",
      "################################  5  ################################\n",
      "Training Loss:  50.68353653\n",
      "################################  10  ################################\n",
      "Training Loss:  49.64920425\n",
      "################################  15  ################################\n",
      "Training Loss:  758.12139893\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  44.72941971\n",
      "################################  25  ################################\n",
      "Training Loss:  42.22228241\n",
      "################################  30  ################################\n",
      "Training Loss:  133.14608765\n",
      "################################  35  ################################\n",
      "Training Loss:  38.71404648\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  129.04960632\n",
      "################################  45  ################################\n",
      "Training Loss:  37.92362213\n",
      "################################  50  ################################\n",
      "Training Loss:  38.16199112\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.00615692\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.0061264\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.00550842\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.00576401\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  38.00576401\n",
      "################################  80  ################################\n",
      "Training Loss:  38.00576401\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  38.00576401\n",
      "################################  90  ################################\n",
      "Training Loss:  38.00576401\n",
      "################################  95  ################################\n",
      "Training Loss:  38.00576401\n",
      "Final training Loss:  38.00576401\n",
      "\n",
      "Running model (trial=6, mod=176, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.71033478\n",
      "################################  5  ################################\n",
      "Training Loss:  83.44506836\n",
      "################################  10  ################################\n",
      "Training Loss:  165.9418335\n",
      "################################  15  ################################\n",
      "Training Loss:  58.13103867\n",
      "################################  20  ################################\n",
      "Training Loss:  1247.49279785\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  37.26131439\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.24956894\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.15867996\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  37.12190247\n",
      "################################  45  ################################\n",
      "Training Loss:  37.13578415\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.13557816\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.13558578\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.13558578\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.13558578\n",
      "################################  70  ################################\n",
      "Training Loss:  37.13558578\n",
      "################################  75  ################################\n",
      "Training Loss:  37.13558578\n",
      "################################  80  ################################\n",
      "Training Loss:  37.13558578\n",
      "################################  85  ################################\n",
      "Training Loss:  37.13558578\n",
      "################################  90  ################################\n",
      "Training Loss:  37.13558578\n",
      "################################  95  ################################\n",
      "Training Loss:  37.13558578\n",
      "Final training Loss:  37.13558578\n",
      "\n",
      "Running model (trial=6, mod=177, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  104.23729706\n",
      "################################  5  ################################\n",
      "Training Loss:  53.13985825\n",
      "################################  10  ################################\n",
      "Training Loss:  1026.92980957\n",
      "################################  15  ################################\n",
      "Training Loss:  67.02797699\n",
      "################################  20  ################################\n",
      "Training Loss:  39.53060532\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  39.27055359\n",
      "################################  30  ################################\n",
      "Training Loss:  1659.0546875\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  38.88972092\n",
      "################################  40  ################################\n",
      "Training Loss:  37.42021942\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.43186951\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.43175507\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.43175507\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.43175507\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  37.43175507\n",
      "################################  70  ################################\n",
      "Training Loss:  37.43175507\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.43175507\n",
      "################################  80  ################################\n",
      "Training Loss:  37.43175507\n",
      "################################  85  ################################\n",
      "Training Loss:  37.43175507\n",
      "################################  90  ################################\n",
      "Training Loss:  37.43175507\n",
      "################################  95  ################################\n",
      "Training Loss:  37.43175507\n",
      "Final training Loss:  37.43175507\n",
      "\n",
      "Running model (trial=6, mod=178, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.29351044\n",
      "################################  5  ################################\n",
      "Training Loss:  50.15673065\n",
      "################################  10  ################################\n",
      "Training Loss:  46.52632904\n",
      "################################  15  ################################\n",
      "Training Loss:  39.32747269\n",
      "################################  20  ################################\n",
      "Training Loss:  38.23303223\n",
      "################################  25  ################################\n",
      "Training Loss:  1859.75964355\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.99736023\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.14142609\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  39.69010162\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  39.38919449\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  39.38959885\n",
      "################################  55  ################################\n",
      "Training Loss:  39.38923645\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.38923645\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.38923645\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.38923645\n",
      "################################  75  ################################\n",
      "Training Loss:  39.38923645\n",
      "################################  80  ################################\n",
      "Training Loss:  39.38923645\n",
      "################################  85  ################################\n",
      "Training Loss:  39.38923645\n",
      "################################  90  ################################\n",
      "Training Loss:  39.38923645\n",
      "################################  95  ################################\n",
      "Training Loss:  39.38923645\n",
      "Final training Loss:  39.38923645\n",
      "\n",
      "Running model (trial=6, mod=179, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.09911728\n",
      "################################  5  ################################\n",
      "Training Loss:  50.7875061\n",
      "################################  10  ################################\n",
      "Training Loss:  50.14823151\n",
      "################################  15  ################################\n",
      "Training Loss:  48.95958328\n",
      "################################  20  ################################\n",
      "Training Loss:  1327.48583984\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  42.85621643\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=180, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  59.39691162\n",
      "################################  5  ################################\n",
      "Training Loss:  37.36946487\n",
      "################################  10  ################################\n",
      "Training Loss:  39.17387009\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  1250.65112305\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  38.98543549\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.23351288\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.17058563\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  38.17059708\n",
      "################################  40  ################################\n",
      "Training Loss:  38.17057037\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.17051697\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.17053223\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.17053223\n",
      "################################  60  ################################\n",
      "Training Loss:  38.17053223\n",
      "################################  65  ################################\n",
      "Training Loss:  38.17053223\n",
      "################################  70  ################################\n",
      "Training Loss:  38.17053223\n",
      "################################  75  ################################\n",
      "Training Loss:  38.17053223\n",
      "################################  80  ################################\n",
      "Training Loss:  38.17053223\n",
      "################################  85  ################################\n",
      "Training Loss:  38.17053223\n",
      "################################  90  ################################\n",
      "Training Loss:  38.17053223\n",
      "################################  95  ################################\n",
      "Training Loss:  38.17053223\n",
      "Final training Loss:  38.17053223\n",
      "\n",
      "Running model (trial=6, mod=181, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  106.96524048\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=182, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  85.34494019\n",
      "################################  5  ################################\n",
      "Training Loss:  76.22065735\n",
      "################################  10  ################################\n",
      "Training Loss:  53.13864136\n",
      "################################  15  ################################\n",
      "Training Loss:  52.86600876\n",
      "################################  20  ################################\n",
      "Training Loss:  52.91003036\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  53.06805038\n",
      "################################  30  ################################\n",
      "Training Loss:  52.23870087\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  52.21684265\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  52.22021866\n",
      "################################  45  ################################\n",
      "Training Loss:  52.45626068\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.45620728\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.45619202\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  52.45619202\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  52.45619583\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  52.45619583\n",
      "################################  75  ################################\n",
      "Training Loss:  52.45619583\n",
      "################################  80  ################################\n",
      "Training Loss:  52.45619583\n",
      "################################  85  ################################\n",
      "Training Loss:  52.45619583\n",
      "################################  90  ################################\n",
      "Training Loss:  52.45619583\n",
      "################################  95  ################################\n",
      "Training Loss:  52.45619583\n",
      "Final training Loss:  52.45619583\n",
      "\n",
      "Running model (trial=7, mod=183, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=184, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  102.69831085\n",
      "################################  5  ################################\n",
      "Training Loss:  102.70462036\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  102.74712372\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  102.71276855\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  102.66029358\n",
      "################################  25  ################################\n",
      "Training Loss:  102.65224457\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  102.65183258\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  102.65183258\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  102.6518631\n",
      "################################  45  ################################\n",
      "Training Loss:  102.6518631\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  102.6518631\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  102.6518631\n",
      "################################  60  ################################\n",
      "Training Loss:  102.6518631\n",
      "################################  65  ################################\n",
      "Training Loss:  102.6518631\n",
      "################################  70  ################################\n",
      "Training Loss:  102.6518631\n",
      "################################  75  ################################\n",
      "Training Loss:  102.6518631\n",
      "################################  80  ################################\n",
      "Training Loss:  102.6518631\n",
      "################################  85  ################################\n",
      "Training Loss:  102.6518631\n",
      "################################  90  ################################\n",
      "Training Loss:  102.6518631\n",
      "################################  95  ################################\n",
      "Training Loss:  102.6518631\n",
      "Final training Loss:  102.6518631\n",
      "\n",
      "Running model (trial=7, mod=185, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  59.45008087\n",
      "################################  5  ################################\n",
      "Training Loss:  41.80966187\n",
      "################################  10  ################################\n",
      "Training Loss:  39.56951141\n",
      "################################  15  ################################\n",
      "Training Loss:  39.52906799\n",
      "################################  20  ################################\n",
      "Training Loss:  39.04977417\n",
      "################################  25  ################################\n",
      "Training Loss:  37.94984436\n",
      "################################  30  ################################\n",
      "Training Loss:  37.03822708\n",
      "################################  35  ################################\n",
      "Training Loss:  36.65813446\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  37.20610428\n",
      "################################  45  ################################\n",
      "Training Loss:  36.79931641\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.87840652\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.85832977\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.9200592\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.90983582\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  36.90986633\n",
      "################################  75  ################################\n",
      "Training Loss:  36.90987396\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.90988159\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  36.90988159\n",
      "################################  90  ################################\n",
      "Training Loss:  36.90988159\n",
      "################################  95  ################################\n",
      "Training Loss:  36.90988159\n",
      "Final training Loss:  36.90988159\n",
      "\n",
      "Running model (trial=7, mod=186, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.8223114\n",
      "################################  5  ################################\n",
      "Training Loss:  43.8716011\n",
      "################################  10  ################################\n",
      "Training Loss:  42.5623703\n",
      "################################  15  ################################\n",
      "Training Loss:  41.87633514\n",
      "################################  20  ################################\n",
      "Training Loss:  41.73557663\n",
      "################################  25  ################################\n",
      "Training Loss:  39.52789688\n",
      "################################  30  ################################\n",
      "Training Loss:  39.10834503\n",
      "################################  35  ################################\n",
      "Training Loss:  39.20231628\n",
      "################################  40  ################################\n",
      "Training Loss:  38.617733\n",
      "################################  45  ################################\n",
      "Training Loss:  36.68576431\n",
      "################################  50  ################################\n",
      "Training Loss:  37.43770218\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.14670944\n",
      "################################  60  ################################\n",
      "Training Loss:  36.66267395\n",
      "################################  65  ################################\n",
      "Training Loss:  36.62988281\n",
      "################################  70  ################################\n",
      "Training Loss:  36.4839859\n",
      "################################  75  ################################\n",
      "Training Loss:  36.57089233\n",
      "################################  80  ################################\n",
      "Training Loss:  36.44976807\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  85  ################################\n",
      "Training Loss:  36.67277908\n",
      "################################  90  ################################\n",
      "Training Loss:  36.17483521\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  36.17483521\n",
      "Final training Loss:  36.17467499\n",
      "\n",
      "Running model (trial=7, mod=187, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  44.21375656\n",
      "################################  5  ################################\n",
      "Training Loss:  41.75274277\n",
      "################################  10  ################################\n",
      "Training Loss:  40.59451675\n",
      "################################  15  ################################\n",
      "Training Loss:  39.62839127\n",
      "################################  20  ################################\n",
      "Training Loss:  38.91592789\n",
      "################################  25  ################################\n",
      "Training Loss:  37.85532761\n",
      "################################  30  ################################\n",
      "Training Loss:  37.20768356\n",
      "################################  35  ################################\n",
      "Training Loss:  36.9483757\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.01261902\n",
      "################################  45  ################################\n",
      "Training Loss:  304.27780151\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  2209.12255859\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.09708023\n",
      "################################  60  ################################\n",
      "Training Loss:  37.07866287\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.08732224\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.08732224\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.08324432\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  37.08324432\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.08324432\n",
      "################################  90  ################################\n",
      "Training Loss:  37.08324432\n",
      "################################  95  ################################\n",
      "Training Loss:  37.08324432\n",
      "Final training Loss:  37.08324432\n",
      "\n",
      "Running model (trial=7, mod=188, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.3862915\n",
      "################################  5  ################################\n",
      "Training Loss:  39.0078392\n",
      "################################  10  ################################\n",
      "Training Loss:  36.0116806\n",
      "################################  15  ################################\n",
      "Training Loss:  35.80994034\n",
      "################################  20  ################################\n",
      "Training Loss:  36.10482025\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=189, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.49234009\n",
      "################################  5  ################################\n",
      "Training Loss:  37.98908234\n",
      "################################  10  ################################\n",
      "Training Loss:  37.13433456\n",
      "################################  15  ################################\n",
      "Training Loss:  36.91262817\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  36.79405212\n",
      "################################  25  ################################\n",
      "Training Loss:  36.8064537\n",
      "################################  30  ################################\n",
      "Training Loss:  4448.21630859\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.96789551\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.17301559\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.76910019\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.66960907\n",
      "################################  55  ################################\n",
      "Training Loss:  37.66962051\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.66962051\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.66962051\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.66962051\n",
      "################################  75  ################################\n",
      "Training Loss:  37.66962051\n",
      "################################  80  ################################\n",
      "Training Loss:  37.66962051\n",
      "################################  85  ################################\n",
      "Training Loss:  37.66962051\n",
      "################################  90  ################################\n",
      "Training Loss:  37.66962051\n",
      "################################  95  ################################\n",
      "Training Loss:  37.66962051\n",
      "Final training Loss:  37.66962051\n",
      "\n",
      "Running model (trial=7, mod=190, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  43.55696487\n",
      "################################  5  ################################\n",
      "Training Loss:  39.69953156\n",
      "################################  10  ################################\n",
      "Training Loss:  36.86632538\n",
      "################################  15  ################################\n",
      "Training Loss:  36.1018219\n",
      "################################  20  ################################\n",
      "Training Loss:  36.15494537\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  36.28259277\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  36.03953552\n",
      "################################  35  ################################\n",
      "Training Loss:  36.04700851\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.09453583\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.13096619\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.13089371\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.13088226\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.13088226\n",
      "################################  65  ################################\n",
      "Training Loss:  36.13088226\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.13088226\n",
      "################################  75  ################################\n",
      "Training Loss:  36.13088226\n",
      "################################  80  ################################\n",
      "Training Loss:  36.13088226\n",
      "################################  85  ################################\n",
      "Training Loss:  36.13088226\n",
      "################################  90  ################################\n",
      "Training Loss:  36.13088226\n",
      "################################  95  ################################\n",
      "Training Loss:  36.13088226\n",
      "Final training Loss:  36.13088226\n",
      "\n",
      "Running model (trial=7, mod=191, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  47.38933945\n",
      "################################  5  ################################\n",
      "Training Loss:  43.17803192\n",
      "################################  10  ################################\n",
      "Training Loss:  42.77470398\n",
      "################################  15  ################################\n",
      "Training Loss:  40.58577347\n",
      "################################  20  ################################\n",
      "Training Loss:  60.69719315\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.4866333\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  38.23470306\n",
      "################################  35  ################################\n",
      "Training Loss:  38.28585052\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.17076111\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.17117691\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.17116928\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.17116165\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  38.17116165\n",
      "################################  65  ################################\n",
      "Training Loss:  38.17116165\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.17116165\n",
      "################################  75  ################################\n",
      "Training Loss:  38.17116165\n",
      "################################  80  ################################\n",
      "Training Loss:  38.17116165\n",
      "################################  85  ################################\n",
      "Training Loss:  38.17116165\n",
      "################################  90  ################################\n",
      "Training Loss:  38.17116165\n",
      "################################  95  ################################\n",
      "Training Loss:  38.17116165\n",
      "Final training Loss:  38.17116165\n",
      "\n",
      "Running model (trial=7, mod=192, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  117.15779877\n",
      "################################  5  ################################\n",
      "Training Loss:  116.64322662\n",
      "################################  10  ################################\n",
      "Training Loss:  111.67106628\n",
      "################################  15  ################################\n",
      "Training Loss:  62.93640137\n",
      "################################  20  ################################\n",
      "Training Loss:  39.5625\n",
      "################################  25  ################################\n",
      "Training Loss:  38.62571716\n",
      "################################  30  ################################\n",
      "Training Loss:  38.61985397\n",
      "################################  35  ################################\n",
      "Training Loss:  38.45654297\n",
      "################################  40  ################################\n",
      "Training Loss:  39.64091492\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.1319046\n",
      "################################  50  ################################\n",
      "Training Loss:  37.04245377\n",
      "################################  55  ################################\n",
      "Training Loss:  36.62530518\n",
      "################################  60  ################################\n",
      "Training Loss:  36.49810791\n",
      "################################  65  ################################\n",
      "Training Loss:  36.38797379\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.56635666\n",
      "################################  75  ################################\n",
      "Training Loss:  35.88408661\n",
      "################################  80  ################################\n",
      "Training Loss:  36.07281113\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  85  ################################\n",
      "Training Loss:  35.90734863\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  90  ################################\n",
      "Training Loss:  36.15633392\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.15633392\n",
      "Final training Loss:  36.15633392\n",
      "\n",
      "Running model (trial=7, mod=193, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  48.09782028\n",
      "################################  5  ################################\n",
      "Training Loss:  41.87440872\n",
      "################################  10  ################################\n",
      "Training Loss:  41.36856461\n",
      "################################  15  ################################\n",
      "Training Loss:  40.43640137\n",
      "################################  20  ################################\n",
      "Training Loss:  40.27334976\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=194, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3667.83422852\n",
      "################################  5  ################################\n",
      "Training Loss:  3667.71948242\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  3667.71948242\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  3667.71948242\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  3667.71948242\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  3667.71948242\n",
      "################################  35  ################################\n",
      "Training Loss:  3667.71948242\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  3667.71948242\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  3667.71948242\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  55  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  60  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  65  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  70  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  75  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  80  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  85  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  90  ################################\n",
      "Training Loss:  3667.71948242\n",
      "################################  95  ################################\n",
      "Training Loss:  3667.71948242\n",
      "Final training Loss:  3667.71948242\n",
      "\n",
      "Running model (trial=7, mod=195, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.74066162\n",
      "################################  5  ################################\n",
      "Training Loss:  52.38549423\n",
      "################################  10  ################################\n",
      "Training Loss:  52.22731781\n",
      "################################  15  ################################\n",
      "Training Loss:  52.00228119\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  52.2127533\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  52.31751633\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  52.24543762\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  52.00362015\n",
      "################################  40  ################################\n",
      "Training Loss:  52.00361252\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  52.00359344\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.00359344\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.00359344\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  52.00359344\n",
      "################################  65  ################################\n",
      "Training Loss:  52.00359344\n",
      "################################  70  ################################\n",
      "Training Loss:  52.00359344\n",
      "################################  75  ################################\n",
      "Training Loss:  52.00359344\n",
      "################################  80  ################################\n",
      "Training Loss:  52.00359344\n",
      "################################  85  ################################\n",
      "Training Loss:  52.00359344\n",
      "################################  90  ################################\n",
      "Training Loss:  52.00359344\n",
      "################################  95  ################################\n",
      "Training Loss:  52.00359344\n",
      "Final training Loss:  52.00359344\n",
      "\n",
      "Running model (trial=7, mod=196, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.85235214\n",
      "################################  5  ################################\n",
      "Training Loss:  51.83714294\n",
      "################################  10  ################################\n",
      "Training Loss:  39.10360336\n",
      "################################  15  ################################\n",
      "Training Loss:  38.50275803\n",
      "################################  20  ################################\n",
      "Training Loss:  35.61881256\n",
      "################################  25  ################################\n",
      "Training Loss:  36.04934692\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  36.61659622\n",
      "################################  35  ################################\n",
      "Training Loss:  35.87640762\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  35.54937363\n",
      "################################  45  ################################\n",
      "Training Loss:  44.9287796\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=197, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.95072174\n",
      "################################  5  ################################\n",
      "Training Loss:  50.97023392\n",
      "################################  10  ################################\n",
      "Training Loss:  50.10061264\n",
      "################################  15  ################################\n",
      "Training Loss:  49.25509262\n",
      "################################  20  ################################\n",
      "Training Loss:  38.80190659\n",
      "################################  25  ################################\n",
      "Training Loss:  119.72431946\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.42048264\n",
      "################################  35  ################################\n",
      "Training Loss:  37.37376022\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.29164886\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.30635452\n",
      "################################  50  ################################\n",
      "Training Loss:  37.3152504\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.31505966\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.31489182\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.31489182\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.31489182\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.31489182\n",
      "################################  80  ################################\n",
      "Training Loss:  37.31489182\n",
      "################################  85  ################################\n",
      "Training Loss:  37.31489182\n",
      "################################  90  ################################\n",
      "Training Loss:  37.31489182\n",
      "################################  95  ################################\n",
      "Training Loss:  37.31489182\n",
      "Final training Loss:  37.31489182\n",
      "\n",
      "Running model (trial=7, mod=198, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.92145157\n",
      "################################  5  ################################\n",
      "Training Loss:  50.43888092\n",
      "################################  10  ################################\n",
      "Training Loss:  49.24536514\n",
      "################################  15  ################################\n",
      "Training Loss:  49.11134338\n",
      "################################  20  ################################\n",
      "Training Loss:  44.28005981\n",
      "################################  25  ################################\n",
      "Training Loss:  92.50810242\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  195.42732239\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  84.26637268\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  81.51259613\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  81.28773499\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  81.2876358\n",
      "################################  55  ################################\n",
      "Training Loss:  81.2875824\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  81.2875824\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  81.2875824\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  81.2875824\n",
      "################################  75  ################################\n",
      "Training Loss:  81.2875824\n",
      "################################  80  ################################\n",
      "Training Loss:  81.2875824\n",
      "################################  85  ################################\n",
      "Training Loss:  81.2875824\n",
      "################################  90  ################################\n",
      "Training Loss:  81.2875824\n",
      "################################  95  ################################\n",
      "Training Loss:  81.2875824\n",
      "Final training Loss:  81.2875824\n",
      "\n",
      "Running model (trial=7, mod=199, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  118.91545868\n",
      "################################  5  ################################\n",
      "Training Loss:  50.68663025\n",
      "################################  10  ################################\n",
      "Training Loss:  777.50549316\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  2768.97631836\n",
      "################################  20  ################################\n",
      "Training Loss:  39.85847473\n",
      "################################  25  ################################\n",
      "Training Loss:  47.1603775\n",
      "################################  30  ################################\n",
      "Training Loss:  39.07778549\n",
      "################################  35  ################################\n",
      "Training Loss:  38.53549957\n",
      "################################  40  ################################\n",
      "Training Loss:  39.15139389\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  39.2127533\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  39.17586136\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  39.13819504\n",
      "################################  60  ################################\n",
      "Training Loss:  39.13823318\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.13819504\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.13819504\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.13819504\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  39.13819504\n",
      "################################  85  ################################\n",
      "Training Loss:  39.13819504\n",
      "################################  90  ################################\n",
      "Training Loss:  39.13819504\n",
      "################################  95  ################################\n",
      "Training Loss:  39.13819504\n",
      "Final training Loss:  39.13819504\n",
      "\n",
      "Running model (trial=7, mod=200, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.85690689\n",
      "################################  5  ################################\n",
      "Training Loss:  50.71474457\n",
      "################################  10  ################################\n",
      "Training Loss:  47.57872772\n",
      "################################  15  ################################\n",
      "Training Loss:  166.37635803\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  38.3284874\n",
      "################################  25  ################################\n",
      "Training Loss:  37.83742905\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  39.67876053\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.96182632\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  39.02481842\n",
      "################################  45  ################################\n",
      "Training Loss:  39.02477646\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  39.02477646\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.02479172\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.0247879\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.0247879\n",
      "################################  70  ################################\n",
      "Training Loss:  39.0247879\n",
      "################################  75  ################################\n",
      "Training Loss:  39.0247879\n",
      "################################  80  ################################\n",
      "Training Loss:  39.0247879\n",
      "################################  85  ################################\n",
      "Training Loss:  39.0247879\n",
      "################################  90  ################################\n",
      "Training Loss:  39.0247879\n",
      "################################  95  ################################\n",
      "Training Loss:  39.0247879\n",
      "Final training Loss:  39.0247879\n",
      "\n",
      "Running model (trial=7, mod=201, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.04889679\n",
      "################################  5  ################################\n",
      "Training Loss:  50.72537613\n",
      "################################  10  ################################\n",
      "Training Loss:  50.66898346\n",
      "################################  15  ################################\n",
      "Training Loss:  4652.3984375\n",
      "################################  20  ################################\n",
      "Training Loss:  39.90432358\n",
      "################################  25  ################################\n",
      "Training Loss:  38.81911087\n",
      "################################  30  ################################\n",
      "Training Loss:  38.91777039\n",
      "################################  35  ################################\n",
      "Training Loss:  247.76022339\n",
      "################################  40  ################################\n",
      "Training Loss:  37.24775314\n",
      "################################  45  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=202, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.78453445\n",
      "################################  5  ################################\n",
      "Training Loss:  10721.93164062\n",
      "################################  10  ################################\n",
      "Training Loss:  38.04841995\n",
      "################################  15  ################################\n",
      "Training Loss:  630.41021729\n",
      "################################  20  ################################\n",
      "Training Loss:  37.13426971\n",
      "################################  25  ################################\n",
      "Training Loss:  2148.9921875\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  225.34814453\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.54289246\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.46708679\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.47003937\n",
      "################################  50  ################################\n",
      "Training Loss:  38.470047\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.47006607\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.47006607\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.47006607\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.47006607\n",
      "################################  75  ################################\n",
      "Training Loss:  38.47006607\n",
      "################################  80  ################################\n",
      "Training Loss:  38.47006607\n",
      "################################  85  ################################\n",
      "Training Loss:  38.47006607\n",
      "################################  90  ################################\n",
      "Training Loss:  38.47006607\n",
      "################################  95  ################################\n",
      "Training Loss:  38.47006607\n",
      "Final training Loss:  38.47006607\n",
      "\n",
      "Running model (trial=7, mod=203, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.10557556\n",
      "################################  5  ################################\n",
      "Training Loss:  48.308815\n",
      "################################  10  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=204, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.17417145\n",
      "################################  5  ################################\n",
      "Training Loss:  49.72639465\n",
      "################################  10  ################################\n",
      "Training Loss:  49.56977844\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  49.23810577\n",
      "################################  20  ################################\n",
      "Training Loss:  275.30215454\n",
      "################################  25  ################################\n",
      "Training Loss:  1369.92382812\n",
      "################################  30  ################################\n",
      "Training Loss:  40.04852295\n",
      "################################  35  ################################\n",
      "Training Loss:  39.4258194\n",
      "################################  40  ################################\n",
      "Training Loss:  40.15343094\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  39.17598724\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  39.37448883\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  39.18458939\n",
      "################################  60  ################################\n",
      "Training Loss:  39.1845665\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.18438721\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.18486786\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.18486786\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  39.18486786\n",
      "################################  85  ################################\n",
      "Training Loss:  39.18486786\n",
      "################################  90  ################################\n",
      "Training Loss:  39.18486786\n",
      "################################  95  ################################\n",
      "Training Loss:  39.18486786\n",
      "Final training Loss:  39.18486786\n",
      "\n",
      "Running model (trial=7, mod=205, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.38285446\n",
      "################################  5  ################################\n",
      "Training Loss:  248.57341003\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  212.66717529\n",
      "################################  15  ################################\n",
      "Training Loss:  147.55143738\n",
      "################################  20  ################################\n",
      "Training Loss:  39.10645294\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.8548584\n",
      "################################  30  ################################\n",
      "Training Loss:  38.42174149\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  42.79682541\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.2420311\n",
      "################################  45  ################################\n",
      "Training Loss:  38.24203491\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.24213028\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.24213409\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.24213409\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.24213409\n",
      "################################  70  ################################\n",
      "Training Loss:  38.24213409\n",
      "################################  75  ################################\n",
      "Training Loss:  38.24213409\n",
      "################################  80  ################################\n",
      "Training Loss:  38.24213409\n",
      "################################  85  ################################\n",
      "Training Loss:  38.24213409\n",
      "################################  90  ################################\n",
      "Training Loss:  38.24213409\n",
      "################################  95  ################################\n",
      "Training Loss:  38.24213409\n",
      "Final training Loss:  38.24213409\n",
      "\n",
      "Running model (trial=7, mod=206, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.38562393\n",
      "################################  5  ################################\n",
      "Training Loss:  276.33499146\n",
      "################################  10  ################################\n",
      "Training Loss:  39.97568512\n",
      "################################  15  ################################\n",
      "Training Loss:  1230.68786621\n",
      "################################  20  ################################\n",
      "Training Loss:  207.27658081\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  133.07872009\n",
      "################################  30  ################################\n",
      "Training Loss:  110.16596222\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.80855942\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  39.16219711\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  39.1541748\n",
      "################################  50  ################################\n",
      "Training Loss:  39.15445709\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.1544838\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.1544838\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.1544838\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.1544838\n",
      "################################  75  ################################\n",
      "Training Loss:  39.1544838\n",
      "################################  80  ################################\n",
      "Training Loss:  39.1544838\n",
      "################################  85  ################################\n",
      "Training Loss:  39.1544838\n",
      "################################  90  ################################\n",
      "Training Loss:  39.1544838\n",
      "################################  95  ################################\n",
      "Training Loss:  39.1544838\n",
      "Final training Loss:  39.1544838\n",
      "\n",
      "Running model (trial=7, mod=207, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  115.67436981\n",
      "################################  5  ################################\n",
      "Training Loss:  114.44916534\n",
      "################################  10  ################################\n",
      "Training Loss:  113.26634979\n",
      "################################  15  ################################\n",
      "Training Loss:  108.6654129\n",
      "################################  20  ################################\n",
      "Training Loss:  108.76576996\n",
      "################################  25  ################################\n",
      "Training Loss:  107.88574219\n",
      "################################  30  ################################\n",
      "Training Loss:  144.88247681\n",
      "################################  35  ################################\n",
      "Training Loss:  167.07563782\n",
      "################################  40  ################################\n",
      "Training Loss:  113.11595917\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  111.04412842\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  109.37004852\n",
      "################################  55  ################################\n",
      "Training Loss:  107.38121033\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  107.32062531\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  107.27048492\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  107.27037811\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  107.27038574\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  107.27038574\n",
      "################################  85  ################################\n",
      "Training Loss:  107.27038574\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  107.27038574\n",
      "################################  95  ################################\n",
      "Training Loss:  107.27038574\n",
      "Final training Loss:  107.27038574\n",
      "\n",
      "Running model (trial=8, mod=208, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.30535126\n",
      "################################  5  ################################\n",
      "Training Loss:  52.23100281\n",
      "################################  10  ################################\n",
      "Training Loss:  52.57639694\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  52.43033218\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  52.2252655\n",
      "################################  25  ################################\n",
      "Training Loss:  52.39547729\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  52.31744003\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  52.44712067\n",
      "################################  40  ################################\n",
      "Training Loss:  52.44709396\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  52.44710159\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.44710159\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.44710159\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  52.44710159\n",
      "################################  65  ################################\n",
      "Training Loss:  52.44710159\n",
      "################################  70  ################################\n",
      "Training Loss:  52.44710159\n",
      "################################  75  ################################\n",
      "Training Loss:  52.44710159\n",
      "################################  80  ################################\n",
      "Training Loss:  52.44710159\n",
      "################################  85  ################################\n",
      "Training Loss:  52.44710159\n",
      "################################  90  ################################\n",
      "Training Loss:  52.44710159\n",
      "################################  95  ################################\n",
      "Training Loss:  52.44710159\n",
      "Final training Loss:  52.44710159\n",
      "\n",
      "Running model (trial=8, mod=209, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=210, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.96047974\n",
      "################################  5  ################################\n",
      "Training Loss:  52.82779312\n",
      "################################  10  ################################\n",
      "Training Loss:  52.24435806\n",
      "################################  15  ################################\n",
      "Training Loss:  52.12446976\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  52.27884293\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  52.2583046\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  52.42071533\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  52.4203949\n",
      "################################  40  ################################\n",
      "Training Loss:  52.42041016\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  52.42042923\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.42042923\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.42042923\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  52.42042923\n",
      "################################  65  ################################\n",
      "Training Loss:  52.42042923\n",
      "################################  70  ################################\n",
      "Training Loss:  52.42042923\n",
      "################################  75  ################################\n",
      "Training Loss:  52.42042923\n",
      "################################  80  ################################\n",
      "Training Loss:  52.42042923\n",
      "################################  85  ################################\n",
      "Training Loss:  52.42042923\n",
      "################################  90  ################################\n",
      "Training Loss:  52.42042923\n",
      "################################  95  ################################\n",
      "Training Loss:  52.42042923\n",
      "Final training Loss:  52.42042923\n",
      "\n",
      "Running model (trial=8, mod=211, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.18445969\n",
      "################################  5  ################################\n",
      "Training Loss:  36.41174698\n",
      "################################  10  ################################\n",
      "Training Loss:  36.80846024\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  36.85263443\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  36.97622299\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  36.9784317\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  36.74291611\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  36.74287415\n",
      "################################  40  ################################\n",
      "Training Loss:  36.74288177\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.74288177\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.74288177\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.74288177\n",
      "################################  60  ################################\n",
      "Training Loss:  36.74288177\n",
      "################################  65  ################################\n",
      "Training Loss:  36.74288177\n",
      "################################  70  ################################\n",
      "Training Loss:  36.74288177\n",
      "################################  75  ################################\n",
      "Training Loss:  36.74288177\n",
      "################################  80  ################################\n",
      "Training Loss:  36.74288177\n",
      "################################  85  ################################\n",
      "Training Loss:  36.74288177\n",
      "################################  90  ################################\n",
      "Training Loss:  36.74288177\n",
      "################################  95  ################################\n",
      "Training Loss:  36.74288177\n",
      "Final training Loss:  36.74288177\n",
      "\n",
      "Running model (trial=8, mod=212, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  48.74427795\n",
      "################################  5  ################################\n",
      "Training Loss:  42.92484283\n",
      "################################  10  ################################\n",
      "Training Loss:  42.70420456\n",
      "################################  15  ################################\n",
      "Training Loss:  38.95482254\n",
      "################################  20  ################################\n",
      "Training Loss:  40.40641785\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  41.37929916\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  39.99588776\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  39.28382492\n",
      "################################  40  ################################\n",
      "Training Loss:  39.05094147\n",
      "################################  45  ################################\n",
      "Training Loss:  38.28685379\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.33156586\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.33218002\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.33217239\n",
      "################################  65  ################################\n",
      "Training Loss:  38.33217239\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.33217239\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  38.33217239\n",
      "################################  80  ################################\n",
      "Training Loss:  38.33217239\n",
      "################################  85  ################################\n",
      "Training Loss:  38.33217239\n",
      "################################  90  ################################\n",
      "Training Loss:  38.33217239\n",
      "################################  95  ################################\n",
      "Training Loss:  38.33217239\n",
      "Final training Loss:  38.33217239\n",
      "\n",
      "Running model (trial=8, mod=213, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  48.14657593\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=214, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  43.46595001\n",
      "################################  5  ################################\n",
      "Training Loss:  36.68740845\n",
      "################################  10  ################################\n",
      "Training Loss:  35.86273193\n",
      "################################  15  ################################\n",
      "Training Loss:  39.49501801\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  39.37441254\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.73907852\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  1046.42431641\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  1932.85681152\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  44.70799637\n",
      "################################  45  ################################\n",
      "Training Loss:  44.70800018\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  44.70800781\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  44.70801163\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  44.70801163\n",
      "################################  65  ################################\n",
      "Training Loss:  44.70801163\n",
      "################################  70  ################################\n",
      "Training Loss:  44.70801163\n",
      "################################  75  ################################\n",
      "Training Loss:  44.70801163\n",
      "################################  80  ################################\n",
      "Training Loss:  44.70801163\n",
      "################################  85  ################################\n",
      "Training Loss:  44.70801163\n",
      "################################  90  ################################\n",
      "Training Loss:  44.70801163\n",
      "################################  95  ################################\n",
      "Training Loss:  44.70801163\n",
      "Final training Loss:  44.70801163\n",
      "\n",
      "Running model (trial=8, mod=215, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  60.34078217\n",
      "################################  5  ################################\n",
      "Training Loss:  52.08296204\n",
      "################################  10  ################################\n",
      "Training Loss:  52.86670685\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  52.22718811\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  52.45663834\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  52.47875977\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  52.41955948\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  52.41947937\n",
      "################################  40  ################################\n",
      "Training Loss:  52.41946793\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  52.41946793\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  52.41946793\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.41946793\n",
      "################################  60  ################################\n",
      "Training Loss:  52.41946793\n",
      "################################  65  ################################\n",
      "Training Loss:  52.41946793\n",
      "################################  70  ################################\n",
      "Training Loss:  52.41946793\n",
      "################################  75  ################################\n",
      "Training Loss:  52.41946793\n",
      "################################  80  ################################\n",
      "Training Loss:  52.41946793\n",
      "################################  85  ################################\n",
      "Training Loss:  52.41946793\n",
      "################################  90  ################################\n",
      "Training Loss:  52.41946793\n",
      "################################  95  ################################\n",
      "Training Loss:  52.41946793\n",
      "Final training Loss:  52.41946793\n",
      "\n",
      "Running model (trial=8, mod=216, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  123.28659058\n",
      "################################  5  ################################\n",
      "Training Loss:  122.02819061\n",
      "################################  10  ################################\n",
      "Training Loss:  121.26687622\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  121.26646423\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  121.2652359\n",
      "################################  25  ################################\n",
      "Training Loss:  121.2652359\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  121.27588654\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  121.27590942\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  121.27589417\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=217, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12724.20507812\n",
      "################################  5  ################################\n",
      "Training Loss:  43.21414948\n",
      "################################  10  ################################\n",
      "Training Loss:  42.70052719\n",
      "################################  15  ################################\n",
      "Training Loss:  42.29450607\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  43.0073204\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  42.63890076\n",
      "################################  30  ################################\n",
      "Training Loss:  42.44889069\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  42.3335228\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  42.30183029\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  42.34401703\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  42.34402084\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  42.34402084\n",
      "################################  60  ################################\n",
      "Training Loss:  42.34402084\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  42.34402084\n",
      "################################  70  ################################\n",
      "Training Loss:  42.34402084\n",
      "################################  75  ################################\n",
      "Training Loss:  42.34402084\n",
      "################################  80  ################################\n",
      "Training Loss:  42.34402084\n",
      "################################  85  ################################\n",
      "Training Loss:  42.34402084\n",
      "################################  90  ################################\n",
      "Training Loss:  42.34402084\n",
      "################################  95  ################################\n",
      "Training Loss:  42.34402084\n",
      "Final training Loss:  42.34402084\n",
      "\n",
      "Running model (trial=8, mod=218, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  45.39818192\n",
      "################################  5  ################################\n",
      "Training Loss:  43.66021729\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  44.24914169\n",
      "################################  15  ################################\n",
      "Training Loss:  44.21030426\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  43.88326263\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  43.75279236\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  43.76368332\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  315.22268677\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  315.22296143\n",
      "################################  45  ################################\n",
      "Training Loss:  315.2232666\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  315.2232666\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  315.2232666\n",
      "################################  60  ################################\n",
      "Training Loss:  315.2232666\n",
      "################################  65  ################################\n",
      "Training Loss:  315.2232666\n",
      "################################  70  ################################\n",
      "Training Loss:  315.2232666\n",
      "################################  75  ################################\n",
      "Training Loss:  315.2232666\n",
      "################################  80  ################################\n",
      "Training Loss:  315.2232666\n",
      "################################  85  ################################\n",
      "Training Loss:  315.2232666\n",
      "################################  90  ################################\n",
      "Training Loss:  315.2232666\n",
      "################################  95  ################################\n",
      "Training Loss:  315.2232666\n",
      "Final training Loss:  315.2232666\n",
      "\n",
      "Running model (trial=8, mod=219, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.25726318\n",
      "################################  5  ################################\n",
      "Training Loss:  42.39559555\n",
      "################################  10  ################################\n",
      "Training Loss:  42.30269241\n",
      "################################  15  ################################\n",
      "Training Loss:  36.99314117\n",
      "################################  20  ################################\n",
      "Training Loss:  36.49983215\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=220, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  105.12182617\n",
      "################################  5  ################################\n",
      "Training Loss:  105.41691589\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  105.41691589\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  105.41503143\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  105.41504669\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  105.41504669\n",
      "################################  35  ################################\n",
      "Training Loss:  105.41504669\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  105.41504669\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  105.41504669\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  55  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  60  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  65  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  70  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  75  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  80  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  85  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  90  ################################\n",
      "Training Loss:  105.41504669\n",
      "################################  95  ################################\n",
      "Training Loss:  105.41504669\n",
      "Final training Loss:  105.41504669\n",
      "\n",
      "Running model (trial=8, mod=221, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.51709366\n",
      "################################  5  ################################\n",
      "Training Loss:  51.65246582\n",
      "################################  10  ################################\n",
      "Training Loss:  51.08066177\n",
      "################################  15  ################################\n",
      "Training Loss:  50.81714249\n",
      "################################  20  ################################\n",
      "Training Loss:  50.39523697\n",
      "################################  25  ################################\n",
      "Training Loss:  50.7710762\n",
      "################################  30  ################################\n",
      "Training Loss:  50.5041008\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  50.61267853\n",
      "################################  40  ################################\n",
      "Training Loss:  50.2375679\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  50.12652206\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  50.4687767\n",
      "################################  55  ################################\n",
      "Training Loss:  50.46840286\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  50.46859741\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  50.46852875\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  50.46852875\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  50.46852875\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  50.46852875\n",
      "################################  85  ################################\n",
      "Training Loss:  50.46852875\n",
      "################################  90  ################################\n",
      "Training Loss:  50.46852875\n",
      "################################  95  ################################\n",
      "Training Loss:  50.46852875\n",
      "Final training Loss:  50.46852875\n",
      "\n",
      "Running model (trial=8, mod=222, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.56053925\n",
      "################################  5  ################################\n",
      "Training Loss:  51.39775848\n",
      "################################  10  ################################\n",
      "Training Loss:  41.61762619\n",
      "################################  15  ################################\n",
      "Training Loss:  547.54522705\n",
      "################################  20  ################################\n",
      "Training Loss:  578.58654785\n",
      "################################  25  ################################\n",
      "Training Loss:  39.60628128\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  220.08886719\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  41.65333557\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  39.58922577\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  39.57173157\n",
      "################################  50  ################################\n",
      "Training Loss:  39.57146835\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.57150269\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.57150269\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.57150269\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.57150269\n",
      "################################  75  ################################\n",
      "Training Loss:  39.57150269\n",
      "################################  80  ################################\n",
      "Training Loss:  39.57150269\n",
      "################################  85  ################################\n",
      "Training Loss:  39.57150269\n",
      "################################  90  ################################\n",
      "Training Loss:  39.57150269\n",
      "################################  95  ################################\n",
      "Training Loss:  39.57150269\n",
      "Final training Loss:  39.57150269\n",
      "\n",
      "Running model (trial=8, mod=223, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.6195488\n",
      "################################  5  ################################\n",
      "Training Loss:  990.45550537\n",
      "################################  10  ################################\n",
      "Training Loss:  502.5965271\n",
      "################################  15  ################################\n",
      "Training Loss:  37.72134018\n",
      "################################  20  ################################\n",
      "Training Loss:  37.21542358\n",
      "################################  25  ################################\n",
      "Training Loss:  533.77459717\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  65.60639191\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.45464325\n",
      "################################  40  ################################\n",
      "Training Loss:  70.98972321\n",
      "################################  45  ################################\n",
      "Training Loss:  37.08008575\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  38.08099747\n",
      "################################  55  ################################\n",
      "Training Loss:  37.37654877\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.36458206\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.36457443\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.36457443\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.36457443\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.36457443\n",
      "################################  85  ################################\n",
      "Training Loss:  37.36457443\n",
      "################################  90  ################################\n",
      "Training Loss:  37.36457443\n",
      "################################  95  ################################\n",
      "Training Loss:  37.36457443\n",
      "Final training Loss:  37.36457443\n",
      "\n",
      "Running model (trial=8, mod=224, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.2295723\n",
      "################################  5  ################################\n",
      "Training Loss:  51.31152725\n",
      "################################  10  ################################\n",
      "Training Loss:  187.2550354\n",
      "################################  15  ################################\n",
      "Training Loss:  42.15864944\n",
      "################################  20  ################################\n",
      "Training Loss:  42.51368713\n",
      "################################  25  ################################\n",
      "Training Loss:  37.213974\n",
      "################################  30  ################################\n",
      "Training Loss:  236.22108459\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.97600174\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.98169327\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.48195648\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.33005905\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  38.33003616\n",
      "################################  60  ################################\n",
      "Training Loss:  38.32981873\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.32981873\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.32981873\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  38.32981873\n",
      "################################  80  ################################\n",
      "Training Loss:  38.32981873\n",
      "################################  85  ################################\n",
      "Training Loss:  38.32981873\n",
      "################################  90  ################################\n",
      "Training Loss:  38.32981873\n",
      "################################  95  ################################\n",
      "Training Loss:  38.32981873\n",
      "Final training Loss:  38.32981873\n",
      "\n",
      "Running model (trial=8, mod=225, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  55.12742996\n",
      "################################  5  ################################\n",
      "Training Loss:  51.51356506\n",
      "################################  10  ################################\n",
      "Training Loss:  50.21884155\n",
      "################################  15  ################################\n",
      "Training Loss:  49.92087936\n",
      "################################  20  ################################\n",
      "Training Loss:  1083.55065918\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  39.79366302\n",
      "################################  30  ################################\n",
      "Training Loss:  416.41394043\n",
      "################################  35  ################################\n",
      "Training Loss:  42.62433624\n",
      "################################  40  ################################\n",
      "Training Loss:  39.27193832\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  187.81219482\n",
      "################################  50  ################################\n",
      "Training Loss:  37.66546631\n",
      "################################  55  ################################\n",
      "Training Loss:  37.80385208\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.68854904\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.68761444\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.68761063\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.68765259\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  37.68765259\n",
      "################################  85  ################################\n",
      "Training Loss:  37.68765259\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  37.68765259\n",
      "################################  95  ################################\n",
      "Training Loss:  37.68765259\n",
      "Final training Loss:  37.68765259\n",
      "\n",
      "Running model (trial=8, mod=226, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  50.83636475\n",
      "################################  5  ################################\n",
      "Training Loss:  180.86456299\n",
      "################################  10  ################################\n",
      "Training Loss:  39.45008469\n",
      "################################  15  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=227, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.3340683\n",
      "################################  5  ################################\n",
      "Training Loss:  51.03470612\n",
      "################################  10  ################################\n",
      "Training Loss:  50.49530792\n",
      "################################  15  ################################\n",
      "Training Loss:  38.33320236\n",
      "################################  20  ################################\n",
      "Training Loss:  38.13869858\n",
      "################################  25  ################################\n",
      "Training Loss:  35.65255356\n",
      "################################  30  ################################\n",
      "Training Loss:  35.96316147\n",
      "################################  35  ################################\n",
      "Training Loss:  35.43090439\n",
      "################################  40  ################################\n",
      "Training Loss:  35.22531509\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=228, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.79995728\n",
      "################################  5  ################################\n",
      "Training Loss:  51.54863358\n",
      "################################  10  ################################\n",
      "Training Loss:  42.87696838\n",
      "################################  15  ################################\n",
      "Training Loss:  35.66331863\n",
      "################################  20  ################################\n",
      "Training Loss:  42.5438652\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=229, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.41147995\n",
      "################################  5  ################################\n",
      "Training Loss:  46.14656067\n",
      "################################  10  ################################\n",
      "Training Loss:  1474.18164062\n",
      "################################  15  ################################\n",
      "Training Loss:  235.16497803\n",
      "################################  20  ################################\n",
      "Training Loss:  60.65755844\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.18717575\n",
      "################################  30  ################################\n",
      "Training Loss:  1331.21484375\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.32047653\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  37.05472946\n",
      "################################  45  ################################\n",
      "Training Loss:  37.03184891\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.0318985\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.0317421\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.03174591\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.03174591\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  37.03174591\n",
      "################################  75  ################################\n",
      "Training Loss:  37.03174591\n",
      "################################  80  ################################\n",
      "Training Loss:  37.03174591\n",
      "################################  85  ################################\n",
      "Training Loss:  37.03174591\n",
      "################################  90  ################################\n",
      "Training Loss:  37.03174591\n",
      "################################  95  ################################\n",
      "Training Loss:  37.03174591\n",
      "Final training Loss:  37.03174591\n",
      "\n",
      "Running model (trial=8, mod=230, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.31871796\n",
      "################################  5  ################################\n",
      "Training Loss:  49.55276871\n",
      "################################  10  ################################\n",
      "Training Loss:  265.90994263\n",
      "################################  15  ################################\n",
      "Training Loss:  236.57597351\n",
      "################################  20  ################################\n",
      "Training Loss:  1239.81188965\n",
      "################################  25  ################################\n",
      "Training Loss:  40.63777542\n",
      "################################  30  ################################\n",
      "Training Loss:  39.47677231\n",
      "################################  35  ################################\n",
      "Training Loss:  38.37715149\n",
      "################################  40  ################################\n",
      "Training Loss:  38.36416245\n",
      "################################  45  ################################\n",
      "Training Loss:  104.77363586\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.38848114\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  39.99331284\n",
      "################################  60  ################################\n",
      "Training Loss:  39.72309113\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  39.65571213\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  39.65581894\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.65580368\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  39.65580368\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  39.65580368\n",
      "################################  90  ################################\n",
      "Training Loss:  39.65580368\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  39.65580368\n",
      "Final training Loss:  39.65580368\n",
      "\n",
      "Running model (trial=8, mod=231, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.64764404\n",
      "################################  5  ################################\n",
      "Training Loss:  702.61712646\n",
      "################################  10  ################################\n",
      "Training Loss:  39.55570984\n",
      "################################  15  ################################\n",
      "Training Loss:  40.3348465\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  38.59537506\n",
      "################################  25  ################################\n",
      "Training Loss:  39.66102982\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  39.22790146\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.91669083\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.9165802\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.91669846\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.91653442\n",
      "################################  55  ################################\n",
      "Training Loss:  38.91653442\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.91653442\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.91653442\n",
      "################################  70  ################################\n",
      "Training Loss:  38.91653442\n",
      "################################  75  ################################\n",
      "Training Loss:  38.91653442\n",
      "################################  80  ################################\n",
      "Training Loss:  38.91653442\n",
      "################################  85  ################################\n",
      "Training Loss:  38.91653442\n",
      "################################  90  ################################\n",
      "Training Loss:  38.91653442\n",
      "################################  95  ################################\n",
      "Training Loss:  38.91653442\n",
      "Final training Loss:  38.91653442\n",
      "\n",
      "Running model (trial=8, mod=232, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  116.81044006\n",
      "################################  5  ################################\n",
      "Training Loss:  114.24115753\n",
      "################################  10  ################################\n",
      "Training Loss:  113.83271027\n",
      "################################  15  ################################\n",
      "Training Loss:  111.60165405\n",
      "################################  20  ################################\n",
      "Training Loss:  111.28227234\n",
      "################################  25  ################################\n",
      "Training Loss:  107.31187439\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  108.99674988\n",
      "################################  35  ################################\n",
      "Training Loss:  107.15605164\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  359.73947144\n",
      "################################  45  ################################\n",
      "Training Loss:  111.9322052\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  111.47963715\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  111.41472626\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  111.27235413\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  111.27233887\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  111.27233124\n",
      "################################  75  ################################\n",
      "Training Loss:  111.27233124\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  111.27233124\n",
      "################################  85  ################################\n",
      "Training Loss:  111.27233124\n",
      "################################  90  ################################\n",
      "Training Loss:  111.27233124\n",
      "################################  95  ################################\n",
      "Training Loss:  111.27233124\n",
      "Final training Loss:  111.27233124\n",
      "\n",
      "Running model (trial=8, mod=233, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  47.3096199\n",
      "################################  5  ################################\n",
      "Training Loss:  1541.92138672\n",
      "################################  10  ################################\n",
      "Training Loss:  105.306633\n",
      "################################  15  ################################\n",
      "Training Loss:  39.57217026\n",
      "################################  20  ################################\n",
      "Training Loss:  131.12580872\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  36.92027664\n",
      "################################  30  ################################\n",
      "Training Loss:  35.87637329\n",
      "################################  35  ################################\n",
      "Training Loss:  35.78633881\n",
      "################################  40  ################################\n",
      "Training Loss:  43.21953964\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=234, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=235, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=236, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  58.04781723\n",
      "################################  5  ################################\n",
      "Training Loss:  38.86844254\n",
      "################################  10  ################################\n",
      "Training Loss:  38.5773201\n",
      "################################  15  ################################\n",
      "Training Loss:  37.22986221\n",
      "################################  20  ################################\n",
      "Training Loss:  37.50536346\n",
      "################################  25  ################################\n",
      "Training Loss:  36.86237717\n",
      "################################  30  ################################\n",
      "Training Loss:  36.68453598\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.88476944\n",
      "################################  40  ################################\n",
      "Training Loss:  35.8627739\n",
      "################################  45  ################################\n",
      "Training Loss:  35.66604614\n",
      "################################  50  ################################\n",
      "Training Loss:  36.40838623\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  35.99594116\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  36.06721497\n",
      "################################  65  ################################\n",
      "Training Loss:  35.85199738\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  35.82020187\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  35.82020187\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  35.82023621\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  35.81624603\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  35.81624603\n",
      "################################  95  ################################\n",
      "Training Loss:  35.81624603\n",
      "Final training Loss:  35.81624603\n",
      "\n",
      "Running model (trial=9, mod=237, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  44.46323776\n",
      "################################  5  ################################\n",
      "Training Loss:  44.08395386\n",
      "################################  10  ################################\n",
      "Training Loss:  42.17409515\n",
      "################################  15  ################################\n",
      "Training Loss:  37.72673035\n",
      "################################  20  ################################\n",
      "Training Loss:  37.58475494\n",
      "################################  25  ################################\n",
      "Training Loss:  37.09835434\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.17407227\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  37.04927826\n",
      "################################  40  ################################\n",
      "Training Loss:  37.02303696\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  36.81951523\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  36.74682236\n",
      "################################  55  ################################\n",
      "Training Loss:  36.74682617\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.74671555\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.74671555\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  36.74671555\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.74671555\n",
      "################################  80  ################################\n",
      "Training Loss:  36.74671555\n",
      "################################  85  ################################\n",
      "Training Loss:  36.74671555\n",
      "################################  90  ################################\n",
      "Training Loss:  36.74671555\n",
      "################################  95  ################################\n",
      "Training Loss:  36.74671555\n",
      "Final training Loss:  36.74671555\n",
      "\n",
      "Running model (trial=9, mod=238, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  47.17977905\n",
      "################################  5  ################################\n",
      "Training Loss:  44.73449707\n",
      "################################  10  ################################\n",
      "Training Loss:  39.85829544\n",
      "################################  15  ################################\n",
      "Training Loss:  39.97960281\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  39.36810303\n",
      "################################  25  ################################\n",
      "Training Loss:  583.67730713\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  42.67438889\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  39.94335938\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  39.48150253\n",
      "################################  45  ################################\n",
      "Training Loss:  39.2844429\n",
      "################################  50  ################################\n",
      "Training Loss:  39.26644516\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  39.26637268\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  39.26633072\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  39.26633072\n",
      "################################  70  ################################\n",
      "Training Loss:  39.26633072\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  39.26633072\n",
      "################################  80  ################################\n",
      "Training Loss:  39.26633072\n",
      "################################  85  ################################\n",
      "Training Loss:  39.26633072\n",
      "################################  90  ################################\n",
      "Training Loss:  39.26633072\n",
      "################################  95  ################################\n",
      "Training Loss:  39.26633072\n",
      "Final training Loss:  39.26633072\n",
      "\n",
      "Running model (trial=9, mod=239, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  42.04783249\n",
      "################################  5  ################################\n",
      "Training Loss:  38.95449448\n",
      "################################  10  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=240, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  95.48692322\n",
      "################################  5  ################################\n",
      "Training Loss:  405.76473999\n",
      "################################  10  ################################\n",
      "Training Loss:  4814.96484375\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  47.47754288\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  44.93033981\n",
      "################################  25  ################################\n",
      "Training Loss:  44.58513641\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  44.55475235\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  44.57771683\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  44.57780075\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  44.57785034\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  44.57782364\n",
      "################################  55  ################################\n",
      "Training Loss:  44.57782364\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  44.57782364\n",
      "################################  65  ################################\n",
      "Training Loss:  44.57782364\n",
      "################################  70  ################################\n",
      "Training Loss:  44.57782364\n",
      "################################  75  ################################\n",
      "Training Loss:  44.57782364\n",
      "################################  80  ################################\n",
      "Training Loss:  44.57782364\n",
      "################################  85  ################################\n",
      "Training Loss:  44.57782364\n",
      "################################  90  ################################\n",
      "Training Loss:  44.57782364\n",
      "################################  95  ################################\n",
      "Training Loss:  44.57782364\n",
      "Final training Loss:  44.57782364\n",
      "\n",
      "Running model (trial=9, mod=241, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  103.09597015\n",
      "################################  5  ################################\n",
      "Training Loss:  102.29682922\n",
      "################################  10  ################################\n",
      "Training Loss:  101.82074738\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  101.64477539\n",
      "################################  20  ################################\n",
      "Training Loss:  101.67158508\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  101.79426575\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  101.7060318\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  101.7060318\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  101.7060318\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  101.7060318\n",
      "################################  50  ################################\n",
      "Training Loss:  101.7060318\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  101.7060318\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  101.7060318\n",
      "################################  65  ################################\n",
      "Training Loss:  101.7060318\n",
      "################################  70  ################################\n",
      "Training Loss:  101.7060318\n",
      "################################  75  ################################\n",
      "Training Loss:  101.7060318\n",
      "################################  80  ################################\n",
      "Training Loss:  101.7060318\n",
      "################################  85  ################################\n",
      "Training Loss:  101.7060318\n",
      "################################  90  ################################\n",
      "Training Loss:  101.7060318\n",
      "################################  95  ################################\n",
      "Training Loss:  101.7060318\n",
      "Final training Loss:  101.7060318\n",
      "\n",
      "Running model (trial=9, mod=242, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  120.92960358\n",
      "################################  5  ################################\n",
      "Training Loss:  103.39810181\n",
      "################################  10  ################################\n",
      "Training Loss:  43.42802429\n",
      "################################  15  ################################\n",
      "Training Loss:  41.99854279\n",
      "################################  20  ################################\n",
      "Training Loss:  37.663414\n",
      "################################  25  ################################\n",
      "Training Loss:  36.97782516\n",
      "################################  30  ################################\n",
      "Training Loss:  37.20160675\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  37.33355713\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  36.92363358\n",
      "################################  45  ################################\n",
      "Training Loss:  36.8313179\n",
      "################################  50  ################################\n",
      "Training Loss:  36.39485931\n",
      "################################  55  ################################\n",
      "Training Loss:  36.18639374\n",
      "################################  60  ################################\n",
      "Training Loss:  36.22064972\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.34001541\n",
      "################################  70  ################################\n",
      "Training Loss:  36.53189087\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  36.2582283\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.2580986\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  36.25815201\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.29943466\n",
      "################################  95  ################################\n",
      "Training Loss:  36.29943466\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  36.29943466\n",
      "\n",
      "Running model (trial=9, mod=243, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  38.91223145\n",
      "################################  5  ################################\n",
      "Training Loss:  37.00082779\n",
      "################################  10  ################################\n",
      "Training Loss:  37.07962799\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  36.71873474\n",
      "################################  20  ################################\n",
      "Training Loss:  36.74542236\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  36.73249054\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  36.89243698\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  37.22375107\n",
      "################################  40  ################################\n",
      "Training Loss:  37.2069664\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.21351242\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.21351242\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.21351242\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.21351242\n",
      "################################  65  ################################\n",
      "Training Loss:  37.21351242\n",
      "################################  70  ################################\n",
      "Training Loss:  37.21351242\n",
      "################################  75  ################################\n",
      "Training Loss:  37.21351242\n",
      "################################  80  ################################\n",
      "Training Loss:  37.21351242\n",
      "################################  85  ################################\n",
      "Training Loss:  37.21351242\n",
      "################################  90  ################################\n",
      "Training Loss:  37.21351242\n",
      "################################  95  ################################\n",
      "Training Loss:  37.21351242\n",
      "Final training Loss:  37.21351242\n",
      "\n",
      "Running model (trial=9, mod=244, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  118.02610016\n",
      "################################  5  ################################\n",
      "Training Loss:  39.15696716\n",
      "################################  10  ################################\n",
      "Training Loss:  38.53687286\n",
      "################################  15  ################################\n",
      "Training Loss:  38.25561523\n",
      "################################  20  ################################\n",
      "Training Loss:  38.0873909\n",
      "################################  25  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=245, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  59.18651962\n",
      "################################  5  ################################\n",
      "Training Loss:  53.88795853\n",
      "################################  10  ################################\n",
      "Training Loss:  48.06642151\n",
      "################################  15  ################################\n",
      "Training Loss:  40.59572983\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=246, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2219.06835938\n",
      "################################  5  ################################\n",
      "Training Loss:  2219.06811523\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  2219.06811523\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  2219.05224609\n",
      "################################  20  ################################\n",
      "Training Loss:  2218.83349609\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  2218.8137207\n",
      "################################  30  ################################\n",
      "Training Loss:  2192.64257812\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  2192.56176758\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2192.56176758\n",
      "################################  45  ################################\n",
      "Training Loss:  2192.56176758\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  2192.56176758\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  2192.56176758\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  2192.56176758\n",
      "################################  65  ################################\n",
      "Training Loss:  2192.56176758\n",
      "################################  70  ################################\n",
      "Training Loss:  2192.56176758\n",
      "################################  75  ################################\n",
      "Training Loss:  2192.56176758\n",
      "################################  80  ################################\n",
      "Training Loss:  2192.56176758\n",
      "################################  85  ################################\n",
      "Training Loss:  2192.56176758\n",
      "################################  90  ################################\n",
      "Training Loss:  2192.56176758\n",
      "################################  95  ################################\n",
      "Training Loss:  2192.56176758\n",
      "Final training Loss:  2192.56176758\n",
      "\n",
      "Running model (trial=9, mod=247, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  122.28230286\n",
      "################################  5  ################################\n",
      "Training Loss:  53.04566193\n",
      "################################  10  ################################\n",
      "Training Loss:  52.18302536\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  52.24289322\n",
      "################################  20  ################################\n",
      "Training Loss:  52.42560577\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  52.79189682\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  52.58335876\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  52.54965591\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  52.54959106\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  52.54961395\n",
      "################################  50  ################################\n",
      "Training Loss:  52.54961395\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  52.54961395\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  52.54961395\n",
      "################################  65  ################################\n",
      "Training Loss:  52.54961395\n",
      "################################  70  ################################\n",
      "Training Loss:  52.54961395\n",
      "################################  75  ################################\n",
      "Training Loss:  52.54961395\n",
      "################################  80  ################################\n",
      "Training Loss:  52.54961395\n",
      "################################  85  ################################\n",
      "Training Loss:  52.54961395\n",
      "################################  90  ################################\n",
      "Training Loss:  52.54961395\n",
      "################################  95  ################################\n",
      "Training Loss:  52.54961395\n",
      "Final training Loss:  52.54961395\n",
      "\n",
      "Running model (trial=9, mod=248, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.76803207\n",
      "################################  5  ################################\n",
      "Training Loss:  51.65500259\n",
      "################################  10  ################################\n",
      "Training Loss:  52.89024353\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  51.60952759\n",
      "################################  20  ################################\n",
      "Training Loss:  47.95401764\n",
      "################################  25  ################################\n",
      "Training Loss:  278.68673706\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  48.16495132\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  48.1835556\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  47.86459732\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  47.86461258\n",
      "################################  50  ################################\n",
      "Training Loss:  47.86461258\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  47.86461258\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  47.86462021\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  47.86462021\n",
      "################################  70  ################################\n",
      "Training Loss:  47.86462021\n",
      "################################  75  ################################\n",
      "Training Loss:  47.86462021\n",
      "################################  80  ################################\n",
      "Training Loss:  47.86462021\n",
      "################################  85  ################################\n",
      "Training Loss:  47.86462021\n",
      "################################  90  ################################\n",
      "Training Loss:  47.86462021\n",
      "################################  95  ################################\n",
      "Training Loss:  47.86462021\n",
      "Final training Loss:  47.86462021\n",
      "\n",
      "Running model (trial=9, mod=249, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.0796051\n",
      "################################  5  ################################\n",
      "Training Loss:  50.93995285\n",
      "################################  10  ################################\n",
      "Training Loss:  259.08972168\n",
      "################################  15  ################################\n",
      "Training Loss:  37.60664368\n",
      "################################  20  ################################\n",
      "Training Loss:  38.38892746\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.06711578\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.40325165\n",
      "################################  35  ################################\n",
      "Training Loss:  37.220047\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.22199249\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.33974457\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  37.33987808\n",
      "################################  55  ################################\n",
      "Training Loss:  37.33991241\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.33991623\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.33988953\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.33988953\n",
      "################################  75  ################################\n",
      "Training Loss:  37.33988953\n",
      "################################  80  ################################\n",
      "Training Loss:  37.33988953\n",
      "################################  85  ################################\n",
      "Training Loss:  37.33988953\n",
      "################################  90  ################################\n",
      "Training Loss:  37.33988953\n",
      "################################  95  ################################\n",
      "Training Loss:  37.33988953\n",
      "Final training Loss:  37.33988953\n",
      "\n",
      "Running model (trial=9, mod=250, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53.01965714\n",
      "################################  5  ################################\n",
      "Training Loss:  47.54860687\n",
      "################################  10  ################################\n",
      "Training Loss:  46.34097672\n",
      "################################  15  ################################\n",
      "Training Loss:  221.20986938\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  2236.64624023\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  38.84812164\n",
      "################################  30  ################################\n",
      "Training Loss:  37.81132889\n",
      "################################  35  ################################\n",
      "Training Loss:  37.58823013\n",
      "################################  40  ################################\n",
      "Training Loss:  42.41683578\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  37.63201523\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  37.6341362\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.6341362\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.6341362\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  37.6341362\n",
      "################################  70  ################################\n",
      "Training Loss:  37.6341362\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  37.6341362\n",
      "################################  80  ################################\n",
      "Training Loss:  37.6341362\n",
      "################################  85  ################################\n",
      "Training Loss:  37.6341362\n",
      "################################  90  ################################\n",
      "Training Loss:  37.6341362\n",
      "################################  95  ################################\n",
      "Training Loss:  37.6341362\n",
      "Final training Loss:  37.6341362\n",
      "\n",
      "Running model (trial=9, mod=251, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.1634903\n",
      "################################  5  ################################\n",
      "Training Loss:  49.59754562\n",
      "################################  10  ################################\n",
      "Training Loss:  262.85470581\n",
      "################################  15  ################################\n",
      "Training Loss:  41.02293777\n",
      "################################  20  ################################\n",
      "Training Loss:  115.19978333\n",
      "################################  25  ################################\n",
      "Training Loss:  41.87733841\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  231.48957825\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.95085907\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  39.12773514\n",
      "################################  45  ################################\n",
      "Training Loss:  38.44191742\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.5435524\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  38.5435524\n",
      "################################  60  ################################\n",
      "Training Loss:  38.54354477\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.54354858\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.54354858\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  38.54354858\n",
      "################################  80  ################################\n",
      "Training Loss:  38.54354858\n",
      "################################  85  ################################\n",
      "Training Loss:  38.54354858\n",
      "################################  90  ################################\n",
      "Training Loss:  38.54354858\n",
      "################################  95  ################################\n",
      "Training Loss:  38.54354858\n",
      "Final training Loss:  38.54354858\n",
      "\n",
      "Running model (trial=9, mod=252, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.84812546\n",
      "################################  5  ################################\n",
      "Training Loss:  49.6486969\n",
      "################################  10  ################################\n",
      "Training Loss:  49.66815567\n",
      "################################  15  ################################\n",
      "Training Loss:  47.8535881\n",
      "################################  20  ################################\n",
      "Training Loss:  46.92423248\n",
      "################################  25  ################################\n",
      "Training Loss:  39.06415558\n",
      "################################  30  ################################\n",
      "Training Loss:  39.07774353\n",
      "################################  35  ################################\n",
      "Training Loss:  38.74165344\n",
      "################################  40  ################################\n",
      "Training Loss:  38.60175705\n",
      "################################  45  ################################\n",
      "Training Loss:  38.14579391\n",
      "################################  50  ################################\n",
      "Training Loss:  38.76764679\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.67381287\n",
      "################################  60  ################################\n",
      "Training Loss:  42.57269669\n",
      "################################  65  ################################\n",
      "Training Loss:  35.9690361\n",
      "################################  70  ################################\n",
      "Training Loss:  35.89331055\n",
      "################################  75  ################################\n",
      "Training Loss:  35.61666489\n",
      "################################  80  ################################\n",
      "Training Loss:  36.58778\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  85  ################################\n",
      "Training Loss:  35.98871994\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  90  ################################\n",
      "Training Loss:  35.98534775\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  95  ################################\n",
      "Training Loss:  35.98534775\n",
      "Epoch   100: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Final training Loss:  35.98534775\n",
      "\n",
      "Running model (trial=9, mod=253, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.31875229\n",
      "################################  5  ################################\n",
      "Training Loss:  50.3218689\n",
      "################################  10  ################################\n",
      "Training Loss:  1314.9597168\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  39.4260025\n",
      "################################  20  ################################\n",
      "Training Loss:  38.07683563\n",
      "################################  25  ################################\n",
      "Training Loss:  343.08837891\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  37.83682632\n",
      "################################  35  ################################\n",
      "Training Loss:  42.19948196\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.22539139\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.16381836\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  38.16368103\n",
      "################################  55  ################################\n",
      "Training Loss:  38.16362762\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.16361618\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.16363525\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  38.16363525\n",
      "################################  75  ################################\n",
      "Training Loss:  38.16363525\n",
      "################################  80  ################################\n",
      "Training Loss:  38.16363525\n",
      "################################  85  ################################\n",
      "Training Loss:  38.16363525\n",
      "################################  90  ################################\n",
      "Training Loss:  38.16363525\n",
      "################################  95  ################################\n",
      "Training Loss:  38.16363525\n",
      "Final training Loss:  38.16363525\n",
      "\n",
      "Running model (trial=9, mod=254, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  52.70641327\n",
      "################################  5  ################################\n",
      "Training Loss:  39.46182632\n",
      "################################  10  ################################\n",
      "Training Loss:  1326.2064209\n",
      "################################  15  ################################\n",
      "Training Loss:  36.50475693\n",
      "################################  20  ################################\n",
      "Training Loss:  161.14994812\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  163.10160828\n",
      "################################  30  ################################\n",
      "Training Loss:  425.96032715\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  36.04850006\n",
      "################################  40  ################################\n",
      "Training Loss:  35.9126091\n",
      "################################  45  ################################\n",
      "Training Loss:  36.89227295\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  36.55657196\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  36.54606247\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  36.54600525\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  36.54600525\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  36.54600525\n",
      "################################  75  ################################\n",
      "Training Loss:  36.54600525\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  36.54600525\n",
      "################################  85  ################################\n",
      "Training Loss:  36.54600525\n",
      "################################  90  ################################\n",
      "Training Loss:  36.54600525\n",
      "################################  95  ################################\n",
      "Training Loss:  36.54600525\n",
      "Final training Loss:  36.54600525\n",
      "\n",
      "Running model (trial=9, mod=255, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  120.84196472\n",
      "################################  5  ################################\n",
      "Training Loss:  50.06867599\n",
      "################################  10  ################################\n",
      "Training Loss:  40.84886932\n",
      "################################  15  ################################\n",
      "Training Loss:  523.36755371\n",
      "################################  20  ################################\n",
      "Training Loss:  39.27518463\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  193.52967834\n",
      "################################  30  ################################\n",
      "Training Loss:  408.94955444\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  54.55866241\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  37.8197937\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  37.54997635\n",
      "################################  50  ################################\n",
      "Training Loss:  37.54997635\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  37.55002975\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  37.55003357\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  37.55003357\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  37.55003357\n",
      "################################  75  ################################\n",
      "Training Loss:  37.55003357\n",
      "################################  80  ################################\n",
      "Training Loss:  37.55003357\n",
      "################################  85  ################################\n",
      "Training Loss:  37.55003357\n",
      "################################  90  ################################\n",
      "Training Loss:  37.55003357\n",
      "################################  95  ################################\n",
      "Training Loss:  37.55003357\n",
      "Final training Loss:  37.55003357\n",
      "\n",
      "Running model (trial=9, mod=256, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.65621567\n",
      "################################  5  ################################\n",
      "Training Loss:  38.83417511\n",
      "################################  10  ################################\n",
      "Training Loss:  38.34867477\n",
      "################################  15  ################################\n",
      "Training Loss:  37.96251297\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  38.73279572\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  38.74076843\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.22362518\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  38.15703964\n",
      "################################  40  ################################\n",
      "Training Loss:  38.15703964\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.15704346\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  38.15704346\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  38.15704346\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.15704346\n",
      "################################  65  ################################\n",
      "Training Loss:  38.15704346\n",
      "################################  70  ################################\n",
      "Training Loss:  38.15704346\n",
      "################################  75  ################################\n",
      "Training Loss:  38.15704346\n",
      "################################  80  ################################\n",
      "Training Loss:  38.15704346\n",
      "################################  85  ################################\n",
      "Training Loss:  38.15704346\n",
      "################################  90  ################################\n",
      "Training Loss:  38.15704346\n",
      "################################  95  ################################\n",
      "Training Loss:  38.15704346\n",
      "Final training Loss:  38.15704346\n",
      "\n",
      "Running model (trial=9, mod=257, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.46209717\n",
      "################################  5  ################################\n",
      "Training Loss:  47.52607727\n",
      "################################  10  ################################\n",
      "Training Loss:  41.01890182\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  38.11168289\n",
      "################################  20  ################################\n",
      "Training Loss:  37.72060776\n",
      "################################  25  ################################\n",
      "Training Loss:  39.30170059\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  38.51815033\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  38.36196136\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  38.36231995\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  38.36287308\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  38.36287308\n",
      "################################  55  ################################\n",
      "Training Loss:  38.36287308\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  38.36287308\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  38.36287308\n",
      "################################  70  ################################\n",
      "Training Loss:  38.36287308\n",
      "################################  75  ################################\n",
      "Training Loss:  38.36287308\n",
      "################################  80  ################################\n",
      "Training Loss:  38.36287308\n",
      "################################  85  ################################\n",
      "Training Loss:  38.36287308\n",
      "################################  90  ################################\n",
      "Training Loss:  38.36287308\n",
      "################################  95  ################################\n",
      "Training Loss:  38.36287308\n",
      "Final training Loss:  38.36287308\n",
      "\n",
      "Running model (trial=9, mod=258, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.79156494\n",
      "################################  5  ################################\n",
      "Training Loss:  36.51477814\n",
      "################################  10  ################################\n",
      "Training Loss:  36.48775482\n",
      "################################  15  ################################\n",
      "Training Loss:  35.75630188\n",
      "################################  20  ################################\n",
      "Training Loss:  35.64892578\n",
      "################################  25  ################################\n",
      "Training Loss:  34.05614471\n",
      "################################  30  ################################\n",
      "Training Loss:  69.97558594\n",
      "################################  35  ################################\n",
      "Training Loss:  34.05840302\n",
      "################################  40  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=259, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x123f2d5e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1248bee50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x1248be9d0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  112.6740799\n",
      "################################  5  ################################\n",
      "Training Loss:  112.33018494\n",
      "################################  10  ################################\n",
      "Training Loss:  111.97214508\n",
      "################################  15  ################################\n",
      "Training Loss:  111.96648407\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  111.98022461\n",
      "################################  25  ################################\n",
      "Training Loss:  111.59457397\n",
      "################################  30  ################################\n",
      "Training Loss:  112.03570557\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  111.64817047\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  111.6063385\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  111.6063385\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  111.60635376\n",
      "################################  55  ################################\n",
      "Training Loss:  111.60633087\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  111.60633087\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  111.60633087\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  111.60633087\n",
      "################################  75  ################################\n",
      "Training Loss:  111.60633087\n",
      "################################  80  ################################\n",
      "Training Loss:  111.60633087\n",
      "################################  85  ################################\n",
      "Training Loss:  111.60633087\n",
      "################################  90  ################################\n",
      "Training Loss:  111.60633087\n",
      "################################  95  ################################\n",
      "Training Loss:  111.60633087\n",
      "Final training Loss:  111.60633087\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [],
   "source": [
    "# plotting\n",
    "disc_points = x_train.detach()\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": disc_points,\n",
    "    \"x_train\": disc_points,\n",
    "    \"y_train\": I1_new,\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"DP solution\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: invalid value encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "activation = np.vectorize(lambda model: model.activation)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: penalty_free_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "loss_array -= d_01 ** 2\n",
    "# make data frame\n",
    "# optims = [\"line search\", \"ADAM\", \"LBFGS\"]*(len(loss_array)//3)\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type,\n",
    "     \"activation\": activation})\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T05:02:07.381475</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m802cd52cbd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"178.481387\" xlink:href=\"#m802cd52cbd\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(169.681387 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"346.993739\" xlink:href=\"#m802cd52cbd\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(338.193739 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m5e446dae41\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"60.696307\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"90.369859\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"111.42358\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"127.754114\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"141.097132\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"152.378493\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"162.150852\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"170.770684\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"229.208659\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"258.882212\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"279.935932\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"296.266466\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"309.609484\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"320.890845\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"330.663205\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"339.283037\" xlink:href=\"#m5e446dae41\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Neurons -->\n     <g transform=\"translate(192.116406 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"199.707031\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"238.570312\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"299.751953\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"363.130859\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_19\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mf3a4a1e35f\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#mf3a4a1e35f\" y=\"146.50068\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 150.299899)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"md4bf934b81\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"217.024754\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"199.850083\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"185.817354\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"173.952852\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"163.675352\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"154.609955\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"93.151278\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"61.943878\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"39.801876\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"22.627204\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#md4bf934b81\" y=\"8.594476\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- $\\Delta E$ -->\n     <g transform=\"translate(14.798438 122.52)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 906 525 \nL 3472 525 \nL 2188 4044 \nz\nM 50 0 \nL 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 50 0 \nz\n\" id=\"DejaVuSans-394\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-394\"/>\n      <use transform=\"translate(68.408203 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 60.696307 56.438139 \nL 60.696307 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 111.42358 132.53856 \nL 111.42358 53.164798 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 162.150852 192.352969 \nL 162.150852 115.977299 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 212.878125 201.22383 \nL 212.878125 69.428472 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 263.605398 214.756364 \nL 263.605398 201.507859 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 314.33267 206.88974 \nL 314.33267 191.159421 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 365.059943 154.338324 \nL 365.059943 123.254106 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 60.696307 164.441781 \nL 60.696307 77.335545 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 111.42358 197.431376 \nL 111.42358 72.538066 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 162.150852 202.331089 \nL 162.150852 80.025164 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 212.878125 208.527372 \nL 212.878125 117.61777 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 263.605398 178.648844 \nL 263.605398 97.017746 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 314.33267 201.027244 \nL 314.33267 174.577044 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 365.059943 203.098394 \nL 365.059943 190.968071 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_31\">\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 60.696307 30.081389 \nL 111.42358 82.811008 \nL 162.150852 145.899091 \nL 212.878125 110.045243 \nL 263.605398 208.458634 \nL 314.33267 198.806638 \nL 365.059943 137.459689 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#p77bbff4d65)\" d=\"M 60.696307 111.772968 \nL 111.42358 113.532473 \nL 162.150852 119.10994 \nL 212.878125 150.810521 \nL 263.605398 128.060369 \nL 314.33267 186.671471 \nL 365.059943 196.94323 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_33\"/>\n   <g id=\"line2d_34\"/>\n   <g id=\"line2d_35\"/>\n   <g id=\"line2d_36\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 318.554688 59.234375 \nL 373.278125 59.234375 \nQ 375.278125 59.234375 375.278125 57.234375 \nL 375.278125 14.2 \nQ 375.278125 12.2 373.278125 12.2 \nL 318.554688 12.2 \nQ 316.554688 12.2 316.554688 14.2 \nL 316.554688 57.234375 \nQ 316.554688 59.234375 318.554688 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_6\">\n     <!-- activation -->\n     <g transform=\"translate(321.153125 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"155.46875\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"183.251953\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"242.431641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"303.710938\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"342.919922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"370.703125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"431.884766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_37\">\n     <path d=\"M 320.554688 34.976562 \nL 340.554688 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_38\"/>\n    <g id=\"text_7\">\n     <!-- relu -->\n     <g transform=\"translate(348.554688 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"38.863281\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"100.386719\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"128.169922\" xlink:href=\"#DejaVuSans-75\"/>\n     </g>\n    </g>\n    <g id=\"line2d_39\">\n     <path d=\"M 320.554688 49.654687 \nL 340.554688 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_40\"/>\n    <g id=\"text_8\">\n     <!-- tanh -->\n     <g transform=\"translate(348.554688 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"100.488281\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"163.867188\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p77bbff4d65\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"activation\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "32 160\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 10]), len(d_results_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T05:02:09.540394</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"medf9c256b3\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.696307\" xlink:href=\"#medf9c256b3\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(51.896307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"262.91113\" xlink:href=\"#medf9c256b3\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(254.11113 239.238437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m49561d1be4\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.443464\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"121.569034\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"157.177297\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"182.441761\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"202.038402\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"218.050024\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"231.587657\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"243.314489\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"253.658287\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"323.783857\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"359.39212\" xlink:href=\"#m49561d1be4\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Hidden layers -->\n     <g transform=\"translate(178.396875 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 2753 \nL 3553 2753 \nL 3553 4666 \nL 4184 4666 \nL 4184 0 \nL 3553 0 \nL 3553 2222 \nL 1259 2222 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-48\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"75.195312\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"102.978516\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"166.455078\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"229.931641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"291.455078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"354.833984\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"386.621094\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"414.404297\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"475.683594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"534.863281\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"596.386719\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"637.5\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m67b57e50ab\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m67b57e50ab\" y=\"146.339238\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 150.138457)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m3ba4750e72\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"208.295186\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"193.207104\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"180.879242\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"170.456184\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"161.427321\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"153.463299\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"99.471372\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"72.055428\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"52.603507\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"37.515424\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"25.187563\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m3ba4750e72\" y=\"14.764505\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- $\\Delta E$ -->\n     <g transform=\"translate(14.798438 122.52)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 906 525 \nL 3472 525 \nL 2188 4044 \nz\nM 50 0 \nL 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 50 0 \nz\n\" id=\"DejaVuSans-394\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-394\"/>\n      <use transform=\"translate(68.408203 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p913b627029)\" d=\"M 60.696307 114.835693 \nL 60.696307 52.183635 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 121.569034 185.812867 \nL 121.569034 120.044928 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 182.441761 170.046269 \nL 182.441761 74.240496 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 243.314489 192.90496 \nL 243.314489 95.277909 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 304.187216 106.671416 \nL 304.187216 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 365.059943 52.454716 \nL 365.059943 23.127005 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p913b627029)\" d=\"M 60.696307 214.756364 \nL 60.696307 200.356826 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 121.569034 195.609696 \nL 121.569034 89.810611 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 182.441761 184.354045 \nL 182.441761 58.126443 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 243.314489 193.742948 \nL 243.314489 90.076467 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 304.187216 121.195565 \nL 304.187216 36.291969 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p913b627029)\" d=\"M 365.059943 108.872582 \nL 365.059943 38.880261 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_27\">\n    <path clip-path=\"url(#p913b627029)\" d=\"M 60.696307 76.541834 \nL 121.569034 145.810737 \nL 182.441761 106.925757 \nL 243.314489 127.425638 \nL 304.187216 48.341734 \nL 365.059943 36.892383 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_28\">\n    <path clip-path=\"url(#p913b627029)\" d=\"M 60.696307 207.598671 \nL 121.569034 122.276259 \nL 182.441761 94.817879 \nL 243.314489 124.318979 \nL 304.187216 66.409695 \nL 365.059943 65.874515 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_29\"/>\n   <g id=\"line2d_30\"/>\n   <g id=\"line2d_31\"/>\n   <g id=\"line2d_32\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 52.478125 59.234375 \nL 107.201563 59.234375 \nQ 109.201563 59.234375 109.201563 57.234375 \nL 109.201563 14.2 \nQ 109.201563 12.2 107.201563 12.2 \nL 52.478125 12.2 \nQ 50.478125 12.2 50.478125 14.2 \nL 50.478125 57.234375 \nQ 50.478125 59.234375 52.478125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_6\">\n     <!-- activation -->\n     <g transform=\"translate(55.076563 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"155.46875\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"183.251953\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"242.431641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"303.710938\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"342.919922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"370.703125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"431.884766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_33\">\n     <path d=\"M 54.478125 34.976562 \nL 74.478125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_34\"/>\n    <g id=\"text_7\">\n     <!-- relu -->\n     <g transform=\"translate(82.478125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"38.863281\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"100.386719\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"128.169922\" xlink:href=\"#DejaVuSans-75\"/>\n     </g>\n    </g>\n    <g id=\"line2d_35\">\n     <path d=\"M 54.478125 49.654687 \nL 74.478125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_36\"/>\n    <g id=\"text_8\">\n     <!-- tanh -->\n     <g transform=\"translate(82.478125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"100.488281\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"163.867188\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p913b627029\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"activation\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Hidden layers\", ylabel=\"Error\")\n",
    "fig_layers.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "35 140\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 10]), len(d_results_layer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 399.036323 262.19625\" width=\"399.036323pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T05:02:10.357545</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 399.036323 262.19625 \nL 399.036323 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 107.878623 \nC 61.852451 107.878623 62.615581 107.562525 63.178162 106.999944 \nC 63.740743 106.437363 64.056842 105.674233 64.056842 104.878623 \nC 64.056842 104.083014 63.740743 103.319884 63.178162 102.757303 \nC 62.615581 102.194722 61.852451 101.878623 61.056842 101.878623 \nC 60.261233 101.878623 59.498102 102.194722 58.935521 102.757303 \nC 58.372941 103.319884 58.056842 104.083014 58.056842 104.878623 \nC 58.056842 105.674233 58.372941 106.437363 58.935521 106.999944 \nC 59.498102 107.562525 60.261233 107.878623 61.056842 107.878623 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 171.69561 \nC 63.006163 171.69561 63.769293 171.379511 64.331874 170.81693 \nC 64.894455 170.254349 65.210554 169.491219 65.210554 168.69561 \nC 65.210554 167.9 64.894455 167.13687 64.331874 166.574289 \nC 63.769293 166.011709 63.006163 165.69561 62.210554 165.69561 \nC 61.414944 165.69561 60.651814 166.011709 60.089233 166.574289 \nC 59.526653 167.13687 59.210554 167.9 59.210554 168.69561 \nC 59.210554 169.491219 59.526653 170.254349 60.089233 170.81693 \nC 60.651814 171.379511 61.414944 171.69561 62.210554 171.69561 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 148.629899 \nC 67.044155 148.629899 67.807285 148.3138 68.369866 147.75122 \nC 68.932447 147.188639 69.248546 146.425509 69.248546 145.629899 \nC 69.248546 144.83429 68.932447 144.07116 68.369866 143.508579 \nC 67.807285 142.945998 67.044155 142.629899 66.248546 142.629899 \nC 65.452936 142.629899 64.689806 142.945998 64.127225 143.508579 \nC 63.564645 144.07116 63.248546 144.83429 63.248546 145.629899 \nC 63.248546 146.425509 63.564645 147.188639 64.127225 147.75122 \nC 64.689806 148.3138 65.452936 148.629899 66.248546 148.629899 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 172.150797 \nC 82.04241 172.150797 82.805541 171.834698 83.368121 171.272117 \nC 83.930702 170.709536 84.246801 169.946406 84.246801 169.150797 \nC 84.246801 168.355187 83.930702 167.592057 83.368121 167.029476 \nC 82.805541 166.466896 82.04241 166.150797 81.246801 166.150797 \nC 80.451192 166.150797 79.688061 166.466896 79.125481 167.029476 \nC 78.5629 167.592057 78.246801 168.355187 78.246801 169.150797 \nC 78.246801 169.946406 78.5629 170.709536 79.125481 171.272117 \nC 79.688061 171.834698 80.451192 172.150797 81.246801 172.150797 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 157.685601 \nC 139.728008 157.685601 140.491139 157.369502 141.053719 156.806921 \nC 141.6163 156.244341 141.932399 155.48121 141.932399 154.685601 \nC 141.932399 153.889992 141.6163 153.126861 141.053719 152.564281 \nC 140.491139 152.0017 139.728008 151.685601 138.932399 151.685601 \nC 138.13679 151.685601 137.373659 152.0017 136.811079 152.564281 \nC 136.248498 153.126861 135.932399 153.889992 135.932399 154.685601 \nC 135.932399 155.48121 136.248498 156.244341 136.811079 156.806921 \nC 137.373659 157.369502 138.13679 157.685601 138.932399 157.685601 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 180.629605 \nC 61.708237 180.629605 62.471367 180.313507 63.033948 179.750926 \nC 63.596529 179.188345 63.912628 178.425215 63.912628 177.629605 \nC 63.912628 176.833996 63.596529 176.070866 63.033948 175.508285 \nC 62.471367 174.945704 61.708237 174.629605 60.912628 174.629605 \nC 60.117019 174.629605 59.353888 174.945704 58.791307 175.508285 \nC 58.228727 176.070866 57.912628 176.833996 57.912628 177.629605 \nC 57.912628 178.425215 58.228727 179.188345 58.791307 179.750926 \nC 59.353888 180.313507 60.117019 180.629605 60.912628 180.629605 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 109.258589 \nC 63.006163 109.258589 63.769293 108.94249 64.331874 108.379909 \nC 64.894455 107.817328 65.210554 107.054198 65.210554 106.258589 \nC 65.210554 105.462979 64.894455 104.699849 64.331874 104.137268 \nC 63.769293 103.574688 63.006163 103.258589 62.210554 103.258589 \nC 61.414944 103.258589 60.651814 103.574688 60.089233 104.137268 \nC 59.526653 104.699849 59.210554 105.462979 59.210554 106.258589 \nC 59.210554 107.054198 59.526653 107.817328 60.089233 108.379909 \nC 60.651814 108.94249 61.414944 109.258589 62.210554 109.258589 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 21.171625 \nC 65.602015 21.171625 66.365145 20.855526 66.927726 20.292945 \nC 67.490307 19.730364 67.806406 18.967234 67.806406 18.171625 \nC 67.806406 17.376015 67.490307 16.612885 66.927726 16.050304 \nC 66.365145 15.487724 65.602015 15.171625 64.806406 15.171625 \nC 64.010796 15.171625 63.247666 15.487724 62.685085 16.050304 \nC 62.122505 16.612885 61.806406 17.376015 61.806406 18.171625 \nC 61.806406 18.967234 62.122505 19.730364 62.685085 20.292945 \nC 63.247666 20.855526 64.010796 21.171625 64.806406 21.171625 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 25.969823 \nC 81.177126 25.969823 81.940257 25.653724 82.502837 25.091144 \nC 83.065418 24.528563 83.381517 23.765433 83.381517 22.969823 \nC 83.381517 22.174214 83.065418 21.411084 82.502837 20.848503 \nC 81.940257 20.285922 81.177126 19.969823 80.381517 19.969823 \nC 79.585908 19.969823 78.822778 20.285922 78.260197 20.848503 \nC 77.697616 21.411084 77.381517 22.174214 77.381517 22.969823 \nC 77.381517 23.765433 77.697616 24.528563 78.260197 25.091144 \nC 78.822778 25.653724 79.585908 25.969823 80.381517 25.969823 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 60.920083 \nC 101.943942 60.920083 102.707072 60.603984 103.269653 60.041403 \nC 103.832233 59.478823 104.148332 58.715692 104.148332 57.920083 \nC 104.148332 57.124474 103.832233 56.361343 103.269653 55.798763 \nC 102.707072 55.236182 101.943942 54.920083 101.148332 54.920083 \nC 100.352723 54.920083 99.589593 55.236182 99.027012 55.798763 \nC 98.464431 56.361343 98.148332 57.124474 98.148332 57.920083 \nC 98.148332 58.715692 98.464431 59.478823 99.027012 60.041403 \nC 99.589593 60.603984 100.352723 60.920083 101.148332 60.920083 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 165.887729 \nC 61.491916 165.887729 62.255046 165.57163 62.817627 165.009049 \nC 63.380208 164.446469 63.696307 163.683338 63.696307 162.887729 \nC 63.696307 162.09212 63.380208 161.32899 62.817627 160.766409 \nC 62.255046 160.203828 61.491916 159.887729 60.696307 159.887729 \nC 59.900698 159.887729 59.137567 160.203828 58.574986 160.766409 \nC 58.012406 161.32899 57.696307 162.09212 57.696307 162.887729 \nC 57.696307 163.683338 58.012406 164.446469 58.574986 165.009049 \nC 59.137567 165.57163 59.900698 165.887729 60.696307 165.887729 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 191.177772 \nC 61.852451 191.177772 62.615581 190.861673 63.178162 190.299093 \nC 63.740743 189.736512 64.056842 188.973382 64.056842 188.177772 \nC 64.056842 187.382163 63.740743 186.619033 63.178162 186.056452 \nC 62.615581 185.493871 61.852451 185.177772 61.056842 185.177772 \nC 60.261233 185.177772 59.498102 185.493871 58.935521 186.056452 \nC 58.372941 186.619033 58.056842 187.382163 58.056842 188.177772 \nC 58.056842 188.973382 58.372941 189.736512 58.935521 190.299093 \nC 59.498102 190.861673 60.261233 191.177772 61.056842 191.177772 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 176.300333 \nC 67.044155 176.300333 67.807285 175.984234 68.369866 175.421653 \nC 68.932447 174.859072 69.248546 174.095942 69.248546 173.300333 \nC 69.248546 172.504724 68.932447 171.741593 68.369866 171.179012 \nC 67.807285 170.616432 67.044155 170.300333 66.248546 170.300333 \nC 65.452936 170.300333 64.689806 170.616432 64.127225 171.179012 \nC 63.564645 171.741593 63.248546 172.504724 63.248546 173.300333 \nC 63.248546 174.095942 63.564645 174.859072 64.127225 175.421653 \nC 64.689806 175.984234 65.452936 176.300333 66.248546 176.300333 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 124.834104 \nC 82.04241 124.834104 82.805541 124.518005 83.368121 123.955425 \nC 83.930702 123.392844 84.246801 122.629714 84.246801 121.834104 \nC 84.246801 121.038495 83.930702 120.275365 83.368121 119.712784 \nC 82.805541 119.150203 82.04241 118.834104 81.246801 118.834104 \nC 80.451192 118.834104 79.688061 119.150203 79.125481 119.712784 \nC 78.5629 120.275365 78.246801 121.038495 78.246801 121.834104 \nC 78.246801 122.629714 78.5629 123.392844 79.125481 123.955425 \nC 79.688061 124.518005 80.451192 124.834104 81.246801 124.834104 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 155.965251 \nC 139.728008 155.965251 140.491139 155.649152 141.053719 155.086571 \nC 141.6163 154.52399 141.932399 153.76086 141.932399 152.965251 \nC 141.932399 152.169641 141.6163 151.406511 141.053719 150.84393 \nC 140.491139 150.28135 139.728008 149.965251 138.932399 149.965251 \nC 138.13679 149.965251 137.373659 150.28135 136.811079 150.84393 \nC 136.248498 151.406511 135.932399 152.169641 135.932399 152.965251 \nC 135.932399 153.76086 136.248498 154.52399 136.811079 155.086571 \nC 137.373659 155.649152 138.13679 155.965251 138.932399 155.965251 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 181.197254 \nC 365.855552 181.197254 366.618683 180.881156 367.181264 180.318575 \nC 367.743844 179.755994 368.059943 178.992864 368.059943 178.197254 \nC 368.059943 177.401645 367.743844 176.638515 367.181264 176.075934 \nC 366.618683 175.513353 365.855552 175.197254 365.059943 175.197254 \nC 364.264334 175.197254 363.501204 175.513353 362.938623 176.075934 \nC 362.376042 176.638515 362.059943 177.401645 362.059943 178.197254 \nC 362.059943 178.992864 362.376042 179.755994 362.938623 180.318575 \nC 363.501204 180.881156 364.264334 181.197254 365.059943 181.197254 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 36.502845 \nC 63.006163 36.502845 63.769293 36.186746 64.331874 35.624165 \nC 64.894455 35.061584 65.210554 34.298454 65.210554 33.502845 \nC 65.210554 32.707235 64.894455 31.944105 64.331874 31.381524 \nC 63.769293 30.818943 63.006163 30.502845 62.210554 30.502845 \nC 61.414944 30.502845 60.651814 30.818943 60.089233 31.381524 \nC 59.526653 31.944105 59.210554 32.707235 59.210554 33.502845 \nC 59.210554 34.298454 59.526653 35.061584 60.089233 35.624165 \nC 60.651814 36.186746 61.414944 36.502845 62.210554 36.502845 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 173.58205 \nC 65.602015 173.58205 66.365145 173.265951 66.927726 172.70337 \nC 67.490307 172.140789 67.806406 171.377659 67.806406 170.58205 \nC 67.806406 169.786441 67.490307 169.02331 66.927726 168.460729 \nC 66.365145 167.898149 65.602015 167.58205 64.806406 167.58205 \nC 64.010796 167.58205 63.247666 167.898149 62.685085 168.460729 \nC 62.122505 169.02331 61.806406 169.786441 61.806406 170.58205 \nC 61.806406 171.377659 62.122505 172.140789 62.685085 172.70337 \nC 63.247666 173.265951 64.010796 173.58205 64.806406 173.58205 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 169.416344 \nC 70.793719 169.416344 71.556849 169.100245 72.11943 168.537665 \nC 72.682011 167.975084 72.998109 167.211954 72.998109 166.416344 \nC 72.998109 165.620735 72.682011 164.857605 72.11943 164.295024 \nC 71.556849 163.732443 70.793719 163.416344 69.998109 163.416344 \nC 69.2025 163.416344 68.43937 163.732443 67.876789 164.295024 \nC 67.314208 164.857605 66.998109 165.620735 66.998109 166.416344 \nC 66.998109 167.211954 67.314208 167.975084 67.876789 168.537665 \nC 68.43937 169.100245 69.2025 169.416344 69.998109 169.416344 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 28.395797 \nC 81.177126 28.395797 81.940257 28.079699 82.502837 27.517118 \nC 83.065418 26.954537 83.381517 26.191407 83.381517 25.395797 \nC 83.381517 24.600188 83.065418 23.837058 82.502837 23.274477 \nC 81.940257 22.711896 81.177126 22.395797 80.381517 22.395797 \nC 79.585908 22.395797 78.822778 22.711896 78.260197 23.274477 \nC 77.697616 23.837058 77.381517 24.600188 77.381517 25.395797 \nC 77.381517 26.191407 77.697616 26.954537 78.260197 27.517118 \nC 78.822778 28.079699 79.585908 28.395797 80.381517 28.395797 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 99.899963 \nC 61.852451 99.899963 62.615581 99.583864 63.178162 99.021283 \nC 63.740743 98.458703 64.056842 97.695572 64.056842 96.899963 \nC 64.056842 96.104354 63.740743 95.341223 63.178162 94.778643 \nC 62.615581 94.216062 61.852451 93.899963 61.056842 93.899963 \nC 60.261233 93.899963 59.498102 94.216062 58.935521 94.778643 \nC 58.372941 95.341223 58.056842 96.104354 58.056842 96.899963 \nC 58.056842 97.695572 58.372941 98.458703 58.935521 99.021283 \nC 59.498102 99.583864 60.261233 99.899963 61.056842 99.899963 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 131.588024 \nC 67.044155 131.588024 67.807285 131.271925 68.369866 130.709344 \nC 68.932447 130.146763 69.248546 129.383633 69.248546 128.588024 \nC 69.248546 127.792415 68.932447 127.029284 68.369866 126.466704 \nC 67.807285 125.904123 67.044155 125.588024 66.248546 125.588024 \nC 65.452936 125.588024 64.689806 125.904123 64.127225 126.466704 \nC 63.564645 127.029284 63.248546 127.792415 63.248546 128.588024 \nC 63.248546 129.383633 63.564645 130.146763 64.127225 130.709344 \nC 64.689806 131.271925 65.452936 131.588024 66.248546 131.588024 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 183.43743 \nC 82.04241 183.43743 82.805541 183.121331 83.368121 182.55875 \nC 83.930702 181.996169 84.246801 181.233039 84.246801 180.43743 \nC 84.246801 179.64182 83.930702 178.87869 83.368121 178.316109 \nC 82.805541 177.753529 82.04241 177.43743 81.246801 177.43743 \nC 80.451192 177.43743 79.688061 177.753529 79.125481 178.316109 \nC 78.5629 178.87869 78.246801 179.64182 78.246801 180.43743 \nC 78.246801 181.233039 78.5629 181.996169 79.125481 182.55875 \nC 79.688061 183.121331 80.451192 183.43743 81.246801 183.43743 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 165.700113 \nC 139.728008 165.700113 140.491139 165.384014 141.053719 164.821433 \nC 141.6163 164.258853 141.932399 163.495722 141.932399 162.700113 \nC 141.932399 161.904504 141.6163 161.141373 141.053719 160.578793 \nC 140.491139 160.016212 139.728008 159.700113 138.932399 159.700113 \nC 138.13679 159.700113 137.373659 160.016212 136.811079 160.578793 \nC 136.248498 161.141373 135.932399 161.904504 135.932399 162.700113 \nC 135.932399 163.495722 136.248498 164.258853 136.811079 164.821433 \nC 137.373659 165.384014 138.13679 165.700113 138.932399 165.700113 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 192.446518 \nC 61.708237 192.446518 62.471367 192.13042 63.033948 191.567839 \nC 63.596529 191.005258 63.912628 190.242128 63.912628 189.446518 \nC 63.912628 188.650909 63.596529 187.887779 63.033948 187.325198 \nC 62.471367 186.762617 61.708237 186.446518 60.912628 186.446518 \nC 60.117019 186.446518 59.353888 186.762617 58.791307 187.325198 \nC 58.228727 187.887779 57.912628 188.650909 57.912628 189.446518 \nC 57.912628 190.242128 58.228727 191.005258 58.791307 191.567839 \nC 59.353888 192.13042 60.117019 192.446518 60.912628 192.446518 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 184.062656 \nC 63.006163 184.062656 63.769293 183.746557 64.331874 183.183976 \nC 64.894455 182.621395 65.210554 181.858265 65.210554 181.062656 \nC 65.210554 180.267046 64.894455 179.503916 64.331874 178.941335 \nC 63.769293 178.378755 63.006163 178.062656 62.210554 178.062656 \nC 61.414944 178.062656 60.651814 178.378755 60.089233 178.941335 \nC 59.526653 179.503916 59.210554 180.267046 59.210554 181.062656 \nC 59.210554 181.858265 59.526653 182.621395 60.089233 183.183976 \nC 60.651814 183.746557 61.414944 184.062656 62.210554 184.062656 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 113.684123 \nC 65.602015 113.684123 66.365145 113.368024 66.927726 112.805443 \nC 67.490307 112.242863 67.806406 111.479732 67.806406 110.684123 \nC 67.806406 109.888514 67.490307 109.125383 66.927726 108.562803 \nC 66.365145 108.000222 65.602015 107.684123 64.806406 107.684123 \nC 64.010796 107.684123 63.247666 108.000222 62.685085 108.562803 \nC 62.122505 109.125383 61.806406 109.888514 61.806406 110.684123 \nC 61.806406 111.479732 62.122505 112.242863 62.685085 112.805443 \nC 63.247666 113.368024 64.010796 113.684123 64.806406 113.684123 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 165.396767 \nC 70.793719 165.396767 71.556849 165.080668 72.11943 164.518087 \nC 72.682011 163.955507 72.998109 163.192376 72.998109 162.396767 \nC 72.998109 161.601158 72.682011 160.838027 72.11943 160.275447 \nC 71.556849 159.712866 70.793719 159.396767 69.998109 159.396767 \nC 69.2025 159.396767 68.43937 159.712866 67.876789 160.275447 \nC 67.314208 160.838027 66.998109 161.601158 66.998109 162.396767 \nC 66.998109 163.192376 67.314208 163.955507 67.876789 164.518087 \nC 68.43937 165.080668 69.2025 165.396767 69.998109 165.396767 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 26.039391 \nC 81.177126 26.039391 81.940257 25.723292 82.502837 25.160711 \nC 83.065418 24.598131 83.381517 23.835 83.381517 23.039391 \nC 83.381517 22.243782 83.065418 21.480651 82.502837 20.918071 \nC 81.940257 20.35549 81.177126 20.039391 80.381517 20.039391 \nC 79.585908 20.039391 78.822778 20.35549 78.260197 20.918071 \nC 77.697616 21.480651 77.381517 22.243782 77.381517 23.039391 \nC 77.381517 23.835 77.697616 24.598131 78.260197 25.160711 \nC 78.822778 25.723292 79.585908 26.039391 80.381517 26.039391 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 27.667878 \nC 61.491916 27.667878 62.255046 27.351779 62.817627 26.789198 \nC 63.380208 26.226617 63.696307 25.463487 63.696307 24.667878 \nC 63.696307 23.872268 63.380208 23.109138 62.817627 22.546557 \nC 62.255046 21.983977 61.491916 21.667878 60.696307 21.667878 \nC 59.900698 21.667878 59.137567 21.983977 58.574986 22.546557 \nC 58.012406 23.109138 57.696307 23.872268 57.696307 24.667878 \nC 57.696307 25.463487 58.012406 26.226617 58.574986 26.789198 \nC 59.137567 27.351779 59.900698 27.667878 60.696307 27.667878 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 26.991259 \nC 63.006163 26.991259 63.769293 26.67516 64.331874 26.112579 \nC 64.894455 25.549999 65.210554 24.786868 65.210554 23.991259 \nC 65.210554 23.19565 64.894455 22.432519 64.331874 21.869939 \nC 63.769293 21.307358 63.006163 20.991259 62.210554 20.991259 \nC 61.414944 20.991259 60.651814 21.307358 60.089233 21.869939 \nC 59.526653 22.432519 59.210554 23.19565 59.210554 23.991259 \nC 59.210554 24.786868 59.526653 25.549999 60.089233 26.112579 \nC 60.651814 26.67516 61.414944 26.991259 62.210554 26.991259 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 171.906935 \nC 67.044155 171.906935 67.807285 171.590836 68.369866 171.028255 \nC 68.932447 170.465674 69.248546 169.702544 69.248546 168.906935 \nC 69.248546 168.111325 68.932447 167.348195 68.369866 166.785614 \nC 67.807285 166.223034 67.044155 165.906935 66.248546 165.906935 \nC 65.452936 165.906935 64.689806 166.223034 64.127225 166.785614 \nC 63.564645 167.348195 63.248546 168.111325 63.248546 168.906935 \nC 63.248546 169.702544 63.564645 170.465674 64.127225 171.028255 \nC 64.689806 171.590836 65.452936 171.906935 66.248546 171.906935 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 138.510156 \nC 139.728008 138.510156 140.491139 138.194057 141.053719 137.631476 \nC 141.6163 137.068895 141.932399 136.305765 141.932399 135.510156 \nC 141.932399 134.714546 141.6163 133.951416 141.053719 133.388835 \nC 140.491139 132.826255 139.728008 132.510156 138.932399 132.510156 \nC 138.13679 132.510156 137.373659 132.826255 136.811079 133.388835 \nC 136.248498 133.951416 135.932399 134.714546 135.932399 135.510156 \nC 135.932399 136.305765 136.248498 137.068895 136.811079 137.631476 \nC 137.373659 138.194057 138.13679 138.510156 138.932399 138.510156 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 162.220798 \nC 365.855552 162.220798 366.618683 161.904699 367.181264 161.342118 \nC 367.743844 160.779537 368.059943 160.016407 368.059943 159.220798 \nC 368.059943 158.425188 367.743844 157.662058 367.181264 157.099477 \nC 366.618683 156.536897 365.855552 156.220798 365.059943 156.220798 \nC 364.264334 156.220798 363.501204 156.536897 362.938623 157.099477 \nC 362.376042 157.662058 362.059943 158.425188 362.059943 159.220798 \nC 362.059943 160.016407 362.376042 160.779537 362.938623 161.342118 \nC 363.501204 161.904699 364.264334 162.220798 365.059943 162.220798 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 178.750316 \nC 61.708237 178.750316 62.471367 178.434217 63.033948 177.871636 \nC 63.596529 177.309055 63.912628 176.545925 63.912628 175.750316 \nC 63.912628 174.954706 63.596529 174.191576 63.033948 173.628995 \nC 62.471367 173.066415 61.708237 172.750316 60.912628 172.750316 \nC 60.117019 172.750316 59.353888 173.066415 58.791307 173.628995 \nC 58.228727 174.191576 57.912628 174.954706 57.912628 175.750316 \nC 57.912628 176.545925 58.228727 177.309055 58.791307 177.871636 \nC 59.353888 178.434217 60.117019 178.750316 60.912628 178.750316 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 168.660164 \nC 63.006163 168.660164 63.769293 168.344066 64.331874 167.781485 \nC 64.894455 167.218904 65.210554 166.455774 65.210554 165.660164 \nC 65.210554 164.864555 64.894455 164.101425 64.331874 163.538844 \nC 63.769293 162.976263 63.006163 162.660164 62.210554 162.660164 \nC 61.414944 162.660164 60.651814 162.976263 60.089233 163.538844 \nC 59.526653 164.101425 59.210554 164.864555 59.210554 165.660164 \nC 59.210554 166.455774 59.526653 167.218904 60.089233 167.781485 \nC 60.651814 168.344066 61.414944 168.660164 62.210554 168.660164 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 210.578686 \nC 70.793719 210.578686 71.556849 210.262587 72.11943 209.700006 \nC 72.682011 209.137425 72.998109 208.374295 72.998109 207.578686 \nC 72.998109 206.783077 72.682011 206.019946 72.11943 205.457365 \nC 71.556849 204.894785 70.793719 204.578686 69.998109 204.578686 \nC 69.2025 204.578686 68.43937 204.894785 67.876789 205.457365 \nC 67.314208 206.019946 66.998109 206.783077 66.998109 207.578686 \nC 66.998109 208.374295 67.314208 209.137425 67.876789 209.700006 \nC 68.43937 210.262587 69.2025 210.578686 69.998109 210.578686 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 159.349353 \nC 81.177126 159.349353 81.940257 159.033254 82.502837 158.470673 \nC 83.065418 157.908092 83.381517 157.144962 83.381517 156.349353 \nC 83.381517 155.553743 83.065418 154.790613 82.502837 154.228032 \nC 81.940257 153.665452 81.177126 153.349353 80.381517 153.349353 \nC 79.585908 153.349353 78.822778 153.665452 78.260197 154.228032 \nC 77.697616 154.790613 77.381517 155.553743 77.381517 156.349353 \nC 77.381517 157.144962 77.697616 157.908092 78.260197 158.470673 \nC 78.822778 159.033254 79.585908 159.349353 80.381517 159.349353 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 185.721641 \nC 101.943942 185.721641 102.707072 185.405542 103.269653 184.842961 \nC 103.832233 184.28038 104.148332 183.51725 104.148332 182.721641 \nC 104.148332 181.926031 103.832233 181.162901 103.269653 180.60032 \nC 102.707072 180.03774 101.943942 179.721641 101.148332 179.721641 \nC 100.352723 179.721641 99.589593 180.03774 99.027012 180.60032 \nC 98.464431 181.162901 98.148332 181.926031 98.148332 182.721641 \nC 98.148332 183.51725 98.464431 184.28038 99.027012 184.842961 \nC 99.589593 185.405542 100.352723 185.721641 101.148332 185.721641 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 42.634798 \nC 61.491916 42.634798 62.255046 42.318699 62.817627 41.756118 \nC 63.380208 41.193538 63.696307 40.430407 63.696307 39.634798 \nC 63.696307 38.839189 63.380208 38.076059 62.817627 37.513478 \nC 62.255046 36.950897 61.491916 36.634798 60.696307 36.634798 \nC 59.900698 36.634798 59.137567 36.950897 58.574986 37.513478 \nC 58.012406 38.076059 57.696307 38.839189 57.696307 39.634798 \nC 57.696307 40.430407 58.012406 41.193538 58.574986 41.756118 \nC 59.137567 42.318699 59.900698 42.634798 60.696307 42.634798 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 110.077366 \nC 61.852451 110.077366 62.615581 109.761268 63.178162 109.198687 \nC 63.740743 108.636106 64.056842 107.872976 64.056842 107.077366 \nC 64.056842 106.281757 63.740743 105.518627 63.178162 104.956046 \nC 62.615581 104.393465 61.852451 104.077366 61.056842 104.077366 \nC 60.261233 104.077366 59.498102 104.393465 58.935521 104.956046 \nC 58.372941 105.518627 58.056842 106.281757 58.056842 107.077366 \nC 58.056842 107.872976 58.372941 108.636106 58.935521 109.198687 \nC 59.498102 109.761268 60.261233 110.077366 61.056842 110.077366 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 108.456546 \nC 63.006163 108.456546 63.769293 108.140447 64.331874 107.577866 \nC 64.894455 107.015286 65.210554 106.252155 65.210554 105.456546 \nC 65.210554 104.660937 64.894455 103.897806 64.331874 103.335226 \nC 63.769293 102.772645 63.006163 102.456546 62.210554 102.456546 \nC 61.414944 102.456546 60.651814 102.772645 60.089233 103.335226 \nC 59.526653 103.897806 59.210554 104.660937 59.210554 105.456546 \nC 59.210554 106.252155 59.526653 107.015286 60.089233 107.577866 \nC 60.651814 108.140447 61.414944 108.456546 62.210554 108.456546 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 184.734639 \nC 82.04241 184.734639 82.805541 184.41854 83.368121 183.855959 \nC 83.930702 183.293379 84.246801 182.530248 84.246801 181.734639 \nC 84.246801 180.93903 83.930702 180.175899 83.368121 179.613319 \nC 82.805541 179.050738 82.04241 178.734639 81.246801 178.734639 \nC 80.451192 178.734639 79.688061 179.050738 79.125481 179.613319 \nC 78.5629 180.175899 78.246801 180.93903 78.246801 181.734639 \nC 78.246801 182.530248 78.5629 183.293379 79.125481 183.855959 \nC 79.688061 184.41854 80.451192 184.734639 81.246801 184.734639 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 142.401809 \nC 365.855552 142.401809 366.618683 142.08571 367.181264 141.52313 \nC 367.743844 140.960549 368.059943 140.197419 368.059943 139.401809 \nC 368.059943 138.6062 367.743844 137.84307 367.181264 137.280489 \nC 366.618683 136.717908 365.855552 136.401809 365.059943 136.401809 \nC 364.264334 136.401809 363.501204 136.717908 362.938623 137.280489 \nC 362.376042 137.84307 362.059943 138.6062 362.059943 139.401809 \nC 362.059943 140.197419 362.376042 140.960549 362.938623 141.52313 \nC 363.501204 142.08571 364.264334 142.401809 365.059943 142.401809 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 44.563832 \nC 61.708237 44.563832 62.471367 44.247733 63.033948 43.685152 \nC 63.596529 43.122572 63.912628 42.359441 63.912628 41.563832 \nC 63.912628 40.768223 63.596529 40.005092 63.033948 39.442512 \nC 62.471367 38.879931 61.708237 38.563832 60.912628 38.563832 \nC 60.117019 38.563832 59.353888 38.879931 58.791307 39.442512 \nC 58.228727 40.005092 57.912628 40.768223 57.912628 41.563832 \nC 57.912628 42.359441 58.228727 43.122572 58.791307 43.685152 \nC 59.353888 44.247733 60.117019 44.563832 60.912628 44.563832 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 185.993534 \nC 63.006163 185.993534 63.769293 185.677435 64.331874 185.114855 \nC 64.894455 184.552274 65.210554 183.789144 65.210554 182.993534 \nC 65.210554 182.197925 64.894455 181.434795 64.331874 180.872214 \nC 63.769293 180.309633 63.006163 179.993534 62.210554 179.993534 \nC 61.414944 179.993534 60.651814 180.309633 60.089233 180.872214 \nC 59.526653 181.434795 59.210554 182.197925 59.210554 182.993534 \nC 59.210554 183.789144 59.526653 184.552274 60.089233 185.114855 \nC 60.651814 185.677435 61.414944 185.993534 62.210554 185.993534 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 42.480222 \nC 70.793719 42.480222 71.556849 42.164123 72.11943 41.601542 \nC 72.682011 41.038961 72.998109 40.275831 72.998109 39.480222 \nC 72.998109 38.684613 72.682011 37.921482 72.11943 37.358901 \nC 71.556849 36.796321 70.793719 36.480222 69.998109 36.480222 \nC 69.2025 36.480222 68.43937 36.796321 67.876789 37.358901 \nC 67.314208 37.921482 66.998109 38.684613 66.998109 39.480222 \nC 66.998109 40.275831 67.314208 41.038961 67.876789 41.601542 \nC 68.43937 42.164123 69.2025 42.480222 69.998109 42.480222 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 188.205644 \nC 81.177126 188.205644 81.940257 187.889545 82.502837 187.326964 \nC 83.065418 186.764383 83.381517 186.001253 83.381517 185.205644 \nC 83.381517 184.410035 83.065418 183.646904 82.502837 183.084323 \nC 81.940257 182.521743 81.177126 182.205644 80.381517 182.205644 \nC 79.585908 182.205644 78.822778 182.521743 78.260197 183.084323 \nC 77.697616 183.646904 77.381517 184.410035 77.381517 185.205644 \nC 77.381517 186.001253 77.697616 186.764383 78.260197 187.326964 \nC 78.822778 187.889545 79.585908 188.205644 80.381517 188.205644 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 184.595225 \nC 61.491916 184.595225 62.255046 184.279126 62.817627 183.716545 \nC 63.380208 183.153964 63.696307 182.390834 63.696307 181.595225 \nC 63.696307 180.799615 63.380208 180.036485 62.817627 179.473904 \nC 62.255046 178.911324 61.491916 178.595225 60.696307 178.595225 \nC 59.900698 178.595225 59.137567 178.911324 58.574986 179.473904 \nC 58.012406 180.036485 57.696307 180.799615 57.696307 181.595225 \nC 57.696307 182.390834 58.012406 183.153964 58.574986 183.716545 \nC 59.137567 184.279126 59.900698 184.595225 60.696307 184.595225 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 180.522537 \nC 61.852451 180.522537 62.615581 180.206438 63.178162 179.643857 \nC 63.740743 179.081277 64.056842 178.318146 64.056842 177.522537 \nC 64.056842 176.726928 63.740743 175.963797 63.178162 175.401217 \nC 62.615581 174.838636 61.852451 174.522537 61.056842 174.522537 \nC 60.261233 174.522537 59.498102 174.838636 58.935521 175.401217 \nC 58.372941 175.963797 58.056842 176.726928 58.056842 177.522537 \nC 58.056842 178.318146 58.372941 179.081277 58.935521 179.643857 \nC 59.498102 180.206438 60.261233 180.522537 61.056842 180.522537 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 172.417265 \nC 63.006163 172.417265 63.769293 172.101166 64.331874 171.538586 \nC 64.894455 170.976005 65.210554 170.212875 65.210554 169.417265 \nC 65.210554 168.621656 64.894455 167.858526 64.331874 167.295945 \nC 63.769293 166.733364 63.006163 166.417265 62.210554 166.417265 \nC 61.414944 166.417265 60.651814 166.733364 60.089233 167.295945 \nC 59.526653 167.858526 59.210554 168.621656 59.210554 169.417265 \nC 59.210554 170.212875 59.526653 170.976005 60.089233 171.538586 \nC 60.651814 172.101166 61.414944 172.417265 62.210554 172.417265 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 192.700319 \nC 67.044155 192.700319 67.807285 192.38422 68.369866 191.82164 \nC 68.932447 191.259059 69.248546 190.495929 69.248546 189.700319 \nC 69.248546 188.90471 68.932447 188.14158 68.369866 187.578999 \nC 67.807285 187.016418 67.044155 186.700319 66.248546 186.700319 \nC 65.452936 186.700319 64.689806 187.016418 64.127225 187.578999 \nC 63.564645 188.14158 63.248546 188.90471 63.248546 189.700319 \nC 63.248546 190.495929 63.564645 191.259059 64.127225 191.82164 \nC 64.689806 192.38422 65.452936 192.700319 66.248546 192.700319 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 170.818112 \nC 82.04241 170.818112 82.805541 170.502013 83.368121 169.939432 \nC 83.930702 169.376852 84.246801 168.613721 84.246801 167.818112 \nC 84.246801 167.022503 83.930702 166.259372 83.368121 165.696792 \nC 82.805541 165.134211 82.04241 164.818112 81.246801 164.818112 \nC 80.451192 164.818112 79.688061 165.134211 79.125481 165.696792 \nC 78.5629 166.259372 78.246801 167.022503 78.246801 167.818112 \nC 78.246801 168.613721 78.5629 169.376852 79.125481 169.939432 \nC 79.688061 170.502013 80.451192 170.818112 81.246801 170.818112 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 161.455075 \nC 139.728008 161.455075 140.491139 161.138976 141.053719 160.576395 \nC 141.6163 160.013814 141.932399 159.250684 141.932399 158.455075 \nC 141.932399 157.659466 141.6163 156.896335 141.053719 156.333755 \nC 140.491139 155.771174 139.728008 155.455075 138.932399 155.455075 \nC 138.13679 155.455075 137.373659 155.771174 136.811079 156.333755 \nC 136.248498 156.896335 135.932399 157.659466 135.932399 158.455075 \nC 135.932399 159.250684 136.248498 160.013814 136.811079 160.576395 \nC 137.373659 161.138976 138.13679 161.455075 138.932399 161.455075 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 206.34531 \nC 61.708237 206.34531 62.471367 206.029211 63.033948 205.466631 \nC 63.596529 204.90405 63.912628 204.14092 63.912628 203.34531 \nC 63.912628 202.549701 63.596529 201.786571 63.033948 201.22399 \nC 62.471367 200.661409 61.708237 200.34531 60.912628 200.34531 \nC 60.117019 200.34531 59.353888 200.661409 58.791307 201.22399 \nC 58.228727 201.786571 57.912628 202.549701 57.912628 203.34531 \nC 57.912628 204.14092 58.228727 204.90405 58.791307 205.466631 \nC 59.353888 206.029211 60.117019 206.34531 60.912628 206.34531 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 171.061164 \nC 63.006163 171.061164 63.769293 170.745065 64.331874 170.182485 \nC 64.894455 169.619904 65.210554 168.856774 65.210554 168.061164 \nC 65.210554 167.265555 64.894455 166.502425 64.331874 165.939844 \nC 63.769293 165.377263 63.006163 165.061164 62.210554 165.061164 \nC 61.414944 165.061164 60.651814 165.377263 60.089233 165.939844 \nC 59.526653 166.502425 59.210554 167.265555 59.210554 168.061164 \nC 59.210554 168.856774 59.526653 169.619904 60.089233 170.182485 \nC 60.651814 170.745065 61.414944 171.061164 62.210554 171.061164 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 29.651828 \nC 65.602015 29.651828 66.365145 29.335729 66.927726 28.773148 \nC 67.490307 28.210567 67.806406 27.447437 67.806406 26.651828 \nC 67.806406 25.856219 67.490307 25.093088 66.927726 24.530508 \nC 66.365145 23.967927 65.602015 23.651828 64.806406 23.651828 \nC 64.010796 23.651828 63.247666 23.967927 62.685085 24.530508 \nC 62.122505 25.093088 61.806406 25.856219 61.806406 26.651828 \nC 61.806406 27.447437 62.122505 28.210567 62.685085 28.773148 \nC 63.247666 29.335729 64.010796 29.651828 64.806406 29.651828 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 36.108978 \nC 70.793719 36.108978 71.556849 35.792879 72.11943 35.230298 \nC 72.682011 34.667717 72.998109 33.904587 72.998109 33.108978 \nC 72.998109 32.313368 72.682011 31.550238 72.11943 30.987657 \nC 71.556849 30.425077 70.793719 30.108978 69.998109 30.108978 \nC 69.2025 30.108978 68.43937 30.425077 67.876789 30.987657 \nC 67.314208 31.550238 66.998109 32.313368 66.998109 33.108978 \nC 66.998109 33.904587 67.314208 34.667717 67.876789 35.230298 \nC 68.43937 35.792879 69.2025 36.108978 69.998109 36.108978 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 25.645025 \nC 81.177126 25.645025 81.940257 25.328926 82.502837 24.766345 \nC 83.065418 24.203765 83.381517 23.440634 83.381517 22.645025 \nC 83.381517 21.849416 83.065418 21.086285 82.502837 20.523705 \nC 81.940257 19.961124 81.177126 19.645025 80.381517 19.645025 \nC 79.585908 19.645025 78.822778 19.961124 78.260197 20.523705 \nC 77.697616 21.086285 77.381517 21.849416 77.381517 22.645025 \nC 77.381517 23.440634 77.697616 24.203765 78.260197 24.766345 \nC 78.822778 25.328926 79.585908 25.645025 80.381517 25.645025 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 170.563391 \nC 101.943942 170.563391 102.707072 170.247292 103.269653 169.684711 \nC 103.832233 169.122131 104.148332 168.359 104.148332 167.563391 \nC 104.148332 166.767782 103.832233 166.004651 103.269653 165.442071 \nC 102.707072 164.87949 101.943942 164.563391 101.148332 164.563391 \nC 100.352723 164.563391 99.589593 164.87949 99.027012 165.442071 \nC 98.464431 166.004651 98.148332 166.767782 98.148332 167.563391 \nC 98.148332 168.359 98.464431 169.122131 99.027012 169.684711 \nC 99.589593 170.247292 100.352723 170.563391 101.148332 170.563391 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 39.803033 \nC 61.491916 39.803033 62.255046 39.486934 62.817627 38.924354 \nC 63.380208 38.361773 63.696307 37.598643 63.696307 36.803033 \nC 63.696307 36.007424 63.380208 35.244294 62.817627 34.681713 \nC 62.255046 34.119132 61.491916 33.803033 60.696307 33.803033 \nC 59.900698 33.803033 59.137567 34.119132 58.574986 34.681713 \nC 58.012406 35.244294 57.696307 36.007424 57.696307 36.803033 \nC 57.696307 37.598643 58.012406 38.361773 58.574986 38.924354 \nC 59.137567 39.486934 59.900698 39.803033 60.696307 39.803033 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 179.635335 \nC 63.006163 179.635335 63.769293 179.319236 64.331874 178.756655 \nC 64.894455 178.194074 65.210554 177.430944 65.210554 176.635335 \nC 65.210554 175.839725 64.894455 175.076595 64.331874 174.514014 \nC 63.769293 173.951433 63.006163 173.635335 62.210554 173.635335 \nC 61.414944 173.635335 60.651814 173.951433 60.089233 174.514014 \nC 59.526653 175.076595 59.210554 175.839725 59.210554 176.635335 \nC 59.210554 177.430944 59.526653 178.194074 60.089233 178.756655 \nC 60.651814 179.319236 61.414944 179.635335 62.210554 179.635335 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 20.083636 \nC 67.044155 20.083636 67.807285 19.767537 68.369866 19.204957 \nC 68.932447 18.642376 69.248546 17.879246 69.248546 17.083636 \nC 69.248546 16.288027 68.932447 15.524897 68.369866 14.962316 \nC 67.807285 14.399735 67.044155 14.083636 66.248546 14.083636 \nC 65.452936 14.083636 64.689806 14.399735 64.127225 14.962316 \nC 63.564645 15.524897 63.248546 16.288027 63.248546 17.083636 \nC 63.248546 17.879246 63.564645 18.642376 64.127225 19.204957 \nC 64.689806 19.767537 65.452936 20.083636 66.248546 20.083636 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 185.031888 \nC 82.04241 185.031888 82.805541 184.71579 83.368121 184.153209 \nC 83.930702 183.590628 84.246801 182.827498 84.246801 182.031888 \nC 84.246801 181.236279 83.930702 180.473149 83.368121 179.910568 \nC 82.805541 179.347987 82.04241 179.031888 81.246801 179.031888 \nC 80.451192 179.031888 79.688061 179.347987 79.125481 179.910568 \nC 78.5629 180.473149 78.246801 181.236279 78.246801 182.031888 \nC 78.246801 182.827498 78.5629 183.590628 79.125481 184.153209 \nC 79.688061 184.71579 80.451192 185.031888 81.246801 185.031888 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 175.793975 \nC 139.728008 175.793975 140.491139 175.477876 141.053719 174.915295 \nC 141.6163 174.352715 141.932399 173.589584 141.932399 172.793975 \nC 141.932399 171.998366 141.6163 171.235235 141.053719 170.672655 \nC 140.491139 170.110074 139.728008 169.793975 138.932399 169.793975 \nC 138.13679 169.793975 137.373659 170.110074 136.811079 170.672655 \nC 136.248498 171.235235 135.932399 171.998366 135.932399 172.793975 \nC 135.932399 173.589584 136.248498 174.352715 136.811079 174.915295 \nC 137.373659 175.477876 138.13679 175.793975 138.932399 175.793975 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 165.511515 \nC 365.855552 165.511515 366.618683 165.195416 367.181264 164.632836 \nC 367.743844 164.070255 368.059943 163.307124 368.059943 162.511515 \nC 368.059943 161.715906 367.743844 160.952776 367.181264 160.390195 \nC 366.618683 159.827614 365.855552 159.511515 365.059943 159.511515 \nC 364.264334 159.511515 363.501204 159.827614 362.938623 160.390195 \nC 362.376042 160.952776 362.059943 161.715906 362.059943 162.511515 \nC 362.059943 163.307124 362.376042 164.070255 362.938623 164.632836 \nC 363.501204 165.195416 364.264334 165.511515 365.059943 165.511515 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 180.505425 \nC 61.708237 180.505425 62.471367 180.189326 63.033948 179.626745 \nC 63.596529 179.064164 63.912628 178.301034 63.912628 177.505425 \nC 63.912628 176.709815 63.596529 175.946685 63.033948 175.384104 \nC 62.471367 174.821523 61.708237 174.505425 60.912628 174.505425 \nC 60.117019 174.505425 59.353888 174.821523 58.791307 175.384104 \nC 58.228727 175.946685 57.912628 176.709815 57.912628 177.505425 \nC 57.912628 178.301034 58.228727 179.064164 58.791307 179.626745 \nC 59.353888 180.189326 60.117019 180.505425 60.912628 180.505425 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 175.901453 \nC 63.006163 175.901453 63.769293 175.585354 64.331874 175.022773 \nC 64.894455 174.460193 65.210554 173.697062 65.210554 172.901453 \nC 65.210554 172.105844 64.894455 171.342713 64.331874 170.780133 \nC 63.769293 170.217552 63.006163 169.901453 62.210554 169.901453 \nC 61.414944 169.901453 60.651814 170.217552 60.089233 170.780133 \nC 59.526653 171.342713 59.210554 172.105844 59.210554 172.901453 \nC 59.210554 173.697062 59.526653 174.460193 60.089233 175.022773 \nC 60.651814 175.585354 61.414944 175.901453 62.210554 175.901453 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 181.231262 \nC 65.602015 181.231262 66.365145 180.915163 66.927726 180.352582 \nC 67.490307 179.790001 67.806406 179.026871 67.806406 178.231262 \nC 67.806406 177.435653 67.490307 176.672522 66.927726 176.109941 \nC 66.365145 175.547361 65.602015 175.231262 64.806406 175.231262 \nC 64.010796 175.231262 63.247666 175.547361 62.685085 176.109941 \nC 62.122505 176.672522 61.806406 177.435653 61.806406 178.231262 \nC 61.806406 179.026871 62.122505 179.790001 62.685085 180.352582 \nC 63.247666 180.915163 64.010796 181.231262 64.806406 181.231262 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 25.310686 \nC 81.177126 25.310686 81.940257 24.994587 82.502837 24.432006 \nC 83.065418 23.869426 83.381517 23.106295 83.381517 22.310686 \nC 83.381517 21.515077 83.065418 20.751946 82.502837 20.189366 \nC 81.940257 19.626785 81.177126 19.310686 80.381517 19.310686 \nC 79.585908 19.310686 78.822778 19.626785 78.260197 20.189366 \nC 77.697616 20.751946 77.381517 21.515077 77.381517 22.310686 \nC 77.381517 23.106295 77.697616 23.869426 78.260197 24.432006 \nC 78.822778 24.994587 79.585908 25.310686 80.381517 25.310686 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 77.286583 \nC 101.943942 77.286583 102.707072 76.970484 103.269653 76.407903 \nC 103.832233 75.845322 104.148332 75.082192 104.148332 74.286583 \nC 104.148332 73.490973 103.832233 72.727843 103.269653 72.165262 \nC 102.707072 71.602682 101.943942 71.286583 101.148332 71.286583 \nC 100.352723 71.286583 99.589593 71.602682 99.027012 72.165262 \nC 98.464431 72.727843 98.148332 73.490973 98.148332 74.286583 \nC 98.148332 75.082192 98.464431 75.845322 99.027012 76.407903 \nC 99.589593 76.970484 100.352723 77.286583 101.148332 77.286583 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 171.525833 \nC 61.491916 171.525833 62.255046 171.209734 62.817627 170.647153 \nC 63.380208 170.084572 63.696307 169.321442 63.696307 168.525833 \nC 63.696307 167.730223 63.380208 166.967093 62.817627 166.404512 \nC 62.255046 165.841931 61.491916 165.525833 60.696307 165.525833 \nC 59.900698 165.525833 59.137567 165.841931 58.574986 166.404512 \nC 58.012406 166.967093 57.696307 167.730223 57.696307 168.525833 \nC 57.696307 169.321442 58.012406 170.084572 58.574986 170.647153 \nC 59.137567 171.209734 59.900698 171.525833 60.696307 171.525833 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 186.208385 \nC 61.852451 186.208385 62.615581 185.892286 63.178162 185.329705 \nC 63.740743 184.767125 64.056842 184.003994 64.056842 183.208385 \nC 64.056842 182.412776 63.740743 181.649645 63.178162 181.087065 \nC 62.615581 180.524484 61.852451 180.208385 61.056842 180.208385 \nC 60.261233 180.208385 59.498102 180.524484 58.935521 181.087065 \nC 58.372941 181.649645 58.056842 182.412776 58.056842 183.208385 \nC 58.056842 184.003994 58.372941 184.767125 58.935521 185.329705 \nC 59.498102 185.892286 60.261233 186.208385 61.056842 186.208385 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 174.289698 \nC 67.044155 174.289698 67.807285 173.973599 68.369866 173.411018 \nC 68.932447 172.848437 69.248546 172.085307 69.248546 171.289698 \nC 69.248546 170.494088 68.932447 169.730958 68.369866 169.168377 \nC 67.807285 168.605796 67.044155 168.289698 66.248546 168.289698 \nC 65.452936 168.289698 64.689806 168.605796 64.127225 169.168377 \nC 63.564645 169.730958 63.248546 170.494088 63.248546 171.289698 \nC 63.248546 172.085307 63.564645 172.848437 64.127225 173.411018 \nC 64.689806 173.973599 65.452936 174.289698 66.248546 174.289698 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 163.049931 \nC 82.04241 163.049931 82.805541 162.733832 83.368121 162.171251 \nC 83.930702 161.60867 84.246801 160.84554 84.246801 160.049931 \nC 84.246801 159.254321 83.930702 158.491191 83.368121 157.92861 \nC 82.805541 157.366029 82.04241 157.049931 81.246801 157.049931 \nC 80.451192 157.049931 79.688061 157.366029 79.125481 157.92861 \nC 78.5629 158.491191 78.246801 159.254321 78.246801 160.049931 \nC 78.246801 160.84554 78.5629 161.60867 79.125481 162.171251 \nC 79.688061 162.733832 80.451192 163.049931 81.246801 163.049931 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 181.470864 \nC 365.855552 181.470864 366.618683 181.154765 367.181264 180.592184 \nC 367.743844 180.029603 368.059943 179.266473 368.059943 178.470864 \nC 368.059943 177.675254 367.743844 176.912124 367.181264 176.349543 \nC 366.618683 175.786962 365.855552 175.470864 365.059943 175.470864 \nC 364.264334 175.470864 363.501204 175.786962 362.938623 176.349543 \nC 362.376042 176.912124 362.059943 177.675254 362.059943 178.470864 \nC 362.059943 179.266473 362.376042 180.029603 362.938623 180.592184 \nC 363.501204 181.154765 364.264334 181.470864 365.059943 181.470864 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 170.486287 \nC 61.708237 170.486287 62.471367 170.170188 63.033948 169.607608 \nC 63.596529 169.045027 63.912628 168.281896 63.912628 167.486287 \nC 63.912628 166.690678 63.596529 165.927548 63.033948 165.364967 \nC 62.471367 164.802386 61.708237 164.486287 60.912628 164.486287 \nC 60.117019 164.486287 59.353888 164.802386 58.791307 165.364967 \nC 58.228727 165.927548 57.912628 166.690678 57.912628 167.486287 \nC 57.912628 168.281896 58.228727 169.045027 58.791307 169.607608 \nC 59.353888 170.170188 60.117019 170.486287 60.912628 170.486287 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 166.687803 \nC 63.006163 166.687803 63.769293 166.371704 64.331874 165.809123 \nC 64.894455 165.246542 65.210554 164.483412 65.210554 163.687803 \nC 65.210554 162.892193 64.894455 162.129063 64.331874 161.566482 \nC 63.769293 161.003902 63.006163 160.687803 62.210554 160.687803 \nC 61.414944 160.687803 60.651814 161.003902 60.089233 161.566482 \nC 59.526653 162.129063 59.210554 162.892193 59.210554 163.687803 \nC 59.210554 164.483412 59.526653 165.246542 60.089233 165.809123 \nC 60.651814 166.371704 61.414944 166.687803 62.210554 166.687803 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 174.176146 \nC 65.602015 174.176146 66.365145 173.860047 66.927726 173.297466 \nC 67.490307 172.734886 67.806406 171.971755 67.806406 171.176146 \nC 67.806406 170.380537 67.490307 169.617406 66.927726 169.054826 \nC 66.365145 168.492245 65.602015 168.176146 64.806406 168.176146 \nC 64.010796 168.176146 63.247666 168.492245 62.685085 169.054826 \nC 62.122505 169.617406 61.806406 170.380537 61.806406 171.176146 \nC 61.806406 171.971755 62.122505 172.734886 62.685085 173.297466 \nC 63.247666 173.860047 64.010796 174.176146 64.806406 174.176146 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 184.428584 \nC 81.177126 184.428584 81.940257 184.112485 82.502837 183.549904 \nC 83.065418 182.987323 83.381517 182.224193 83.381517 181.428584 \nC 83.381517 180.632974 83.065418 179.869844 82.502837 179.307263 \nC 81.940257 178.744682 81.177126 178.428584 80.381517 178.428584 \nC 79.585908 178.428584 78.822778 178.744682 78.260197 179.307263 \nC 77.697616 179.869844 77.381517 180.632974 77.381517 181.428584 \nC 77.381517 182.224193 77.697616 182.987323 78.260197 183.549904 \nC 78.822778 184.112485 79.585908 184.428584 80.381517 184.428584 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 193.182315 \nC 63.006163 193.182315 63.769293 192.866216 64.331874 192.303635 \nC 64.894455 191.741055 65.210554 190.977924 65.210554 190.182315 \nC 65.210554 189.386706 64.894455 188.623575 64.331874 188.060995 \nC 63.769293 187.498414 63.006163 187.182315 62.210554 187.182315 \nC 61.414944 187.182315 60.651814 187.498414 60.089233 188.060995 \nC 59.526653 188.623575 59.210554 189.386706 59.210554 190.182315 \nC 59.210554 190.977924 59.526653 191.741055 60.089233 192.303635 \nC 60.651814 192.866216 61.414944 193.182315 62.210554 193.182315 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 181.001319 \nC 67.044155 181.001319 67.807285 180.68522 68.369866 180.122639 \nC 68.932447 179.560058 69.248546 178.796928 69.248546 178.001319 \nC 69.248546 177.205709 68.932447 176.442579 68.369866 175.879998 \nC 67.807285 175.317418 67.044155 175.001319 66.248546 175.001319 \nC 65.452936 175.001319 64.689806 175.317418 64.127225 175.879998 \nC 63.564645 176.442579 63.248546 177.205709 63.248546 178.001319 \nC 63.248546 178.796928 63.564645 179.560058 64.127225 180.122639 \nC 64.689806 180.68522 65.452936 181.001319 66.248546 181.001319 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 179.264442 \nC 82.04241 179.264442 82.805541 178.948343 83.368121 178.385763 \nC 83.930702 177.823182 84.246801 177.060052 84.246801 176.264442 \nC 84.246801 175.468833 83.930702 174.705703 83.368121 174.143122 \nC 82.805541 173.580541 82.04241 173.264442 81.246801 173.264442 \nC 80.451192 173.264442 79.688061 173.580541 79.125481 174.143122 \nC 78.5629 174.705703 78.246801 175.468833 78.246801 176.264442 \nC 78.246801 177.060052 78.5629 177.823182 79.125481 178.385763 \nC 79.688061 178.948343 80.451192 179.264442 81.246801 179.264442 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 162.005068 \nC 139.728008 162.005068 140.491139 161.688969 141.053719 161.126388 \nC 141.6163 160.563807 141.932399 159.800677 141.932399 159.005068 \nC 141.932399 158.209458 141.6163 157.446328 141.053719 156.883747 \nC 140.491139 156.321167 139.728008 156.005068 138.932399 156.005068 \nC 138.13679 156.005068 137.373659 156.321167 136.811079 156.883747 \nC 136.248498 157.446328 135.932399 158.209458 135.932399 159.005068 \nC 135.932399 159.800677 136.248498 160.563807 136.811079 161.126388 \nC 137.373659 161.688969 138.13679 162.005068 138.932399 162.005068 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 151.817991 \nC 365.855552 151.817991 366.618683 151.501892 367.181264 150.939312 \nC 367.743844 150.376731 368.059943 149.613601 368.059943 148.817991 \nC 368.059943 148.022382 367.743844 147.259252 367.181264 146.696671 \nC 366.618683 146.13409 365.855552 145.817991 365.059943 145.817991 \nC 364.264334 145.817991 363.501204 146.13409 362.938623 146.696671 \nC 362.376042 147.259252 362.059943 148.022382 362.059943 148.817991 \nC 362.059943 149.613601 362.376042 150.376731 362.938623 150.939312 \nC 363.501204 151.501892 364.264334 151.817991 365.059943 151.817991 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 103.990903 \nC 61.708237 103.990903 62.471367 103.674804 63.033948 103.112223 \nC 63.596529 102.549643 63.912628 101.786512 63.912628 100.990903 \nC 63.912628 100.195294 63.596529 99.432163 63.033948 98.869583 \nC 62.471367 98.307002 61.708237 97.990903 60.912628 97.990903 \nC 60.117019 97.990903 59.353888 98.307002 58.791307 98.869583 \nC 58.228727 99.432163 57.912628 100.195294 57.912628 100.990903 \nC 57.912628 101.786512 58.228727 102.549643 58.791307 103.112223 \nC 59.353888 103.674804 60.117019 103.990903 60.912628 103.990903 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 131.708972 \nC 65.602015 131.708972 66.365145 131.392873 66.927726 130.830293 \nC 67.490307 130.267712 67.806406 129.504581 67.806406 128.708972 \nC 67.806406 127.913363 67.490307 127.150233 66.927726 126.587652 \nC 66.365145 126.025071 65.602015 125.708972 64.806406 125.708972 \nC 64.010796 125.708972 63.247666 126.025071 62.685085 126.587652 \nC 62.122505 127.150233 61.806406 127.913363 61.806406 128.708972 \nC 61.806406 129.504581 62.122505 130.267712 62.685085 130.830293 \nC 63.247666 131.392873 64.010796 131.708972 64.806406 131.708972 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 173.503457 \nC 70.793719 173.503457 71.556849 173.187359 72.11943 172.624778 \nC 72.682011 172.062197 72.998109 171.299067 72.998109 170.503457 \nC 72.998109 169.707848 72.682011 168.944718 72.11943 168.382137 \nC 71.556849 167.819556 70.793719 167.503457 69.998109 167.503457 \nC 69.2025 167.503457 68.43937 167.819556 67.876789 168.382137 \nC 67.314208 168.944718 66.998109 169.707848 66.998109 170.503457 \nC 66.998109 171.299067 67.314208 172.062197 67.876789 172.624778 \nC 68.43937 173.187359 69.2025 173.503457 69.998109 173.503457 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 158.016563 \nC 81.177126 158.016563 81.940257 157.700464 82.502837 157.137883 \nC 83.065418 156.575302 83.381517 155.812172 83.381517 155.016563 \nC 83.381517 154.220953 83.065418 153.457823 82.502837 152.895242 \nC 81.940257 152.332662 81.177126 152.016563 80.381517 152.016563 \nC 79.585908 152.016563 78.822778 152.332662 78.260197 152.895242 \nC 77.697616 153.457823 77.381517 154.220953 77.381517 155.016563 \nC 77.381517 155.812172 77.697616 156.575302 78.260197 157.137883 \nC 78.822778 157.700464 79.585908 158.016563 80.381517 158.016563 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 26.402419 \nC 101.943942 26.402419 102.707072 26.08632 103.269653 25.52374 \nC 103.832233 24.961159 104.148332 24.198029 104.148332 23.402419 \nC 104.148332 22.60681 103.832233 21.84368 103.269653 21.281099 \nC 102.707072 20.718518 101.943942 20.402419 101.148332 20.402419 \nC 100.352723 20.402419 99.589593 20.718518 99.027012 21.281099 \nC 98.464431 21.84368 98.148332 22.60681 98.148332 23.402419 \nC 98.148332 24.198029 98.464431 24.961159 99.027012 25.52374 \nC 99.589593 26.08632 100.352723 26.402419 101.148332 26.402419 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 169.590734 \nC 61.491916 169.590734 62.255046 169.274635 62.817627 168.712054 \nC 63.380208 168.149474 63.696307 167.386343 63.696307 166.590734 \nC 63.696307 165.795125 63.380208 165.031995 62.817627 164.469414 \nC 62.255046 163.906833 61.491916 163.590734 60.696307 163.590734 \nC 59.900698 163.590734 59.137567 163.906833 58.574986 164.469414 \nC 58.012406 165.031995 57.696307 165.795125 57.696307 166.590734 \nC 57.696307 167.386343 58.012406 168.149474 58.574986 168.712054 \nC 59.137567 169.274635 59.900698 169.590734 60.696307 169.590734 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 184.620682 \nC 61.852451 184.620682 62.615581 184.304583 63.178162 183.742002 \nC 63.740743 183.179421 64.056842 182.416291 64.056842 181.620682 \nC 64.056842 180.825073 63.740743 180.061942 63.178162 179.499361 \nC 62.615581 178.936781 61.852451 178.620682 61.056842 178.620682 \nC 60.261233 178.620682 59.498102 178.936781 58.935521 179.499361 \nC 58.372941 180.061942 58.056842 180.825073 58.056842 181.620682 \nC 58.056842 182.416291 58.372941 183.179421 58.935521 183.742002 \nC 59.498102 184.304583 60.261233 184.620682 61.056842 184.620682 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 44.17148 \nC 82.04241 44.17148 82.805541 43.855381 83.368121 43.2928 \nC 83.930702 42.73022 84.246801 41.967089 84.246801 41.17148 \nC 84.246801 40.375871 83.930702 39.61274 83.368121 39.05016 \nC 82.805541 38.487579 82.04241 38.17148 81.246801 38.17148 \nC 80.451192 38.17148 79.688061 38.487579 79.125481 39.05016 \nC 78.5629 39.61274 78.246801 40.375871 78.246801 41.17148 \nC 78.246801 41.967089 78.5629 42.73022 79.125481 43.2928 \nC 79.688061 43.855381 80.451192 44.17148 81.246801 44.17148 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 179.083175 \nC 139.728008 179.083175 140.491139 178.767076 141.053719 178.204495 \nC 141.6163 177.641914 141.932399 176.878784 141.932399 176.083175 \nC 141.932399 175.287566 141.6163 174.524435 141.053719 173.961854 \nC 140.491139 173.399274 139.728008 173.083175 138.932399 173.083175 \nC 138.13679 173.083175 137.373659 173.399274 136.811079 173.961854 \nC 136.248498 174.524435 135.932399 175.287566 135.932399 176.083175 \nC 135.932399 176.878784 136.248498 177.641914 136.811079 178.204495 \nC 137.373659 178.767076 138.13679 179.083175 138.932399 179.083175 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 155.773943 \nC 365.855552 155.773943 366.618683 155.457844 367.181264 154.895263 \nC 367.743844 154.332683 368.059943 153.569552 368.059943 152.773943 \nC 368.059943 151.978334 367.743844 151.215203 367.181264 150.652623 \nC 366.618683 150.090042 365.855552 149.773943 365.059943 149.773943 \nC 364.264334 149.773943 363.501204 150.090042 362.938623 150.652623 \nC 362.376042 151.215203 362.059943 151.978334 362.059943 152.773943 \nC 362.059943 153.569552 362.376042 154.332683 362.938623 154.895263 \nC 363.501204 155.457844 364.264334 155.773943 365.059943 155.773943 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 176.389989 \nC 61.708237 176.389989 62.471367 176.07389 63.033948 175.511309 \nC 63.596529 174.948728 63.912628 174.185598 63.912628 173.389989 \nC 63.912628 172.59438 63.596529 171.831249 63.033948 171.268669 \nC 62.471367 170.706088 61.708237 170.389989 60.912628 170.389989 \nC 60.117019 170.389989 59.353888 170.706088 58.791307 171.268669 \nC 58.228727 171.831249 57.912628 172.59438 57.912628 173.389989 \nC 57.912628 174.185598 58.228727 174.948728 58.791307 175.511309 \nC 59.353888 176.07389 60.117019 176.389989 60.912628 176.389989 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 166.342643 \nC 63.006163 166.342643 63.769293 166.026544 64.331874 165.463964 \nC 64.894455 164.901383 65.210554 164.138253 65.210554 163.342643 \nC 65.210554 162.547034 64.894455 161.783904 64.331874 161.221323 \nC 63.769293 160.658742 63.006163 160.342643 62.210554 160.342643 \nC 61.414944 160.342643 60.651814 160.658742 60.089233 161.221323 \nC 59.526653 161.783904 59.210554 162.547034 59.210554 163.342643 \nC 59.210554 164.138253 59.526653 164.901383 60.089233 165.463964 \nC 60.651814 166.026544 61.414944 166.342643 62.210554 166.342643 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 190.583935 \nC 65.602015 190.583935 66.365145 190.267836 66.927726 189.705255 \nC 67.490307 189.142675 67.806406 188.379544 67.806406 187.583935 \nC 67.806406 186.788326 67.490307 186.025195 66.927726 185.462615 \nC 66.365145 184.900034 65.602015 184.583935 64.806406 184.583935 \nC 64.010796 184.583935 63.247666 184.900034 62.685085 185.462615 \nC 62.122505 186.025195 61.806406 186.788326 61.806406 187.583935 \nC 61.806406 188.379544 62.122505 189.142675 62.685085 189.705255 \nC 63.247666 190.267836 64.010796 190.583935 64.806406 190.583935 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 158.999476 \nC 70.793719 158.999476 71.556849 158.683377 72.11943 158.120796 \nC 72.682011 157.558216 72.998109 156.795085 72.998109 155.999476 \nC 72.998109 155.203867 72.682011 154.440737 72.11943 153.878156 \nC 71.556849 153.315575 70.793719 152.999476 69.998109 152.999476 \nC 69.2025 152.999476 68.43937 153.315575 67.876789 153.878156 \nC 67.314208 154.440737 66.998109 155.203867 66.998109 155.999476 \nC 66.998109 156.795085 67.314208 157.558216 67.876789 158.120796 \nC 68.43937 158.683377 69.2025 158.999476 69.998109 158.999476 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 42.680281 \nC 61.491916 42.680281 62.255046 42.364182 62.817627 41.801601 \nC 63.380208 41.23902 63.696307 40.47589 63.696307 39.680281 \nC 63.696307 38.884671 63.380208 38.121541 62.817627 37.55896 \nC 62.255046 36.99638 61.491916 36.680281 60.696307 36.680281 \nC 59.900698 36.680281 59.137567 36.99638 58.574986 37.55896 \nC 58.012406 38.121541 57.696307 38.884671 57.696307 39.680281 \nC 57.696307 40.47589 58.012406 41.23902 58.574986 41.801601 \nC 59.137567 42.364182 59.900698 42.680281 60.696307 42.680281 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 42.670778 \nC 61.852451 42.670778 62.615581 42.354679 63.178162 41.792099 \nC 63.740743 41.229518 64.056842 40.466388 64.056842 39.670778 \nC 64.056842 38.875169 63.740743 38.112039 63.178162 37.549458 \nC 62.615581 36.986877 61.852451 36.670778 61.056842 36.670778 \nC 60.261233 36.670778 59.498102 36.986877 58.935521 37.549458 \nC 58.372941 38.112039 58.056842 38.875169 58.056842 39.670778 \nC 58.056842 40.466388 58.372941 41.229518 58.935521 41.792099 \nC 59.498102 42.354679 60.261233 42.670778 61.056842 42.670778 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 191.742411 \nC 63.006163 191.742411 63.769293 191.426312 64.331874 190.863731 \nC 64.894455 190.301151 65.210554 189.53802 65.210554 188.742411 \nC 65.210554 187.946802 64.894455 187.183671 64.331874 186.621091 \nC 63.769293 186.05851 63.006163 185.742411 62.210554 185.742411 \nC 61.414944 185.742411 60.651814 186.05851 60.089233 186.621091 \nC 59.526653 187.183671 59.210554 187.946802 59.210554 188.742411 \nC 59.210554 189.53802 59.526653 190.301151 60.089233 190.863731 \nC 60.651814 191.426312 61.414944 191.742411 62.210554 191.742411 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 187.026434 \nC 139.728008 187.026434 140.491139 186.710335 141.053719 186.147754 \nC 141.6163 185.585174 141.932399 184.822043 141.932399 184.026434 \nC 141.932399 183.230825 141.6163 182.467694 141.053719 181.905114 \nC 140.491139 181.342533 139.728008 181.026434 138.932399 181.026434 \nC 138.13679 181.026434 137.373659 181.342533 136.811079 181.905114 \nC 136.248498 182.467694 135.932399 183.230825 135.932399 184.026434 \nC 135.932399 184.822043 136.248498 185.585174 136.811079 186.147754 \nC 137.373659 186.710335 138.13679 187.026434 138.932399 187.026434 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 95.389626 \nC 365.855552 95.389626 366.618683 95.073527 367.181264 94.510946 \nC 367.743844 93.948366 368.059943 93.185235 368.059943 92.389626 \nC 368.059943 91.594017 367.743844 90.830886 367.181264 90.268306 \nC 366.618683 89.705725 365.855552 89.389626 365.059943 89.389626 \nC 364.264334 89.389626 363.501204 89.705725 362.938623 90.268306 \nC 362.376042 90.830886 362.059943 91.594017 362.059943 92.389626 \nC 362.059943 93.185235 362.376042 93.948366 362.938623 94.510946 \nC 363.501204 95.073527 364.264334 95.389626 365.059943 95.389626 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 181.600842 \nC 63.006163 181.600842 63.769293 181.284743 64.331874 180.722162 \nC 64.894455 180.159581 65.210554 179.396451 65.210554 178.600842 \nC 65.210554 177.805232 64.894455 177.042102 64.331874 176.479521 \nC 63.769293 175.916941 63.006163 175.600842 62.210554 175.600842 \nC 61.414944 175.600842 60.651814 175.916941 60.089233 176.479521 \nC 59.526653 177.042102 59.210554 177.805232 59.210554 178.600842 \nC 59.210554 179.396451 59.526653 180.159581 60.089233 180.722162 \nC 60.651814 181.284743 61.414944 181.600842 62.210554 181.600842 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 184.955473 \nC 70.793719 184.955473 71.556849 184.639374 72.11943 184.076793 \nC 72.682011 183.514212 72.998109 182.751082 72.998109 181.955473 \nC 72.998109 181.159864 72.682011 180.396733 72.11943 179.834153 \nC 71.556849 179.271572 70.793719 178.955473 69.998109 178.955473 \nC 69.2025 178.955473 68.43937 179.271572 67.876789 179.834153 \nC 67.314208 180.396733 66.998109 181.159864 66.998109 181.955473 \nC 66.998109 182.751082 67.314208 183.514212 67.876789 184.076793 \nC 68.43937 184.639374 69.2025 184.955473 69.998109 184.955473 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 185.224022 \nC 81.177126 185.224022 81.940257 184.907923 82.502837 184.345342 \nC 83.065418 183.782761 83.381517 183.019631 83.381517 182.224022 \nC 83.381517 181.428413 83.065418 180.665282 82.502837 180.102702 \nC 81.940257 179.540121 81.177126 179.224022 80.381517 179.224022 \nC 79.585908 179.224022 78.822778 179.540121 78.260197 180.102702 \nC 77.697616 180.665282 77.381517 181.428413 77.381517 182.224022 \nC 77.381517 183.019631 77.697616 183.782761 78.260197 184.345342 \nC 78.822778 184.907923 79.585908 185.224022 80.381517 185.224022 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 73.092858 \nC 101.943942 73.092858 102.707072 72.77676 103.269653 72.214179 \nC 103.832233 71.651598 104.148332 70.888468 104.148332 70.092858 \nC 104.148332 69.297249 103.832233 68.534119 103.269653 67.971538 \nC 102.707072 67.408957 101.943942 67.092858 101.148332 67.092858 \nC 100.352723 67.092858 99.589593 67.408957 99.027012 67.971538 \nC 98.464431 68.534119 98.148332 69.297249 98.148332 70.092858 \nC 98.148332 70.888468 98.464431 71.651598 99.027012 72.214179 \nC 99.589593 72.77676 100.352723 73.092858 101.148332 73.092858 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 178.904162 \nC 61.491916 178.904162 62.255046 178.588063 62.817627 178.025482 \nC 63.380208 177.462901 63.696307 176.699771 63.696307 175.904162 \nC 63.696307 175.108552 63.380208 174.345422 62.817627 173.782841 \nC 62.255046 173.220261 61.491916 172.904162 60.696307 172.904162 \nC 59.900698 172.904162 59.137567 173.220261 58.574986 173.782841 \nC 58.012406 174.345422 57.696307 175.108552 57.696307 175.904162 \nC 57.696307 176.699771 58.012406 177.462901 58.574986 178.025482 \nC 59.137567 178.588063 59.900698 178.904162 60.696307 178.904162 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 176.692841 \nC 67.044155 176.692841 67.807285 176.376742 68.369866 175.814161 \nC 68.932447 175.25158 69.248546 174.48845 69.248546 173.692841 \nC 69.248546 172.897232 68.932447 172.134101 68.369866 171.571521 \nC 67.807285 171.00894 67.044155 170.692841 66.248546 170.692841 \nC 65.452936 170.692841 64.689806 171.00894 64.127225 171.571521 \nC 63.564645 172.134101 63.248546 172.897232 63.248546 173.692841 \nC 63.248546 174.48845 63.564645 175.25158 64.127225 175.814161 \nC 64.689806 176.376742 65.452936 176.692841 66.248546 176.692841 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 127.236891 \nC 82.04241 127.236891 82.805541 126.920792 83.368121 126.358211 \nC 83.930702 125.795631 84.246801 125.0325 84.246801 124.236891 \nC 84.246801 123.441282 83.930702 122.678151 83.368121 122.115571 \nC 82.805541 121.55299 82.04241 121.236891 81.246801 121.236891 \nC 80.451192 121.236891 79.688061 121.55299 79.125481 122.115571 \nC 78.5629 122.678151 78.246801 123.441282 78.246801 124.236891 \nC 78.246801 125.0325 78.5629 125.795631 79.125481 126.358211 \nC 79.688061 126.920792 80.451192 127.236891 81.246801 127.236891 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 169.679375 \nC 365.855552 169.679375 366.618683 169.363276 367.181264 168.800695 \nC 367.743844 168.238115 368.059943 167.474984 368.059943 166.679375 \nC 368.059943 165.883766 367.743844 165.120635 367.181264 164.558055 \nC 366.618683 163.995474 365.855552 163.679375 365.059943 163.679375 \nC 364.264334 163.679375 363.501204 163.995474 362.938623 164.558055 \nC 362.376042 165.120635 362.059943 165.883766 362.059943 166.679375 \nC 362.059943 167.474984 362.376042 168.238115 362.938623 168.800695 \nC 363.501204 169.363276 364.264334 169.679375 365.059943 169.679375 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 206.321092 \nC 61.708237 206.321092 62.471367 206.004993 63.033948 205.442413 \nC 63.596529 204.879832 63.912628 204.116702 63.912628 203.321092 \nC 63.912628 202.525483 63.596529 201.762353 63.033948 201.199772 \nC 62.471367 200.637191 61.708237 200.321092 60.912628 200.321092 \nC 60.117019 200.321092 59.353888 200.637191 58.791307 201.199772 \nC 58.228727 201.762353 57.912628 202.525483 57.912628 203.321092 \nC 57.912628 204.116702 58.228727 204.879832 58.791307 205.442413 \nC 59.353888 206.004993 60.117019 206.321092 60.912628 206.321092 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 25.819598 \nC 65.602015 25.819598 66.365145 25.503499 66.927726 24.940918 \nC 67.490307 24.378338 67.806406 23.615207 67.806406 22.819598 \nC 67.806406 22.023989 67.490307 21.260858 66.927726 20.698278 \nC 66.365145 20.135697 65.602015 19.819598 64.806406 19.819598 \nC 64.010796 19.819598 63.247666 20.135697 62.685085 20.698278 \nC 62.122505 21.260858 61.806406 22.023989 61.806406 22.819598 \nC 61.806406 23.615207 62.122505 24.378338 62.685085 24.940918 \nC 63.247666 25.503499 64.010796 25.819598 64.806406 25.819598 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 190.299188 \nC 81.177126 190.299188 81.940257 189.983089 82.502837 189.420508 \nC 83.065418 188.857927 83.381517 188.094797 83.381517 187.299188 \nC 83.381517 186.503578 83.065418 185.740448 82.502837 185.177867 \nC 81.940257 184.615287 81.177126 184.299188 80.381517 184.299188 \nC 79.585908 184.299188 78.822778 184.615287 78.260197 185.177867 \nC 77.697616 185.740448 77.381517 186.503578 77.381517 187.299188 \nC 77.381517 188.094797 77.697616 188.857927 78.260197 189.420508 \nC 78.822778 189.983089 79.585908 190.299188 80.381517 190.299188 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 203.557502 \nC 101.943942 203.557502 102.707072 203.241403 103.269653 202.678822 \nC 103.832233 202.116242 104.148332 201.353111 104.148332 200.557502 \nC 104.148332 199.761893 103.832233 198.998762 103.269653 198.436182 \nC 102.707072 197.873601 101.943942 197.557502 101.148332 197.557502 \nC 100.352723 197.557502 99.589593 197.873601 99.027012 198.436182 \nC 98.464431 198.998762 98.148332 199.761893 98.148332 200.557502 \nC 98.148332 201.353111 98.464431 202.116242 99.027012 202.678822 \nC 99.589593 203.241403 100.352723 203.557502 101.148332 203.557502 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 39.784837 \nC 61.491916 39.784837 62.255046 39.468738 62.817627 38.906157 \nC 63.380208 38.343576 63.696307 37.580446 63.696307 36.784837 \nC 63.696307 35.989227 63.380208 35.226097 62.817627 34.663516 \nC 62.255046 34.100935 61.491916 33.784837 60.696307 33.784837 \nC 59.900698 33.784837 59.137567 34.100935 58.574986 34.663516 \nC 58.012406 35.226097 57.696307 35.989227 57.696307 36.784837 \nC 57.696307 37.580446 58.012406 38.343576 58.574986 38.906157 \nC 59.137567 39.468738 59.900698 39.784837 60.696307 39.784837 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 184.096479 \nC 61.852451 184.096479 62.615581 183.78038 63.178162 183.217799 \nC 63.740743 182.655218 64.056842 181.892088 64.056842 181.096479 \nC 64.056842 180.300869 63.740743 179.537739 63.178162 178.975158 \nC 62.615581 178.412578 61.852451 178.096479 61.056842 178.096479 \nC 60.261233 178.096479 59.498102 178.412578 58.935521 178.975158 \nC 58.372941 179.537739 58.056842 180.300869 58.056842 181.096479 \nC 58.056842 181.892088 58.372941 182.655218 58.935521 183.217799 \nC 59.498102 183.78038 60.261233 184.096479 61.056842 184.096479 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 182.828176 \nC 67.044155 182.828176 67.807285 182.512077 68.369866 181.949496 \nC 68.932447 181.386916 69.248546 180.623785 69.248546 179.828176 \nC 69.248546 179.032567 68.932447 178.269436 68.369866 177.706856 \nC 67.807285 177.144275 67.044155 176.828176 66.248546 176.828176 \nC 65.452936 176.828176 64.689806 177.144275 64.127225 177.706856 \nC 63.564645 178.269436 63.248546 179.032567 63.248546 179.828176 \nC 63.248546 180.623785 63.564645 181.386916 64.127225 181.949496 \nC 64.689806 182.512077 65.452936 182.828176 66.248546 182.828176 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 177.141678 \nC 82.04241 177.141678 82.805541 176.825579 83.368121 176.262999 \nC 83.930702 175.700418 84.246801 174.937288 84.246801 174.141678 \nC 84.246801 173.346069 83.930702 172.582939 83.368121 172.020358 \nC 82.805541 171.457777 82.04241 171.141678 81.246801 171.141678 \nC 80.451192 171.141678 79.688061 171.457777 79.125481 172.020358 \nC 78.5629 172.582939 78.246801 173.346069 78.246801 174.141678 \nC 78.246801 174.937288 78.5629 175.700418 79.125481 176.262999 \nC 79.688061 176.825579 80.451192 177.141678 81.246801 177.141678 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 121.954925 \nC 365.855552 121.954925 366.618683 121.638826 367.181264 121.076246 \nC 367.743844 120.513665 368.059943 119.750535 368.059943 118.954925 \nC 368.059943 118.159316 367.743844 117.396186 367.181264 116.833605 \nC 366.618683 116.271024 365.855552 115.954925 365.059943 115.954925 \nC 364.264334 115.954925 363.501204 116.271024 362.938623 116.833605 \nC 362.376042 117.396186 362.059943 118.159316 362.059943 118.954925 \nC 362.059943 119.750535 362.376042 120.513665 362.938623 121.076246 \nC 363.501204 121.638826 364.264334 121.954925 365.059943 121.954925 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 24.123379 \nC 61.708237 24.123379 62.471367 23.80728 63.033948 23.2447 \nC 63.596529 22.682119 63.912628 21.918989 63.912628 21.123379 \nC 63.912628 20.32777 63.596529 19.56464 63.033948 19.002059 \nC 62.471367 18.439478 61.708237 18.123379 60.912628 18.123379 \nC 60.117019 18.123379 59.353888 18.439478 58.791307 19.002059 \nC 58.228727 19.56464 57.912628 20.32777 57.912628 21.123379 \nC 57.912628 21.918989 58.228727 22.682119 58.791307 23.2447 \nC 59.353888 23.80728 60.117019 24.123379 60.912628 24.123379 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 181.130567 \nC 63.006163 181.130567 63.769293 180.814468 64.331874 180.251888 \nC 64.894455 179.689307 65.210554 178.926177 65.210554 178.130567 \nC 65.210554 177.334958 64.894455 176.571828 64.331874 176.009247 \nC 63.769293 175.446666 63.006163 175.130567 62.210554 175.130567 \nC 61.414944 175.130567 60.651814 175.446666 60.089233 176.009247 \nC 59.526653 176.571828 59.210554 177.334958 59.210554 178.130567 \nC 59.210554 178.926177 59.526653 179.689307 60.089233 180.251888 \nC 60.651814 180.814468 61.414944 181.130567 62.210554 181.130567 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 159.506938 \nC 65.602015 159.506938 66.365145 159.190839 66.927726 158.628258 \nC 67.490307 158.065677 67.806406 157.302547 67.806406 156.506938 \nC 67.806406 155.711329 67.490307 154.948198 66.927726 154.385618 \nC 66.365145 153.823037 65.602015 153.506938 64.806406 153.506938 \nC 64.010796 153.506938 63.247666 153.823037 62.685085 154.385618 \nC 62.122505 154.948198 61.806406 155.711329 61.806406 156.506938 \nC 61.806406 157.302547 62.122505 158.065677 62.685085 158.628258 \nC 63.247666 159.190839 64.010796 159.506938 64.806406 159.506938 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 162.423426 \nC 70.793719 162.423426 71.556849 162.107327 72.11943 161.544746 \nC 72.682011 160.982166 72.998109 160.219035 72.998109 159.423426 \nC 72.998109 158.627817 72.682011 157.864686 72.11943 157.302106 \nC 71.556849 156.739525 70.793719 156.423426 69.998109 156.423426 \nC 69.2025 156.423426 68.43937 156.739525 67.876789 157.302106 \nC 67.314208 157.864686 66.998109 158.627817 66.998109 159.423426 \nC 66.998109 160.219035 67.314208 160.982166 67.876789 161.544746 \nC 68.43937 162.107327 69.2025 162.423426 69.998109 162.423426 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 21.781372 \nC 101.943942 21.781372 102.707072 21.465273 103.269653 20.902693 \nC 103.832233 20.340112 104.148332 19.576982 104.148332 18.781372 \nC 104.148332 17.985763 103.832233 17.222633 103.269653 16.660052 \nC 102.707072 16.097471 101.943942 15.781372 101.148332 15.781372 \nC 100.352723 15.781372 99.589593 16.097471 99.027012 16.660052 \nC 98.464431 17.222633 98.148332 17.985763 98.148332 18.781372 \nC 98.148332 19.576982 98.464431 20.340112 99.027012 20.902693 \nC 99.589593 21.465273 100.352723 21.781372 101.148332 21.781372 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 217.756364 \nC 61.491916 217.756364 62.255046 217.440265 62.817627 216.877684 \nC 63.380208 216.315103 63.696307 215.551973 63.696307 214.756364 \nC 63.696307 213.960754 63.380208 213.197624 62.817627 212.635043 \nC 62.255046 212.072463 61.491916 211.756364 60.696307 211.756364 \nC 59.900698 211.756364 59.137567 212.072463 58.574986 212.635043 \nC 58.012406 213.197624 57.696307 213.960754 57.696307 214.756364 \nC 57.696307 215.551973 58.012406 216.315103 58.574986 216.877684 \nC 59.137567 217.440265 59.900698 217.756364 60.696307 217.756364 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 32.054153 \nC 61.852451 32.054153 62.615581 31.738054 63.178162 31.175473 \nC 63.740743 30.612893 64.056842 29.849762 64.056842 29.054153 \nC 64.056842 28.258544 63.740743 27.495413 63.178162 26.932833 \nC 62.615581 26.370252 61.852451 26.054153 61.056842 26.054153 \nC 60.261233 26.054153 59.498102 26.370252 58.935521 26.932833 \nC 58.372941 27.495413 58.056842 28.258544 58.056842 29.054153 \nC 58.056842 29.849762 58.372941 30.612893 58.935521 31.175473 \nC 59.498102 31.738054 60.261233 32.054153 61.056842 32.054153 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 203.889132 \nC 63.006163 203.889132 63.769293 203.573033 64.331874 203.010452 \nC 64.894455 202.447871 65.210554 201.684741 65.210554 200.889132 \nC 65.210554 200.093522 64.894455 199.330392 64.331874 198.767811 \nC 63.769293 198.20523 63.006163 197.889132 62.210554 197.889132 \nC 61.414944 197.889132 60.651814 198.20523 60.089233 198.767811 \nC 59.526653 199.330392 59.210554 200.093522 59.210554 200.889132 \nC 59.210554 201.684741 59.526653 202.447871 60.089233 203.010452 \nC 60.651814 203.573033 61.414944 203.889132 62.210554 203.889132 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 177.377167 \nC 67.044155 177.377167 67.807285 177.061068 68.369866 176.498487 \nC 68.932447 175.935906 69.248546 175.172776 69.248546 174.377167 \nC 69.248546 173.581557 68.932447 172.818427 68.369866 172.255846 \nC 67.807285 171.693266 67.044155 171.377167 66.248546 171.377167 \nC 65.452936 171.377167 64.689806 171.693266 64.127225 172.255846 \nC 63.564645 172.818427 63.248546 173.581557 63.248546 174.377167 \nC 63.248546 175.172776 63.564645 175.935906 64.127225 176.498487 \nC 64.689806 177.061068 65.452936 177.377167 66.248546 177.377167 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 158.141345 \nC 82.04241 158.141345 82.805541 157.825247 83.368121 157.262666 \nC 83.930702 156.700085 84.246801 155.936955 84.246801 155.141345 \nC 84.246801 154.345736 83.930702 153.582606 83.368121 153.020025 \nC 82.805541 152.457444 82.04241 152.141345 81.246801 152.141345 \nC 80.451192 152.141345 79.688061 152.457444 79.125481 153.020025 \nC 78.5629 153.582606 78.246801 154.345736 78.246801 155.141345 \nC 78.246801 155.936955 78.5629 156.700085 79.125481 157.262666 \nC 79.688061 157.825247 80.451192 158.141345 81.246801 158.141345 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 168.654856 \nC 365.855552 168.654856 366.618683 168.338757 367.181264 167.776176 \nC 367.743844 167.213596 368.059943 166.450465 368.059943 165.654856 \nC 368.059943 164.859247 367.743844 164.096117 367.181264 163.533536 \nC 366.618683 162.970955 365.855552 162.654856 365.059943 162.654856 \nC 364.264334 162.654856 363.501204 162.970955 362.938623 163.533536 \nC 362.376042 164.096117 362.059943 164.859247 362.059943 165.654856 \nC 362.059943 166.450465 362.376042 167.213596 362.938623 167.776176 \nC 363.501204 168.338757 364.264334 168.654856 365.059943 168.654856 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 179.616767 \nC 61.708237 179.616767 62.471367 179.300668 63.033948 178.738088 \nC 63.596529 178.175507 63.912628 177.412376 63.912628 176.616767 \nC 63.912628 175.821158 63.596529 175.058028 63.033948 174.495447 \nC 62.471367 173.932866 61.708237 173.616767 60.912628 173.616767 \nC 60.117019 173.616767 59.353888 173.932866 58.791307 174.495447 \nC 58.228727 175.058028 57.912628 175.821158 57.912628 176.616767 \nC 57.912628 177.412376 58.228727 178.175507 58.791307 178.738088 \nC 59.353888 179.300668 60.117019 179.616767 60.912628 179.616767 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 173.801973 \nC 63.006163 173.801973 63.769293 173.485874 64.331874 172.923294 \nC 64.894455 172.360713 65.210554 171.597583 65.210554 170.801973 \nC 65.210554 170.006364 64.894455 169.243234 64.331874 168.680653 \nC 63.769293 168.118072 63.006163 167.801973 62.210554 167.801973 \nC 61.414944 167.801973 60.651814 168.118072 60.089233 168.680653 \nC 59.526653 169.243234 59.210554 170.006364 59.210554 170.801973 \nC 59.210554 171.597583 59.526653 172.360713 60.089233 172.923294 \nC 60.651814 173.485874 61.414944 173.801973 62.210554 173.801973 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 161.933344 \nC 65.602015 161.933344 66.365145 161.617245 66.927726 161.054665 \nC 67.490307 160.492084 67.806406 159.728954 67.806406 158.933344 \nC 67.806406 158.137735 67.490307 157.374605 66.927726 156.812024 \nC 66.365145 156.249443 65.602015 155.933344 64.806406 155.933344 \nC 64.010796 155.933344 63.247666 156.249443 62.685085 156.812024 \nC 62.122505 157.374605 61.806406 158.137735 61.806406 158.933344 \nC 61.806406 159.728954 62.122505 160.492084 62.685085 161.054665 \nC 63.247666 161.617245 64.010796 161.933344 64.806406 161.933344 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 168.151992 \nC 81.177126 168.151992 81.940257 167.835893 82.502837 167.273312 \nC 83.065418 166.710732 83.381517 165.947601 83.381517 165.151992 \nC 83.381517 164.356383 83.065418 163.593252 82.502837 163.030672 \nC 81.940257 162.468091 81.177126 162.151992 80.381517 162.151992 \nC 79.585908 162.151992 78.822778 162.468091 78.260197 163.030672 \nC 77.697616 163.593252 77.381517 164.356383 77.381517 165.151992 \nC 77.381517 165.947601 77.697616 166.710732 78.260197 167.273312 \nC 78.822778 167.835893 79.585908 168.151992 80.381517 168.151992 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 114.101764 \nC 61.491916 114.101764 62.255046 113.785665 62.817627 113.223084 \nC 63.380208 112.660503 63.696307 111.897373 63.696307 111.101764 \nC 63.696307 110.306154 63.380208 109.543024 62.817627 108.980443 \nC 62.255046 108.417862 61.491916 108.101764 60.696307 108.101764 \nC 59.900698 108.101764 59.137567 108.417862 58.574986 108.980443 \nC 58.012406 109.543024 57.696307 110.306154 57.696307 111.101764 \nC 57.696307 111.897373 58.012406 112.660503 58.574986 113.223084 \nC 59.137567 113.785665 59.900698 114.101764 60.696307 114.101764 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 37.079252 \nC 63.006163 37.079252 63.769293 36.763153 64.331874 36.200572 \nC 64.894455 35.637992 65.210554 34.874861 65.210554 34.079252 \nC 65.210554 33.283643 64.894455 32.520512 64.331874 31.957932 \nC 63.769293 31.395351 63.006163 31.079252 62.210554 31.079252 \nC 61.414944 31.079252 60.651814 31.395351 60.089233 31.957932 \nC 59.526653 32.520512 59.210554 33.283643 59.210554 34.079252 \nC 59.210554 34.874861 59.526653 35.637992 60.089233 36.200572 \nC 60.651814 36.763153 61.414944 37.079252 62.210554 37.079252 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 180.08342 \nC 67.044155 180.08342 67.807285 179.767321 68.369866 179.20474 \nC 68.932447 178.642159 69.248546 177.879029 69.248546 177.08342 \nC 69.248546 176.28781 68.932447 175.52468 68.369866 174.962099 \nC 67.807285 174.399519 67.044155 174.08342 66.248546 174.08342 \nC 65.452936 174.08342 64.689806 174.399519 64.127225 174.962099 \nC 63.564645 175.52468 63.248546 176.28781 63.248546 177.08342 \nC 63.248546 177.879029 63.564645 178.642159 64.127225 179.20474 \nC 64.689806 179.767321 65.452936 180.08342 66.248546 180.08342 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 190.247634 \nC 82.04241 190.247634 82.805541 189.931536 83.368121 189.368955 \nC 83.930702 188.806374 84.246801 188.043244 84.246801 187.247634 \nC 84.246801 186.452025 83.930702 185.688895 83.368121 185.126314 \nC 82.805541 184.563733 82.04241 184.247634 81.246801 184.247634 \nC 80.451192 184.247634 79.688061 184.563733 79.125481 185.126314 \nC 78.5629 185.688895 78.246801 186.452025 78.246801 187.247634 \nC 78.246801 188.043244 78.5629 188.806374 79.125481 189.368955 \nC 79.688061 189.931536 80.451192 190.247634 81.246801 190.247634 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 178.265764 \nC 139.728008 178.265764 140.491139 177.949665 141.053719 177.387084 \nC 141.6163 176.824503 141.932399 176.061373 141.932399 175.265764 \nC 141.932399 174.470154 141.6163 173.707024 141.053719 173.144443 \nC 140.491139 172.581863 139.728008 172.265764 138.932399 172.265764 \nC 138.13679 172.265764 137.373659 172.581863 136.811079 173.144443 \nC 136.248498 173.707024 135.932399 174.470154 135.932399 175.265764 \nC 135.932399 176.061373 136.248498 176.824503 136.811079 177.387084 \nC 137.373659 177.949665 138.13679 178.265764 138.932399 178.265764 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 172.32881 \nC 61.708237 172.32881 62.471367 172.012711 63.033948 171.45013 \nC 63.596529 170.88755 63.912628 170.124419 63.912628 169.32881 \nC 63.912628 168.533201 63.596529 167.77007 63.033948 167.20749 \nC 62.471367 166.644909 61.708237 166.32881 60.912628 166.32881 \nC 60.117019 166.32881 59.353888 166.644909 58.791307 167.20749 \nC 58.228727 167.77007 57.912628 168.533201 57.912628 169.32881 \nC 57.912628 170.124419 58.228727 170.88755 58.791307 171.45013 \nC 59.353888 172.012711 60.117019 172.32881 60.912628 172.32881 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 191.888137 \nC 63.006163 191.888137 63.769293 191.572038 64.331874 191.009457 \nC 64.894455 190.446877 65.210554 189.683746 65.210554 188.888137 \nC 65.210554 188.092528 64.894455 187.329397 64.331874 186.766817 \nC 63.769293 186.204236 63.006163 185.888137 62.210554 185.888137 \nC 61.414944 185.888137 60.651814 186.204236 60.089233 186.766817 \nC 59.526653 187.329397 59.210554 188.092528 59.210554 188.888137 \nC 59.210554 189.683746 59.526653 190.446877 60.089233 191.009457 \nC 60.651814 191.572038 61.414944 191.888137 62.210554 191.888137 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 166.189981 \nC 65.602015 166.189981 66.365145 165.873882 66.927726 165.311302 \nC 67.490307 164.748721 67.806406 163.985591 67.806406 163.189981 \nC 67.806406 162.394372 67.490307 161.631242 66.927726 161.068661 \nC 66.365145 160.50608 65.602015 160.189981 64.806406 160.189981 \nC 64.010796 160.189981 63.247666 160.50608 62.685085 161.068661 \nC 62.122505 161.631242 61.806406 162.394372 61.806406 163.189981 \nC 61.806406 163.985591 62.122505 164.748721 62.685085 165.311302 \nC 63.247666 165.873882 64.010796 166.189981 64.806406 166.189981 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 190.334819 \nC 70.793719 190.334819 71.556849 190.01872 72.11943 189.456139 \nC 72.682011 188.893559 72.998109 188.130428 72.998109 187.334819 \nC 72.998109 186.53921 72.682011 185.77608 72.11943 185.213499 \nC 71.556849 184.650918 70.793719 184.334819 69.998109 184.334819 \nC 69.2025 184.334819 68.43937 184.650918 67.876789 185.213499 \nC 67.314208 185.77608 66.998109 186.53921 66.998109 187.334819 \nC 66.998109 188.130428 67.314208 188.893559 67.876789 189.456139 \nC 68.43937 190.01872 69.2025 190.334819 69.998109 190.334819 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 80.232435 \nC 101.943942 80.232435 102.707072 79.916336 103.269653 79.353755 \nC 103.832233 78.791175 104.148332 78.028044 104.148332 77.232435 \nC 104.148332 76.436826 103.832233 75.673696 103.269653 75.111115 \nC 102.707072 74.548534 101.943942 74.232435 101.148332 74.232435 \nC 100.352723 74.232435 99.589593 74.548534 99.027012 75.111115 \nC 98.464431 75.673696 98.148332 76.436826 98.148332 77.232435 \nC 98.148332 78.028044 98.464431 78.791175 99.027012 79.353755 \nC 99.589593 79.916336 100.352723 80.232435 101.148332 80.232435 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 103.298221 \nC 61.491916 103.298221 62.255046 102.982122 62.817627 102.419541 \nC 63.380208 101.85696 63.696307 101.09383 63.696307 100.298221 \nC 63.696307 99.502612 63.380208 98.739481 62.817627 98.176901 \nC 62.255046 97.61432 61.491916 97.298221 60.696307 97.298221 \nC 59.900698 97.298221 59.137567 97.61432 58.574986 98.176901 \nC 58.012406 98.739481 57.696307 99.502612 57.696307 100.298221 \nC 57.696307 101.09383 58.012406 101.85696 58.574986 102.419541 \nC 59.137567 102.982122 59.900698 103.298221 60.696307 103.298221 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 176.679567 \nC 63.006163 176.679567 63.769293 176.363468 64.331874 175.800887 \nC 64.894455 175.238307 65.210554 174.475176 65.210554 173.679567 \nC 65.210554 172.883958 64.894455 172.120827 64.331874 171.558247 \nC 63.769293 170.995666 63.006163 170.679567 62.210554 170.679567 \nC 61.414944 170.679567 60.651814 170.995666 60.089233 171.558247 \nC 59.526653 172.120827 59.210554 172.883958 59.210554 173.679567 \nC 59.210554 174.475176 59.526653 175.238307 60.089233 175.800887 \nC 60.651814 176.363468 61.414944 176.679567 62.210554 176.679567 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 52.192841 \nC 67.044155 52.192841 67.807285 51.876742 68.369866 51.314161 \nC 68.932447 50.75158 69.248546 49.98845 69.248546 49.192841 \nC 69.248546 48.397231 68.932447 47.634101 68.369866 47.07152 \nC 67.807285 46.508939 67.044155 46.192841 66.248546 46.192841 \nC 65.452936 46.192841 64.689806 46.508939 64.127225 47.07152 \nC 63.564645 47.634101 63.248546 48.397231 63.248546 49.192841 \nC 63.248546 49.98845 63.564645 50.75158 64.127225 51.314161 \nC 64.689806 51.876742 65.452936 52.192841 66.248546 52.192841 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 158.861199 \nC 82.04241 158.861199 82.805541 158.5451 83.368121 157.982519 \nC 83.930702 157.419939 84.246801 156.656808 84.246801 155.861199 \nC 84.246801 155.06559 83.930702 154.302459 83.368121 153.739879 \nC 82.805541 153.177298 82.04241 152.861199 81.246801 152.861199 \nC 80.451192 152.861199 79.688061 153.177298 79.125481 153.739879 \nC 78.5629 154.302459 78.246801 155.06559 78.246801 155.861199 \nC 78.246801 156.656808 78.5629 157.419939 79.125481 157.982519 \nC 79.688061 158.5451 80.451192 158.861199 81.246801 158.861199 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 159.67982 \nC 139.728008 159.67982 140.491139 159.363721 141.053719 158.801141 \nC 141.6163 158.23856 141.932399 157.47543 141.932399 156.67982 \nC 141.932399 155.884211 141.6163 155.121081 141.053719 154.5585 \nC 140.491139 153.995919 139.728008 153.67982 138.932399 153.67982 \nC 138.13679 153.67982 137.373659 153.995919 136.811079 154.5585 \nC 136.248498 155.121081 135.932399 155.884211 135.932399 156.67982 \nC 135.932399 157.47543 136.248498 158.23856 136.811079 158.801141 \nC 137.373659 159.363721 138.13679 159.67982 138.932399 159.67982 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 166.887612 \nC 61.708237 166.887612 62.471367 166.571513 63.033948 166.008932 \nC 63.596529 165.446351 63.912628 164.683221 63.912628 163.887612 \nC 63.912628 163.092003 63.596529 162.328872 63.033948 161.766292 \nC 62.471367 161.203711 61.708237 160.887612 60.912628 160.887612 \nC 60.117019 160.887612 59.353888 161.203711 58.791307 161.766292 \nC 58.228727 162.328872 57.912628 163.092003 57.912628 163.887612 \nC 57.912628 164.683221 58.228727 165.446351 58.791307 166.008932 \nC 59.353888 166.571513 60.117019 166.887612 60.912628 166.887612 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 157.72052 \nC 65.602015 157.72052 66.365145 157.404421 66.927726 156.84184 \nC 67.490307 156.279259 67.806406 155.516129 67.806406 154.72052 \nC 67.806406 153.92491 67.490307 153.16178 66.927726 152.599199 \nC 66.365145 152.036619 65.602015 151.72052 64.806406 151.72052 \nC 64.010796 151.72052 63.247666 152.036619 62.685085 152.599199 \nC 62.122505 153.16178 61.806406 153.92491 61.806406 154.72052 \nC 61.806406 155.516129 62.122505 156.279259 62.685085 156.84184 \nC 63.247666 157.404421 64.010796 157.72052 64.806406 157.72052 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 165.943856 \nC 70.793719 165.943856 71.556849 165.627757 72.11943 165.065176 \nC 72.682011 164.502595 72.998109 163.739465 72.998109 162.943856 \nC 72.998109 162.148246 72.682011 161.385116 72.11943 160.822535 \nC 71.556849 160.259955 70.793719 159.943856 69.998109 159.943856 \nC 69.2025 159.943856 68.43937 160.259955 67.876789 160.822535 \nC 67.314208 161.385116 66.998109 162.148246 66.998109 162.943856 \nC 66.998109 163.739465 67.314208 164.502595 67.876789 165.065176 \nC 68.43937 165.627757 69.2025 165.943856 69.998109 165.943856 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 158.932938 \nC 81.177126 158.932938 81.940257 158.616839 82.502837 158.054259 \nC 83.065418 157.491678 83.381517 156.728548 83.381517 155.932938 \nC 83.381517 155.137329 83.065418 154.374199 82.502837 153.811618 \nC 81.940257 153.249037 81.177126 152.932938 80.381517 152.932938 \nC 79.585908 152.932938 78.822778 153.249037 78.260197 153.811618 \nC 77.697616 154.374199 77.381517 155.137329 77.381517 155.932938 \nC 77.381517 156.728548 77.697616 157.491678 78.260197 158.054259 \nC 78.822778 158.616839 79.585908 158.932938 80.381517 158.932938 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 30.122361 \nC 101.943942 30.122361 102.707072 29.806262 103.269653 29.243681 \nC 103.832233 28.6811 104.148332 27.91797 104.148332 27.122361 \nC 104.148332 26.326752 103.832233 25.563621 103.269653 25.001041 \nC 102.707072 24.43846 101.943942 24.122361 101.148332 24.122361 \nC 100.352723 24.122361 99.589593 24.43846 99.027012 25.001041 \nC 98.464431 25.563621 98.148332 26.326752 98.148332 27.122361 \nC 98.148332 27.91797 98.464431 28.6811 99.027012 29.243681 \nC 99.589593 29.806262 100.352723 30.122361 101.148332 30.122361 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 105.169345 \nC 61.491916 105.169345 62.255046 104.853246 62.817627 104.290665 \nC 63.380208 103.728084 63.696307 102.964954 63.696307 102.169345 \nC 63.696307 101.373735 63.380208 100.610605 62.817627 100.048024 \nC 62.255046 99.485443 61.491916 99.169345 60.696307 99.169345 \nC 59.900698 99.169345 59.137567 99.485443 58.574986 100.048024 \nC 58.012406 100.610605 57.696307 101.373735 57.696307 102.169345 \nC 57.696307 102.964954 58.012406 103.728084 58.574986 104.290665 \nC 59.137567 104.853246 59.900698 105.169345 60.696307 105.169345 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 105.396488 \nC 63.006163 105.396488 63.769293 105.080389 64.331874 104.517809 \nC 64.894455 103.955228 65.210554 103.192098 65.210554 102.396488 \nC 65.210554 101.600879 64.894455 100.837749 64.331874 100.275168 \nC 63.769293 99.712587 63.006163 99.396488 62.210554 99.396488 \nC 61.414944 99.396488 60.651814 99.712587 60.089233 100.275168 \nC 59.526653 100.837749 59.210554 101.600879 59.210554 102.396488 \nC 59.210554 103.192098 59.526653 103.955228 60.089233 104.517809 \nC 60.651814 105.080389 61.414944 105.396488 62.210554 105.396488 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 182.342141 \nC 67.044155 182.342141 67.807285 182.026042 68.369866 181.463462 \nC 68.932447 180.900881 69.248546 180.13775 69.248546 179.342141 \nC 69.248546 178.546532 68.932447 177.783402 68.369866 177.220821 \nC 67.807285 176.65824 67.044155 176.342141 66.248546 176.342141 \nC 65.452936 176.342141 64.689806 176.65824 64.127225 177.220821 \nC 63.564645 177.783402 63.248546 178.546532 63.248546 179.342141 \nC 63.248546 180.13775 63.564645 180.900881 64.127225 181.463462 \nC 64.689806 182.026042 65.452936 182.342141 66.248546 182.342141 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 169.590967 \nC 82.04241 169.590967 82.805541 169.274868 83.368121 168.712288 \nC 83.930702 168.149707 84.246801 167.386577 84.246801 166.590967 \nC 84.246801 165.795358 83.930702 165.032228 83.368121 164.469647 \nC 82.805541 163.907066 82.04241 163.590967 81.246801 163.590967 \nC 80.451192 163.590967 79.688061 163.907066 79.125481 164.469647 \nC 78.5629 165.032228 78.246801 165.795358 78.246801 166.590967 \nC 78.246801 167.386577 78.5629 168.149707 79.125481 168.712288 \nC 79.688061 169.274868 80.451192 169.590967 81.246801 169.590967 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 126.042954 \nC 365.855552 126.042954 366.618683 125.726855 367.181264 125.164275 \nC 367.743844 124.601694 368.059943 123.838564 368.059943 123.042954 \nC 368.059943 122.247345 367.743844 121.484215 367.181264 120.921634 \nC 366.618683 120.359053 365.855552 120.042954 365.059943 120.042954 \nC 364.264334 120.042954 363.501204 120.359053 362.938623 120.921634 \nC 362.376042 121.484215 362.059943 122.247345 362.059943 123.042954 \nC 362.059943 123.838564 362.376042 124.601694 362.938623 125.164275 \nC 363.501204 125.726855 364.264334 126.042954 365.059943 126.042954 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 105.568312 \nC 61.708237 105.568312 62.471367 105.252214 63.033948 104.689633 \nC 63.596529 104.127052 63.912628 103.363922 63.912628 102.568312 \nC 63.912628 101.772703 63.596529 101.009573 63.033948 100.446992 \nC 62.471367 99.884411 61.708237 99.568312 60.912628 99.568312 \nC 60.117019 99.568312 59.353888 99.884411 58.791307 100.446992 \nC 58.228727 101.009573 57.912628 101.772703 57.912628 102.568312 \nC 57.912628 103.363922 58.228727 104.127052 58.791307 104.689633 \nC 59.353888 105.252214 60.117019 105.568312 60.912628 105.568312 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 137.605998 \nC 65.602015 137.605998 66.365145 137.289899 66.927726 136.727318 \nC 67.490307 136.164737 67.806406 135.401607 67.806406 134.605998 \nC 67.806406 133.810388 67.490307 133.047258 66.927726 132.484677 \nC 66.365145 131.922097 65.602015 131.605998 64.806406 131.605998 \nC 64.010796 131.605998 63.247666 131.922097 62.685085 132.484677 \nC 62.122505 133.047258 61.806406 133.810388 61.806406 134.605998 \nC 61.806406 135.401607 62.122505 136.164737 62.685085 136.727318 \nC 63.247666 137.289899 64.010796 137.605998 64.806406 137.605998 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 131.37357 \nC 70.793719 131.37357 71.556849 131.057471 72.11943 130.49489 \nC 72.682011 129.93231 72.998109 129.169179 72.998109 128.37357 \nC 72.998109 127.577961 72.682011 126.81483 72.11943 126.25225 \nC 71.556849 125.689669 70.793719 125.37357 69.998109 125.37357 \nC 69.2025 125.37357 68.43937 125.689669 67.876789 126.25225 \nC 67.314208 126.81483 66.998109 127.577961 66.998109 128.37357 \nC 66.998109 129.169179 67.314208 129.93231 67.876789 130.49489 \nC 68.43937 131.057471 69.2025 131.37357 69.998109 131.37357 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 32.330201 \nC 101.943942 32.330201 102.707072 32.014102 103.269653 31.451521 \nC 103.832233 30.88894 104.148332 30.12581 104.148332 29.330201 \nC 104.148332 28.534591 103.832233 27.771461 103.269653 27.20888 \nC 102.707072 26.646299 101.943942 26.330201 101.148332 26.330201 \nC 100.352723 26.330201 99.589593 26.646299 99.027012 27.20888 \nC 98.464431 27.771461 98.148332 28.534591 98.148332 29.330201 \nC 98.148332 30.12581 98.464431 30.88894 99.027012 31.451521 \nC 99.589593 32.014102 100.352723 32.330201 101.148332 32.330201 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 104.842575 \nC 61.491916 104.842575 62.255046 104.526476 62.817627 103.963895 \nC 63.380208 103.401314 63.696307 102.638184 63.696307 101.842575 \nC 63.696307 101.046965 63.380208 100.283835 62.817627 99.721254 \nC 62.255046 99.158673 61.491916 98.842575 60.696307 98.842575 \nC 59.900698 98.842575 59.137567 99.158673 58.574986 99.721254 \nC 58.012406 100.283835 57.696307 101.046965 57.696307 101.842575 \nC 57.696307 102.638184 58.012406 103.401314 58.574986 103.963895 \nC 59.137567 104.526476 59.900698 104.842575 60.696307 104.842575 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 159.160336 \nC 61.852451 159.160336 62.615581 158.844237 63.178162 158.281656 \nC 63.740743 157.719075 64.056842 156.955945 64.056842 156.160336 \nC 64.056842 155.364726 63.740743 154.601596 63.178162 154.039015 \nC 62.615581 153.476434 61.852451 153.160336 61.056842 153.160336 \nC 60.261233 153.160336 59.498102 153.476434 58.935521 154.039015 \nC 58.372941 154.601596 58.056842 155.364726 58.056842 156.160336 \nC 58.056842 156.955945 58.372941 157.719075 58.935521 158.281656 \nC 59.498102 158.844237 60.261233 159.160336 61.056842 159.160336 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 176.352045 \nC 63.006163 176.352045 63.769293 176.035947 64.331874 175.473366 \nC 64.894455 174.910785 65.210554 174.147655 65.210554 173.352045 \nC 65.210554 172.556436 64.894455 171.793306 64.331874 171.230725 \nC 63.769293 170.668144 63.006163 170.352045 62.210554 170.352045 \nC 61.414944 170.352045 60.651814 170.668144 60.089233 171.230725 \nC 59.526653 171.793306 59.210554 172.556436 59.210554 173.352045 \nC 59.210554 174.147655 59.526653 174.910785 60.089233 175.473366 \nC 60.651814 176.035947 61.414944 176.352045 62.210554 176.352045 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 166.537319 \nC 67.044155 166.537319 67.807285 166.22122 68.369866 165.658639 \nC 68.932447 165.096058 69.248546 164.332928 69.248546 163.537319 \nC 69.248546 162.741709 68.932447 161.978579 68.369866 161.415998 \nC 67.807285 160.853418 67.044155 160.537319 66.248546 160.537319 \nC 65.452936 160.537319 64.689806 160.853418 64.127225 161.415998 \nC 63.564645 161.978579 63.248546 162.741709 63.248546 163.537319 \nC 63.248546 164.332928 63.564645 165.096058 64.127225 165.658639 \nC 64.689806 166.22122 65.452936 166.537319 66.248546 166.537319 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 171.60255 \nC 82.04241 171.60255 82.805541 171.286451 83.368121 170.72387 \nC 83.930702 170.161289 84.246801 169.398159 84.246801 168.60255 \nC 84.246801 167.80694 83.930702 167.04381 83.368121 166.481229 \nC 82.805541 165.918649 82.04241 165.60255 81.246801 165.60255 \nC 80.451192 165.60255 79.688061 165.918649 79.125481 166.481229 \nC 78.5629 167.04381 78.246801 167.80694 78.246801 168.60255 \nC 78.246801 169.398159 78.5629 170.161289 79.125481 170.72387 \nC 79.688061 171.286451 80.451192 171.60255 81.246801 171.60255 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 179.75812 \nC 63.006163 179.75812 63.769293 179.442022 64.331874 178.879441 \nC 64.894455 178.31686 65.210554 177.55373 65.210554 176.75812 \nC 65.210554 175.962511 64.894455 175.199381 64.331874 174.6368 \nC 63.769293 174.074219 63.006163 173.75812 62.210554 173.75812 \nC 61.414944 173.75812 60.651814 174.074219 60.089233 174.6368 \nC 59.526653 175.199381 59.210554 175.962511 59.210554 176.75812 \nC 59.210554 177.55373 59.526653 178.31686 60.089233 178.879441 \nC 60.651814 179.442022 61.414944 179.75812 62.210554 179.75812 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 154.687852 \nC 65.602015 154.687852 66.365145 154.371753 66.927726 153.809172 \nC 67.490307 153.246591 67.806406 152.483461 67.806406 151.687852 \nC 67.806406 150.892242 67.490307 150.129112 66.927726 149.566531 \nC 66.365145 149.003951 65.602015 148.687852 64.806406 148.687852 \nC 64.010796 148.687852 63.247666 149.003951 62.685085 149.566531 \nC 62.122505 150.129112 61.806406 150.892242 61.806406 151.687852 \nC 61.806406 152.483461 62.122505 153.246591 62.685085 153.809172 \nC 63.247666 154.371753 64.010796 154.687852 64.806406 154.687852 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 159.890776 \nC 70.793719 159.890776 71.556849 159.574677 72.11943 159.012096 \nC 72.682011 158.449515 72.998109 157.686385 72.998109 156.890776 \nC 72.998109 156.095167 72.682011 155.332036 72.11943 154.769455 \nC 71.556849 154.206875 70.793719 153.890776 69.998109 153.890776 \nC 69.2025 153.890776 68.43937 154.206875 67.876789 154.769455 \nC 67.314208 155.332036 66.998109 156.095167 66.998109 156.890776 \nC 66.998109 157.686385 67.314208 158.449515 67.876789 159.012096 \nC 68.43937 159.574677 69.2025 159.890776 69.998109 159.890776 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 80.381517 27.251729 \nC 81.177126 27.251729 81.940257 26.93563 82.502837 26.37305 \nC 83.065418 25.810469 83.381517 25.047339 83.381517 24.251729 \nC 83.381517 23.45612 83.065418 22.69299 82.502837 22.130409 \nC 81.940257 21.567828 81.177126 21.251729 80.381517 21.251729 \nC 79.585908 21.251729 78.822778 21.567828 78.260197 22.130409 \nC 77.697616 22.69299 77.381517 23.45612 77.381517 24.251729 \nC 77.381517 25.047339 77.697616 25.810469 78.260197 26.37305 \nC 78.822778 26.93563 79.585908 27.251729 80.381517 27.251729 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 195.949266 \nC 63.006163 195.949266 63.769293 195.633167 64.331874 195.070586 \nC 64.894455 194.508006 65.210554 193.744875 65.210554 192.949266 \nC 65.210554 192.153657 64.894455 191.390527 64.331874 190.827946 \nC 63.769293 190.265365 63.006163 189.949266 62.210554 189.949266 \nC 61.414944 189.949266 60.651814 190.265365 60.089233 190.827946 \nC 59.526653 191.390527 59.210554 192.153657 59.210554 192.949266 \nC 59.210554 193.744875 59.526653 194.508006 60.089233 195.070586 \nC 60.651814 195.633167 61.414944 195.949266 62.210554 195.949266 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 182.075147 \nC 67.044155 182.075147 67.807285 181.759048 68.369866 181.196467 \nC 68.932447 180.633886 69.248546 179.870756 69.248546 179.075147 \nC 69.248546 178.279537 68.932447 177.516407 68.369866 176.953826 \nC 67.807285 176.391246 67.044155 176.075147 66.248546 176.075147 \nC 65.452936 176.075147 64.689806 176.391246 64.127225 176.953826 \nC 63.564645 177.516407 63.248546 178.279537 63.248546 179.075147 \nC 63.248546 179.870756 63.564645 180.633886 64.127225 181.196467 \nC 64.689806 181.759048 65.452936 182.075147 66.248546 182.075147 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 156.665088 \nC 82.04241 156.665088 82.805541 156.348989 83.368121 155.786408 \nC 83.930702 155.223828 84.246801 154.460697 84.246801 153.665088 \nC 84.246801 152.869479 83.930702 152.106349 83.368121 151.543768 \nC 82.805541 150.981187 82.04241 150.665088 81.246801 150.665088 \nC 80.451192 150.665088 79.688061 150.981187 79.125481 151.543768 \nC 78.5629 152.106349 78.246801 152.869479 78.246801 153.665088 \nC 78.246801 154.460697 78.5629 155.223828 79.125481 155.786408 \nC 79.688061 156.348989 80.451192 156.665088 81.246801 156.665088 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 126.925032 \nC 365.855552 126.925032 366.618683 126.608933 367.181264 126.046352 \nC 367.743844 125.483771 368.059943 124.720641 368.059943 123.925032 \nC 368.059943 123.129422 367.743844 122.366292 367.181264 121.803711 \nC 366.618683 121.241131 365.855552 120.925032 365.059943 120.925032 \nC 364.264334 120.925032 363.501204 121.241131 362.938623 121.803711 \nC 362.376042 122.366292 362.059943 123.129422 362.059943 123.925032 \nC 362.059943 124.720641 362.376042 125.483771 362.938623 126.046352 \nC 363.501204 126.608933 364.264334 126.925032 365.059943 126.925032 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 40.052114 \nC 61.708237 40.052114 62.471367 39.736015 63.033948 39.173434 \nC 63.596529 38.610853 63.912628 37.847723 63.912628 37.052114 \nC 63.912628 36.256504 63.596529 35.493374 63.033948 34.930793 \nC 62.471367 34.368212 61.708237 34.052114 60.912628 34.052114 \nC 60.117019 34.052114 59.353888 34.368212 58.791307 34.930793 \nC 58.228727 35.493374 57.912628 36.256504 57.912628 37.052114 \nC 57.912628 37.847723 58.228727 38.610853 58.791307 39.173434 \nC 59.353888 39.736015 60.117019 40.052114 60.912628 40.052114 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 188.78888 \nC 63.006163 188.78888 63.769293 188.472781 64.331874 187.9102 \nC 64.894455 187.347619 65.210554 186.584489 65.210554 185.78888 \nC 65.210554 184.99327 64.894455 184.23014 64.331874 183.667559 \nC 63.769293 183.104978 63.006163 182.78888 62.210554 182.78888 \nC 61.414944 182.78888 60.651814 183.104978 60.089233 183.667559 \nC 59.526653 184.23014 59.210554 184.99327 59.210554 185.78888 \nC 59.210554 186.584489 59.526653 187.347619 60.089233 187.9102 \nC 60.651814 188.472781 61.414944 188.78888 62.210554 188.78888 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 176.274916 \nC 65.602015 176.274916 66.365145 175.958817 66.927726 175.396236 \nC 67.490307 174.833655 67.806406 174.070525 67.806406 173.274916 \nC 67.806406 172.479306 67.490307 171.716176 66.927726 171.153595 \nC 66.365145 170.591014 65.602015 170.274916 64.806406 170.274916 \nC 64.010796 170.274916 63.247666 170.591014 62.685085 171.153595 \nC 62.122505 171.716176 61.806406 172.479306 61.806406 173.274916 \nC 61.806406 174.070525 62.122505 174.833655 62.685085 175.396236 \nC 63.247666 175.958817 64.010796 176.274916 64.806406 176.274916 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 70.996462 \nC 101.943942 70.996462 102.707072 70.680364 103.269653 70.117783 \nC 103.832233 69.555202 104.148332 68.792072 104.148332 67.996462 \nC 104.148332 67.200853 103.832233 66.437723 103.269653 65.875142 \nC 102.707072 65.312561 101.943942 64.996462 101.148332 64.996462 \nC 100.352723 64.996462 99.589593 65.312561 99.027012 65.875142 \nC 98.464431 66.437723 98.148332 67.200853 98.148332 67.996462 \nC 98.148332 68.792072 98.464431 69.555202 99.027012 70.117783 \nC 99.589593 70.680364 100.352723 70.996462 101.148332 70.996462 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.696307 105.068557 \nC 61.491916 105.068557 62.255046 104.752458 62.817627 104.189877 \nC 63.380208 103.627296 63.696307 102.864166 63.696307 102.068557 \nC 63.696307 101.272947 63.380208 100.509817 62.817627 99.947236 \nC 62.255046 99.384655 61.491916 99.068557 60.696307 99.068557 \nC 59.900698 99.068557 59.137567 99.384655 58.574986 99.947236 \nC 58.012406 100.509817 57.696307 101.272947 57.696307 102.068557 \nC 57.696307 102.864166 58.012406 103.627296 58.574986 104.189877 \nC 59.137567 104.752458 59.900698 105.068557 60.696307 105.068557 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 61.056842 114.865067 \nC 61.852451 114.865067 62.615581 114.548968 63.178162 113.986388 \nC 63.740743 113.423807 64.056842 112.660677 64.056842 111.865067 \nC 64.056842 111.069458 63.740743 110.306328 63.178162 109.743747 \nC 62.615581 109.181166 61.852451 108.865067 61.056842 108.865067 \nC 60.261233 108.865067 59.498102 109.181166 58.935521 109.743747 \nC 58.372941 110.306328 58.056842 111.069458 58.056842 111.865067 \nC 58.056842 112.660677 58.372941 113.423807 58.935521 113.986388 \nC 59.498102 114.548968 60.261233 114.865067 61.056842 114.865067 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 175.784649 \nC 63.006163 175.784649 63.769293 175.46855 64.331874 174.905969 \nC 64.894455 174.343389 65.210554 173.580258 65.210554 172.784649 \nC 65.210554 171.98904 64.894455 171.225909 64.331874 170.663329 \nC 63.769293 170.100748 63.006163 169.784649 62.210554 169.784649 \nC 61.414944 169.784649 60.651814 170.100748 60.089233 170.663329 \nC 59.526653 171.225909 59.210554 171.98904 59.210554 172.784649 \nC 59.210554 173.580258 59.526653 174.343389 60.089233 174.905969 \nC 60.651814 175.46855 61.414944 175.784649 62.210554 175.784649 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 66.248546 172.386451 \nC 67.044155 172.386451 67.807285 172.070352 68.369866 171.507771 \nC 68.932447 170.945191 69.248546 170.18206 69.248546 169.386451 \nC 69.248546 168.590842 68.932447 167.827711 68.369866 167.265131 \nC 67.807285 166.70255 67.044155 166.386451 66.248546 166.386451 \nC 65.452936 166.386451 64.689806 166.70255 64.127225 167.265131 \nC 63.564645 167.827711 63.248546 168.590842 63.248546 169.386451 \nC 63.248546 170.18206 63.564645 170.945191 64.127225 171.507771 \nC 64.689806 172.070352 65.452936 172.386451 66.248546 172.386451 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 81.246801 163.796132 \nC 82.04241 163.796132 82.805541 163.480033 83.368121 162.917453 \nC 83.930702 162.354872 84.246801 161.591741 84.246801 160.796132 \nC 84.246801 160.000523 83.930702 159.237393 83.368121 158.674812 \nC 82.805541 158.112231 82.04241 157.796132 81.246801 157.796132 \nC 80.451192 157.796132 79.688061 158.112231 79.125481 158.674812 \nC 78.5629 159.237393 78.246801 160.000523 78.246801 160.796132 \nC 78.246801 161.591741 78.5629 162.354872 79.125481 162.917453 \nC 79.688061 163.480033 80.451192 163.796132 81.246801 163.796132 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 138.932399 193.618929 \nC 139.728008 193.618929 140.491139 193.30283 141.053719 192.740249 \nC 141.6163 192.177668 141.932399 191.414538 141.932399 190.618929 \nC 141.932399 189.82332 141.6163 189.060189 141.053719 188.497608 \nC 140.491139 187.935028 139.728008 187.618929 138.932399 187.618929 \nC 138.13679 187.618929 137.373659 187.935028 136.811079 188.497608 \nC 136.248498 189.060189 135.932399 189.82332 135.932399 190.618929 \nC 135.932399 191.414538 136.248498 192.177668 136.811079 192.740249 \nC 137.373659 193.30283 138.13679 193.618929 138.932399 193.618929 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 365.059943 167.916144 \nC 365.855552 167.916144 366.618683 167.600045 367.181264 167.037465 \nC 367.743844 166.474884 368.059943 165.711754 368.059943 164.916144 \nC 368.059943 164.120535 367.743844 163.357405 367.181264 162.794824 \nC 366.618683 162.232243 365.855552 161.916144 365.059943 161.916144 \nC 364.264334 161.916144 363.501204 162.232243 362.938623 162.794824 \nC 362.376042 163.357405 362.059943 164.120535 362.059943 164.916144 \nC 362.059943 165.711754 362.376042 166.474884 362.938623 167.037465 \nC 363.501204 167.600045 364.264334 167.916144 365.059943 167.916144 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 60.912628 184.645373 \nC 61.708237 184.645373 62.471367 184.329274 63.033948 183.766693 \nC 63.596529 183.204113 63.912628 182.440982 63.912628 181.645373 \nC 63.912628 180.849764 63.596529 180.086633 63.033948 179.524053 \nC 62.471367 178.961472 61.708237 178.645373 60.912628 178.645373 \nC 60.117019 178.645373 59.353888 178.961472 58.791307 179.524053 \nC 58.228727 180.086633 57.912628 180.849764 57.912628 181.645373 \nC 57.912628 182.440982 58.228727 183.204113 58.791307 183.766693 \nC 59.353888 184.329274 60.117019 184.645373 60.912628 184.645373 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 62.210554 176.704781 \nC 63.006163 176.704781 63.769293 176.388682 64.331874 175.826102 \nC 64.894455 175.263521 65.210554 174.500391 65.210554 173.704781 \nC 65.210554 172.909172 64.894455 172.146042 64.331874 171.583461 \nC 63.769293 171.02088 63.006163 170.704781 62.210554 170.704781 \nC 61.414944 170.704781 60.651814 171.02088 60.089233 171.583461 \nC 59.526653 172.146042 59.210554 172.909172 59.210554 173.704781 \nC 59.210554 174.500391 59.526653 175.263521 60.089233 175.826102 \nC 60.651814 176.388682 61.414944 176.704781 62.210554 176.704781 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 64.806406 166.560608 \nC 65.602015 166.560608 66.365145 166.244509 66.927726 165.681929 \nC 67.490307 165.119348 67.806406 164.356218 67.806406 163.560608 \nC 67.806406 162.764999 67.490307 162.001869 66.927726 161.439288 \nC 66.365145 160.876707 65.602015 160.560608 64.806406 160.560608 \nC 64.010796 160.560608 63.247666 160.876707 62.685085 161.439288 \nC 62.122505 162.001869 61.806406 162.764999 61.806406 163.560608 \nC 61.806406 164.356218 62.122505 165.119348 62.685085 165.681929 \nC 63.247666 166.244509 64.010796 166.560608 64.806406 166.560608 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 69.998109 166.537172 \nC 70.793719 166.537172 71.556849 166.221073 72.11943 165.658492 \nC 72.682011 165.095912 72.998109 164.332781 72.998109 163.537172 \nC 72.998109 162.741563 72.682011 161.978432 72.11943 161.415852 \nC 71.556849 160.853271 70.793719 160.537172 69.998109 160.537172 \nC 69.2025 160.537172 68.43937 160.853271 67.876789 161.415852 \nC 67.314208 161.978432 66.998109 162.741563 66.998109 163.537172 \nC 66.998109 164.332781 67.314208 165.095912 67.876789 165.658492 \nC 68.43937 166.221073 69.2025 166.537172 69.998109 166.537172 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p292833166d)\" d=\"M 101.148332 26.999953 \nC 101.943942 26.999953 102.707072 26.683854 103.269653 26.121273 \nC 103.832233 25.558692 104.148332 24.795562 104.148332 23.999953 \nC 104.148332 23.204343 103.832233 22.441213 103.269653 21.878632 \nC 102.707072 21.316051 101.943942 20.999953 101.148332 20.999953 \nC 100.352723 20.999953 99.589593 21.316051 99.027012 21.878632 \nC 98.464431 22.441213 98.148332 23.204343 98.148332 23.999953 \nC 98.148332 24.795562 98.464431 25.558692 99.027012 26.121273 \nC 99.589593 26.683854 100.352723 26.999953 101.148332 26.999953 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n   </g>\n   <g id=\"PathCollection_2\"/>\n   <g id=\"PathCollection_3\"/>\n   <g id=\"PathCollection_4\"/>\n   <g id=\"PathCollection_5\"/>\n   <g id=\"PathCollection_6\"/>\n   <g id=\"PathCollection_7\"/>\n   <g id=\"PathCollection_8\"/>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m31fe7edc8c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.461959\" xlink:href=\"#m31fe7edc8c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(57.280709 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"105.528833\" xlink:href=\"#m31fe7edc8c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2500 -->\n      <g transform=\"translate(92.803833 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"150.595706\" xlink:href=\"#m31fe7edc8c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5000 -->\n      <g transform=\"translate(137.870706 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"195.662579\" xlink:href=\"#m31fe7edc8c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 7500 -->\n      <g transform=\"translate(182.937579 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"240.729453\" xlink:href=\"#m31fe7edc8c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10000 -->\n      <g transform=\"translate(224.823203 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"285.796326\" xlink:href=\"#m31fe7edc8c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12500 -->\n      <g transform=\"translate(269.890076 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"330.8632\" xlink:href=\"#m31fe7edc8c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 15000 -->\n      <g transform=\"translate(314.95695 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"375.930073\" xlink:href=\"#m31fe7edc8c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 17500 -->\n      <g transform=\"translate(360.023823 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- parameters -->\n     <g transform=\"translate(183.876563 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"165.869141\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"324.560547\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"386.083984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"425.292969\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"486.816406\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"527.929688\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m122331e8f4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m122331e8f4\" y=\"131.654086\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 135.453305)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m122331e8f4\" y=\"11.531296\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 15.330515)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m4a5827bc97\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"215.616313\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"194.46374\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"179.45575\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"167.814649\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"158.303177\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"150.261342\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"143.295187\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"137.150604\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"95.493523\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"74.34095\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"59.33296\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"47.691859\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"38.180387\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"30.138552\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"23.172397\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#m4a5827bc97\" y=\"17.027814\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 125.577812)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 179.170313 219.64 \nL 246.585938 219.64 \nQ 248.585938 219.64 248.585938 217.64 \nL 248.585938 86.536875 \nQ 248.585938 84.536875 246.585938 84.536875 \nL 179.170313 84.536875 \nQ 177.170313 84.536875 177.170313 86.536875 \nL 177.170313 217.64 \nQ 177.170313 219.64 179.170313 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- layers -->\n     <g transform=\"translate(189.170313 96.135312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"209.765625\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"250.878906\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_9\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m36066c5829\" style=\"stroke:#edd1cb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#edd1cb;stroke:#edd1cb;\" x=\"191.170313\" xlink:href=\"#m36066c5829\" y=\"108.188438\"/>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- 1 -->\n     <g transform=\"translate(209.170313 110.813438)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_10\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"md8a10382c1\" style=\"stroke:#ebcac5;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ebcac5;stroke:#ebcac5;\" x=\"191.170313\" xlink:href=\"#md8a10382c1\" y=\"122.866562\"/>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- 2 -->\n     <g transform=\"translate(209.170313 125.491562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_11\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m34ef70774a\" style=\"stroke:#e5bcbb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#e5bcbb;stroke:#e5bcbb;\" x=\"191.170313\" xlink:href=\"#m34ef70774a\" y=\"137.544688\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- 4 -->\n     <g transform=\"translate(209.170313 140.169688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-34\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_12\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m958f65e431\" style=\"stroke:#d89faa;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#d89faa;stroke:#d89faa;\" x=\"191.170313\" xlink:href=\"#m958f65e431\" y=\"152.222813\"/>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- 8 -->\n     <g transform=\"translate(209.170313 154.847813)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-38\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_13\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m642013b9b2\" style=\"stroke:#ae6b91;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ae6b91;stroke:#ae6b91;\" x=\"191.170313\" xlink:href=\"#m642013b9b2\" y=\"166.900937\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- 16 -->\n     <g transform=\"translate(209.170313 169.525937)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_14\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mf628d9abd9\" style=\"stroke:#2d1e3e;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#2d1e3e;stroke:#2d1e3e;\" x=\"191.170313\" xlink:href=\"#mf628d9abd9\" y=\"181.579062\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- 32 -->\n     <g transform=\"translate(209.170313 184.204062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- model -->\n     <g transform=\"translate(189.170313 198.882187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_15\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m44a198cf38\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"191.170313\" xlink:href=\"#m44a198cf38\" y=\"210.935312\"/>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- ResNet -->\n     <g transform=\"translate(209.170313 213.560312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p292833166d\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}