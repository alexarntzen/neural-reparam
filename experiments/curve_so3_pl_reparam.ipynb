{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Test on piecewise linear curves in $SO(3)^n$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from itertools import chain\n",
    "\n",
    "set_matplotlib_formats(\"pdf\", \"svg\")\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN\n",
    "from deepthermal.validation import (\n",
    "    create_subdictionary_iterator,\n",
    "    k_fold_cv_grid,\n",
    "    add_dictionary_iterators,\n",
    ")\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from neural_reparam.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from neural_reparam.interpolation import get_pl_curve_from_data\n",
    "from neural_reparam.models import CNN\n",
    "from neural_reparam.ResNet import ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "run/jog\n",
      "run/jog\n",
      "(23, 161, 3, 3)\n",
      "(23, 137, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from animation.animation_manager import fetch_animations, unpack\n",
    "from so3.curves import move_origin_to_zero, dynamic_distance\n",
    "from so3.helpers import crop_curve\n",
    "from so3.dynamic_distance import (\n",
    "    find_optimal_diffeomorphism,\n",
    "    create_shared_parameterization,\n",
    ")\n",
    "from so3.clustering.id_set import crop_curve_based_on_id, get_id_set\n",
    "from so3.transformations import skew_to_vector, SRVT\n",
    "from so3 import animation_to_SO3\n",
    "\n",
    "max_frame_count = 180\n",
    "\n",
    "id_set = get_id_set()\n",
    "print(\"Load data\")\n",
    "\n",
    "data = [\n",
    "    fetch_animations(1, file_name=\"39_02.amc\"),  # walk 6.5 steps\n",
    "    fetch_animations(1, file_name=\"35_26.amc\"),  # run/jog 3 steps\n",
    "    fetch_animations(1, file_name=\"16_35.amc\"),  # run/jog 3 steps\n",
    "]\n",
    "\n",
    "# walk\n",
    "subject, animation, desc0 = unpack(data[2])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=240)  # first 2 seconds\n",
    "c_0 = move_origin_to_zero(curve)\n",
    "print(desc0)\n",
    "\n",
    "# run\n",
    "subject, animation, desc1 = unpack(data[1])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=240)  # first 2 seconds\n",
    "c_1 = move_origin_to_zero(curve)\n",
    "print(desc1)\n",
    "print(c_0.shape)\n",
    "print(c_1.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# calculate distances\n",
    "I0 = np.linspace(0, 1, c_0.shape[1])\n",
    "I1 = np.linspace(0, 1, c_1.shape[1])\n",
    "q_data_ = skew_to_vector(SRVT(c_0, I0))\n",
    "r_data_ = skew_to_vector(SRVT(c_1, I1))\n",
    "I, q_data, r_data = create_shared_parameterization(q0=q_data_, q1=r_data_, I0=I0, I1=I1)\n",
    "shared_frames = I.shape[0]\n",
    "q_func = get_pl_curve_from_data(data=q_data)\n",
    "r_func = get_pl_curve_from_data(data=r_data)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.718828369966287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexander/git_repos/git_skole/paalel-master/code/so3/transformations.py:38: RuntimeWarning: invalid value encountered in arccos\n",
      "  theta = arccos(0.5*(trace(R)-1))\n"
     ]
    }
   ],
   "source": [
    "d_01 = dynamic_distance(c_0, c_1, depth=10)\n",
    "print(d_01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "I1_new = find_optimal_diffeomorphism(q0=q_data, q1=r_data, I0=I, I1=I, depth=10)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_so3/\"\n",
    "SET_NAME = \"pl_eks_7\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "FOLDS = 1\n",
    "N = shared_frames  # training points internal\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optim, mode=\"min\", factor=0.1, patience=5, verbose=True\n",
    ")\n",
    "loss_func = get_elastic_metric_loss(r=r_func, constrain_cost=1e4, verbose=False)\n",
    "penalty_free_loss_func = get_elastic_metric_loss(\n",
    "    r=r_func, constrain_cost=0, verbose=False\n",
    ")\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"n_hidden_layers\": [2],  # ,8,16,64],\n",
    "    \"model\": [ResNet],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128],\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"model\": [CNN, ResNet],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"activation\": [\"relu\", \"tanh\"],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N // 2],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [100],\n",
    "    \"learning_rate\": [1],\n",
    "    \"stop_stochastic\": [90],\n",
    "    \"lr_scheduler\": [lr_scheduler],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = torch.tensor(q_data)\n",
    "data = TensorDataset(x_train, q_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(\n",
    "    TRAINING_PARAMS_EXPERIMENT, product=False\n",
    ")\n",
    "exp_training_params_iter = add_dictionary_iterators(\n",
    "    training_exp_iter, training_params_iter\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a0944c0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12a8e9e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x12a8e9dc0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a0944c0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12a8e9e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x12a8e9dc0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  51.78935623\n",
      "################################  5  ################################\n",
      "Training Loss:  38.57539749\n",
      "################################  10  ################################\n",
      "Training Loss:  37.29899597\n",
      "################################  15  ################################\n",
      "Training Loss:  8272.50878906\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  98.83892822\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  96.65584564\n",
      "################################  30  ################################\n",
      "Training Loss:  95.21520996\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  95.27440643\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  95.22267151\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  95.22265625\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  95.22265625\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  95.22265625\n",
      "################################  60  ################################\n",
      "Training Loss:  95.22265625\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  95.22265625\n",
      "################################  70  ################################\n",
      "Training Loss:  95.22265625\n",
      "################################  75  ################################\n",
      "Training Loss:  95.22265625\n",
      "################################  80  ################################\n",
      "Training Loss:  95.22265625\n",
      "################################  85  ################################\n",
      "Training Loss:  95.22265625\n",
      "################################  90  ################################\n",
      "Training Loss:  95.22265625\n",
      "################################  95  ################################\n",
      "Training Loss:  95.22265625\n",
      "Final training Loss:  95.22265625\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a0944c0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12a8e9e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x12a8e9dc0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  103.41870117\n",
      "################################  5  ################################\n",
      "Training Loss:  103.36598206\n",
      "################################  10  ################################\n",
      "Training Loss:  101.70259094\n",
      "################################  15  ################################\n",
      "Training Loss:  101.65219116\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  101.7205658\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  101.72922516\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  101.72922516\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  101.72922516\n",
      "################################  40  ################################\n",
      "Training Loss:  101.72922516\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  101.72922516\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  101.72922516\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  101.72922516\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  101.72922516\n",
      "################################  65  ################################\n",
      "Training Loss:  101.72922516\n",
      "################################  70  ################################\n",
      "Training Loss:  101.72922516\n",
      "################################  75  ################################\n",
      "Training Loss:  101.72922516\n",
      "################################  80  ################################\n",
      "Training Loss:  101.72922516\n",
      "################################  85  ################################\n",
      "Training Loss:  101.72922516\n",
      "################################  90  ################################\n",
      "Training Loss:  101.72922516\n",
      "################################  95  ################################\n",
      "Training Loss:  101.72922516\n",
      "Final training Loss:  101.72922516\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 144, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a0944c0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12a8e9e50>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'stop_stochastic': 90, 'lr_scheduler': <function <lambda> at 0x12a8e9dc0>})\n",
      "################################  0  ################################\n",
      "Training Loss:  44.96590424\n",
      "################################  5  ################################\n",
      "Training Loss:  40.62969971\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-8ce513209ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cv_results = k_fold_cv_grid(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_model_params_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_FFNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtraining_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_training_params_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/git_skole/deepthermal/deepthermal/validation.py\u001b[0m in \u001b[0;36mk_fold_cv_grid\u001b[0;34m(model_params, training_params, data, val_data, fit, folds, trials, partial, verbose, get_error)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mmodel_param_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mmodel_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_param_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_param_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             model_instance, loss_history_train, loss_history_val = fit(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_instance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mtraining_param\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/git_skole/deepthermal/deepthermal/FFNN_model.py\u001b[0m in \u001b[0;36mfit_FFNN\u001b[0;34m(model, data, num_epochs, batch_size, optimizer, init, regularization_param, regularization_exp, data_val, track_history, track_epoch, verbose, verbose_interval, learning_rate, init_weight_seed, lr_scheduler, stop_stochastic, loss_func, compute_loss, max_nan_steps, **kwargs)\u001b[0m\n\u001b[1;32m    235\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m             \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrack_epoch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrack_history\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;31m# multiply by initial Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plotting\n",
    "disc_points = x_train.detach()\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": disc_points,\n",
    "    \"x_train\": disc_points,\n",
    "    \"y_train\": I1_new,\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"DP solution\",\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(\n",
    "    models\n",
    ").flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "activation = np.vectorize(lambda model: model.activation)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(\n",
    "    lambda model: penalty_free_loss_func(model, x_train, q_train).detach()\n",
    ")(models).flatten()\n",
    "loss_array -= d_01**2\n",
    "# make data frame\n",
    "# optims = [\"line search\", \"ADAM\", \"LBFGS\"]*(len(loss_array)//3)\n",
    "d_results = pd.DataFrame(\n",
    "    {\n",
    "        \"loss\": loss_array,\n",
    "        \"neurons\": neurons,\n",
    "        \"layers\": layers,\n",
    "        \"parameters\": parameters,\n",
    "        \"model\": model_type,\n",
    "        \"activation\": activation,\n",
    "    }\n",
    ")\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_neurons = sns.lineplot(\n",
    "    data=d_results_neurons,\n",
    "    y=\"loss\",\n",
    "    x=\"neurons\",\n",
    "    hue=\"activation\",\n",
    "    ci=80,\n",
    "    err_style=\"bars\",\n",
    ")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 10]), len(d_results_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_layers = sns.lineplot(\n",
    "    data=d_results_layer,\n",
    "    y=\"loss\",\n",
    "    x=\"layers\",\n",
    "    hue=\"activation\",\n",
    "    ci=80,\n",
    "    err_style=\"bars\",\n",
    ")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Hidden layers\", ylabel=\"Error\")\n",
    "fig_layers.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss > 10]), len(d_results_layer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig_scatter = sns.scatterplot(\n",
    "    data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\"\n",
    ")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}