{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Test on piecewise linear curves in $SO(3)^n$"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "from itertools import chain\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from deep_reparametrization.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from deep_reparametrization.helpers import get_pc_curve_from_data, get_pl_curve_from_data\n",
    "from deep_reparametrization.ResNET import ResNET\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load data\n",
      "run/jog\n",
      "run/jog\n",
      "(23, 161, 3, 3)\n",
      "(23, 137, 3, 3)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"../../\")\n",
    "from animation.animation_manager import fetch_animations, unpack\n",
    "from so3.curves import move_origin_to_zero, dynamic_distance\n",
    "from so3.helpers import crop_curve\n",
    "from so3.dynamic_distance import find_optimal_diffeomorphism, create_shared_parameterization\n",
    "from so3.clustering.id_set import crop_curve_based_on_id, get_id_set\n",
    "from so3.transformations import skew_to_vector, SRVT\n",
    "from so3 import animation_to_SO3\n",
    "\n",
    "max_frame_count = 180\n",
    "\n",
    "id_set = get_id_set()\n",
    "print(\"Load data\")\n",
    "\n",
    "data = [fetch_animations(1, file_name=\"39_02.amc\"),  #walk 6.5 steps\n",
    "        fetch_animations(1, file_name=\"35_26.amc\"),  # run/jog 3 steps\n",
    "        fetch_animations(1, file_name=\"16_35.amc\")  # run/jog 3 steps\n",
    "        ]\n",
    "\n",
    "# walk\n",
    "subject, animation, desc0 = unpack(data[2])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=240)  #first 2 seconds\n",
    "c_0 = move_origin_to_zero(curve)\n",
    "print(desc0)\n",
    "\n",
    "# run\n",
    "subject, animation, desc1 = unpack(data[1])\n",
    "curve_full = animation_to_SO3(subject, animation)\n",
    "curve = crop_curve(curve_full, stop=240)  #first 2 seconds\n",
    "c_1 = move_origin_to_zero(curve)\n",
    "print(desc1)\n",
    "print(c_0.shape)\n",
    "print(c_1.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "#calculate distances\n",
    "I0 = np.linspace(0, 1, c_0.shape[1])\n",
    "I1 = np.linspace(0, 1, c_1.shape[1])\n",
    "q_data_ = skew_to_vector(SRVT(c_0, I0))\n",
    "r_data_ = skew_to_vector(SRVT(c_1, I1))\n",
    "I, q_data, r_data = create_shared_parameterization(q0=q_data_, q1=r_data_, I0=I0, I1=I1)\n",
    "shared_frames = I.shape[0]\n",
    "q_func = get_pl_curve_from_data(data=q_data)\n",
    "r_func = get_pl_curve_from_data(data=r_data)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.718828369966287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alexander/git_repos/git_skole/paalel-master/code/so3/transformations.py:38: RuntimeWarning: invalid value encountered in arccos\n",
      "  theta = arccos(0.5*(trace(R)-1))\n"
     ]
    }
   ],
   "source": [
    "d_01 = dynamic_distance(c_0, c_1, depth=10)\n",
    "print(d_01)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "I1_new = find_optimal_diffeomorphism(q0=q_data, q1=r_data, I0=I, I1=I, depth=10)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_so3/\"\n",
    "SET_NAME = \"pl_eks_6\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "FOLDS = 1\n",
    "N = shared_frames  # training points internal\n",
    "\n",
    "loss_func = get_elastic_metric_loss(r=r_func, constrain_cost=1e4, verbose=False)\n",
    "penalty_free_loss_func = get_elastic_metric_loss(r=r_func, constrain_cost=0, verbose=False)\n",
    "lr_scheduler = lambda optimizer: torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.8, patience=200,\n",
    "                                                                            verbose=True)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"n_hidden_layers\": [2],  #,8,16,64],\n",
    "    \"model\": [ResNET],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"model\": [ResNET],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32, 64],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"activation\": [\"tanh\", \"relu\"],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"ADAM\", ],\n",
    "    \"num_epochs\": [2000, ],\n",
    "    \"learning_rate\": [0.05, ],\n",
    "    \"lr_scheduler\": [lr_scheduler]\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = torch.tensor(q_data)\n",
    "data = TensorDataset(x_train, q_train)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  17117.55078125\n",
      "################################  100  ################################\n",
      "Training Loss:  117.51451874\n",
      "################################  200  ################################\n",
      "Training Loss:  114.95751953\n",
      "################################  300  ################################\n",
      "Training Loss:  115.26902771\n",
      "################################  400  ################################\n",
      "Training Loss:  116.19284058\n",
      "################################  500  ################################\n",
      "Training Loss:  114.47418213\n",
      "################################  600  ################################\n",
      "Training Loss:  115.21520996\n",
      "Epoch   696: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  114.73408508\n",
      "################################  800  ################################\n",
      "Training Loss:  113.82884216\n",
      "Epoch   897: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  114.38658142\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.02301025\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.18758392\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.19084167\n",
      "Epoch  1259: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.00466156\n",
      "################################  1400  ################################\n",
      "Training Loss:  113.93198395\n",
      "Epoch  1460: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.35402679\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.25992584\n",
      "Epoch  1661: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  112.78282166\n",
      "################################  1800  ################################\n",
      "Training Loss:  112.50786591\n",
      "Epoch  1862: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.49947357\n",
      "Final training Loss:  113.13395691\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4026.16455078\n",
      "################################  100  ################################\n",
      "Training Loss:  97.02843475\n",
      "################################  200  ################################\n",
      "Training Loss:  58.59313202\n",
      "################################  300  ################################\n",
      "Training Loss:  57.54114151\n",
      "Epoch   330: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  58.9521904\n",
      "################################  500  ################################\n",
      "Training Loss:  74.65904999\n",
      "Epoch   531: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  53.76684952\n",
      "################################  700  ################################\n",
      "Training Loss:  64.42063141\n",
      "Epoch   732: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  61.88114166\n",
      "################################  900  ################################\n",
      "Training Loss:  55.3990593\n",
      "Epoch   933: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  57.19807816\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.86401367\n",
      "Epoch  1134: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  56.47880173\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.89240265\n",
      "Epoch  1335: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.69366455\n",
      "################################  1500  ################################\n",
      "Training Loss:  63.20994186\n",
      "Epoch  1536: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.14031601\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.23230362\n",
      "Epoch  1737: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.87200928\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.1476326\n",
      "Epoch  1938: reducing learning rate of group 0 to 6.7109e-03.\n",
      "Final training Loss:  54.1776619\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1354.81188965\n",
      "################################  100  ################################\n",
      "Training Loss:  119.99923706\n",
      "################################  200  ################################\n",
      "Training Loss:  117.99586487\n",
      "################################  300  ################################\n",
      "Training Loss:  119.84302521\n",
      "################################  400  ################################\n",
      "Training Loss:  115.5763092\n",
      "################################  500  ################################\n",
      "Training Loss:  114.64910889\n",
      "Epoch   583: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  123.68868256\n",
      "################################  700  ################################\n",
      "Training Loss:  116.24854279\n",
      "Epoch   784: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  114.97932434\n",
      "################################  900  ################################\n",
      "Training Loss:  114.1880722\n",
      "Epoch   985: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.58422852\n",
      "################################  1100  ################################\n",
      "Training Loss:  116.53627777\n",
      "Epoch  1186: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.18136597\n",
      "################################  1300  ################################\n",
      "Training Loss:  113.16459656\n",
      "Epoch  1387: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.94213104\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.31342316\n",
      "Epoch  1588: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.71842957\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.63994598\n",
      "################################  1800  ################################\n",
      "Training Loss:  113.02310944\n",
      "Epoch  1854: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.83306122\n",
      "Final training Loss:  115.13498688\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9160.01855469\n",
      "################################  100  ################################\n",
      "Training Loss:  117.26747131\n",
      "################################  200  ################################\n",
      "Training Loss:  115.38463593\n",
      "################################  300  ################################\n",
      "Training Loss:  115.06368256\n",
      "################################  400  ################################\n",
      "Training Loss:  116.6924057\n",
      "################################  500  ################################\n",
      "Training Loss:  116.47805023\n",
      "################################  600  ################################\n",
      "Training Loss:  114.41693115\n",
      "Epoch   641: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  116.30493927\n",
      "################################  800  ################################\n",
      "Training Loss:  116.32267761\n",
      "################################  900  ################################\n",
      "Training Loss:  119.67586517\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.4932251\n",
      "Epoch  1019: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  111.88710785\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.79167938\n",
      "Epoch  1231: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  112.03640747\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.16705322\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.32913208\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.66122437\n",
      "################################  1700  ################################\n",
      "Training Loss:  112.86762238\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.12290192\n",
      "Epoch  1836: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.18229675\n",
      "Final training Loss:  116.9257431\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  569659.3125\n",
      "################################  100  ################################\n",
      "Training Loss:  122.59565735\n",
      "################################  200  ################################\n",
      "Training Loss:  111.85626984\n",
      "################################  300  ################################\n",
      "Training Loss:  111.77423096\n",
      "################################  400  ################################\n",
      "Training Loss:  113.24449921\n",
      "Epoch   453: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  111.80532837\n",
      "################################  600  ################################\n",
      "Training Loss:  113.29143524\n",
      "Epoch   654: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  111.99867249\n",
      "################################  800  ################################\n",
      "Training Loss:  113.00765228\n",
      "################################  900  ################################\n",
      "Training Loss:  111.72718811\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.099617\n",
      "Epoch  1043: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  111.7012558\n",
      "################################  1200  ################################\n",
      "Training Loss:  111.70282745\n",
      "Epoch  1266: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  112.79321289\n",
      "################################  1400  ################################\n",
      "Training Loss:  112.37255859\n",
      "Epoch  1472: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  111.90552521\n",
      "################################  1600  ################################\n",
      "Training Loss:  111.82476807\n",
      "Epoch  1673: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  111.66608429\n",
      "################################  1800  ################################\n",
      "Training Loss:  111.66195679\n",
      "Epoch  1874: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  111.66131592\n",
      "Final training Loss:  111.62875366\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6019.9140625\n",
      "################################  100  ################################\n",
      "Training Loss:  108.79167938\n",
      "################################  200  ################################\n",
      "Training Loss:  56.04384995\n",
      "################################  300  ################################\n",
      "Training Loss:  57.37150574\n",
      "Epoch   396: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  53.64129639\n",
      "################################  500  ################################\n",
      "Training Loss:  53.81337738\n",
      "################################  600  ################################\n",
      "Training Loss:  52.68081284\n",
      "################################  700  ################################\n",
      "Training Loss:  52.53599548\n",
      "################################  800  ################################\n",
      "Training Loss:  205.59634399\n",
      "################################  900  ################################\n",
      "Training Loss:  113.43351746\n",
      "Epoch   978: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  97.1247406\n",
      "################################  1100  ################################\n",
      "Training Loss:  55.85616302\n",
      "Epoch  1179: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.78326797\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.40068817\n",
      "Epoch  1380: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.44437408\n",
      "################################  1500  ################################\n",
      "Training Loss:  55.48246384\n",
      "################################  1600  ################################\n",
      "Training Loss:  52.26456451\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.03898621\n",
      "Epoch  1711: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.00603485\n",
      "################################  1900  ################################\n",
      "Training Loss:  65.59333801\n",
      "Epoch  1912: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  54.13714981\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9212.69433594\n",
      "################################  100  ################################\n",
      "Training Loss:  140.11138916\n",
      "################################  200  ################################\n",
      "Training Loss:  99.79680634\n",
      "Epoch   253: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  81.49645233\n",
      "################################  400  ################################\n",
      "Training Loss:  57.77523422\n",
      "################################  500  ################################\n",
      "Training Loss:  78.21364594\n",
      "################################  600  ################################\n",
      "Training Loss:  66.94682312\n",
      "################################  700  ################################\n",
      "Training Loss:  49.0796051\n",
      "################################  800  ################################\n",
      "Training Loss:  62.02371216\n",
      "################################  900  ################################\n",
      "Training Loss:  124.70844269\n",
      "################################  1000  ################################\n",
      "Training Loss:  89.62728882\n",
      "Epoch  1063: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  70.53585815\n",
      "################################  1200  ################################\n",
      "Training Loss:  70.48221588\n",
      "Epoch  1264: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  76.60437012\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.2774086\n",
      "Epoch  1465: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  55.69837189\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.93161011\n",
      "Epoch  1666: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  55.03384781\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.06402206\n",
      "Epoch  1867: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.18700027\n",
      "Final training Loss:  54.33956909\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1004253.1875\n",
      "################################  100  ################################\n",
      "Training Loss:  140.7947998\n",
      "################################  200  ################################\n",
      "Training Loss:  118.62579346\n",
      "Epoch   288: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  117.63267517\n",
      "################################  400  ################################\n",
      "Training Loss:  113.98782349\n",
      "################################  500  ################################\n",
      "Training Loss:  109.3667984\n",
      "################################  600  ################################\n",
      "Training Loss:  102.81834412\n",
      "################################  700  ################################\n",
      "Training Loss:  93.82646179\n",
      "################################  800  ################################\n",
      "Training Loss:  84.4822998\n",
      "################################  900  ################################\n",
      "Training Loss:  81.77768707\n",
      "################################  1000  ################################\n",
      "Training Loss:  75.57874298\n",
      "Epoch  1086: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  61.92050934\n",
      "################################  1200  ################################\n",
      "Training Loss:  84.8316803\n",
      "Epoch  1287: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  58.54276276\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.1337471\n",
      "Epoch  1488: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.90368652\n",
      "################################  1600  ################################\n",
      "Training Loss:  58.56987381\n",
      "Epoch  1689: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  64.83667755\n",
      "################################  1800  ################################\n",
      "Training Loss:  55.32865143\n",
      "Epoch  1890: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.35488129\n",
      "Final training Loss:  65.43151855\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  686178.375\n",
      "################################  100  ################################\n",
      "Training Loss:  117.3885498\n",
      "################################  200  ################################\n",
      "Training Loss:  112.79341125\n",
      "Epoch   265: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  107.20121765\n",
      "################################  400  ################################\n",
      "Training Loss:  99.65805054\n",
      "################################  500  ################################\n",
      "Training Loss:  91.1208725\n",
      "################################  600  ################################\n",
      "Training Loss:  73.83242798\n",
      "################################  700  ################################\n",
      "Training Loss:  54.08328247\n",
      "################################  800  ################################\n",
      "Training Loss:  63.2694397\n",
      "################################  900  ################################\n",
      "Training Loss:  53.55288315\n",
      "Epoch   936: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.85065079\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.26948547\n",
      "Epoch  1137: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  51.61565018\n",
      "################################  1300  ################################\n",
      "Training Loss:  51.61201096\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.48127365\n",
      "################################  1500  ################################\n",
      "Training Loss:  51.64759445\n",
      "Epoch  1535: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.05009079\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.57793427\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.10481262\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.69931412\n",
      "Final training Loss:  51.39011002\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8836.21679688\n",
      "################################  100  ################################\n",
      "Training Loss:  110.89727783\n",
      "################################  200  ################################\n",
      "Training Loss:  91.67570496\n",
      "################################  300  ################################\n",
      "Training Loss:  96.49775696\n",
      "################################  400  ################################\n",
      "Training Loss:  164.08232117\n",
      "################################  500  ################################\n",
      "Training Loss:  53.78680801\n",
      "Epoch   508: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  53.94526672\n",
      "################################  700  ################################\n",
      "Training Loss:  53.5229187\n",
      "Epoch   709: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  54.997612\n",
      "################################  900  ################################\n",
      "Training Loss:  53.98004913\n",
      "Epoch   910: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.10046387\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.76948547\n",
      "Epoch  1111: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  57.71743774\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.45778656\n",
      "Epoch  1312: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.05117798\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.07286835\n",
      "Epoch  1513: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.85726166\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.10292053\n",
      "Epoch  1714: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  62.16094208\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.09764862\n",
      "Epoch  1915: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Final training Loss:  51.62044907\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2446.46362305\n",
      "################################  100  ################################\n",
      "Training Loss:  119.83686066\n",
      "################################  200  ################################\n",
      "Training Loss:  117.51361084\n",
      "################################  300  ################################\n",
      "Training Loss:  121.0641861\n",
      "################################  400  ################################\n",
      "Training Loss:  119.19267273\n",
      "################################  500  ################################\n",
      "Training Loss:  114.3549881\n",
      "################################  600  ################################\n",
      "Training Loss:  118.88381195\n",
      "################################  700  ################################\n",
      "Training Loss:  85.05839539\n",
      "################################  800  ################################\n",
      "Training Loss:  59.60737991\n",
      "################################  900  ################################\n",
      "Training Loss:  63.35481262\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.16836548\n",
      "################################  1100  ################################\n",
      "Training Loss:  91.5349884\n",
      "################################  1200  ################################\n",
      "Training Loss:  47.31385803\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.09312439\n",
      "################################  1400  ################################\n",
      "Training Loss:  9000.16894531\n",
      "################################  1500  ################################\n",
      "Training Loss:  110.29176331\n",
      "Epoch  1529: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  110.21041107\n",
      "################################  1700  ################################\n",
      "Training Loss:  109.93159485\n",
      "Epoch  1730: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  109.83777618\n",
      "################################  1900  ################################\n",
      "Training Loss:  109.73802948\n",
      "Epoch  1931: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  109.13845062\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  876558.3125\n",
      "################################  100  ################################\n",
      "Training Loss:  72.49291992\n",
      "################################  200  ################################\n",
      "Training Loss:  60.18104172\n",
      "################################  300  ################################\n",
      "Training Loss:  56.11537933\n",
      "################################  400  ################################\n",
      "Training Loss:  54.88877487\n",
      "################################  500  ################################\n",
      "Training Loss:  57.21392059\n",
      "Epoch   516: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  57.36403275\n",
      "################################  700  ################################\n",
      "Training Loss:  68.99219513\n",
      "Epoch   717: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  54.33272552\n",
      "################################  900  ################################\n",
      "Training Loss:  54.63687515\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.38300705\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.04047775\n",
      "Epoch  1168: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  56.16339874\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.94548416\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.11854553\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.91622162\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.30374146\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.39383698\n",
      "################################  1800  ################################\n",
      "Training Loss:  55.9434967\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.46646118\n",
      "Final training Loss:  53.33456421\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  90585.4140625\n",
      "################################  100  ################################\n",
      "Training Loss:  90.67140198\n",
      "################################  200  ################################\n",
      "Training Loss:  63.43263245\n",
      "################################  300  ################################\n",
      "Training Loss:  63.80133438\n",
      "################################  400  ################################\n",
      "Training Loss:  140.94927979\n",
      "################################  500  ################################\n",
      "Training Loss:  67.41417694\n",
      "Epoch   589: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  54.02945709\n",
      "################################  700  ################################\n",
      "Training Loss:  49.68951797\n",
      "################################  800  ################################\n",
      "Training Loss:  119.90492249\n",
      "################################  900  ################################\n",
      "Training Loss:  119.08308411\n",
      "Epoch   946: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  121.51711273\n",
      "################################  1100  ################################\n",
      "Training Loss:  129.71913147\n",
      "Epoch  1147: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  119.39833832\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.39109802\n",
      "Epoch  1348: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  122.09953308\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.77552795\n",
      "Epoch  1549: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.25753021\n",
      "################################  1700  ################################\n",
      "Training Loss:  112.86122131\n",
      "Epoch  1750: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  119.98822784\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.39663696\n",
      "Epoch  1951: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  114.0589447\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5601665.0\n",
      "################################  100  ################################\n",
      "Training Loss:  124.65390778\n",
      "################################  200  ################################\n",
      "Training Loss:  132.86172485\n",
      "################################  300  ################################\n",
      "Training Loss:  118.1089859\n",
      "################################  400  ################################\n",
      "Training Loss:  122.84955597\n",
      "################################  500  ################################\n",
      "Training Loss:  115.41149902\n",
      "################################  600  ################################\n",
      "Training Loss:  116.26184845\n",
      "################################  700  ################################\n",
      "Training Loss:  129.68624878\n",
      "################################  800  ################################\n",
      "Training Loss:  181.18328857\n",
      "################################  900  ################################\n",
      "Training Loss:  135.58198547\n",
      "Epoch   967: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  180.59770203\n",
      "################################  1100  ################################\n",
      "Training Loss:  224.04669189\n",
      "Epoch  1168: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  129.64466858\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.69373322\n",
      "################################  1400  ################################\n",
      "Training Loss:  116.15844727\n",
      "################################  1500  ################################\n",
      "Training Loss:  112.91162109\n",
      "Epoch  1580: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  118.16368103\n",
      "################################  1700  ################################\n",
      "Training Loss:  127.06693268\n",
      "Epoch  1781: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.18369293\n",
      "################################  1900  ################################\n",
      "Training Loss:  174.36178589\n",
      "Epoch  1998: reducing learning rate of group 0 to 1.6384e-02.\n",
      "Final training Loss:  116.23122406\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  181630.65625\n",
      "################################  100  ################################\n",
      "Training Loss:  117.40170288\n",
      "################################  200  ################################\n",
      "Training Loss:  114.0135498\n",
      "Epoch   256: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  122.58862305\n",
      "################################  400  ################################\n",
      "Training Loss:  114.07641602\n",
      "Epoch   457: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  114.07181549\n",
      "################################  600  ################################\n",
      "Training Loss:  113.70684814\n",
      "################################  700  ################################\n",
      "Training Loss:  90.88396454\n",
      "################################  800  ################################\n",
      "Training Loss:  72.99542236\n",
      "################################  900  ################################\n",
      "Training Loss:  105.60134888\n",
      "################################  1000  ################################\n",
      "Training Loss:  104.9380722\n",
      "################################  1100  ################################\n",
      "Training Loss:  138.50985718\n",
      "################################  1200  ################################\n",
      "Training Loss:  66.86441803\n",
      "################################  1300  ################################\n",
      "Training Loss:  103.099823\n",
      "################################  1400  ################################\n",
      "Training Loss:  119.09144592\n",
      "################################  1500  ################################\n",
      "Training Loss:  104.17884827\n",
      "Epoch  1536: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  105.52017212\n",
      "################################  1700  ################################\n",
      "Training Loss:  98.8107605\n",
      "################################  1800  ################################\n",
      "Training Loss:  86.18443298\n",
      "Epoch  1879: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  82.76268768\n",
      "Final training Loss:  55.03560257\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  285200.3125\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  210392.671875\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1688270.125\n",
      "################################  100  ################################\n",
      "Training Loss:  97.68467712\n",
      "################################  200  ################################\n",
      "Training Loss:  97.56098175\n",
      "################################  300  ################################\n",
      "Training Loss:  95.65620422\n",
      "################################  400  ################################\n",
      "Training Loss:  91.91004181\n",
      "################################  500  ################################\n",
      "Training Loss:  87.52637482\n",
      "################################  600  ################################\n",
      "Training Loss:  48.18127823\n",
      "################################  700  ################################\n",
      "Training Loss:  45.07402802\n",
      "################################  800  ################################\n",
      "Training Loss:  43.80283356\n",
      "################################  900  ################################\n",
      "Training Loss:  60.32682037\n",
      "################################  1000  ################################\n",
      "Training Loss:  47.11508942\n",
      "################################  1100  ################################\n",
      "Training Loss:  45.72325134\n",
      "################################  1200  ################################\n",
      "Training Loss:  42.2551384\n",
      "Epoch  1215: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  43.94028091\n",
      "################################  1400  ################################\n",
      "Training Loss:  44.60106277\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.74592209\n",
      "################################  1600  ################################\n",
      "Training Loss:  42.11874008\n",
      "################################  1700  ################################\n",
      "Training Loss:  42.00788116\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.67734909\n",
      "################################  1900  ################################\n",
      "Training Loss:  40.80784607\n",
      "Epoch  1973: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  41.41735077\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4821.94628906\n",
      "################################  100  ################################\n",
      "Training Loss:  69.69945526\n",
      "################################  200  ################################\n",
      "Training Loss:  49.75505447\n",
      "################################  300  ################################\n",
      "Training Loss:  47.12306976\n",
      "################################  400  ################################\n",
      "Training Loss:  45.65067673\n",
      "################################  500  ################################\n",
      "Training Loss:  48.44024658\n",
      "################################  600  ################################\n",
      "Training Loss:  52.52032471\n",
      "################################  700  ################################\n",
      "Training Loss:  44.24364471\n",
      "################################  800  ################################\n",
      "Training Loss:  40.83253098\n",
      "################################  900  ################################\n",
      "Training Loss:  45.48561859\n",
      "################################  1000  ################################\n",
      "Training Loss:  41.03440857\n",
      "################################  1100  ################################\n",
      "Training Loss:  128.92453003\n",
      "################################  1200  ################################\n",
      "Training Loss:  119.85844421\n",
      "Epoch  1246: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  112.49718475\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.03801727\n",
      "Epoch  1447: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  110.59078217\n",
      "################################  1600  ################################\n",
      "Training Loss:  106.05511475\n",
      "Epoch  1648: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.73960495\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.19837189\n",
      "Epoch  1849: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  50.16091537\n",
      "Final training Loss:  49.39974213\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4142.17919922\n",
      "################################  100  ################################\n",
      "Training Loss:  114.49996185\n",
      "################################  200  ################################\n",
      "Training Loss:  114.10292816\n",
      "Epoch   270: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  114.19844055\n",
      "################################  400  ################################\n",
      "Training Loss:  112.98564148\n",
      "################################  500  ################################\n",
      "Training Loss:  68.01126862\n",
      "################################  600  ################################\n",
      "Training Loss:  57.56878281\n",
      "################################  700  ################################\n",
      "Training Loss:  50.82141113\n",
      "################################  800  ################################\n",
      "Training Loss:  47.28896332\n",
      "################################  900  ################################\n",
      "Training Loss:  47.23465729\n",
      "################################  1000  ################################\n",
      "Training Loss:  44.8477478\n",
      "################################  1100  ################################\n",
      "Training Loss:  45.31175613\n",
      "################################  1200  ################################\n",
      "Training Loss:  43.56472778\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.54534531\n",
      "################################  1400  ################################\n",
      "Training Loss:  66.65057373\n",
      "Epoch  1428: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  45.94161987\n",
      "################################  1600  ################################\n",
      "Training Loss:  45.1477623\n",
      "################################  1700  ################################\n",
      "Training Loss:  40.37491226\n",
      "################################  1800  ################################\n",
      "Training Loss:  55.43507767\n",
      "Epoch  1832: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.82544708\n",
      "Final training Loss:  40.10351562\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2318.32788086\n",
      "################################  100  ################################\n",
      "Training Loss:  3524.72949219\n",
      "################################  200  ################################\n",
      "Training Loss:  97.17024231\n",
      "Epoch   295: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  50.26118088\n",
      "################################  400  ################################\n",
      "Training Loss:  71.80957031\n",
      "Epoch   496: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  52.20604324\n",
      "################################  600  ################################\n",
      "Training Loss:  56.20135498\n",
      "################################  700  ################################\n",
      "Training Loss:  41.77613449\n",
      "################################  800  ################################\n",
      "Training Loss:  58.52960205\n",
      "################################  900  ################################\n",
      "Training Loss:  55.06967926\n",
      "################################  1000  ################################\n",
      "Training Loss:  58.90313339\n",
      "################################  1100  ################################\n",
      "Training Loss:  235.72967529\n",
      "Epoch  1177: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.40614319\n",
      "################################  1300  ################################\n",
      "Training Loss:  118.77214813\n",
      "Epoch  1378: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  121.50963593\n",
      "################################  1500  ################################\n",
      "Training Loss:  77.18800354\n",
      "Epoch  1579: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  49.83986282\n",
      "################################  1700  ################################\n",
      "Training Loss:  112.60799408\n",
      "Epoch  1780: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  111.53961182\n",
      "################################  1900  ################################\n",
      "Training Loss:  110.82250977\n",
      "Epoch  1981: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  110.45897675\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  433119.125\n",
      "################################  100  ################################\n",
      "Training Loss:  159.09642029\n",
      "################################  200  ################################\n",
      "Training Loss:  118.94549561\n",
      "Epoch   287: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  119.17583466\n",
      "################################  400  ################################\n",
      "Training Loss:  119.11399841\n",
      "Epoch   488: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  119.42926788\n",
      "################################  600  ################################\n",
      "Training Loss:  119.63911438\n",
      "Epoch   689: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  119.63829041\n",
      "################################  800  ################################\n",
      "Training Loss:  120.08236694\n",
      "Epoch   890: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  119.44602203\n",
      "################################  1000  ################################\n",
      "Training Loss:  119.39391327\n",
      "Epoch  1091: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.20511627\n",
      "################################  1200  ################################\n",
      "Training Loss:  119.11158752\n",
      "Epoch  1292: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  119.01029968\n",
      "################################  1400  ################################\n",
      "Training Loss:  118.87960052\n",
      "Epoch  1493: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  119.2957077\n",
      "################################  1600  ################################\n",
      "Training Loss:  119.23345184\n",
      "Epoch  1694: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  118.81375122\n",
      "################################  1800  ################################\n",
      "Training Loss:  118.69667053\n",
      "Epoch  1895: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  118.69630432\n",
      "Final training Loss:  118.66255951\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2622.61352539\n",
      "################################  100  ################################\n",
      "Training Loss:  9295.56347656\n",
      "################################  200  ################################\n",
      "Training Loss:  125.7613678\n",
      "################################  300  ################################\n",
      "Training Loss:  118.14693451\n",
      "################################  400  ################################\n",
      "Training Loss:  113.85650635\n",
      "################################  500  ################################\n",
      "Training Loss:  114.3972168\n",
      "################################  600  ################################\n",
      "Training Loss:  113.94777679\n",
      "################################  700  ################################\n",
      "Training Loss:  112.89574432\n",
      "Epoch   774: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  128.62576294\n",
      "################################  900  ################################\n",
      "Training Loss:  121.20529938\n",
      "Epoch   975: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.14813232\n",
      "################################  1100  ################################\n",
      "Training Loss:  121.47908783\n",
      "Epoch  1176: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.03663635\n",
      "################################  1300  ################################\n",
      "Training Loss:  113.72010803\n",
      "Epoch  1377: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  113.37692261\n",
      "################################  1500  ################################\n",
      "Training Loss:  138.75520325\n",
      "Epoch  1578: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.19364929\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.16693115\n",
      "Epoch  1779: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.40415192\n",
      "################################  1900  ################################\n",
      "Training Loss:  117.31973267\n",
      "Final training Loss:  114.1111908\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9366.39355469\n",
      "################################  100  ################################\n",
      "Training Loss:  49.09030914\n",
      "################################  200  ################################\n",
      "Training Loss:  48.2201767\n",
      "################################  300  ################################\n",
      "Training Loss:  43.94377518\n",
      "################################  400  ################################\n",
      "Training Loss:  45.05131149\n",
      "################################  500  ################################\n",
      "Training Loss:  49.5948143\n",
      "Epoch   549: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  45.51533508\n",
      "################################  700  ################################\n",
      "Training Loss:  40.95140839\n",
      "################################  800  ################################\n",
      "Training Loss:  46.27127838\n",
      "################################  900  ################################\n",
      "Training Loss:  64.10318756\n",
      "################################  1000  ################################\n",
      "Training Loss:  40.9397049\n",
      "Epoch  1066: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  39.66866302\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.73748398\n",
      "################################  1300  ################################\n",
      "Training Loss:  42.67822266\n",
      "################################  1400  ################################\n",
      "Training Loss:  40.03544998\n",
      "################################  1500  ################################\n",
      "Training Loss:  39.47056198\n",
      "################################  1600  ################################\n",
      "Training Loss:  39.08446121\n",
      "################################  1700  ################################\n",
      "Training Loss:  38.93789291\n",
      "Epoch  1770: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.87659836\n",
      "################################  1900  ################################\n",
      "Training Loss:  39.94235992\n",
      "Epoch  1971: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  37.01804352\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3553.83642578\n",
      "################################  100  ################################\n",
      "Training Loss:  48.4825058\n",
      "################################  200  ################################\n",
      "Training Loss:  53.36817169\n",
      "################################  300  ################################\n",
      "Training Loss:  75.61817932\n",
      "################################  400  ################################\n",
      "Training Loss:  119.91003418\n",
      "Epoch   460: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  115.36463165\n",
      "################################  600  ################################\n",
      "Training Loss:  120.68993378\n",
      "Epoch   661: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  124.3129425\n",
      "################################  800  ################################\n",
      "Training Loss:  117.25852203\n",
      "Epoch   862: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  117.90115356\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.22904205\n",
      "Epoch  1063: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1034719.625\n",
      "################################  100  ################################\n",
      "Training Loss:  72.20326233\n",
      "################################  200  ################################\n",
      "Training Loss:  62.59107208\n",
      "################################  300  ################################\n",
      "Training Loss:  41.41432953\n",
      "################################  400  ################################\n",
      "Training Loss:  41.77009583\n",
      "Epoch   481: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  53.05399704\n",
      "################################  600  ################################\n",
      "Training Loss:  39.7732811\n",
      "################################  700  ################################\n",
      "Training Loss:  39.08362198\n",
      "################################  800  ################################\n",
      "Training Loss:  125.50715637\n",
      "################################  900  ################################\n",
      "Training Loss:  84.31866455\n",
      "Epoch   974: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  45.1836853\n",
      "################################  1100  ################################\n",
      "Training Loss:  44.95874405\n",
      "Epoch  1175: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  44.62983704\n",
      "################################  1300  ################################\n",
      "Training Loss:  44.47335434\n",
      "Epoch  1376: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  45.40439606\n",
      "################################  1500  ################################\n",
      "Training Loss:  44.45137787\n",
      "Epoch  1577: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  44.72499847\n",
      "################################  1700  ################################\n",
      "Training Loss:  45.38906479\n",
      "Epoch  1778: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  43.7604332\n",
      "################################  1900  ################################\n",
      "Training Loss:  43.62842178\n",
      "Epoch  1979: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  43.75098801\n",
      "\n",
      "Running model (trial=0, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  100009.875\n",
      "################################  100  ################################\n",
      "Training Loss:  89.33694458\n",
      "################################  200  ################################\n",
      "Training Loss:  58.63560104\n",
      "################################  300  ################################\n",
      "Training Loss:  54.71825409\n",
      "################################  400  ################################\n",
      "Training Loss:  44.84357452\n",
      "################################  500  ################################\n",
      "Training Loss:  52.30398941\n",
      "################################  600  ################################\n",
      "Training Loss:  54.62008286\n",
      "################################  700  ################################\n",
      "Training Loss:  49.93320847\n",
      "################################  800  ################################\n",
      "Training Loss:  54.13863373\n",
      "################################  900  ################################\n",
      "Training Loss:  45.52323914\n",
      "################################  1000  ################################\n",
      "Training Loss:  58.51528931\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.80335236\n",
      "Epoch  1167: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  45.06312561\n",
      "################################  1300  ################################\n",
      "Training Loss:  44.36795425\n",
      "Epoch  1368: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  40.82260132\n",
      "################################  1500  ################################\n",
      "Training Loss:  42.2553215\n",
      "Epoch  1569: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  39.34825516\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.32328033\n",
      "Epoch  1770: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  67.91316986\n",
      "################################  1900  ################################\n",
      "Training Loss:  40.41983414\n",
      "Final training Loss:  42.0735321\n",
      "\n",
      "Running model (trial=0, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  573250.6875\n",
      "################################  100  ################################\n",
      "Training Loss:  74.54067993\n",
      "################################  200  ################################\n",
      "Training Loss:  47.52151871\n",
      "################################  300  ################################\n",
      "Training Loss:  47.91606522\n",
      "################################  400  ################################\n",
      "Training Loss:  44.12473297\n",
      "################################  500  ################################\n",
      "Training Loss:  40.64792252\n",
      "################################  600  ################################\n",
      "Training Loss:  70.66291809\n",
      "################################  700  ################################\n",
      "Training Loss:  52.4431076\n",
      "Epoch   743: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  50.79813004\n",
      "################################  900  ################################\n",
      "Training Loss:  46.48580551\n",
      "Epoch   944: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  46.26089478\n",
      "################################  1100  ################################\n",
      "Training Loss:  44.8802681\n",
      "Epoch  1145: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.00424194\n",
      "################################  1300  ################################\n",
      "Training Loss:  46.10404968\n",
      "Epoch  1346: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  44.92713547\n",
      "################################  1500  ################################\n",
      "Training Loss:  44.64596558\n",
      "Epoch  1547: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  44.50123596\n",
      "################################  1700  ################################\n",
      "Training Loss:  44.31975937\n",
      "Epoch  1748: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  45.84989929\n",
      "################################  1900  ################################\n",
      "Training Loss:  44.60821533\n",
      "Epoch  1949: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  44.61899185\n",
      "\n",
      "Running model (trial=0, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1820796544.0\n",
      "################################  100  ################################\n",
      "Training Loss:  215250.28125\n",
      "################################  200  ################################\n",
      "Training Loss:  31319.3984375\n",
      "################################  300  ################################\n",
      "Training Loss:  9063.57519531\n",
      "################################  400  ################################\n",
      "Training Loss:  6775.37548828\n",
      "################################  500  ################################\n",
      "Training Loss:  6979.87402344\n",
      "################################  600  ################################\n",
      "Training Loss:  5918.34472656\n",
      "################################  700  ################################\n",
      "Training Loss:  3181.49682617\n",
      "################################  800  ################################\n",
      "Training Loss:  2220.46728516\n",
      "################################  900  ################################\n",
      "Training Loss:  2669.04296875\n",
      "################################  1000  ################################\n",
      "Training Loss:  814.26794434\n",
      "################################  1100  ################################\n",
      "Training Loss:  499.22576904\n",
      "################################  1200  ################################\n",
      "Training Loss:  703.2734375\n",
      "################################  1300  ################################\n",
      "Training Loss:  207.29463196\n",
      "################################  1400  ################################\n",
      "Training Loss:  1519.89172363\n",
      "################################  1500  ################################\n",
      "Training Loss:  134.60777283\n",
      "################################  1600  ################################\n",
      "Training Loss:  124.49869537\n",
      "################################  1700  ################################\n",
      "Training Loss:  120.27879333\n",
      "################################  1800  ################################\n",
      "Training Loss:  119.21614838\n",
      "Epoch  1895: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  118.42797089\n",
      "Final training Loss:  118.3839035\n",
      "\n",
      "Running model (trial=0, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  232953120555008.0\n",
      "################################  100  ################################\n",
      "Training Loss:  1906211968.0\n",
      "################################  200  ################################\n",
      "Training Loss:  495670880.0\n",
      "################################  300  ################################\n",
      "Training Loss:  93483568.0\n",
      "################################  400  ################################\n",
      "Training Loss:  17613884.0\n",
      "################################  500  ################################\n",
      "Training Loss:  2916405.5\n",
      "################################  600  ################################\n",
      "Training Loss:  630086.25\n",
      "################################  700  ################################\n",
      "Training Loss:  527104.75\n",
      "################################  800  ################################\n",
      "Training Loss:  420430.40625\n",
      "################################  900  ################################\n",
      "Training Loss:  320646.3125\n",
      "################################  1000  ################################\n",
      "Training Loss:  241766.671875\n",
      "################################  1100  ################################\n",
      "Training Loss:  205789.5625\n",
      "################################  1200  ################################\n",
      "Training Loss:  226356.28125\n",
      "################################  1300  ################################\n",
      "Training Loss:  504899.9375\n",
      "Epoch  1385: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  727878.25\n",
      "################################  1500  ################################\n",
      "Training Loss:  953342.4375\n",
      "Epoch  1586: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  1043760.875\n",
      "################################  1700  ################################\n",
      "Training Loss:  1080051.0\n",
      "Epoch  1787: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  1064147.25\n",
      "################################  1900  ################################\n",
      "Training Loss:  1107997.625\n",
      "Epoch  1988: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  1032203.0625\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  483882.84375\n",
      "################################  100  ################################\n",
      "Training Loss:  1524.38208008\n",
      "################################  200  ################################\n",
      "Training Loss:  86.49064636\n",
      "################################  300  ################################\n",
      "Training Loss:  70.27097321\n",
      "################################  400  ################################\n",
      "Training Loss:  59.1829834\n",
      "################################  500  ################################\n",
      "Training Loss:  58.60893631\n",
      "################################  600  ################################\n",
      "Training Loss:  56.59026718\n",
      "################################  700  ################################\n",
      "Training Loss:  52.96056366\n",
      "################################  800  ################################\n",
      "Training Loss:  53.96010971\n",
      "################################  900  ################################\n",
      "Training Loss:  60.87730408\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.71274185\n",
      "Epoch  1034: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.22610092\n",
      "################################  1200  ################################\n",
      "Training Loss:  59.45087433\n",
      "Epoch  1235: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.26776123\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.72496033\n",
      "Epoch  1436: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.48810959\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.63139343\n",
      "Epoch  1637: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.32949066\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.7399292\n",
      "Epoch  1838: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.2580986\n",
      "Final training Loss:  52.32598495\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3914225.5\n",
      "################################  100  ################################\n",
      "Training Loss:  4072.97094727\n",
      "################################  200  ################################\n",
      "Training Loss:  2786.61865234\n",
      "################################  300  ################################\n",
      "Training Loss:  173.25946045\n",
      "################################  400  ################################\n",
      "Training Loss:  84.22074127\n",
      "################################  500  ################################\n",
      "Training Loss:  85.39001465\n",
      "################################  600  ################################\n",
      "Training Loss:  83.90407562\n",
      "################################  700  ################################\n",
      "Training Loss:  79.78887939\n",
      "################################  800  ################################\n",
      "Training Loss:  83.23965454\n",
      "################################  900  ################################\n",
      "Training Loss:  75.3734436\n",
      "################################  1000  ################################\n",
      "Training Loss:  80.93180847\n",
      "################################  1100  ################################\n",
      "Training Loss:  71.50611115\n",
      "################################  1200  ################################\n",
      "Training Loss:  70.28501892\n",
      "################################  1300  ################################\n",
      "Training Loss:  71.37198639\n",
      "################################  1400  ################################\n",
      "Training Loss:  70.11138153\n",
      "################################  1500  ################################\n",
      "Training Loss:  62.17332458\n",
      "################################  1600  ################################\n",
      "Training Loss:  61.70741272\n",
      "################################  1700  ################################\n",
      "Training Loss:  58.56108475\n",
      "################################  1800  ################################\n",
      "Training Loss:  57.09438705\n",
      "Epoch  1875: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.68132401\n",
      "Final training Loss:  55.40362549\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2300.86328125\n",
      "################################  100  ################################\n",
      "Training Loss:  76.90702057\n",
      "################################  200  ################################\n",
      "Training Loss:  59.25969315\n",
      "################################  300  ################################\n",
      "Training Loss:  73.16636658\n",
      "################################  400  ################################\n",
      "Training Loss:  55.26225281\n",
      "Epoch   437: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  59.74723816\n",
      "################################  600  ################################\n",
      "Training Loss:  59.89138794\n",
      "################################  700  ################################\n",
      "Training Loss:  56.48667908\n",
      "Epoch   787: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  61.19028473\n",
      "################################  900  ################################\n",
      "Training Loss:  54.2406044\n",
      "Epoch   988: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.44593811\n",
      "################################  1100  ################################\n",
      "Training Loss:  57.99249649\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.46566772\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.52119446\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.03954315\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.85813904\n",
      "################################  1600  ################################\n",
      "Training Loss:  50.62216949\n",
      "################################  1700  ################################\n",
      "Training Loss:  45.02142334\n",
      "################################  1800  ################################\n",
      "Training Loss:  49.80770493\n",
      "################################  1900  ################################\n",
      "Training Loss:  46.99387741\n",
      "Epoch  1921: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  43.03144455\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1340650.75\n",
      "################################  100  ################################\n",
      "Training Loss:  123.60519409\n",
      "################################  200  ################################\n",
      "Training Loss:  89.43230438\n",
      "################################  300  ################################\n",
      "Training Loss:  54.11407852\n",
      "################################  400  ################################\n",
      "Training Loss:  51.61322784\n",
      "Epoch   419: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  53.73340607\n",
      "################################  600  ################################\n",
      "Training Loss:  52.00022888\n",
      "################################  700  ################################\n",
      "Training Loss:  55.79520035\n",
      "################################  800  ################################\n",
      "Training Loss:  53.1818428\n",
      "################################  900  ################################\n",
      "Training Loss:  50.64097977\n",
      "################################  1000  ################################\n",
      "Training Loss:  50.32553482\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.07430267\n",
      "################################  1200  ################################\n",
      "Training Loss:  124.27526093\n",
      "Epoch  1240: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  95.09784698\n",
      "################################  1400  ################################\n",
      "Training Loss:  73.40393066\n",
      "Epoch  1441: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  57.32324219\n",
      "################################  1600  ################################\n",
      "Training Loss:  62.95787048\n",
      "Epoch  1642: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  55.85187912\n",
      "################################  1800  ################################\n",
      "Training Loss:  59.34444046\n",
      "Epoch  1843: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  59.0521698\n",
      "Final training Loss:  55.78341293\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  55467.39453125\n",
      "################################  100  ################################\n",
      "Training Loss:  134.78385925\n",
      "################################  200  ################################\n",
      "Training Loss:  122.29590607\n",
      "################################  300  ################################\n",
      "Training Loss:  121.69367218\n",
      "Epoch   302: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  121.30601501\n",
      "################################  500  ################################\n",
      "Training Loss:  121.01338959\n",
      "Epoch   503: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  120.95024872\n",
      "################################  700  ################################\n",
      "Training Loss:  120.64226532\n",
      "Epoch   704: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  120.68943787\n",
      "################################  900  ################################\n",
      "Training Loss:  120.73588562\n",
      "Epoch   905: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  120.57988739\n",
      "################################  1100  ################################\n",
      "Training Loss:  120.67427826\n",
      "Epoch  1106: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  120.50950623\n",
      "################################  1300  ################################\n",
      "Training Loss:  120.48571014\n",
      "Epoch  1307: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  120.48200989\n",
      "################################  1500  ################################\n",
      "Training Loss:  120.50533295\n",
      "Epoch  1508: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  120.43161011\n",
      "################################  1700  ################################\n",
      "Training Loss:  120.53826141\n",
      "Epoch  1709: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  120.42511749\n",
      "################################  1900  ################################\n",
      "Training Loss:  120.41744232\n",
      "Epoch  1993: reducing learning rate of group 0 to 6.7109e-03.\n",
      "Final training Loss:  120.43929291\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2276.59570312\n",
      "################################  100  ################################\n",
      "Training Loss:  64.48097992\n",
      "################################  200  ################################\n",
      "Training Loss:  55.54232025\n",
      "################################  300  ################################\n",
      "Training Loss:  54.20800018\n",
      "################################  400  ################################\n",
      "Training Loss:  53.04768753\n",
      "################################  500  ################################\n",
      "Training Loss:  52.28358841\n",
      "################################  600  ################################\n",
      "Training Loss:  59.47909546\n",
      "################################  700  ################################\n",
      "Training Loss:  53.2865181\n",
      "################################  800  ################################\n",
      "Training Loss:  52.51403046\n",
      "################################  900  ################################\n",
      "Training Loss:  48.20615768\n",
      "################################  1000  ################################\n",
      "Training Loss:  49.29165649\n",
      "################################  1100  ################################\n",
      "Training Loss:  161.67245483\n",
      "################################  1200  ################################\n",
      "Training Loss:  101.44219971\n",
      "Epoch  1216: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  158.57629395\n",
      "################################  1400  ################################\n",
      "Training Loss:  160.05879211\n",
      "Epoch  1417: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  57.15763092\n",
      "################################  1600  ################################\n",
      "Training Loss:  65.32128906\n",
      "Epoch  1618: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.35877609\n",
      "################################  1800  ################################\n",
      "Training Loss:  54.42518616\n",
      "Epoch  1819: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  70.10878754\n",
      "Final training Loss:  62.76496506\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8304.39550781\n",
      "################################  100  ################################\n",
      "Training Loss:  122.11695099\n",
      "################################  200  ################################\n",
      "Training Loss:  117.23888397\n",
      "Epoch   262: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  115.27654266\n",
      "################################  400  ################################\n",
      "Training Loss:  114.20478058\n",
      "################################  500  ################################\n",
      "Training Loss:  110.728302\n",
      "################################  600  ################################\n",
      "Training Loss:  114.55903625\n",
      "################################  700  ################################\n",
      "Training Loss:  91.10190582\n",
      "################################  800  ################################\n",
      "Training Loss:  76.82587433\n",
      "################################  900  ################################\n",
      "Training Loss:  59.50630569\n",
      "################################  1000  ################################\n",
      "Training Loss:  58.88201904\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.35000992\n",
      "################################  1200  ################################\n",
      "Training Loss:  60.18447113\n",
      "Epoch  1217: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  55.07276535\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.66401291\n",
      "Epoch  1418: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  61.7246933\n",
      "################################  1600  ################################\n",
      "Training Loss:  62.54800034\n",
      "Epoch  1619: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  51.88528061\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.99088287\n",
      "Epoch  1820: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.67498016\n",
      "Final training Loss:  56.02384949\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4801.61132812\n",
      "################################  100  ################################\n",
      "Training Loss:  160.92730713\n",
      "################################  200  ################################\n",
      "Training Loss:  118.12548065\n",
      "################################  300  ################################\n",
      "Training Loss:  118.62243652\n",
      "################################  400  ################################\n",
      "Training Loss:  127.80440521\n",
      "################################  500  ################################\n",
      "Training Loss:  118.70640564\n",
      "Epoch   575: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  130.2507019\n",
      "################################  700  ################################\n",
      "Training Loss:  121.07336426\n",
      "Epoch   789: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  120.25854492\n",
      "################################  900  ################################\n",
      "Training Loss:  116.4862442\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.21347809\n",
      "################################  1100  ################################\n",
      "Training Loss:  118.00224304\n",
      "Epoch  1189: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  121.59229279\n",
      "################################  1300  ################################\n",
      "Training Loss:  119.00085449\n",
      "Epoch  1390: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.60774231\n",
      "################################  1500  ################################\n",
      "Training Loss:  147.05078125\n",
      "Epoch  1591: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.59282684\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.4132843\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.05968475\n",
      "################################  1900  ################################\n",
      "Training Loss:  113.33772278\n",
      "Epoch  1993: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  115.12791443\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  120335.75\n",
      "################################  100  ################################\n",
      "Training Loss:  74.42170715\n",
      "################################  200  ################################\n",
      "Training Loss:  53.68446732\n",
      "################################  300  ################################\n",
      "Training Loss:  54.5079155\n",
      "################################  400  ################################\n",
      "Training Loss:  54.64920044\n",
      "################################  500  ################################\n",
      "Training Loss:  53.39293671\n",
      "################################  600  ################################\n",
      "Training Loss:  54.39723206\n",
      "################################  700  ################################\n",
      "Training Loss:  56.42985916\n",
      "################################  800  ################################\n",
      "Training Loss:  53.23364639\n",
      "################################  900  ################################\n",
      "Training Loss:  63.4282608\n",
      "################################  1000  ################################\n",
      "Training Loss:  57.44588852\n",
      "################################  1100  ################################\n",
      "Training Loss:  64.16719818\n",
      "################################  1200  ################################\n",
      "Training Loss:  56.1722641\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.80273819\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.52012634\n",
      "################################  1500  ################################\n",
      "Training Loss:  49.6622467\n",
      "################################  1600  ################################\n",
      "Training Loss:  52.50148773\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.44450378\n",
      "################################  1800  ################################\n",
      "Training Loss:  59.12182617\n",
      "################################  1900  ################################\n",
      "Training Loss:  48.50069427\n",
      "Final training Loss:  47.25419998\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1102508.375\n",
      "################################  100  ################################\n",
      "Training Loss:  124.4039917\n",
      "################################  200  ################################\n",
      "Training Loss:  122.09505463\n",
      "Epoch   249: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  121.83713531\n",
      "################################  400  ################################\n",
      "Training Loss:  121.34468842\n",
      "################################  500  ################################\n",
      "Training Loss:  120.58636475\n",
      "################################  600  ################################\n",
      "Training Loss:  120.21244812\n",
      "################################  700  ################################\n",
      "Training Loss:  120.21375275\n",
      "################################  800  ################################\n",
      "Training Loss:  119.87834167\n",
      "################################  900  ################################\n",
      "Training Loss:  119.69049072\n",
      "################################  1000  ################################\n",
      "Training Loss:  119.6404953\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.83347321\n",
      "################################  1200  ################################\n",
      "Training Loss:  118.93089294\n",
      "################################  1300  ################################\n",
      "Training Loss:  118.38267517\n",
      "################################  1400  ################################\n",
      "Training Loss:  119.59178925\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.41629791\n",
      "################################  1600  ################################\n",
      "Training Loss:  116.72481537\n",
      "################################  1700  ################################\n",
      "Training Loss:  115.58878326\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.00552368\n",
      "################################  1900  ################################\n",
      "Training Loss:  118.77981567\n",
      "Epoch  1904: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  116.04337311\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3562.33496094\n",
      "################################  100  ################################\n",
      "Training Loss:  99.51693726\n",
      "################################  200  ################################\n",
      "Training Loss:  61.43797684\n",
      "################################  300  ################################\n",
      "Training Loss:  61.15193939\n",
      "################################  400  ################################\n",
      "Training Loss:  55.42737579\n",
      "################################  500  ################################\n",
      "Training Loss:  56.47804642\n",
      "################################  600  ################################\n",
      "Training Loss:  52.39087296\n",
      "################################  700  ################################\n",
      "Training Loss:  55.11642075\n",
      "################################  800  ################################\n",
      "Training Loss:  56.91272736\n",
      "################################  900  ################################\n",
      "Training Loss:  50.75141907\n",
      "################################  1000  ################################\n",
      "Training Loss:  49.48460388\n",
      "################################  1100  ################################\n",
      "Training Loss:  50.73843384\n",
      "################################  1200  ################################\n",
      "Training Loss:  48.45146942\n",
      "################################  1300  ################################\n",
      "Training Loss:  55.02799225\n",
      "################################  1400  ################################\n",
      "Training Loss:  48.23765564\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.58163071\n",
      "################################  1600  ################################\n",
      "Training Loss:  226.86730957\n",
      "################################  1700  ################################\n",
      "Training Loss:  125.6623764\n",
      "Epoch  1752: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  123.84506989\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.70642853\n",
      "Epoch  1953: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  58.83946228\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  379336.0\n",
      "################################  100  ################################\n",
      "Training Loss:  175.43247986\n",
      "################################  200  ################################\n",
      "Training Loss:  114.55696106\n",
      "################################  300  ################################\n",
      "Training Loss:  110.45320129\n",
      "################################  400  ################################\n",
      "Training Loss:  102.87152863\n",
      "################################  500  ################################\n",
      "Training Loss:  96.72441101\n",
      "################################  600  ################################\n",
      "Training Loss:  90.27094269\n",
      "################################  700  ################################\n",
      "Training Loss:  81.59671783\n",
      "################################  800  ################################\n",
      "Training Loss:  61.22796249\n",
      "################################  900  ################################\n",
      "Training Loss:  52.89435196\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.30300522\n",
      "Epoch  1010: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.07542801\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.15348053\n",
      "Epoch  1211: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.02932739\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.00255585\n",
      "Epoch  1412: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.70912552\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.95783615\n",
      "Epoch  1613: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  51.60072708\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.66711426\n",
      "Epoch  1814: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.52054977\n",
      "Final training Loss:  50.80831528\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  31286.79492188\n",
      "################################  100  ################################\n",
      "Training Loss:  133.13487244\n",
      "################################  200  ################################\n",
      "Training Loss:  127.02624512\n",
      "################################  300  ################################\n",
      "Training Loss:  117.91599274\n",
      "################################  400  ################################\n",
      "Training Loss:  116.79833984\n",
      "################################  500  ################################\n",
      "Training Loss:  118.65557098\n",
      "################################  600  ################################\n",
      "Training Loss:  121.10391235\n",
      "################################  700  ################################\n",
      "Training Loss:  117.65856934\n",
      "################################  800  ################################\n",
      "Epoch   801: reducing learning rate of group 0 to 4.0000e-02.\n",
      "Training Loss:  136.26287842\n",
      "################################  900  ################################\n",
      "Training Loss:  117.84039307\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.96169281\n",
      "################################  1100  ################################\n",
      "Training Loss:  121.99564362\n",
      "Epoch  1195: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  118.35249329\n",
      "################################  1300  ################################\n",
      "Training Loss:  117.3895874\n",
      "################################  1400  ################################\n",
      "Training Loss:  234.35748291\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.37651825\n",
      "################################  1600  ################################\n",
      "Training Loss:  129.24871826\n",
      "Epoch  1668: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  160.84757996\n",
      "################################  1800  ################################\n",
      "Training Loss:  117.75821686\n",
      "################################  1900  ################################\n",
      "Training Loss:  122.01487732\n",
      "Final training Loss:  140.1713562\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6921277.5\n",
      "################################  100  ################################\n",
      "Training Loss:  56.1219635\n",
      "################################  200  ################################\n",
      "Training Loss:  57.40842819\n",
      "################################  300  ################################\n",
      "Training Loss:  62.13299179\n",
      "################################  400  ################################\n",
      "Training Loss:  67.87770081\n",
      "################################  500  ################################\n",
      "Training Loss:  77.68309784\n",
      "Epoch   574: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  59.33524704\n",
      "################################  700  ################################\n",
      "Training Loss:  64.45076752\n",
      "Epoch   775: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  66.96382904\n",
      "################################  900  ################################\n",
      "Training Loss:  58.63552094\n",
      "Epoch   976: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.82675171\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.25234222\n",
      "Epoch  1177: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  57.73950577\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.9636116\n",
      "Epoch  1378: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  55.92553329\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.99500275\n",
      "Epoch  1579: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  59.57429504\n",
      "################################  1700  ################################\n",
      "Training Loss:  57.12887192\n",
      "Epoch  1780: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.23252869\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.29052353\n",
      "Final training Loss:  62.94806671\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9141890.0\n",
      "################################  100  ################################\n",
      "Training Loss:  76.56637573\n",
      "################################  200  ################################\n",
      "Training Loss:  95.01332092\n",
      "################################  300  ################################\n",
      "Training Loss:  82.39543152\n",
      "################################  400  ################################\n",
      "Training Loss:  221.48669434\n",
      "Epoch   414: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  59.47354126\n",
      "################################  600  ################################\n",
      "Training Loss:  121.45954132\n",
      "################################  700  ################################\n",
      "Training Loss:  187.04403687\n",
      "################################  800  ################################\n",
      "Training Loss:  52.90176773\n",
      "################################  900  ################################\n",
      "Training Loss:  56.90956497\n",
      "Epoch   927: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.56815338\n",
      "################################  1100  ################################\n",
      "Training Loss:  127.09054565\n",
      "Epoch  1128: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  75.56950378\n",
      "################################  1300  ################################\n",
      "Training Loss:  63.82255554\n",
      "Epoch  1329: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  70.26054382\n",
      "################################  1500  ################################\n",
      "Training Loss:  72.07037354\n",
      "Epoch  1530: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.58321381\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.45761871\n",
      "Epoch  1731: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  60.85507202\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.19500732\n",
      "Epoch  1932: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  52.7793808\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  732361.8125\n",
      "################################  100  ################################\n",
      "Training Loss:  116.68978882\n",
      "################################  200  ################################\n",
      "Training Loss:  131.10299683\n",
      "Epoch   254: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  114.86656189\n",
      "################################  400  ################################\n",
      "Training Loss:  115.5850296\n",
      "Epoch   455: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  128.64151001\n",
      "################################  600  ################################\n",
      "Training Loss:  141.63053894\n",
      "Epoch   656: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  118.66812134\n",
      "################################  800  ################################\n",
      "Training Loss:  115.41146088\n",
      "Epoch   857: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  117.01813507\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.75592041\n",
      "Epoch  1058: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.45440674\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.30463409\n",
      "Epoch  1259: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.8184967\n",
      "################################  1400  ################################\n",
      "Training Loss:  113.85914612\n",
      "Epoch  1460: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  114.54573822\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.35578156\n",
      "Epoch  1661: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.5957489\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.01503754\n",
      "Epoch  1862: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.01259613\n",
      "Final training Loss:  114.22550964\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  158355.5625\n",
      "################################  100  ################################\n",
      "Training Loss:  54.17670059\n",
      "################################  200  ################################\n",
      "Training Loss:  54.0920639\n",
      "################################  300  ################################\n",
      "Training Loss:  53.59825134\n",
      "################################  400  ################################\n",
      "Training Loss:  49.28212738\n",
      "################################  500  ################################\n",
      "Training Loss:  53.00901413\n",
      "################################  600  ################################\n",
      "Training Loss:  47.24529648\n",
      "################################  700  ################################\n",
      "Training Loss:  45.71392441\n",
      "################################  800  ################################\n",
      "Training Loss:  58.78599167\n",
      "################################  900  ################################\n",
      "Training Loss:  53.39968491\n",
      "Epoch   999: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  46.7336998\n",
      "################################  1100  ################################\n",
      "Training Loss:  45.39507294\n",
      "################################  1200  ################################\n",
      "Training Loss:  43.10907745\n",
      "################################  1300  ################################\n",
      "Training Loss:  45.31365585\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.92634583\n",
      "################################  1500  ################################\n",
      "Training Loss:  41.43249893\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.08903503\n",
      "################################  1700  ################################\n",
      "Training Loss:  43.30968475\n",
      "################################  1800  ################################\n",
      "Training Loss:  42.64681244\n",
      "################################  1900  ################################\n",
      "Training Loss:  49.28033066\n",
      "Final training Loss:  69.33106995\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  60752.296875\n",
      "################################  100  ################################\n",
      "Training Loss:  53.24991989\n",
      "################################  200  ################################\n",
      "Training Loss:  53.45410919\n",
      "################################  300  ################################\n",
      "Training Loss:  53.73276901\n",
      "################################  400  ################################\n",
      "Training Loss:  52.96193314\n",
      "################################  500  ################################\n",
      "Training Loss:  54.3968544\n",
      "Epoch   583: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  56.35163498\n",
      "################################  700  ################################\n",
      "Training Loss:  54.21960831\n",
      "Epoch   784: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  53.54682922\n",
      "################################  900  ################################\n",
      "Training Loss:  52.93013\n",
      "Epoch   985: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.18392563\n",
      "################################  1100  ################################\n",
      "Training Loss:  57.95814133\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.32128143\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.59233856\n",
      "################################  1400  ################################\n",
      "Training Loss:  57.01753998\n",
      "Epoch  1470: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.41954422\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.26691818\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.25152206\n",
      "Epoch  1729: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.53572845\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.94872284\n",
      "Epoch  1930: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  52.53186417\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  29341.15820312\n",
      "################################  100  ################################\n",
      "Training Loss:  50.01967239\n",
      "################################  200  ################################\n",
      "Training Loss:  57.5271225\n",
      "################################  300  ################################\n",
      "Training Loss:  114.17593384\n",
      "Epoch   375: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  120.49706268\n",
      "################################  500  ################################\n",
      "Training Loss:  119.80973053\n",
      "Epoch   576: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  224.63240051\n",
      "################################  700  ################################\n",
      "Training Loss:  60.34418488\n",
      "Epoch   777: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  58.44380569\n",
      "################################  900  ################################\n",
      "Training Loss:  59.57304001\n",
      "Epoch   978: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  56.26760864\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.57886505\n",
      "Epoch  1179: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.67709732\n",
      "################################  1300  ################################\n",
      "Training Loss:  59.34856796\n",
      "Epoch  1380: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.50703812\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.07564163\n",
      "Epoch  1581: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.79899979\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.64809036\n",
      "Epoch  1782: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.21527863\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.76399612\n",
      "Epoch  1983: reducing learning rate of group 0 to 6.7109e-03.\n",
      "Final training Loss:  54.45309067\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  439463.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  126.69348145\n",
      "################################  200  ################################\n",
      "Training Loss:  124.79773712\n",
      "Epoch   258: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  117.08341217\n",
      "################################  400  ################################\n",
      "Training Loss:  117.47877502\n",
      "Epoch   459: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  134.97836304\n",
      "################################  600  ################################\n",
      "Training Loss:  130.84388733\n",
      "Epoch   660: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  120.21335602\n",
      "################################  800  ################################\n",
      "Training Loss:  120.75037384\n",
      "Epoch   861: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  118.52102661\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.83805847\n",
      "Epoch  1062: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.61016083\n",
      "################################  1200  ################################\n",
      "Training Loss:  117.74259949\n",
      "Epoch  1263: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  119.07927704\n",
      "################################  1400  ################################\n",
      "Training Loss:  118.7411499\n",
      "Epoch  1464: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  120.44122314\n",
      "################################  1600  ################################\n",
      "Training Loss:  124.66589355\n",
      "Epoch  1665: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  121.14555359\n",
      "################################  1800  ################################\n",
      "Training Loss:  119.13162994\n",
      "Epoch  1866: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  117.57468414\n",
      "Final training Loss:  117.64487457\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  694715.8125\n",
      "################################  100  ################################\n",
      "Training Loss:  67.96316528\n",
      "################################  200  ################################\n",
      "Training Loss:  64.68746185\n",
      "################################  300  ################################\n",
      "Training Loss:  65.83628082\n",
      "################################  400  ################################\n",
      "Training Loss:  61.4379921\n",
      "################################  500  ################################\n",
      "Training Loss:  56.15624237\n",
      "################################  600  ################################\n",
      "Training Loss:  53.09630966\n",
      "################################  700  ################################\n",
      "Training Loss:  49.66326904\n",
      "################################  800  ################################\n",
      "Training Loss:  52.66313553\n",
      "################################  900  ################################\n",
      "Training Loss:  50.06475067\n",
      "################################  1000  ################################\n",
      "Training Loss:  46.83014297\n",
      "################################  1100  ################################\n",
      "Training Loss:  43.31751251\n",
      "################################  1200  ################################\n",
      "Training Loss:  42.90563583\n",
      "################################  1300  ################################\n",
      "Training Loss:  43.46575928\n",
      "################################  1400  ################################\n",
      "Training Loss:  81.88525391\n",
      "################################  1500  ################################\n",
      "Training Loss:  55.26993179\n",
      "Epoch  1549: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  60.49972534\n",
      "################################  1700  ################################\n",
      "Training Loss:  46.9572258\n",
      "Epoch  1750: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  48.8940239\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.61231995\n",
      "Epoch  1951: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  105.34655762\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  821261.5\n",
      "################################  100  ################################\n",
      "Training Loss:  125.1884613\n",
      "################################  200  ################################\n",
      "Training Loss:  111.20212555\n",
      "################################  300  ################################\n",
      "Training Loss:  110.7694397\n",
      "################################  400  ################################\n",
      "Training Loss:  119.38157654\n",
      "################################  500  ################################\n",
      "Training Loss:  107.13903809\n",
      "################################  600  ################################\n",
      "Training Loss:  125.03079224\n",
      "################################  700  ################################\n",
      "Training Loss:  102.90898895\n",
      "################################  800  ################################\n",
      "Training Loss:  102.31535339\n",
      "################################  900  ################################\n",
      "Training Loss:  113.87635803\n",
      "Epoch   932: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  172.76199341\n",
      "################################  1100  ################################\n",
      "Training Loss:  112.22073364\n",
      "################################  1200  ################################\n",
      "Training Loss:  167.63208008\n",
      "################################  1300  ################################\n",
      "Training Loss:  139.46325684\n",
      "################################  1400  ################################\n",
      "Training Loss:  100.77700806\n",
      "################################  1500  ################################\n",
      "Training Loss:  97.45581055\n",
      "################################  1600  ################################\n",
      "Training Loss:  97.33052063\n",
      "################################  1700  ################################\n",
      "Training Loss:  105.44943237\n",
      "################################  1800  ################################\n",
      "Training Loss:  89.68688202\n",
      "################################  1900  ################################\n",
      "Training Loss:  83.80203247\n",
      "Final training Loss:  77.79649353\n",
      "\n",
      "Running model (trial=1, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  797417.6875\n",
      "################################  100  ################################\n",
      "Training Loss:  180.49754333\n",
      "################################  200  ################################\n",
      "Training Loss:  120.49961853\n",
      "################################  300  ################################\n",
      "Training Loss:  118.45307159\n",
      "################################  400  ################################\n",
      "Training Loss:  124.99974823\n",
      "Epoch   443: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  125.72663879\n",
      "################################  600  ################################\n",
      "Training Loss:  119.29428101\n",
      "################################  700  ################################\n",
      "Training Loss:  139.32476807\n",
      "################################  800  ################################\n",
      "Training Loss:  134.50376892\n",
      "################################  900  ################################\n",
      "Training Loss:  123.64345551\n",
      "################################  1000  ################################\n",
      "Training Loss:  132.54777527\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.14863586\n",
      "################################  1200  ################################\n",
      "Training Loss:  131.58647156\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.93675232\n",
      "################################  1400  ################################\n",
      "Training Loss:  117.66760254\n",
      "################################  1500  ################################\n",
      "Training Loss:  133.39782715\n",
      "################################  1600  ################################\n",
      "Training Loss:  119.44002533\n",
      "################################  1700  ################################\n",
      "Training Loss:  197.04681396\n",
      "################################  1800  ################################\n",
      "Training Loss:  124.41809082\n",
      "################################  1900  ################################\n",
      "Training Loss:  410.63146973\n",
      "Final training Loss:  132.01600647\n",
      "\n",
      "Running model (trial=1, mod=53, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  76528.4609375\n",
      "################################  100  ################################\n",
      "Training Loss:  57.05068207\n",
      "################################  200  ################################\n",
      "Training Loss:  48.17792511\n",
      "################################  300  ################################\n",
      "Training Loss:  48.12052536\n",
      "################################  400  ################################\n",
      "Training Loss:  47.23931885\n",
      "################################  500  ################################\n",
      "Training Loss:  45.73077774\n",
      "################################  600  ################################\n",
      "Training Loss:  45.13401413\n",
      "################################  700  ################################\n",
      "Training Loss:  54.7395401\n",
      "################################  800  ################################\n",
      "Training Loss:  39.81255722\n",
      "################################  900  ################################\n",
      "Training Loss:  53.38434601\n",
      "################################  1000  ################################\n",
      "Training Loss:  51.25492477\n",
      "################################  1100  ################################\n",
      "Training Loss:  39.2402153\n",
      "################################  1200  ################################\n",
      "Training Loss:  47.17095947\n",
      "################################  1300  ################################\n",
      "Training Loss:  44.56031036\n",
      "################################  1400  ################################\n",
      "Training Loss:  44.38101578\n",
      "Epoch  1475: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  40.02096558\n",
      "################################  1600  ################################\n",
      "Training Loss:  43.24408722\n",
      "################################  1700  ################################\n",
      "Training Loss:  38.48220062\n",
      "################################  1800  ################################\n",
      "Training Loss:  39.05109787\n",
      "Epoch  1832: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  44.1747818\n",
      "Final training Loss:  38.38228226\n",
      "\n",
      "Running model (trial=1, mod=54, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1067752.0\n",
      "################################  100  ################################\n",
      "Training Loss:  55.22167587\n",
      "################################  200  ################################\n",
      "Training Loss:  56.78504562\n",
      "################################  300  ################################\n",
      "Training Loss:  53.51930237\n",
      "################################  400  ################################\n",
      "Training Loss:  45.45049286\n",
      "################################  500  ################################\n",
      "Training Loss:  63.04690552\n",
      "################################  600  ################################\n",
      "Training Loss:  120.80758667\n",
      "################################  700  ################################\n",
      "Training Loss:  114.56118011\n",
      "Epoch   731: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  114.53088379\n",
      "################################  900  ################################\n",
      "Training Loss:  114.27145386\n",
      "Epoch   932: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  113.4943924\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.76966095\n",
      "Epoch  1133: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.04476166\n",
      "################################  1300  ################################\n",
      "Training Loss:  113.74351501\n",
      "Epoch  1334: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.00767517\n",
      "################################  1500  ################################\n",
      "Training Loss:  117.26859283\n",
      "Epoch  1535: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.50180054\n",
      "################################  1700  ################################\n",
      "Training Loss:  113.48569489\n",
      "Epoch  1736: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  113.53573608\n",
      "################################  1900  ################################\n",
      "Training Loss:  113.61421204\n",
      "Epoch  1937: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  113.52073669\n",
      "\n",
      "Running model (trial=1, mod=55, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  75309.078125\n",
      "################################  100  ################################\n",
      "Training Loss:  80.22113037\n",
      "################################  200  ################################\n",
      "Training Loss:  45.99399185\n",
      "################################  300  ################################\n",
      "Training Loss:  45.91311646\n",
      "################################  400  ################################\n",
      "Training Loss:  43.7694664\n",
      "################################  500  ################################\n",
      "Training Loss:  41.63313293\n",
      "################################  600  ################################\n",
      "Training Loss:  43.3255806\n",
      "################################  700  ################################\n",
      "Training Loss:  42.08695221\n",
      "################################  800  ################################\n",
      "Training Loss:  39.66315079\n",
      "################################  900  ################################\n",
      "Training Loss:  98.7869873\n",
      "################################  1000  ################################\n",
      "Training Loss:  62.63735962\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.1611557\n",
      "Epoch  1158: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  41.80606079\n",
      "################################  1300  ################################\n",
      "Training Loss:  40.58227921\n",
      "################################  1400  ################################\n",
      "Training Loss:  46.77428436\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.66937637\n",
      "################################  1600  ################################\n",
      "Training Loss:  47.47195816\n",
      "################################  1700  ################################\n",
      "Training Loss:  42.24346924\n",
      "Epoch  1703: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  46.39572525\n",
      "################################  1900  ################################\n",
      "Training Loss:  38.88755417\n",
      "Epoch  1904: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  39.70461273\n",
      "\n",
      "Running model (trial=1, mod=56, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  105041.7734375\n",
      "################################  100  ################################\n",
      "Training Loss:  59.97711563\n",
      "################################  200  ################################\n",
      "Training Loss:  51.53292084\n",
      "################################  300  ################################\n",
      "Training Loss:  51.76621246\n",
      "################################  400  ################################\n",
      "Training Loss:  51.36104202\n",
      "################################  500  ################################\n",
      "Training Loss:  51.25294113\n",
      "################################  600  ################################\n",
      "Training Loss:  53.48604202\n",
      "################################  700  ################################\n",
      "Training Loss:  53.11955643\n",
      "################################  800  ################################\n",
      "Training Loss:  52.09475327\n",
      "################################  900  ################################\n",
      "Training Loss:  54.50485611\n",
      "################################  1000  ################################\n",
      "Training Loss:  50.54684067\n",
      "################################  1100  ################################\n",
      "Training Loss:  48.01296234\n",
      "################################  1200  ################################\n",
      "Training Loss:  45.69866943\n",
      "################################  1300  ################################\n",
      "Training Loss:  45.28332138\n",
      "################################  1400  ################################\n",
      "Training Loss:  45.50156784\n",
      "################################  1500  ################################\n",
      "Training Loss:  40.68323135\n",
      "################################  1600  ################################\n",
      "Training Loss:  40.81503296\n",
      "################################  1700  ################################\n",
      "Training Loss:  55.81858063\n",
      "################################  1800  ################################\n",
      "Training Loss:  39.55231476\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.99557495\n",
      "Final training Loss:  39.25595093\n",
      "\n",
      "Running model (trial=1, mod=57, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  119607.8359375\n",
      "################################  100  ################################\n",
      "Training Loss:  57.68527603\n",
      "################################  200  ################################\n",
      "Training Loss:  47.48257828\n",
      "################################  300  ################################\n",
      "Training Loss:  46.40708542\n",
      "Epoch   351: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  46.31098175\n",
      "################################  500  ################################\n",
      "Training Loss:  45.7636261\n",
      "################################  600  ################################\n",
      "Training Loss:  45.68980026\n",
      "################################  700  ################################\n",
      "Training Loss:  45.17121887\n",
      "################################  800  ################################\n",
      "Training Loss:  44.91658783\n",
      "################################  900  ################################\n",
      "Training Loss:  44.77077866\n",
      "################################  1000  ################################\n",
      "Training Loss:  44.68236542\n",
      "################################  1100  ################################\n",
      "Training Loss:  44.49407959\n",
      "################################  1200  ################################\n",
      "Training Loss:  44.33866119\n",
      "################################  1300  ################################\n",
      "Training Loss:  44.28560638\n",
      "################################  1400  ################################\n",
      "Training Loss:  44.20405579\n",
      "################################  1500  ################################\n",
      "Training Loss:  44.17201614\n",
      "################################  1600  ################################\n",
      "Training Loss:  43.89027023\n",
      "################################  1700  ################################\n",
      "Training Loss:  43.8375206\n",
      "Epoch  1703: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  43.62654495\n",
      "################################  1900  ################################\n",
      "Training Loss:  43.62298965\n",
      "Epoch  1904: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  43.59253693\n",
      "\n",
      "Running model (trial=1, mod=58, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  11992052736.0\n",
      "################################  100  ################################\n",
      "Training Loss:  145696.40625\n",
      "################################  200  ################################\n",
      "Training Loss:  51378.3515625\n",
      "################################  300  ################################\n",
      "Training Loss:  21724.22070312\n",
      "################################  400  ################################\n",
      "Training Loss:  9611.37207031\n",
      "################################  500  ################################\n",
      "Training Loss:  4074.25683594\n",
      "################################  600  ################################\n",
      "Training Loss:  1808.58898926\n",
      "################################  700  ################################\n",
      "Training Loss:  1150.95690918\n",
      "################################  800  ################################\n",
      "Training Loss:  875.37182617\n",
      "################################  900  ################################\n",
      "Training Loss:  764.61297607\n",
      "################################  1000  ################################\n",
      "Training Loss:  719.94128418\n",
      "################################  1100  ################################\n",
      "Training Loss:  695.38293457\n",
      "################################  1200  ################################\n",
      "Training Loss:  679.62945557\n",
      "################################  1300  ################################\n",
      "Training Loss:  662.82275391\n",
      "################################  1400  ################################\n",
      "Training Loss:  646.35144043\n",
      "################################  1500  ################################\n",
      "Training Loss:  630.38244629\n",
      "################################  1600  ################################\n",
      "Training Loss:  614.41699219\n",
      "################################  1700  ################################\n",
      "Training Loss:  597.18719482\n",
      "################################  1800  ################################\n",
      "Training Loss:  579.3572998\n",
      "################################  1900  ################################\n",
      "Training Loss:  560.90533447\n",
      "Final training Loss:  541.69897461\n",
      "\n",
      "Running model (trial=1, mod=59, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  479083670536192.0\n",
      "################################  100  ################################\n",
      "Training Loss:  7522973696.0\n",
      "################################  200  ################################\n",
      "Training Loss:  2909348608.0\n",
      "Epoch   206: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  1244551680.0\n",
      "################################  400  ################################\n",
      "Training Loss:  530395616.0\n",
      "################################  500  ################################\n",
      "Training Loss:  251585984.0\n",
      "################################  600  ################################\n",
      "Training Loss:  142602272.0\n",
      "################################  700  ################################\n",
      "Training Loss:  87033200.0\n",
      "################################  800  ################################\n",
      "Training Loss:  60591752.0\n",
      "################################  900  ################################\n",
      "Training Loss:  46139600.0\n",
      "################################  1000  ################################\n",
      "Training Loss:  34074824.0\n",
      "################################  1100  ################################\n",
      "Training Loss:  27837394.0\n",
      "################################  1200  ################################\n",
      "Training Loss:  26076132.0\n",
      "################################  1300  ################################\n",
      "Training Loss:  24336784.0\n",
      "################################  1400  ################################\n",
      "Training Loss:  22609412.0\n",
      "################################  1500  ################################\n",
      "Training Loss:  20328930.0\n",
      "################################  1600  ################################\n",
      "Training Loss:  18199232.0\n",
      "################################  1700  ################################\n",
      "Training Loss:  16110066.0\n",
      "################################  1800  ################################\n",
      "Training Loss:  14387872.0\n",
      "################################  1900  ################################\n",
      "Training Loss:  13631524.0\n",
      "Final training Loss:  12966192.0\n",
      "\n",
      "Running model (trial=2, mod=60, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  41530.203125\n",
      "################################  100  ################################\n",
      "Training Loss:  94.76689911\n",
      "################################  200  ################################\n",
      "Training Loss:  55.17570877\n",
      "################################  300  ################################\n",
      "Training Loss:  55.12059402\n",
      "################################  400  ################################\n",
      "Training Loss:  55.18404007\n",
      "################################  500  ################################\n",
      "Training Loss:  54.24777603\n",
      "################################  600  ################################\n",
      "Training Loss:  54.4086113\n",
      "################################  700  ################################\n",
      "Training Loss:  58.9666748\n",
      "################################  800  ################################\n",
      "Training Loss:  55.07752228\n",
      "################################  900  ################################\n",
      "Training Loss:  55.90337372\n",
      "Epoch   922: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.83909607\n",
      "################################  1100  ################################\n",
      "Training Loss:  58.0014801\n",
      "################################  1200  ################################\n",
      "Training Loss:  53.15769196\n",
      "################################  1300  ################################\n",
      "Training Loss:  57.45404434\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.33642197\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.3956871\n",
      "Epoch  1567: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.00431824\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.18551254\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.82678986\n",
      "Epoch  1819: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.79135895\n",
      "Final training Loss:  54.14883423\n",
      "\n",
      "Running model (trial=2, mod=61, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  16857.64453125\n",
      "################################  100  ################################\n",
      "Training Loss:  55.24811172\n",
      "################################  200  ################################\n",
      "Training Loss:  53.75156403\n",
      "################################  300  ################################\n",
      "Training Loss:  60.89209366\n",
      "################################  400  ################################\n",
      "Training Loss:  54.03609848\n",
      "################################  500  ################################\n",
      "Training Loss:  57.1289444\n",
      "Epoch   575: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  54.8612709\n",
      "################################  700  ################################\n",
      "Training Loss:  55.43207932\n",
      "################################  800  ################################\n",
      "Training Loss:  57.60099792\n",
      "################################  900  ################################\n",
      "Training Loss:  60.3899498\n",
      "Epoch   904: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  56.3218689\n",
      "################################  1100  ################################\n",
      "Training Loss:  55.61413574\n",
      "Epoch  1105: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.10493088\n",
      "################################  1300  ################################\n",
      "Training Loss:  67.31518555\n",
      "################################  1400  ################################\n",
      "Training Loss:  64.25250244\n",
      "################################  1500  ################################\n",
      "Training Loss:  56.53100204\n",
      "Epoch  1600: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  57.89823914\n",
      "################################  1700  ################################\n",
      "Training Loss:  55.28631592\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.88592148\n",
      "Epoch  1898: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.5692482\n",
      "Final training Loss:  52.11112213\n",
      "\n",
      "Running model (trial=2, mod=62, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2769.36425781\n",
      "################################  100  ################################\n",
      "Training Loss:  118.70668793\n",
      "################################  200  ################################\n",
      "Training Loss:  116.87538147\n",
      "################################  300  ################################\n",
      "Training Loss:  116.85820007\n",
      "################################  400  ################################\n",
      "Training Loss:  118.00521088\n",
      "################################  500  ################################\n",
      "Training Loss:  115.91437531\n",
      "################################  600  ################################\n",
      "Training Loss:  124.23643494\n",
      "Epoch   649: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  120.24002838\n",
      "################################  800  ################################\n",
      "Training Loss:  119.83673859\n",
      "Epoch   850: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  121.02742004\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.38516998\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.21492767\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.63534546\n",
      "Epoch  1245: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  117.63983154\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.7797699\n",
      "################################  1500  ################################\n",
      "Training Loss:  124.08722687\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.13249207\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.84693909\n",
      "Epoch  1800: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.06173706\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.29206848\n",
      "Final training Loss:  115.75527191\n",
      "\n",
      "Running model (trial=2, mod=63, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  161424.5625\n",
      "################################  100  ################################\n",
      "Training Loss:  139.5684967\n",
      "################################  200  ################################\n",
      "Training Loss:  123.28307343\n",
      "Epoch   267: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  123.25450134\n",
      "################################  400  ################################\n",
      "Training Loss:  123.22781372\n",
      "Epoch   468: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  123.20055389\n",
      "################################  600  ################################\n",
      "Training Loss:  123.17523193\n",
      "Epoch   669: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  123.15125275\n",
      "################################  800  ################################\n",
      "Training Loss:  123.13130951\n",
      "Epoch   870: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  123.11575317\n",
      "################################  1000  ################################\n",
      "Training Loss:  123.10218811\n",
      "Epoch  1071: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  123.06931305\n",
      "################################  1200  ################################\n",
      "Training Loss:  123.05001831\n",
      "Epoch  1272: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  123.0368042\n",
      "################################  1400  ################################\n",
      "Training Loss:  123.02832031\n",
      "Epoch  1473: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  123.02262878\n",
      "################################  1600  ################################\n",
      "Training Loss:  123.01911926\n",
      "Epoch  1674: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  123.0165863\n",
      "################################  1800  ################################\n",
      "Training Loss:  123.01491547\n",
      "Epoch  1875: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  123.01345825\n",
      "Final training Loss:  123.012146\n",
      "\n",
      "Running model (trial=2, mod=64, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  18975.30859375\n",
      "################################  100  ################################\n",
      "Training Loss:  121.75854492\n",
      "################################  200  ################################\n",
      "Training Loss:  119.78967285\n",
      "Epoch   252: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  116.70343018\n",
      "################################  400  ################################\n",
      "Training Loss:  101.43531799\n",
      "################################  500  ################################\n",
      "Training Loss:  59.29173279\n",
      "################################  600  ################################\n",
      "Training Loss:  57.559021\n",
      "################################  700  ################################\n",
      "Training Loss:  57.23491669\n",
      "################################  800  ################################\n",
      "Training Loss:  54.6919899\n",
      "################################  900  ################################\n",
      "Training Loss:  61.09669113\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.70267868\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.43887329\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.34371948\n",
      "################################  1300  ################################\n",
      "Training Loss:  55.47540283\n",
      "################################  1400  ################################\n",
      "Training Loss:  58.51789093\n",
      "Epoch  1440: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  57.39143753\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.62141418\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.74582672\n",
      "################################  1800  ################################\n",
      "Training Loss:  58.76053238\n",
      "Epoch  1829: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  56.65511703\n",
      "Final training Loss:  51.11089325\n",
      "\n",
      "Running model (trial=2, mod=65, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  415969.46875\n",
      "################################  100  ################################\n",
      "Training Loss:  118.66993713\n",
      "################################  200  ################################\n",
      "Training Loss:  117.44659424\n",
      "Epoch   253: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  117.30541229\n",
      "################################  400  ################################\n",
      "Training Loss:  119.54954529\n",
      "Epoch   454: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  120.90761566\n",
      "################################  600  ################################\n",
      "Training Loss:  93.1084671\n",
      "################################  700  ################################\n",
      "Training Loss:  66.69722748\n",
      "################################  800  ################################\n",
      "Training Loss:  63.66533661\n",
      "################################  900  ################################\n",
      "Training Loss:  63.07919693\n",
      "################################  1000  ################################\n",
      "Training Loss:  67.0644989\n",
      "################################  1100  ################################\n",
      "Training Loss:  61.10661697\n",
      "################################  1200  ################################\n",
      "Training Loss:  67.3780365\n",
      "Epoch  1289: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  59.69977951\n",
      "################################  1400  ################################\n",
      "Training Loss:  59.88496017\n",
      "################################  1500  ################################\n",
      "Training Loss:  64.36250305\n",
      "################################  1600  ################################\n",
      "Training Loss:  57.55878448\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.62075043\n",
      "################################  1800  ################################\n",
      "Training Loss:  59.98174667\n",
      "################################  1900  ################################\n",
      "Training Loss:  65.70674133\n",
      "Final training Loss:  56.63938904\n",
      "\n",
      "Running model (trial=2, mod=66, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  169686.6875\n",
      "################################  100  ################################\n",
      "Training Loss:  147.91508484\n",
      "################################  200  ################################\n",
      "Training Loss:  118.6595459\n",
      "Epoch   280: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  118.74143982\n",
      "################################  400  ################################\n",
      "Training Loss:  118.2855835\n",
      "Epoch   481: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  118.54985809\n",
      "################################  600  ################################\n",
      "Training Loss:  119.32279968\n",
      "################################  700  ################################\n",
      "Training Loss:  117.91815186\n",
      "################################  800  ################################\n",
      "Training Loss:  117.51525116\n",
      "################################  900  ################################\n",
      "Training Loss:  116.76377106\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.73092651\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.1975708\n",
      "Epoch  1132: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  120.50643921\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.30213165\n",
      "################################  1400  ################################\n",
      "Training Loss:  123.72087097\n",
      "################################  1500  ################################\n",
      "Training Loss:  137.12145996\n",
      "################################  1600  ################################\n",
      "Training Loss:  147.23292542\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.90472412\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.46137238\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.37072754\n",
      "Final training Loss:  125.05826569\n",
      "\n",
      "Running model (trial=2, mod=67, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  392909.125\n",
      "################################  100  ################################\n",
      "Training Loss:  148.69326782\n",
      "################################  200  ################################\n",
      "Training Loss:  63.88000488\n",
      "################################  300  ################################\n",
      "Training Loss:  53.42652512\n",
      "################################  400  ################################\n",
      "Training Loss:  49.63136292\n",
      "Epoch   442: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  53.52634048\n",
      "################################  600  ################################\n",
      "Training Loss:  49.04686356\n",
      "Epoch   643: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  52.28082657\n",
      "################################  800  ################################\n",
      "Training Loss:  57.44284439\n",
      "Epoch   844: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  47.77031708\n",
      "################################  1000  ################################\n",
      "Training Loss:  50.73618698\n",
      "################################  1100  ################################\n",
      "Training Loss:  48.58963013\n",
      "################################  1200  ################################\n",
      "Training Loss:  64.91156769\n",
      "################################  1300  ################################\n",
      "Training Loss:  48.4867897\n",
      "Epoch  1365: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.40932083\n",
      "################################  1500  ################################\n",
      "Training Loss:  58.89221191\n",
      "################################  1600  ################################\n",
      "Training Loss:  176.67144775\n",
      "Epoch  1618: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.67399216\n",
      "################################  1800  ################################\n",
      "Training Loss:  68.68551636\n",
      "Epoch  1819: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.25525284\n",
      "Final training Loss:  55.86115265\n",
      "\n",
      "Running model (trial=2, mod=68, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  387466.15625\n",
      "################################  100  ################################\n",
      "Training Loss:  121.56954956\n",
      "################################  200  ################################\n",
      "Training Loss:  121.19166565\n",
      "################################  300  ################################\n",
      "Training Loss:  111.44373322\n",
      "################################  400  ################################\n",
      "Training Loss:  92.75222015\n",
      "################################  500  ################################\n",
      "Training Loss:  69.60951996\n",
      "################################  600  ################################\n",
      "Training Loss:  54.84916306\n",
      "Epoch   679: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  55.39067841\n",
      "################################  800  ################################\n",
      "Training Loss:  54.65994644\n",
      "Epoch   880: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  52.2433548\n",
      "################################  1000  ################################\n",
      "Training Loss:  52.30745316\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.65893555\n",
      "Epoch  1134: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  58.64729691\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.69739151\n",
      "Epoch  1335: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.58004761\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.29660797\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.79319763\n",
      "Epoch  1678: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.46853256\n",
      "################################  1800  ################################\n",
      "Training Loss:  54.14512634\n",
      "Epoch  1879: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.36922836\n",
      "Final training Loss:  51.57027435\n",
      "\n",
      "Running model (trial=2, mod=69, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  603323.5\n",
      "################################  100  ################################\n",
      "Training Loss:  118.71316528\n",
      "################################  200  ################################\n",
      "Training Loss:  108.87268066\n",
      "################################  300  ################################\n",
      "Training Loss:  91.63755035\n",
      "################################  400  ################################\n",
      "Training Loss:  57.14502716\n",
      "################################  500  ################################\n",
      "Training Loss:  61.0345459\n",
      "################################  600  ################################\n",
      "Training Loss:  52.87345505\n",
      "Epoch   625: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  54.39284134\n",
      "################################  800  ################################\n",
      "Training Loss:  55.22715378\n",
      "Epoch   826: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  55.4258728\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.43941879\n",
      "Epoch  1027: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  57.77135468\n",
      "################################  1200  ################################\n",
      "Training Loss:  53.54739761\n",
      "Epoch  1228: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  62.49621964\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.37923813\n",
      "Epoch  1429: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.0912323\n",
      "################################  1600  ################################\n",
      "Training Loss:  70.67160034\n",
      "Epoch  1630: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.11707687\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.22963715\n",
      "Epoch  1831: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.76685333\n",
      "Final training Loss:  52.14253998\n",
      "\n",
      "Running model (trial=2, mod=70, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  254821.9375\n",
      "################################  100  ################################\n",
      "Training Loss:  97.85284424\n",
      "################################  200  ################################\n",
      "Training Loss:  74.32765961\n",
      "################################  300  ################################\n",
      "Training Loss:  56.75290298\n",
      "################################  400  ################################\n",
      "Training Loss:  54.51266098\n",
      "################################  500  ################################\n",
      "Training Loss:  54.25202942\n",
      "################################  600  ################################\n",
      "Training Loss:  54.12364197\n",
      "################################  700  ################################\n",
      "Training Loss:  54.46191788\n",
      "################################  800  ################################\n",
      "Training Loss:  58.28639221\n",
      "Epoch   832: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  54.69101334\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.45365906\n",
      "Epoch  1033: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.0085144\n",
      "################################  1200  ################################\n",
      "Training Loss:  56.69410324\n",
      "################################  1300  ################################\n",
      "Training Loss:  57.32625961\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.73884583\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.64694214\n",
      "Epoch  1584: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.39543533\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.93059921\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.81206512\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.61005402\n",
      "Epoch  1944: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  58.02655411\n",
      "\n",
      "Running model (trial=2, mod=71, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2931362.5\n",
      "################################  100  ################################\n",
      "Training Loss:  82.28278351\n",
      "################################  200  ################################\n",
      "Training Loss:  61.23705292\n",
      "################################  300  ################################\n",
      "Training Loss:  57.63728333\n",
      "################################  400  ################################\n",
      "Training Loss:  56.50409317\n",
      "################################  500  ################################\n",
      "Training Loss:  56.83469772\n",
      "################################  600  ################################\n",
      "Training Loss:  56.41832733\n",
      "################################  700  ################################\n",
      "Training Loss:  55.30360413\n",
      "################################  800  ################################\n",
      "Training Loss:  55.07042313\n",
      "################################  900  ################################\n",
      "Training Loss:  54.82317734\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.36635208\n",
      "################################  1100  ################################\n",
      "Training Loss:  55.38615036\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.93354034\n",
      "################################  1300  ################################\n",
      "Training Loss:  55.23628235\n",
      "################################  1400  ################################\n",
      "Training Loss:  56.76736832\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.62361145\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.13662338\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.91647339\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.57209015\n",
      "################################  1900  ################################\n",
      "Training Loss:  68.82659149\n",
      "Epoch  1928: reducing learning rate of group 0 to 4.0000e-02.\n",
      "Final training Loss:  55.0769043\n",
      "\n",
      "Running model (trial=2, mod=72, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1712680.5\n",
      "################################  100  ################################\n",
      "Training Loss:  114.42229462\n",
      "################################  200  ################################\n",
      "Training Loss:  119.62007141\n",
      "Epoch   251: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  120.31028748\n",
      "################################  400  ################################\n",
      "Training Loss:  118.7885437\n",
      "Epoch   452: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  114.86781311\n",
      "################################  600  ################################\n",
      "Training Loss:  119.33648682\n",
      "################################  700  ################################\n",
      "Training Loss:  117.74733734\n",
      "Epoch   791: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  118.37536621\n",
      "################################  900  ################################\n",
      "Training Loss:  138.30895996\n",
      "Epoch   992: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  113.75304413\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.39361572\n",
      "Epoch  1193: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.93914032\n",
      "################################  1300  ################################\n",
      "Training Loss:  121.26037598\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.04496765\n",
      "################################  1500  ################################\n",
      "Training Loss:  118.61013794\n",
      "################################  1600  ################################\n",
      "Training Loss:  125.04697418\n",
      "Epoch  1691: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  113.41893005\n",
      "################################  1800  ################################\n",
      "Training Loss:  120.06530762\n",
      "Epoch  1892: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.36290741\n",
      "Final training Loss:  120.1140976\n",
      "\n",
      "Running model (trial=2, mod=73, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  643811.875\n",
      "################################  100  ################################\n",
      "Training Loss:  471.6786499\n",
      "################################  200  ################################\n",
      "Training Loss:  66.43585968\n",
      "################################  300  ################################\n",
      "Training Loss:  61.79457474\n",
      "################################  400  ################################\n",
      "Training Loss:  57.79461288\n",
      "################################  500  ################################\n",
      "Training Loss:  64.18951416\n",
      "################################  600  ################################\n",
      "Training Loss:  67.66903687\n",
      "Epoch   677: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  82.14614868\n",
      "################################  800  ################################\n",
      "Training Loss:  52.98498535\n",
      "Epoch   878: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  54.00325775\n",
      "################################  1000  ################################\n",
      "Training Loss:  60.84915924\n",
      "Epoch  1079: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.8459816\n",
      "################################  1200  ################################\n",
      "Training Loss:  60.46658325\n",
      "Epoch  1280: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  79.40776062\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.07399368\n",
      "Epoch  1481: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.35902786\n",
      "################################  1600  ################################\n",
      "Training Loss:  59.40979385\n",
      "Epoch  1682: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.53112793\n",
      "################################  1800  ################################\n",
      "Training Loss:  54.03847504\n",
      "Epoch  1883: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.40008545\n",
      "Final training Loss:  68.52176666\n",
      "\n",
      "Running model (trial=2, mod=74, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  179458.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  3224.86328125\n",
      "################################  200  ################################\n",
      "Training Loss:  150.12615967\n",
      "################################  300  ################################\n",
      "Training Loss:  102.00494385\n",
      "################################  400  ################################\n",
      "Training Loss:  81.29701233\n",
      "################################  500  ################################\n",
      "Training Loss:  62.93870544\n",
      "################################  600  ################################\n",
      "Training Loss:  80.03543854\n",
      "################################  700  ################################\n",
      "Training Loss:  68.31066132\n",
      "Epoch   734: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  53.78292465\n",
      "################################  900  ################################\n",
      "Training Loss:  55.28601837\n",
      "Epoch   935: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  68.19821167\n",
      "################################  1100  ################################\n",
      "Training Loss:  73.40274811\n",
      "Epoch  1136: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  78.35102081\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.55890656\n",
      "Epoch  1337: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.83781433\n",
      "################################  1500  ################################\n",
      "Training Loss:  66.78151703\n",
      "Epoch  1538: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.22356415\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.43931961\n",
      "Epoch  1739: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  75.26853943\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.12932205\n",
      "Epoch  1940: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  59.66594315\n",
      "\n",
      "Running model (trial=2, mod=75, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4402.79345703\n",
      "################################  100  ################################\n",
      "Training Loss:  79.84411621\n",
      "################################  200  ################################\n",
      "Training Loss:  66.29891968\n",
      "################################  300  ################################\n",
      "Training Loss:  64.59332275\n",
      "################################  400  ################################\n",
      "Training Loss:  66.4596405\n",
      "################################  500  ################################\n",
      "Training Loss:  79.35005951\n",
      "################################  600  ################################\n",
      "Training Loss:  56.63030243\n",
      "################################  700  ################################\n",
      "Training Loss:  53.28462219\n",
      "################################  800  ################################\n",
      "Training Loss:  53.42655563\n",
      "Epoch   890: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  66.31187439\n",
      "################################  1000  ################################\n",
      "Training Loss:  52.82661819\n",
      "################################  1100  ################################\n",
      "Training Loss:  57.36695862\n",
      "Epoch  1172: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.37258911\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.5108757\n",
      "Epoch  1373: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.25853348\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.85707474\n",
      "Epoch  1574: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.44378662\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.06153488\n",
      "Epoch  1788: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.79377365\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.17016602\n",
      "Epoch  1989: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  51.80944824\n",
      "\n",
      "Running model (trial=2, mod=76, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6323.18164062\n",
      "################################  100  ################################\n",
      "Training Loss:  85.59280396\n",
      "################################  200  ################################\n",
      "Training Loss:  53.30743027\n",
      "################################  300  ################################\n",
      "Training Loss:  43.52972794\n",
      "################################  400  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=77, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1001173.5625\n",
      "################################  100  ################################\n",
      "Training Loss:  74.63064575\n",
      "################################  200  ################################\n",
      "Training Loss:  70.3052063\n",
      "################################  300  ################################\n",
      "Training Loss:  63.26224518\n",
      "################################  400  ################################\n",
      "Training Loss:  60.94165802\n",
      "################################  500  ################################\n",
      "Training Loss:  57.70209503\n",
      "################################  600  ################################\n",
      "Training Loss:  59.07475281\n",
      "################################  700  ################################\n",
      "Training Loss:  55.61523819\n",
      "################################  800  ################################\n",
      "Training Loss:  53.02848053\n",
      "################################  900  ################################\n",
      "Training Loss:  51.29838943\n",
      "################################  1000  ################################\n",
      "Training Loss:  50.78718567\n",
      "################################  1100  ################################\n",
      "Training Loss:  50.01023865\n",
      "################################  1200  ################################\n",
      "Training Loss:  49.32262421\n",
      "################################  1300  ################################\n",
      "Training Loss:  49.24686813\n",
      "################################  1400  ################################\n",
      "Training Loss:  47.57725525\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.8126297\n",
      "################################  1600  ################################\n",
      "Training Loss:  46.09181976\n",
      "################################  1700  ################################\n",
      "Training Loss:  47.22089005\n",
      "################################  1800  ################################\n",
      "Training Loss:  46.23042297\n",
      "################################  1900  ################################\n",
      "Training Loss:  47.26639938\n",
      "Final training Loss:  67.0072937\n",
      "\n",
      "Running model (trial=2, mod=78, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  465714.25\n",
      "################################  100  ################################\n",
      "Training Loss:  129.84968567\n",
      "################################  200  ################################\n",
      "Training Loss:  133.92280579\n",
      "################################  300  ################################\n",
      "Training Loss:  102.20021057\n",
      "################################  400  ################################\n",
      "Training Loss:  60.73688889\n",
      "################################  500  ################################\n",
      "Training Loss:  60.65853119\n",
      "################################  600  ################################\n",
      "Training Loss:  61.6726532\n",
      "################################  700  ################################\n",
      "Training Loss:  89.62163544\n",
      "################################  800  ################################\n",
      "Training Loss:  41.29758835\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=79, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6163.51269531\n",
      "################################  100  ################################\n",
      "Training Loss:  119.04129791\n",
      "################################  200  ################################\n",
      "Training Loss:  117.07447052\n",
      "################################  300  ################################\n",
      "Training Loss:  117.8130188\n",
      "################################  400  ################################\n",
      "Training Loss:  118.86593628\n",
      "Epoch   408: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  118.61711884\n",
      "################################  600  ################################\n",
      "Training Loss:  118.8108139\n",
      "Epoch   609: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  118.73108673\n",
      "################################  800  ################################\n",
      "Training Loss:  120.64512634\n",
      "Epoch   810: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  120.62470245\n",
      "################################  1000  ################################\n",
      "Training Loss:  121.59240723\n",
      "Epoch  1011: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.10404968\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.66177368\n",
      "################################  1300  ################################\n",
      "Training Loss:  117.28486633\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.25473022\n",
      "Epoch  1419: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  115.77481079\n",
      "################################  1600  ################################\n",
      "Training Loss:  127.79066467\n",
      "Epoch  1620: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.90090179\n",
      "################################  1800  ################################\n",
      "Training Loss:  520.68487549\n",
      "################################  1900  ################################\n",
      "Training Loss:  110.87931824\n",
      "Final training Loss:  82.99656677\n",
      "\n",
      "Running model (trial=2, mod=80, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  25937.56640625\n",
      "################################  100  ################################\n",
      "Training Loss:  122.98292542\n",
      "################################  200  ################################\n",
      "Training Loss:  51.3657074\n",
      "################################  300  ################################\n",
      "Training Loss:  48.00730515\n",
      "################################  400  ################################\n",
      "Training Loss:  46.33686066\n",
      "################################  500  ################################\n",
      "Training Loss:  46.84210968\n",
      "################################  600  ################################\n",
      "Training Loss:  44.07103729\n",
      "################################  700  ################################\n",
      "Training Loss:  45.79531479\n",
      "################################  800  ################################\n",
      "Training Loss:  43.94234085\n",
      "################################  900  ################################\n",
      "Training Loss:  50.05958939\n",
      "################################  1000  ################################\n",
      "Training Loss:  45.75701904\n",
      "################################  1100  ################################\n",
      "Training Loss:  38.1724968\n",
      "################################  1200  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=81, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3512.20507812\n",
      "################################  100  ################################\n",
      "Training Loss:  149.55029297\n",
      "################################  200  ################################\n",
      "Training Loss:  124.21047211\n",
      "Epoch   223: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  104.15068817\n",
      "################################  400  ################################\n",
      "Training Loss:  84.02510834\n",
      "Epoch   424: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  63.47602463\n",
      "################################  600  ################################\n",
      "Training Loss:  54.98949814\n",
      "################################  700  ################################\n",
      "Training Loss:  53.52792358\n",
      "################################  800  ################################\n",
      "Training Loss:  50.15607452\n",
      "################################  900  ################################\n",
      "Training Loss:  48.18305969\n",
      "################################  1000  ################################\n",
      "Training Loss:  47.37488937\n",
      "################################  1100  ################################\n",
      "Training Loss:  46.75240326\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.37487411\n",
      "################################  1300  ################################\n",
      "Training Loss:  45.61349106\n",
      "################################  1400  ################################\n",
      "Training Loss:  44.84469986\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.26102829\n",
      "################################  1600  ################################\n",
      "Training Loss:  44.73705673\n",
      "################################  1700  ################################\n",
      "Training Loss:  44.27875519\n",
      "################################  1800  ################################\n",
      "Training Loss:  44.29489136\n",
      "################################  1900  ################################\n",
      "Training Loss:  45.6341095\n",
      "Final training Loss:  43.77713776\n",
      "\n",
      "Running model (trial=2, mod=82, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  364337.71875\n",
      "################################  100  ################################\n",
      "Training Loss:  7896.29589844\n",
      "################################  200  ################################\n",
      "Training Loss:  112.10861969\n",
      "################################  300  ################################\n",
      "Training Loss:  108.4691925\n",
      "################################  400  ################################\n",
      "Training Loss:  105.62950897\n",
      "################################  500  ################################\n",
      "Training Loss:  104.25183868\n",
      "################################  600  ################################\n",
      "Training Loss:  102.43994141\n",
      "################################  700  ################################\n",
      "Training Loss:  101.48368073\n",
      "################################  800  ################################\n",
      "Training Loss:  99.72180176\n",
      "################################  900  ################################\n",
      "Training Loss:  98.22648621\n",
      "################################  1000  ################################\n",
      "Training Loss:  96.45597839\n",
      "################################  1100  ################################\n",
      "Training Loss:  98.84051514\n",
      "################################  1200  ################################\n",
      "Training Loss:  95.26903534\n",
      "################################  1300  ################################\n",
      "Training Loss:  88.6787796\n",
      "################################  1400  ################################\n",
      "Training Loss:  84.62844086\n",
      "################################  1500  ################################\n",
      "Training Loss:  76.7646637\n",
      "################################  1600  ################################\n",
      "Training Loss:  70.16246033\n",
      "################################  1700  ################################\n",
      "Training Loss:  66.06846619\n",
      "################################  1800  ################################\n",
      "Training Loss:  65.21842194\n",
      "################################  1900  ################################\n",
      "Training Loss:  56.86253357\n",
      "Final training Loss:  56.26770782\n",
      "\n",
      "Running model (trial=2, mod=83, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5631.89501953\n",
      "################################  100  ################################\n",
      "Training Loss:  87.54336548\n",
      "################################  200  ################################\n",
      "Training Loss:  56.11189651\n",
      "################################  300  ################################\n",
      "Training Loss:  173.76766968\n",
      "################################  400  ################################\n",
      "Training Loss:  49.75165558\n",
      "################################  500  ################################\n",
      "Training Loss:  46.04067993\n",
      "################################  600  ################################\n",
      "Training Loss:  63.40110016\n",
      "################################  700  ################################\n",
      "Training Loss:  45.00878906\n",
      "################################  800  ################################\n",
      "Training Loss:  43.30007553\n",
      "################################  900  ################################\n",
      "Training Loss:  46.2435112\n",
      "Epoch   988: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  40.59296799\n",
      "################################  1100  ################################\n",
      "Training Loss:  48.49187469\n",
      "################################  1200  ################################\n",
      "Training Loss:  63.3454361\n",
      "################################  1300  ################################\n",
      "Training Loss:  75.53612518\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.98077011\n",
      "Epoch  1447: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  41.07995605\n",
      "################################  1600  ################################\n",
      "Training Loss:  38.94915009\n",
      "################################  1700  ################################\n",
      "Training Loss:  37.52337646\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.20595932\n",
      "Epoch  1860: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  39.62733459\n",
      "Final training Loss:  37.40985489\n",
      "\n",
      "Running model (trial=2, mod=84, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2512.72680664\n",
      "################################  100  ################################\n",
      "Training Loss:  53.02719879\n",
      "################################  200  ################################\n",
      "Training Loss:  51.43364716\n",
      "################################  300  ################################\n",
      "Training Loss:  53.75172043\n",
      "################################  400  ################################\n",
      "Training Loss:  45.21327209\n",
      "Epoch   464: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  42.58712387\n",
      "################################  600  ################################\n",
      "Training Loss:  51.18514633\n",
      "################################  700  ################################\n",
      "Training Loss:  42.18082809\n",
      "################################  800  ################################\n",
      "Training Loss:  42.37615585\n",
      "################################  900  ################################\n",
      "Training Loss:  52.44615936\n",
      "################################  1000  ################################\n",
      "Training Loss:  44.42595673\n",
      "################################  1100  ################################\n",
      "Training Loss:  41.25535202\n",
      "################################  1200  ################################\n",
      "Training Loss:  39.86209488\n",
      "################################  1300  ################################\n",
      "Training Loss:  39.32047653\n",
      "Epoch  1310: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  39.67858887\n",
      "################################  1500  ################################\n",
      "Training Loss:  41.68102264\n",
      "Epoch  1532: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  45.7808609\n",
      "################################  1700  ################################\n",
      "Training Loss:  39.97148895\n",
      "################################  1800  ################################\n",
      "Training Loss:  37.54613113\n",
      "################################  1900  ################################\n",
      "Training Loss:  38.0018425\n",
      "Final training Loss:  39.50997162\n",
      "\n",
      "Running model (trial=2, mod=85, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6382.84472656\n",
      "################################  100  ################################\n",
      "Training Loss:  54.71312332\n",
      "################################  200  ################################\n",
      "Training Loss:  45.49480438\n",
      "################################  300  ################################\n",
      "Training Loss:  74.07609558\n",
      "################################  400  ################################\n",
      "Training Loss:  40.45734406\n",
      "################################  500  ################################\n",
      "Training Loss:  39.79410934\n",
      "################################  600  ################################\n",
      "Training Loss:  39.35680389\n",
      "################################  700  ################################\n",
      "Training Loss:  43.88010025\n",
      "################################  800  ################################\n",
      "Epoch   801: reducing learning rate of group 0 to 4.0000e-02.\n",
      "Training Loss:  47.96600342\n",
      "################################  900  ################################\n",
      "Training Loss:  48.13141251\n",
      "################################  1000  ################################\n",
      "Training Loss:  39.77183533\n",
      "Epoch  1079: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  62.97166443\n",
      "################################  1200  ################################\n",
      "Training Loss:  51.11543274\n",
      "Epoch  1280: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  40.05845642\n",
      "################################  1400  ################################\n",
      "Training Loss:  41.35676575\n",
      "Epoch  1481: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  45.37434769\n",
      "################################  1600  ################################\n",
      "Training Loss:  40.15194321\n",
      "Epoch  1682: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  39.09579849\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.88501358\n",
      "Epoch  1883: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  38.10295105\n",
      "Final training Loss:  44.67518616\n",
      "\n",
      "Running model (trial=2, mod=86, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  102608.265625\n",
      "################################  100  ################################\n",
      "Training Loss:  93.6416626\n",
      "################################  200  ################################\n",
      "Training Loss:  92.49484253\n",
      "################################  300  ################################\n",
      "Training Loss:  91.12179565\n",
      "################################  400  ################################\n",
      "Training Loss:  96.31062317\n",
      "################################  500  ################################\n",
      "Training Loss:  88.24679565\n",
      "################################  600  ################################\n",
      "Training Loss:  84.06174469\n",
      "################################  700  ################################\n",
      "Training Loss:  88.86408997\n",
      "################################  800  ################################\n",
      "Training Loss:  74.49008179\n",
      "################################  900  ################################\n",
      "Training Loss:  74.22701263\n",
      "################################  1000  ################################\n",
      "Training Loss:  69.19985199\n",
      "################################  1100  ################################\n",
      "Training Loss:  67.85132599\n",
      "################################  1200  ################################\n",
      "Training Loss:  69.71623993\n",
      "################################  1300  ################################\n",
      "Training Loss:  63.7412262\n",
      "Epoch  1357: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  66.17933655\n",
      "################################  1500  ################################\n",
      "Training Loss:  74.82202148\n",
      "################################  1600  ################################\n",
      "Training Loss:  65.69630432\n",
      "################################  1700  ################################\n",
      "Training Loss:  70.9240799\n",
      "################################  1800  ################################\n",
      "Training Loss:  61.53306198\n",
      "################################  1900  ################################\n",
      "Training Loss:  59.40132904\n",
      "Epoch  1931: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  59.97425079\n",
      "\n",
      "Running model (trial=2, mod=87, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2833818.75\n",
      "################################  100  ################################\n",
      "Training Loss:  423.99356079\n",
      "################################  200  ################################\n",
      "Training Loss:  122.22425842\n",
      "################################  300  ################################\n",
      "Training Loss:  122.09583282\n",
      "Epoch   344: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  122.04679871\n",
      "################################  500  ################################\n",
      "Training Loss:  122.07655334\n",
      "Epoch   545: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  122.03370667\n",
      "################################  700  ################################\n",
      "Training Loss:  122.01239777\n",
      "Epoch   746: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  121.98609924\n",
      "################################  900  ################################\n",
      "Training Loss:  121.96225739\n",
      "Epoch   947: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  121.94332886\n",
      "################################  1100  ################################\n",
      "Training Loss:  121.93585968\n",
      "Epoch  1148: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  121.93195343\n",
      "################################  1300  ################################\n",
      "Training Loss:  121.92980194\n",
      "Epoch  1349: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  121.87823486\n",
      "################################  1500  ################################\n",
      "Training Loss:  121.86148834\n",
      "Epoch  1550: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  121.90336609\n",
      "################################  1700  ################################\n",
      "Training Loss:  121.89456177\n",
      "Epoch  1751: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  121.88963318\n",
      "################################  1900  ################################\n",
      "Training Loss:  121.88768005\n",
      "Epoch  1952: reducing learning rate of group 0 to 6.7109e-03.\n",
      "Final training Loss:  121.88564301\n",
      "\n",
      "Running model (trial=2, mod=88, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1091597824.0\n",
      "################################  100  ################################\n",
      "Training Loss:  242.16516113\n",
      "################################  200  ################################\n",
      "Training Loss:  161.52328491\n",
      "################################  300  ################################\n",
      "Training Loss:  112.75424194\n",
      "################################  400  ################################\n",
      "Training Loss:  96.88787842\n",
      "################################  500  ################################\n",
      "Training Loss:  91.09973145\n",
      "################################  600  ################################\n",
      "Training Loss:  90.41419983\n",
      "################################  700  ################################\n",
      "Training Loss:  90.46875\n",
      "Epoch   764: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  90.50151062\n",
      "################################  900  ################################\n",
      "Training Loss:  90.35055542\n",
      "Epoch   965: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  90.33020782\n",
      "################################  1100  ################################\n",
      "Training Loss:  90.318367\n",
      "Epoch  1166: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  90.32269287\n",
      "################################  1300  ################################\n",
      "Training Loss:  90.28953552\n",
      "################################  1400  ################################\n",
      "Training Loss:  90.26565552\n",
      "################################  1500  ################################\n",
      "Training Loss:  90.2350769\n",
      "################################  1600  ################################\n",
      "Training Loss:  90.30175781\n",
      "################################  1700  ################################\n",
      "Training Loss:  90.29888153\n",
      "Epoch  1759: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  90.3431015\n",
      "################################  1900  ################################\n",
      "Training Loss:  90.32164001\n",
      "Epoch  1960: reducing learning rate of group 0 to 1.6384e-02.\n",
      "Final training Loss:  90.31040955\n",
      "\n",
      "Running model (trial=2, mod=89, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.9667896493932544e+16\n",
      "################################  100  ################################\n",
      "Training Loss:  7657691648.0\n",
      "################################  200  ################################\n",
      "Training Loss:  2565540608.0\n",
      "################################  300  ################################\n",
      "Training Loss:  1130574592.0\n",
      "################################  400  ################################\n",
      "Training Loss:  625681536.0\n",
      "################################  500  ################################\n",
      "Training Loss:  377977984.0\n",
      "################################  600  ################################\n",
      "Training Loss:  244755904.0\n",
      "################################  700  ################################\n",
      "Training Loss:  188279200.0\n",
      "################################  800  ################################\n",
      "Training Loss:  156331808.0\n",
      "################################  900  ################################\n",
      "Training Loss:  141631136.0\n",
      "################################  1000  ################################\n",
      "Training Loss:  134233952.0\n",
      "################################  1100  ################################\n",
      "Training Loss:  123126176.0\n",
      "################################  1200  ################################\n",
      "Training Loss:  112217456.0\n",
      "################################  1300  ################################\n",
      "Training Loss:  100693904.0\n",
      "################################  1400  ################################\n",
      "Training Loss:  85882160.0\n",
      "################################  1500  ################################\n",
      "Training Loss:  75835120.0\n",
      "################################  1600  ################################\n",
      "Training Loss:  68388552.0\n",
      "################################  1700  ################################\n",
      "Training Loss:  60277700.0\n",
      "################################  1800  ################################\n",
      "Training Loss:  52475664.0\n",
      "################################  1900  ################################\n",
      "Training Loss:  44970712.0\n",
      "Final training Loss:  40411108.0\n",
      "\n",
      "Running model (trial=3, mod=90, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  38429.41796875\n",
      "################################  100  ################################\n",
      "Training Loss:  118.53046417\n",
      "################################  200  ################################\n",
      "Training Loss:  127.2588501\n",
      "################################  300  ################################\n",
      "Training Loss:  103.7850647\n",
      "################################  400  ################################\n",
      "Training Loss:  90.12451172\n",
      "################################  500  ################################\n",
      "Training Loss:  67.28256226\n",
      "################################  600  ################################\n",
      "Training Loss:  71.83302307\n",
      "################################  700  ################################\n",
      "Training Loss:  54.40303421\n",
      "Epoch   798: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  63.58802795\n",
      "################################  900  ################################\n",
      "Training Loss:  65.74524689\n",
      "Epoch   999: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.43031693\n",
      "################################  1100  ################################\n",
      "Training Loss:  57.8288765\n",
      "Epoch  1200: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  57.22297668\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.02417755\n",
      "################################  1400  ################################\n",
      "Epoch  1401: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Training Loss:  54.90249252\n",
      "################################  1500  ################################\n",
      "Training Loss:  55.06201172\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.47143173\n",
      "Epoch  1602: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.28588104\n",
      "################################  1800  ################################\n",
      "Training Loss:  54.79010773\n",
      "Epoch  1803: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.51818466\n",
      "Final training Loss:  54.19100189\n",
      "\n",
      "Running model (trial=3, mod=91, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  779868.5625\n",
      "################################  100  ################################\n",
      "Training Loss:  112.82066345\n",
      "################################  200  ################################\n",
      "Training Loss:  104.50320435\n",
      "################################  300  ################################\n",
      "Training Loss:  97.23112488\n",
      "################################  400  ################################\n",
      "Training Loss:  87.31109619\n",
      "################################  500  ################################\n",
      "Training Loss:  65.97496033\n",
      "################################  600  ################################\n",
      "Training Loss:  55.65919113\n",
      "################################  700  ################################\n",
      "Training Loss:  54.12917328\n",
      "################################  800  ################################\n",
      "Training Loss:  56.74967194\n",
      "################################  900  ################################\n",
      "Training Loss:  53.65002823\n",
      "Epoch   930: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  56.68529129\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.8102684\n",
      "Epoch  1131: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  55.03483582\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.60547638\n",
      "Epoch  1332: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.03477478\n",
      "################################  1500  ################################\n",
      "Training Loss:  58.82914734\n",
      "Epoch  1533: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.08507156\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.4182663\n",
      "Epoch  1734: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.90433502\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.78417587\n",
      "Epoch  1935: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  53.68853378\n",
      "\n",
      "Running model (trial=3, mod=92, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1468116.25\n",
      "################################  100  ################################\n",
      "Training Loss:  123.87236023\n",
      "################################  200  ################################\n",
      "Training Loss:  120.47037506\n",
      "Epoch   265: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  120.19936371\n",
      "################################  400  ################################\n",
      "Training Loss:  120.20983887\n",
      "Epoch   466: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  120.09175873\n",
      "################################  600  ################################\n",
      "Training Loss:  120.00406647\n",
      "Epoch   667: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  119.60076904\n",
      "################################  800  ################################\n",
      "Training Loss:  118.92564392\n",
      "################################  900  ################################\n",
      "Training Loss:  117.86478424\n",
      "################################  1000  ################################\n",
      "Training Loss:  117.05921936\n",
      "################################  1100  ################################\n",
      "Training Loss:  116.40927887\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.172966\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.89111328\n",
      "################################  1400  ################################\n",
      "Training Loss:  116.82983398\n",
      "################################  1500  ################################\n",
      "Training Loss:  117.79695129\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.66159821\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.85604858\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.73657227\n",
      "################################  1900  ################################\n",
      "Training Loss:  117.1822052\n",
      "Final training Loss:  115.14967346\n",
      "\n",
      "Running model (trial=3, mod=93, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10660.98046875\n",
      "################################  100  ################################\n",
      "Training Loss:  118.5244751\n",
      "################################  200  ################################\n",
      "Training Loss:  120.86328125\n",
      "################################  300  ################################\n",
      "Training Loss:  118.38595581\n",
      "################################  400  ################################\n",
      "Training Loss:  140.15679932\n",
      "################################  500  ################################\n",
      "Training Loss:  123.00882721\n",
      "Epoch   572: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  116.02802277\n",
      "################################  700  ################################\n",
      "Training Loss:  114.75453949\n",
      "Epoch   773: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  113.27281952\n",
      "################################  900  ################################\n",
      "Training Loss:  115.51179504\n",
      "################################  1000  ################################\n",
      "Training Loss:  113.55404663\n",
      "################################  1100  ################################\n",
      "Training Loss:  125.65151978\n",
      "Epoch  1168: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  117.26679993\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.09412384\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.27020264\n",
      "################################  1500  ################################\n",
      "Training Loss:  115.17220306\n",
      "Epoch  1544: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.70317841\n",
      "################################  1700  ################################\n",
      "Training Loss:  113.33281708\n",
      "Epoch  1745: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.09470367\n",
      "################################  1900  ################################\n",
      "Training Loss:  113.35359192\n",
      "Final training Loss:  113.30497742\n",
      "\n",
      "Running model (trial=3, mod=94, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  20655.76367188\n",
      "################################  100  ################################\n",
      "Training Loss:  126.88356781\n",
      "################################  200  ################################\n",
      "Training Loss:  119.12375641\n",
      "################################  300  ################################\n",
      "Training Loss:  119.3316803\n",
      "################################  400  ################################\n",
      "Training Loss:  122.42703247\n",
      "################################  500  ################################\n",
      "Training Loss:  118.02774048\n",
      "################################  600  ################################\n",
      "Training Loss:  116.41996765\n",
      "################################  700  ################################\n",
      "Training Loss:  114.09281158\n",
      "################################  800  ################################\n",
      "Training Loss:  115.43371582\n",
      "################################  900  ################################\n",
      "Training Loss:  115.62627411\n",
      "Epoch   974: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  117.16014099\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.14177704\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.65210724\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.11902618\n",
      "################################  1400  ################################\n",
      "Training Loss:  125.92069244\n",
      "Epoch  1417: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  118.024086\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.8973999\n",
      "Epoch  1618: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  130.29302979\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.55394745\n",
      "Epoch  1819: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  117.37879944\n",
      "Final training Loss:  115.52428436\n",
      "\n",
      "Running model (trial=3, mod=95, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  425474.125\n",
      "################################  100  ################################\n",
      "Training Loss:  130.30245972\n",
      "################################  200  ################################\n",
      "Training Loss:  121.79289246\n",
      "Epoch   295: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  121.67961121\n",
      "################################  400  ################################\n",
      "Training Loss:  121.18228149\n",
      "Epoch   496: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  120.94965363\n",
      "################################  600  ################################\n",
      "Training Loss:  120.66157532\n",
      "Epoch   697: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  120.52077484\n",
      "################################  800  ################################\n",
      "Training Loss:  120.52391052\n",
      "Epoch   898: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  120.54962158\n",
      "################################  1000  ################################\n",
      "Training Loss:  120.44424438\n",
      "Epoch  1099: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  120.48802948\n",
      "################################  1200  ################################\n",
      "Training Loss:  120.4122467\n",
      "################################  1300  ################################\n",
      "Training Loss:  120.260849\n",
      "################################  1400  ################################\n",
      "Training Loss:  120.25310516\n",
      "################################  1500  ################################\n",
      "Training Loss:  120.23485565\n",
      "################################  1600  ################################\n",
      "Training Loss:  120.30221558\n",
      "################################  1700  ################################\n",
      "Training Loss:  120.5238266\n",
      "################################  1800  ################################\n",
      "Training Loss:  120.40663147\n",
      "################################  1900  ################################\n",
      "Training Loss:  120.18000031\n",
      "Epoch  1995: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  120.05664062\n",
      "\n",
      "Running model (trial=3, mod=96, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10878.13378906\n",
      "################################  100  ################################\n",
      "Training Loss:  257.02545166\n",
      "################################  200  ################################\n",
      "Training Loss:  124.70039368\n",
      "Epoch   285: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  125.0557785\n",
      "################################  400  ################################\n",
      "Training Loss:  122.70759583\n",
      "Epoch   486: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  122.8529892\n",
      "################################  600  ################################\n",
      "Training Loss:  122.50713348\n",
      "Epoch   687: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  122.24768829\n",
      "################################  800  ################################\n",
      "Training Loss:  122.24037933\n",
      "Epoch   888: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  122.97073364\n",
      "################################  1000  ################################\n",
      "Training Loss:  122.5071106\n",
      "Epoch  1089: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  121.88169098\n",
      "################################  1200  ################################\n",
      "Training Loss:  121.59395599\n",
      "Epoch  1290: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  120.6047821\n",
      "################################  1400  ################################\n",
      "Training Loss:  119.63808441\n",
      "Epoch  1491: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  119.3242569\n",
      "################################  1600  ################################\n",
      "Training Loss:  118.78144836\n",
      "Epoch  1692: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  118.46450043\n",
      "################################  1800  ################################\n",
      "Training Loss:  119.22785187\n",
      "Epoch  1893: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  119.9095993\n",
      "Final training Loss:  117.66868591\n",
      "\n",
      "Running model (trial=3, mod=97, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4577.68945312\n",
      "################################  100  ################################\n",
      "Training Loss:  173.35121155\n",
      "################################  200  ################################\n",
      "Training Loss:  124.03665161\n",
      "################################  300  ################################\n",
      "Training Loss:  123.97386932\n",
      "################################  400  ################################\n",
      "Training Loss:  123.82621002\n",
      "################################  500  ################################\n",
      "Training Loss:  123.88626099\n",
      "################################  600  ################################\n",
      "Training Loss:  123.58765411\n",
      "################################  700  ################################\n",
      "Training Loss:  128.43400574\n",
      "################################  800  ################################\n",
      "Training Loss:  127.8443985\n",
      "################################  900  ################################\n",
      "Training Loss:  124.24098206\n",
      "################################  1000  ################################\n",
      "Training Loss:  123.17292023\n",
      "################################  1100  ################################\n",
      "Training Loss:  120.44546509\n",
      "################################  1200  ################################\n",
      "Training Loss:  125.92605591\n",
      "################################  1300  ################################\n",
      "Training Loss:  141.58728027\n",
      "################################  1400  ################################\n",
      "Training Loss:  133.40055847\n",
      "################################  1500  ################################\n",
      "Training Loss:  123.17799377\n",
      "################################  1600  ################################\n",
      "Training Loss:  129.53453064\n",
      "################################  1700  ################################\n",
      "Training Loss:  118.89832306\n",
      "Epoch  1705: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  119.30710602\n",
      "################################  1900  ################################\n",
      "Training Loss:  139.7175293\n",
      "Final training Loss:  119.49827576\n",
      "\n",
      "Running model (trial=3, mod=98, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  19804.91210938\n",
      "################################  100  ################################\n",
      "Training Loss:  119.52942657\n",
      "################################  200  ################################\n",
      "Training Loss:  118.17353821\n",
      "################################  300  ################################\n",
      "Training Loss:  115.89962006\n",
      "################################  400  ################################\n",
      "Training Loss:  116.93560791\n",
      "################################  500  ################################\n",
      "Training Loss:  118.93383026\n",
      "################################  600  ################################\n",
      "Training Loss:  114.43369293\n",
      "################################  700  ################################\n",
      "Training Loss:  120.14873505\n",
      "################################  800  ################################\n",
      "Training Loss:  134.85256958\n",
      "################################  900  ################################\n",
      "Training Loss:  115.79777527\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.73127747\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.1513443\n",
      "################################  1200  ################################\n",
      "Training Loss:  121.37311554\n",
      "Epoch  1271: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.47686005\n",
      "################################  1400  ################################\n",
      "Training Loss:  118.29962921\n",
      "################################  1500  ################################\n",
      "Training Loss:  121.53295135\n",
      "Epoch  1517: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.56263733\n",
      "################################  1700  ################################\n",
      "Training Loss:  117.9705658\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.26840973\n",
      "################################  1900  ################################\n",
      "Training Loss:  117.09742737\n",
      "Final training Loss:  124.08422089\n",
      "\n",
      "Running model (trial=3, mod=99, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  62757.4921875\n",
      "################################  100  ################################\n",
      "Training Loss:  120.18933868\n",
      "################################  200  ################################\n",
      "Training Loss:  118.27545166\n",
      "################################  300  ################################\n",
      "Training Loss:  116.61776733\n",
      "################################  400  ################################\n",
      "Training Loss:  121.05568695\n",
      "################################  500  ################################\n",
      "Training Loss:  118.08892822\n",
      "################################  600  ################################\n",
      "Training Loss:  116.59384918\n",
      "################################  700  ################################\n",
      "Training Loss:  118.46426392\n",
      "################################  800  ################################\n",
      "Training Loss:  114.55924225\n",
      "Epoch   883: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  116.03185272\n",
      "################################  1000  ################################\n",
      "Training Loss:  117.69611359\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.08718109\n",
      "Epoch  1195: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  118.8859787\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.04346466\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.17713165\n",
      "Epoch  1441: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.28902435\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.74668121\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.52319336\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.26554108\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.3607254\n",
      "Final training Loss:  113.75689697\n",
      "\n",
      "Running model (trial=3, mod=100, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3622.7878418\n",
      "################################  100  ################################\n",
      "Training Loss:  114.69037628\n",
      "################################  200  ################################\n",
      "Training Loss:  530.44781494\n",
      "################################  300  ################################\n",
      "Training Loss:  117.83677673\n",
      "Epoch   343: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  115.8484726\n",
      "################################  500  ################################\n",
      "Training Loss:  115.03627777\n",
      "Epoch   544: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  114.20393372\n",
      "################################  700  ################################\n",
      "Training Loss:  114.68750763\n",
      "Epoch   745: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  114.48092651\n",
      "################################  900  ################################\n",
      "Training Loss:  113.92362213\n",
      "Epoch   946: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.21737671\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.68476868\n",
      "Epoch  1147: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.74391174\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.00252533\n",
      "Epoch  1348: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  113.74008942\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.71718597\n",
      "Epoch  1549: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.03098297\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.42092896\n",
      "Epoch  1750: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  113.9078064\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.04953766\n",
      "Epoch  1951: reducing learning rate of group 0 to 6.7109e-03.\n",
      "Final training Loss:  113.78773499\n",
      "\n",
      "Running model (trial=3, mod=101, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3485033.5\n",
      "################################  100  ################################\n",
      "Training Loss:  118.89417267\n",
      "################################  200  ################################\n",
      "Training Loss:  119.04071808\n",
      "Epoch   259: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  118.59054565\n",
      "################################  400  ################################\n",
      "Training Loss:  116.92425537\n",
      "################################  500  ################################\n",
      "Training Loss:  115.35781097\n",
      "################################  600  ################################\n",
      "Training Loss:  115.60411835\n",
      "################################  700  ################################\n",
      "Training Loss:  115.73409271\n",
      "################################  800  ################################\n",
      "Training Loss:  114.58003235\n",
      "################################  900  ################################\n",
      "Training Loss:  114.65007782\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.84139252\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.35939026\n",
      "Epoch  1118: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  117.27804565\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.52352142\n",
      "Epoch  1361: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.61620331\n",
      "################################  1500  ################################\n",
      "Training Loss:  114.05010223\n",
      "################################  1600  ################################\n",
      "Training Loss:  120.83472443\n",
      "Epoch  1644: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.39762878\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.04959106\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.25602722\n",
      "Final training Loss:  113.92108917\n",
      "\n",
      "Running model (trial=3, mod=102, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1736.10791016\n",
      "################################  100  ################################\n",
      "Training Loss:  121.19664001\n",
      "################################  200  ################################\n",
      "Training Loss:  117.99565887\n",
      "Epoch   241: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  117.3107605\n",
      "################################  400  ################################\n",
      "Training Loss:  115.76365662\n",
      "Epoch   442: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  120.91260529\n",
      "################################  600  ################################\n",
      "Training Loss:  114.15595245\n",
      "Epoch   643: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  114.51826477\n",
      "################################  800  ################################\n",
      "Training Loss:  113.1490097\n",
      "Epoch   844: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  112.89059448\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.18021393\n",
      "################################  1100  ################################\n",
      "Training Loss:  782.49432373\n",
      "################################  1200  ################################\n",
      "Training Loss:  103.23638916\n",
      "################################  1300  ################################\n",
      "Training Loss:  61.03384399\n",
      "################################  1400  ################################\n",
      "Training Loss:  58.70965195\n",
      "################################  1500  ################################\n",
      "Training Loss:  57.31332016\n",
      "################################  1600  ################################\n",
      "Training Loss:  56.81417847\n",
      "################################  1700  ################################\n",
      "Training Loss:  57.34972382\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.47333527\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.97386932\n",
      "Final training Loss:  54.0937233\n",
      "\n",
      "Running model (trial=3, mod=103, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  632521.875\n",
      "################################  100  ################################\n",
      "Training Loss:  126.86412811\n",
      "################################  200  ################################\n",
      "Training Loss:  123.59184265\n",
      "Epoch   288: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  122.22122192\n",
      "################################  400  ################################\n",
      "Training Loss:  121.67810059\n",
      "################################  500  ################################\n",
      "Training Loss:  159.02938843\n",
      "################################  600  ################################\n",
      "Training Loss:  119.89868164\n",
      "################################  700  ################################\n",
      "Training Loss:  120.94369507\n",
      "################################  800  ################################\n",
      "Training Loss:  120.69786072\n",
      "################################  900  ################################\n",
      "Training Loss:  119.92995453\n",
      "################################  1000  ################################\n",
      "Training Loss:  127.8444519\n",
      "################################  1100  ################################\n",
      "Training Loss:  120.12365723\n",
      "################################  1200  ################################\n",
      "Training Loss:  119.91165924\n",
      "################################  1300  ################################\n",
      "Training Loss:  120.09074402\n",
      "################################  1400  ################################\n",
      "Training Loss:  125.63580322\n",
      "################################  1500  ################################\n",
      "Training Loss:  163.25457764\n",
      "Epoch  1509: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  133.20385742\n",
      "################################  1700  ################################\n",
      "Training Loss:  120.19523621\n",
      "Epoch  1710: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  120.86671448\n",
      "################################  1900  ################################\n",
      "Training Loss:  125.83092499\n",
      "Final training Loss:  127.27037048\n",
      "\n",
      "Running model (trial=3, mod=104, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  399.88418579\n",
      "################################  100  ################################\n",
      "Training Loss:  116.86989594\n",
      "################################  200  ################################\n",
      "Training Loss:  139.40084839\n",
      "################################  300  ################################\n",
      "Training Loss:  143.77340698\n",
      "Epoch   320: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  116.6547699\n",
      "################################  500  ################################\n",
      "Training Loss:  138.349823\n",
      "Epoch   521: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  114.76702881\n",
      "################################  700  ################################\n",
      "Training Loss:  114.16682434\n",
      "Epoch   722: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  137.8556366\n",
      "################################  900  ################################\n",
      "Training Loss:  117.48722839\n",
      "Epoch   923: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.18032074\n",
      "################################  1100  ################################\n",
      "Training Loss:  120.86003113\n",
      "################################  1200  ################################\n",
      "Training Loss:  127.71998596\n",
      "################################  1300  ################################\n",
      "Training Loss:  188.98446655\n",
      "################################  1400  ################################\n",
      "Training Loss:  85.5655365\n",
      "################################  1500  ################################\n",
      "Training Loss:  140.19656372\n",
      "################################  1600  ################################\n",
      "Training Loss:  89.56292725\n",
      "Epoch  1622: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  127.76335144\n",
      "################################  1800  ################################\n",
      "Training Loss:  128.52441406\n",
      "################################  1900  ################################\n",
      "Training Loss:  81.56734467\n",
      "Final training Loss:  256.81231689\n",
      "\n",
      "Running model (trial=3, mod=105, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  49968.65234375\n",
      "################################  100  ################################\n",
      "Training Loss:  1403.50402832\n",
      "################################  200  ################################\n",
      "Training Loss:  211.65963745\n",
      "################################  300  ################################\n",
      "Training Loss:  126.82775879\n",
      "Epoch   375: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  109.92740631\n",
      "################################  500  ################################\n",
      "Training Loss:  102.39469147\n",
      "Epoch   576: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  102.4271698\n",
      "################################  700  ################################\n",
      "Training Loss:  102.4846344\n",
      "################################  800  ################################\n",
      "Training Loss:  112.55002594\n",
      "Epoch   845: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  146.06126404\n",
      "################################  1000  ################################\n",
      "Training Loss:  119.4710083\n",
      "Epoch  1046: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  104.12818146\n",
      "################################  1200  ################################\n",
      "Training Loss:  104.4024353\n",
      "Epoch  1247: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  111.43413544\n",
      "################################  1400  ################################\n",
      "Training Loss:  108.02405548\n",
      "################################  1500  ################################\n",
      "Training Loss:  118.14926147\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.96250916\n",
      "Epoch  1635: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  103.32289886\n",
      "################################  1800  ################################\n",
      "Training Loss:  106.0688324\n",
      "Epoch  1836: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  110.05975342\n",
      "Final training Loss:  128.20417786\n",
      "\n",
      "Running model (trial=3, mod=106, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1064728.0\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=107, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3388.99707031\n",
      "################################  100  ################################\n",
      "Training Loss:  61.21350098\n",
      "################################  200  ################################\n",
      "Training Loss:  75.47909546\n",
      "################################  300  ################################\n",
      "Training Loss:  54.65156937\n",
      "################################  400  ################################\n",
      "Training Loss:  52.86490631\n",
      "################################  500  ################################\n",
      "Training Loss:  51.8536911\n",
      "################################  600  ################################\n",
      "Training Loss:  58.73365021\n",
      "################################  700  ################################\n",
      "Training Loss:  50.87337494\n",
      "################################  800  ################################\n",
      "Training Loss:  63.46265411\n",
      "Epoch   890: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  52.77664948\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.20927811\n",
      "################################  1100  ################################\n",
      "Training Loss:  46.65914154\n",
      "################################  1200  ################################\n",
      "Training Loss:  39.70588684\n",
      "################################  1300  ################################\n",
      "Training Loss:  38.16998291\n",
      "################################  1400  ################################\n",
      "Training Loss:  40.03588104\n",
      "################################  1500  ################################\n",
      "Training Loss:  nan\n",
      "Epoch  1520: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=108, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  71961.65625\n",
      "################################  100  ################################\n",
      "Training Loss:  55.77164078\n",
      "################################  200  ################################\n",
      "Training Loss:  53.476017\n",
      "################################  300  ################################\n",
      "Training Loss:  53.21535492\n",
      "Epoch   366: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  53.15956497\n",
      "################################  500  ################################\n",
      "Training Loss:  54.32697678\n",
      "################################  600  ################################\n",
      "Training Loss:  48.22160721\n",
      "################################  700  ################################\n",
      "Training Loss:  49.91227722\n",
      "################################  800  ################################\n",
      "Training Loss:  48.89252472\n",
      "################################  900  ################################\n",
      "Training Loss:  43.51410675\n",
      "################################  1000  ################################\n",
      "Training Loss:  48.52822113\n",
      "################################  1100  ################################\n",
      "Training Loss:  44.20939636\n",
      "################################  1200  ################################\n",
      "Training Loss:  41.5837326\n",
      "################################  1300  ################################\n",
      "Training Loss:  44.86026001\n",
      "################################  1400  ################################\n",
      "Training Loss:  45.84622574\n",
      "################################  1500  ################################\n",
      "Training Loss:  41.87765121\n",
      "################################  1600  ################################\n",
      "Training Loss:  45.42798615\n",
      "################################  1700  ################################\n",
      "Training Loss:  44.7452507\n",
      "Epoch  1751: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  46.3122673\n",
      "################################  1900  ################################\n",
      "Training Loss:  39.64889526\n",
      "Final training Loss:  38.56603241\n",
      "\n",
      "Running model (trial=3, mod=109, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3260.51831055\n",
      "################################  100  ################################\n",
      "Training Loss:  119.9457016\n",
      "################################  200  ################################\n",
      "Training Loss:  117.92892456\n",
      "################################  300  ################################\n",
      "Training Loss:  117.73672485\n",
      "################################  400  ################################\n",
      "Training Loss:  117.76721954\n",
      "################################  500  ################################\n",
      "Training Loss:  118.01168823\n",
      "################################  600  ################################\n",
      "Training Loss:  118.6227951\n",
      "################################  700  ################################\n",
      "Training Loss:  119.77399445\n",
      "Epoch   740: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  78.60346222\n",
      "################################  900  ################################\n",
      "Training Loss:  51.72684479\n",
      "################################  1000  ################################\n",
      "Training Loss:  44.96331787\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.18615723\n",
      "################################  1200  ################################\n",
      "Training Loss:  41.99512863\n",
      "################################  1300  ################################\n",
      "Training Loss:  39.47637177\n",
      "################################  1400  ################################\n",
      "Training Loss:  135.78111267\n",
      "Epoch  1484: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  120.16755676\n",
      "################################  1600  ################################\n",
      "Training Loss:  119.74198914\n",
      "Epoch  1685: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  120.63050842\n",
      "################################  1800  ################################\n",
      "Training Loss:  120.77345276\n",
      "Epoch  1886: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  120.44916534\n",
      "Final training Loss:  121.50404358\n",
      "\n",
      "Running model (trial=3, mod=110, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  704023.4375\n",
      "################################  100  ################################\n",
      "Training Loss:  586.83148193\n",
      "################################  200  ################################\n",
      "Training Loss:  78.83805847\n",
      "################################  300  ################################\n",
      "Training Loss:  63.07287979\n",
      "################################  400  ################################\n",
      "Training Loss:  59.95033264\n",
      "################################  500  ################################\n",
      "Training Loss:  55.94643021\n",
      "################################  600  ################################\n",
      "Training Loss:  54.91801453\n",
      "################################  700  ################################\n",
      "Training Loss:  54.76649475\n",
      "################################  800  ################################\n",
      "Training Loss:  56.59898376\n",
      "################################  900  ################################\n",
      "Training Loss:  53.51432419\n",
      "################################  1000  ################################\n",
      "Training Loss:  56.49186325\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.31459808\n",
      "################################  1200  ################################\n",
      "Training Loss:  49.86138916\n",
      "################################  1300  ################################\n",
      "Training Loss:  46.6856041\n",
      "################################  1400  ################################\n",
      "Training Loss:  60.85016632\n",
      "################################  1500  ################################\n",
      "Training Loss:  51.43810654\n",
      "################################  1600  ################################\n",
      "Training Loss:  46.27229309\n",
      "################################  1700  ################################\n",
      "Training Loss:  70.13856506\n",
      "################################  1800  ################################\n",
      "Training Loss:  47.27473831\n",
      "################################  1900  ################################\n",
      "Training Loss:  42.64664459\n",
      "Final training Loss:  70.15291595\n",
      "\n",
      "Running model (trial=3, mod=111, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  824568.625\n",
      "################################  100  ################################\n",
      "Training Loss:  63.33887863\n",
      "################################  200  ################################\n",
      "Training Loss:  54.68693542\n",
      "################################  300  ################################\n",
      "Training Loss:  52.38336563\n",
      "################################  400  ################################\n",
      "Training Loss:  63.74618149\n",
      "################################  500  ################################\n",
      "Training Loss:  47.11598587\n",
      "################################  600  ################################\n",
      "Training Loss:  118.35539246\n",
      "################################  700  ################################\n",
      "Training Loss:  104.00569916\n",
      "Epoch   727: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  111.41244507\n",
      "################################  900  ################################\n",
      "Training Loss:  89.19218445\n",
      "Epoch   928: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  79.49518585\n",
      "################################  1100  ################################\n",
      "Training Loss:  67.15148926\n",
      "Epoch  1129: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  61.87518311\n",
      "################################  1300  ################################\n",
      "Training Loss:  59.2114296\n",
      "Epoch  1330: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.49564362\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.86268616\n",
      "Epoch  1531: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.7084465\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.11407089\n",
      "Epoch  1732: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.64644623\n",
      "################################  1900  ################################\n",
      "Training Loss:  48.19974136\n",
      "Epoch  1933: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  46.60149765\n",
      "\n",
      "Running model (trial=3, mod=112, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  13242.51464844\n",
      "################################  100  ################################\n",
      "Training Loss:  69258.90625\n",
      "################################  200  ################################\n",
      "Training Loss:  117.3384552\n",
      "################################  300  ################################\n",
      "Training Loss:  112.84355927\n",
      "################################  400  ################################\n",
      "Training Loss:  112.80579376\n",
      "Epoch   412: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  112.90271759\n",
      "################################  600  ################################\n",
      "Training Loss:  112.93183899\n",
      "Epoch   613: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  112.90113831\n",
      "################################  800  ################################\n",
      "Training Loss:  112.64442444\n",
      "Epoch   814: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  112.81797028\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.73038483\n",
      "Epoch  1015: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  112.28549194\n",
      "################################  1200  ################################\n",
      "Training Loss:  110.04982758\n",
      "################################  1300  ################################\n",
      "Training Loss:  109.60331726\n",
      "################################  1400  ################################\n",
      "Training Loss:  109.41861725\n",
      "################################  1500  ################################\n",
      "Training Loss:  109.07064056\n",
      "################################  1600  ################################\n",
      "Training Loss:  106.94757843\n",
      "################################  1700  ################################\n",
      "Training Loss:  106.10306549\n",
      "################################  1800  ################################\n",
      "Training Loss:  105.64983368\n",
      "################################  1900  ################################\n",
      "Training Loss:  105.79048157\n",
      "Final training Loss:  105.37646484\n",
      "\n",
      "Running model (trial=3, mod=113, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  156735.125\n",
      "################################  100  ################################\n",
      "Training Loss:  121.40851593\n",
      "################################  200  ################################\n",
      "Training Loss:  120.41075134\n",
      "################################  300  ################################\n",
      "Training Loss:  121.71308899\n",
      "Epoch   338: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  121.3372345\n",
      "################################  500  ################################\n",
      "Training Loss:  114.95054626\n",
      "################################  600  ################################\n",
      "Training Loss:  110.70919037\n",
      "################################  700  ################################\n",
      "Training Loss:  170.04437256\n",
      "################################  800  ################################\n",
      "Training Loss:  111.68609619\n",
      "################################  900  ################################\n",
      "Training Loss:  108.13161469\n",
      "################################  1000  ################################\n",
      "Training Loss:  109.16361237\n",
      "################################  1100  ################################\n",
      "Training Loss:  111.70294189\n",
      "Epoch  1110: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  103.25566864\n",
      "################################  1300  ################################\n",
      "Training Loss:  102.62754822\n",
      "Epoch  1351: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  102.10632324\n",
      "################################  1500  ################################\n",
      "Training Loss:  106.00635529\n",
      "################################  1600  ################################\n",
      "Training Loss:  123.04840851\n",
      "################################  1700  ################################\n",
      "Training Loss:  105.77255249\n",
      "Epoch  1722: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  103.06187439\n",
      "################################  1900  ################################\n",
      "Training Loss:  101.83413696\n",
      "Epoch  1923: reducing learning rate of group 0 to 1.6384e-02.\n",
      "Final training Loss:  120.03645325\n",
      "\n",
      "Running model (trial=3, mod=114, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  71347.78125\n",
      "################################  100  ################################\n",
      "Training Loss:  47.1002121\n",
      "################################  200  ################################\n",
      "Training Loss:  42.88988113\n",
      "################################  300  ################################\n",
      "Training Loss:  57.4058609\n",
      "################################  400  ################################\n",
      "Training Loss:  50.04347992\n",
      "################################  500  ################################\n",
      "Training Loss:  44.13256454\n",
      "################################  600  ################################\n",
      "Training Loss:  42.21344757\n",
      "Epoch   616: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  43.82725525\n",
      "################################  800  ################################\n",
      "Training Loss:  43.60731888\n",
      "################################  900  ################################\n",
      "Training Loss:  44.04772568\n",
      "################################  1000  ################################\n",
      "Training Loss:  45.22607422\n",
      "Epoch  1034: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.19064331\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.21400452\n",
      "Epoch  1235: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.86917114\n",
      "################################  1400  ################################\n",
      "Training Loss:  46.10809326\n",
      "################################  1500  ################################\n",
      "Training Loss:  55.07472229\n",
      "Epoch  1555: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  49.90036011\n",
      "################################  1700  ################################\n",
      "Training Loss:  42.52205276\n",
      "################################  1800  ################################\n",
      "Training Loss:  40.57800293\n",
      "################################  1900  ################################\n",
      "Training Loss:  43.4570961\n",
      "Epoch  1968: reducing learning rate of group 0 to 1.6384e-02.\n",
      "Final training Loss:  54.6629715\n",
      "\n",
      "Running model (trial=3, mod=115, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  144414.265625\n",
      "################################  100  ################################\n",
      "Training Loss:  126.68507385\n",
      "################################  200  ################################\n",
      "Training Loss:  58.15594482\n",
      "################################  300  ################################\n",
      "Training Loss:  43.43705368\n",
      "################################  400  ################################\n",
      "Training Loss:  64.39398956\n",
      "################################  500  ################################\n",
      "Training Loss:  63.90139771\n",
      "################################  600  ################################\n",
      "Training Loss:  50.41110229\n",
      "################################  700  ################################\n",
      "Training Loss:  39.36241913\n",
      "################################  800  ################################\n",
      "Training Loss:  40.55018234\n",
      "################################  900  ################################\n",
      "Training Loss:  49.01553345\n",
      "################################  1000  ################################\n",
      "Training Loss:  38.23104858\n",
      "################################  1100  ################################\n",
      "Training Loss:  59.33701706\n",
      "################################  1200  ################################\n",
      "Training Loss:  45.34529495\n",
      "################################  1300  ################################\n",
      "Training Loss:  38.0685997\n",
      "Epoch  1376: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  37.87181091\n",
      "################################  1500  ################################\n",
      "Training Loss:  38.45516586\n",
      "Epoch  1577: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  42.080616\n",
      "################################  1700  ################################\n",
      "Training Loss:  37.59583282\n",
      "Epoch  1778: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  37.13656998\n",
      "################################  1900  ################################\n",
      "Training Loss:  37.94590378\n",
      "Final training Loss:  38.32686234\n",
      "\n",
      "Running model (trial=3, mod=116, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  819746.1875\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=117, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3381972.5\n",
      "################################  100  ################################\n",
      "Training Loss:  101.83995819\n",
      "################################  200  ################################\n",
      "Training Loss:  97.38032532\n",
      "################################  300  ################################\n",
      "Training Loss:  97.630867\n",
      "################################  400  ################################\n",
      "Training Loss:  97.14250946\n",
      "################################  500  ################################\n",
      "Training Loss:  96.43281555\n",
      "################################  600  ################################\n",
      "Training Loss:  95.35140991\n",
      "################################  700  ################################\n",
      "Training Loss:  100.01482391\n",
      "################################  800  ################################\n",
      "Training Loss:  97.29374695\n",
      "################################  900  ################################\n",
      "Training Loss:  95.07646179\n",
      "################################  1000  ################################\n",
      "Training Loss:  90.89034271\n",
      "################################  1100  ################################\n",
      "Training Loss:  86.17668915\n",
      "################################  1200  ################################\n",
      "Training Loss:  87.85201263\n",
      "################################  1300  ################################\n",
      "Training Loss:  87.99642944\n",
      "################################  1400  ################################\n",
      "Training Loss:  85.58798218\n",
      "################################  1500  ################################\n",
      "Training Loss:  85.08275604\n",
      "################################  1600  ################################\n",
      "Training Loss:  88.81835175\n",
      "################################  1700  ################################\n",
      "Training Loss:  87.9521637\n",
      "################################  1800  ################################\n",
      "Training Loss:  84.4712677\n",
      "Epoch  1814: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  84.98486328\n",
      "Final training Loss:  83.6622467\n",
      "\n",
      "Running model (trial=3, mod=118, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  15413332992.0\n",
      "################################  100  ################################\n",
      "Training Loss:  1357378.75\n",
      "################################  200  ################################\n",
      "Training Loss:  420798.78125\n",
      "################################  300  ################################\n",
      "Training Loss:  195485.40625\n",
      "################################  400  ################################\n",
      "Training Loss:  103668.609375\n",
      "################################  500  ################################\n",
      "Training Loss:  80014.21875\n",
      "################################  600  ################################\n",
      "Training Loss:  63464.1875\n",
      "################################  700  ################################\n",
      "Training Loss:  51125.4140625\n",
      "################################  800  ################################\n",
      "Training Loss:  44751.89453125\n",
      "################################  900  ################################\n",
      "Training Loss:  38658.56640625\n",
      "################################  1000  ################################\n",
      "Training Loss:  33581.98046875\n",
      "################################  1100  ################################\n",
      "Training Loss:  30430.63085938\n",
      "################################  1200  ################################\n",
      "Training Loss:  26294.56640625\n",
      "################################  1300  ################################\n",
      "Training Loss:  23909.58007812\n",
      "################################  1400  ################################\n",
      "Training Loss:  21310.82421875\n",
      "################################  1500  ################################\n",
      "Training Loss:  16090.11914062\n",
      "################################  1600  ################################\n",
      "Training Loss:  13334.92382812\n",
      "################################  1700  ################################\n",
      "Training Loss:  10024.60351562\n",
      "################################  1800  ################################\n",
      "Training Loss:  5467.8828125\n",
      "################################  1900  ################################\n",
      "Training Loss:  1382.33447266\n",
      "Final training Loss:  593.83691406\n",
      "\n",
      "Running model (trial=3, mod=119, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.0432755808600064e+16\n",
      "################################  100  ################################\n",
      "Training Loss:  264451456.0\n",
      "################################  200  ################################\n",
      "Training Loss:  262051376.0\n",
      "Epoch   204: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  259426208.0\n",
      "################################  400  ################################\n",
      "Training Loss:  256307344.0\n",
      "Epoch   405: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  253410208.0\n",
      "################################  600  ################################\n",
      "Training Loss:  250206144.0\n",
      "Epoch   606: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  247344832.0\n",
      "################################  800  ################################\n",
      "Training Loss:  244275392.0\n",
      "Epoch   807: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  241581280.0\n",
      "################################  1000  ################################\n",
      "Training Loss:  238739936.0\n",
      "Epoch  1008: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  236267904.0\n",
      "################################  1200  ################################\n",
      "Training Loss:  233688256.0\n",
      "Epoch  1209: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  231453616.0\n",
      "################################  1400  ################################\n",
      "Training Loss:  229140608.0\n",
      "Epoch  1410: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  227139024.0\n",
      "################################  1600  ################################\n",
      "Training Loss:  225078512.0\n",
      "Epoch  1611: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  223297120.0\n",
      "################################  1800  ################################\n",
      "Training Loss:  221471744.0\n",
      "Epoch  1812: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  219892640.0\n",
      "Final training Loss:  218296496.0\n",
      "\n",
      "Running model (trial=4, mod=120, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9836.85839844\n",
      "################################  100  ################################\n",
      "Training Loss:  117.69621277\n",
      "################################  200  ################################\n",
      "Training Loss:  116.65898895\n",
      "################################  300  ################################\n",
      "Training Loss:  118.12663269\n",
      "################################  400  ################################\n",
      "Training Loss:  116.35150146\n",
      "################################  500  ################################\n",
      "Training Loss:  115.53723145\n",
      "################################  600  ################################\n",
      "Training Loss:  116.68972015\n",
      "################################  700  ################################\n",
      "Training Loss:  115.85108185\n",
      "Epoch   723: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  126.89720917\n",
      "################################  900  ################################\n",
      "Training Loss:  122.47861481\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.98771667\n",
      "Epoch  1038: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.22968292\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.51534271\n",
      "Epoch  1239: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.42259216\n",
      "################################  1400  ################################\n",
      "Training Loss:  121.87532806\n",
      "Epoch  1440: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.36346436\n",
      "################################  1600  ################################\n",
      "Training Loss:  112.9959259\n",
      "Epoch  1641: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  112.81015778\n",
      "################################  1800  ################################\n",
      "Training Loss:  113.61962128\n",
      "Epoch  1842: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  113.54519653\n",
      "Final training Loss:  120.13332367\n",
      "\n",
      "Running model (trial=4, mod=121, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  943816.125\n",
      "################################  100  ################################\n",
      "Training Loss:  131.51072693\n",
      "################################  200  ################################\n",
      "Training Loss:  124.24765778\n",
      "################################  300  ################################\n",
      "Training Loss:  122.47502899\n",
      "################################  400  ################################\n",
      "Training Loss:  122.96913147\n",
      "Epoch   451: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  122.19865417\n",
      "################################  600  ################################\n",
      "Training Loss:  122.30044556\n",
      "################################  700  ################################\n",
      "Training Loss:  122.71157074\n",
      "################################  800  ################################\n",
      "Training Loss:  121.11497498\n",
      "################################  900  ################################\n",
      "Training Loss:  117.80163574\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.0125351\n",
      "################################  1100  ################################\n",
      "Training Loss:  111.272789\n",
      "################################  1200  ################################\n",
      "Training Loss:  107.8348999\n",
      "################################  1300  ################################\n",
      "Training Loss:  103.63973999\n",
      "################################  1400  ################################\n",
      "Training Loss:  98.972229\n",
      "################################  1500  ################################\n",
      "Training Loss:  93.96166992\n",
      "################################  1600  ################################\n",
      "Training Loss:  88.60897827\n",
      "################################  1700  ################################\n",
      "Training Loss:  83.37516022\n",
      "################################  1800  ################################\n",
      "Training Loss:  70.57559967\n",
      "################################  1900  ################################\n",
      "Training Loss:  64.05165863\n",
      "Final training Loss:  77.50163269\n",
      "\n",
      "Running model (trial=4, mod=122, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  14563.39257812\n",
      "################################  100  ################################\n",
      "Training Loss:  92.32736206\n",
      "################################  200  ################################\n",
      "Training Loss:  69.27185059\n",
      "################################  300  ################################\n",
      "Training Loss:  59.63399124\n",
      "Epoch   362: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  56.06070328\n",
      "################################  500  ################################\n",
      "Training Loss:  55.01861191\n",
      "Epoch   563: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  53.51237488\n",
      "################################  700  ################################\n",
      "Training Loss:  55.65956497\n",
      "Epoch   764: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  55.95087814\n",
      "################################  900  ################################\n",
      "Training Loss:  54.81777573\n",
      "Epoch   965: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.17806625\n",
      "################################  1100  ################################\n",
      "Training Loss:  55.52046967\n",
      "Epoch  1166: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.61746216\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.33734131\n",
      "Epoch  1367: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.09350586\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.47426987\n",
      "Epoch  1568: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  52.85538483\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.78743744\n",
      "Epoch  1769: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  55.34443665\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.4893074\n",
      "Epoch  1970: reducing learning rate of group 0 to 6.7109e-03.\n",
      "Final training Loss:  51.95457077\n",
      "\n",
      "Running model (trial=4, mod=123, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1076216.75\n",
      "################################  100  ################################\n",
      "Training Loss:  117.63887024\n",
      "################################  200  ################################\n",
      "Training Loss:  118.39713287\n",
      "Epoch   295: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  118.31513977\n",
      "################################  400  ################################\n",
      "Training Loss:  118.29101562\n",
      "Epoch   496: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  118.21932983\n",
      "################################  600  ################################\n",
      "Training Loss:  118.23796844\n",
      "Epoch   697: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  118.4920578\n",
      "################################  800  ################################\n",
      "Training Loss:  115.26267242\n",
      "################################  900  ################################\n",
      "Training Loss:  115.10816193\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.49783325\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.61302948\n",
      "################################  1200  ################################\n",
      "Training Loss:  117.70588684\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.28091431\n",
      "Epoch  1384: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.25108337\n",
      "################################  1500  ################################\n",
      "Training Loss:  114.24401855\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.76111603\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.25022125\n",
      "Epoch  1740: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.69554901\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.22407532\n",
      "Epoch  1941: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  114.29450226\n",
      "\n",
      "Running model (trial=4, mod=124, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1275674.25\n",
      "################################  100  ################################\n",
      "Training Loss:  112.61418152\n",
      "################################  200  ################################\n",
      "Training Loss:  111.46160126\n",
      "################################  300  ################################\n",
      "Training Loss:  111.56125641\n",
      "################################  400  ################################\n",
      "Training Loss:  111.15132141\n",
      "Epoch   428: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  112.54921722\n",
      "################################  600  ################################\n",
      "Training Loss:  112.22214508\n",
      "Epoch   629: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  112.08835602\n",
      "################################  800  ################################\n",
      "Training Loss:  111.59664917\n",
      "Epoch   900: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  111.67017365\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.42493439\n",
      "################################  1100  ################################\n",
      "Epoch  1101: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Training Loss:  112.19256592\n",
      "################################  1200  ################################\n",
      "Training Loss:  112.07324219\n",
      "################################  1300  ################################\n",
      "Training Loss:  112.07304382\n",
      "Epoch  1399: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  111.37508392\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.47685242\n",
      "Epoch  1600: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.28100586\n",
      "################################  1700  ################################\n",
      "Training Loss:  113.87696075\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.07312012\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.07041931\n",
      "Epoch  1961: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  111.14219666\n",
      "\n",
      "Running model (trial=4, mod=125, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  49578.109375\n",
      "################################  100  ################################\n",
      "Training Loss:  117.44547272\n",
      "################################  200  ################################\n",
      "Training Loss:  118.41364288\n",
      "Epoch   233: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  121.88182831\n",
      "################################  400  ################################\n",
      "Training Loss:  119.59001923\n",
      "Epoch   434: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  118.95209503\n",
      "################################  600  ################################\n",
      "Training Loss:  116.05176544\n",
      "################################  700  ################################\n",
      "Training Loss:  120.61364746\n",
      "################################  800  ################################\n",
      "Training Loss:  122.90623474\n",
      "################################  900  ################################\n",
      "Training Loss:  113.58432007\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.79827881\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.48435211\n",
      "Epoch  1167: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.77127075\n",
      "################################  1300  ################################\n",
      "Training Loss:  113.79723358\n",
      "Epoch  1368: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  116.65401459\n",
      "################################  1500  ################################\n",
      "Training Loss:  112.66764832\n",
      "Epoch  1569: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  117.31620026\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.16422272\n",
      "Epoch  1770: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  113.53416443\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.36875153\n",
      "Final training Loss:  114.01309967\n",
      "\n",
      "Running model (trial=4, mod=126, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5731.66308594\n",
      "################################  100  ################################\n",
      "Training Loss:  135.81091309\n",
      "################################  200  ################################\n",
      "Training Loss:  121.64676666\n",
      "################################  300  ################################\n",
      "Training Loss:  120.68917847\n",
      "################################  400  ################################\n",
      "Training Loss:  123.75779724\n",
      "################################  500  ################################\n",
      "Training Loss:  120.35764313\n",
      "################################  600  ################################\n",
      "Training Loss:  120.35887909\n",
      "################################  700  ################################\n",
      "Training Loss:  118.03610992\n",
      "################################  800  ################################\n",
      "Training Loss:  119.08629608\n",
      "################################  900  ################################\n",
      "Training Loss:  119.47744751\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.69271851\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.6002655\n",
      "Epoch  1170: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  121.73149109\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.29151917\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.64003754\n",
      "################################  1500  ################################\n",
      "Training Loss:  129.06282043\n",
      "################################  1600  ################################\n",
      "Training Loss:  119.77098083\n",
      "Epoch  1682: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.38512421\n",
      "################################  1800  ################################\n",
      "Training Loss:  123.20554352\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.60834503\n",
      "Final training Loss:  129.52919006\n",
      "\n",
      "Running model (trial=4, mod=127, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2650.94799805\n",
      "################################  100  ################################\n",
      "Training Loss:  125.59783936\n",
      "################################  200  ################################\n",
      "Training Loss:  121.33248901\n",
      "################################  300  ################################\n",
      "Training Loss:  119.03049469\n",
      "################################  400  ################################\n",
      "Training Loss:  164.52709961\n",
      "################################  500  ################################\n",
      "Training Loss:  121.34710693\n",
      "Epoch   509: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  123.45631409\n",
      "################################  700  ################################\n",
      "Training Loss:  116.39588165\n",
      "################################  800  ################################\n",
      "Training Loss:  143.74554443\n",
      "################################  900  ################################\n",
      "Training Loss:  124.20900726\n",
      "################################  1000  ################################\n",
      "Training Loss:  121.47254944\n",
      "################################  1100  ################################\n",
      "Training Loss:  143.00146484\n",
      "Epoch  1193: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  122.03192139\n",
      "################################  1300  ################################\n",
      "Training Loss:  113.69463348\n",
      "################################  1400  ################################\n",
      "Training Loss:  116.44284058\n",
      "################################  1500  ################################\n",
      "Training Loss:  114.46404266\n",
      "################################  1600  ################################\n",
      "Training Loss:  150.64942932\n",
      "################################  1700  ################################\n",
      "Training Loss:  110.67678833\n",
      "################################  1800  ################################\n",
      "Training Loss:  72.26303101\n",
      "################################  1900  ################################\n",
      "Training Loss:  194.91833496\n",
      "Final training Loss:  353.61953735\n",
      "\n",
      "Running model (trial=4, mod=128, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  606646.25\n",
      "################################  100  ################################\n",
      "Training Loss:  112.19029236\n",
      "################################  200  ################################\n",
      "Training Loss:  97.75183868\n",
      "################################  300  ################################\n",
      "Training Loss:  69.65609741\n",
      "################################  400  ################################\n",
      "Training Loss:  53.60891342\n",
      "################################  500  ################################\n",
      "Training Loss:  53.68397522\n",
      "################################  600  ################################\n",
      "Training Loss:  55.1158905\n",
      "################################  700  ################################\n",
      "Training Loss:  63.2789917\n",
      "################################  800  ################################\n",
      "Training Loss:  51.73484802\n",
      "################################  900  ################################\n",
      "Training Loss:  52.53054428\n",
      "Epoch   928: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.49494934\n",
      "################################  1100  ################################\n",
      "Training Loss:  50.6034584\n",
      "################################  1200  ################################\n",
      "Training Loss:  53.46697235\n",
      "################################  1300  ################################\n",
      "Training Loss:  56.06822205\n",
      "################################  1400  ################################\n",
      "Training Loss:  69.491539\n",
      "################################  1500  ################################\n",
      "Training Loss:  50.80367661\n",
      "################################  1600  ################################\n",
      "Training Loss:  50.95677567\n",
      "################################  1700  ################################\n",
      "Training Loss:  51.05820084\n",
      "Epoch  1718: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  49.62965393\n",
      "################################  1900  ################################\n",
      "Training Loss:  50.32931137\n",
      "Final training Loss:  57.21354675\n",
      "\n",
      "Running model (trial=4, mod=129, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  863747.375\n",
      "################################  100  ################################\n",
      "Training Loss:  113.37612152\n",
      "################################  200  ################################\n",
      "Training Loss:  115.97270966\n",
      "################################  300  ################################\n",
      "Training Loss:  113.45485687\n",
      "################################  400  ################################\n",
      "Training Loss:  114.487854\n",
      "################################  500  ################################\n",
      "Training Loss:  115.14131927\n",
      "Epoch   518: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  113.0737915\n",
      "################################  700  ################################\n",
      "Training Loss:  114.39264679\n",
      "Epoch   719: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  113.30723572\n",
      "################################  900  ################################\n",
      "Training Loss:  112.08433533\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.96530914\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.31916809\n",
      "Epoch  1124: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.53525543\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.21755219\n",
      "################################  1400  ################################\n",
      "Training Loss:  110.93716431\n",
      "################################  1500  ################################\n",
      "Training Loss:  62.33162689\n",
      "################################  1600  ################################\n",
      "Training Loss:  64.4536972\n",
      "################################  1700  ################################\n",
      "Training Loss:  59.02295303\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.32994843\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.4310379\n",
      "Final training Loss:  54.94327545\n",
      "\n",
      "Running model (trial=4, mod=130, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  28572.6953125\n",
      "################################  100  ################################\n",
      "Training Loss:  122.49529266\n",
      "################################  200  ################################\n",
      "Training Loss:  119.2959671\n",
      "################################  300  ################################\n",
      "Training Loss:  119.77684784\n",
      "################################  400  ################################\n",
      "Training Loss:  127.04573822\n",
      "################################  500  ################################\n",
      "Training Loss:  128.26525879\n",
      "################################  600  ################################\n",
      "Training Loss:  122.83124542\n",
      "################################  700  ################################\n",
      "Training Loss:  126.08873749\n",
      "################################  800  ################################\n",
      "Training Loss:  135.62353516\n",
      "Epoch   859: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  118.67313385\n",
      "################################  1000  ################################\n",
      "Training Loss:  113.51567841\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.08240509\n",
      "Epoch  1183: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.51720428\n",
      "################################  1300  ################################\n",
      "Training Loss:  122.53986359\n",
      "################################  1400  ################################\n",
      "Training Loss:  128.64764404\n",
      "################################  1500  ################################\n",
      "Training Loss:  112.73565674\n",
      "Epoch  1514: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.81655884\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.55380249\n",
      "Epoch  1715: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.0428772\n",
      "################################  1900  ################################\n",
      "Training Loss:  118.92198181\n",
      "Epoch  1916: reducing learning rate of group 0 to 1.6384e-02.\n",
      "Final training Loss:  115.52809143\n",
      "\n",
      "Running model (trial=4, mod=131, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5476.45117188\n",
      "################################  100  ################################\n",
      "Training Loss:  58.58015823\n",
      "################################  200  ################################\n",
      "Training Loss:  56.55242157\n",
      "################################  300  ################################\n",
      "Training Loss:  74.54737854\n",
      "################################  400  ################################\n",
      "Training Loss:  57.98361969\n",
      "################################  500  ################################\n",
      "Training Loss:  49.66786194\n",
      "################################  600  ################################\n",
      "Training Loss:  559.41436768\n",
      "################################  700  ################################\n",
      "Training Loss:  61.76010513\n",
      "Epoch   780: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  53.65882111\n",
      "################################  900  ################################\n",
      "Training Loss:  60.38679504\n",
      "################################  1000  ################################\n",
      "Training Loss:  51.97417831\n",
      "################################  1100  ################################\n",
      "Training Loss:  43.15554428\n",
      "################################  1200  ################################\n",
      "Training Loss:  150.04766846\n",
      "################################  1300  ################################\n",
      "Training Loss:  121.03128815\n",
      "Epoch  1380: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  119.2787323\n",
      "################################  1500  ################################\n",
      "Training Loss:  117.5148468\n",
      "Epoch  1581: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.75160217\n",
      "################################  1700  ################################\n",
      "Training Loss:  118.63717651\n",
      "Epoch  1782: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.1584549\n",
      "################################  1900  ################################\n",
      "Training Loss:  120.39400482\n",
      "Epoch  1983: reducing learning rate of group 0 to 1.6384e-02.\n",
      "Final training Loss:  118.12734222\n",
      "\n",
      "Running model (trial=4, mod=132, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10657.39746094\n",
      "################################  100  ################################\n",
      "Training Loss:  118.19815063\n",
      "################################  200  ################################\n",
      "Training Loss:  115.54676056\n",
      "################################  300  ################################\n",
      "Training Loss:  113.56204987\n",
      "################################  400  ################################\n",
      "Training Loss:  114.38722229\n",
      "################################  500  ################################\n",
      "Training Loss:  113.54034424\n",
      "################################  600  ################################\n",
      "Training Loss:  113.68791962\n",
      "################################  700  ################################\n",
      "Training Loss:  115.01480103\n",
      "################################  800  ################################\n",
      "Training Loss:  120.17526245\n",
      "################################  900  ################################\n",
      "Training Loss:  116.8875885\n",
      "Epoch   944: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  119.31721497\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.3502121\n",
      "Epoch  1145: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.86652374\n",
      "################################  1300  ################################\n",
      "Training Loss:  113.0958786\n",
      "Epoch  1346: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.6969223\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.31840515\n",
      "################################  1600  ################################\n",
      "Training Loss:  112.56879425\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.70615005\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.30000687\n",
      "################################  1900  ################################\n",
      "Training Loss:  48.00587463\n",
      "Final training Loss:  55.07320404\n",
      "\n",
      "Running model (trial=4, mod=133, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2035259.75\n",
      "################################  100  ################################\n",
      "Training Loss:  134.28005981\n",
      "################################  200  ################################\n",
      "Training Loss:  116.69908905\n",
      "################################  300  ################################\n",
      "Training Loss:  118.32165527\n",
      "################################  400  ################################\n",
      "Training Loss:  116.4985733\n",
      "################################  500  ################################\n",
      "Training Loss:  117.04071045\n",
      "################################  600  ################################\n",
      "Training Loss:  120.96224976\n",
      "################################  700  ################################\n",
      "Training Loss:  116.88798523\n",
      "Epoch   781: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  121.30575562\n",
      "################################  900  ################################\n",
      "Training Loss:  120.74428558\n",
      "Epoch   982: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.97096252\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.01799774\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.82134247\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.51285553\n",
      "################################  1400  ################################\n",
      "Training Loss:  122.16138458\n",
      "################################  1500  ################################\n",
      "Training Loss:  119.45254517\n",
      "################################  1600  ################################\n",
      "Training Loss:  116.7036972\n",
      "Epoch  1659: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.40035248\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.49996185\n",
      "Epoch  1860: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  118.98834991\n",
      "Final training Loss:  117.42793274\n",
      "\n",
      "Running model (trial=4, mod=134, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8528945.0\n",
      "################################  100  ################################\n",
      "Training Loss:  65.44207001\n",
      "################################  200  ################################\n",
      "Training Loss:  58.20496368\n",
      "################################  300  ################################\n",
      "Training Loss:  76.64302826\n",
      "Epoch   317: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  63.01469421\n",
      "################################  500  ################################\n",
      "Training Loss:  84.17402649\n",
      "Epoch   518: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  56.28058624\n",
      "################################  700  ################################\n",
      "Training Loss:  60.10325623\n",
      "################################  800  ################################\n",
      "Training Loss:  102.0266571\n",
      "################################  900  ################################\n",
      "Training Loss:  55.1468277\n",
      "################################  1000  ################################\n",
      "Training Loss:  57.92323685\n",
      "################################  1100  ################################\n",
      "Training Loss:  57.9718895\n",
      "Epoch  1108: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.96174622\n",
      "################################  1300  ################################\n",
      "Training Loss:  56.17223358\n",
      "Epoch  1309: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  77.20980072\n",
      "################################  1500  ################################\n",
      "Training Loss:  55.29560089\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.98086166\n",
      "Epoch  1686: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  89.57582092\n",
      "################################  1800  ################################\n",
      "Training Loss:  54.5495491\n",
      "Epoch  1887: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.39888\n",
      "Final training Loss:  57.83652496\n",
      "\n",
      "Running model (trial=4, mod=135, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  41631.9921875\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=136, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5282.4765625\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=137, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1011167.8125\n",
      "################################  100  ################################\n",
      "Training Loss:  79.02526855\n",
      "################################  200  ################################\n",
      "Training Loss:  62.45914459\n",
      "################################  300  ################################\n",
      "Training Loss:  61.75062943\n",
      "################################  400  ################################\n",
      "Training Loss:  59.96569443\n",
      "################################  500  ################################\n",
      "Training Loss:  58.69137192\n",
      "################################  600  ################################\n",
      "Training Loss:  54.97164536\n",
      "################################  700  ################################\n",
      "Training Loss:  50.03932571\n",
      "################################  800  ################################\n",
      "Training Loss:  49.00675964\n",
      "################################  900  ################################\n",
      "Training Loss:  44.08803177\n",
      "################################  1000  ################################\n",
      "Training Loss:  44.34726334\n",
      "################################  1100  ################################\n",
      "Training Loss:  43.06912231\n",
      "################################  1200  ################################\n",
      "Training Loss:  43.34368134\n",
      "################################  1300  ################################\n",
      "Training Loss:  42.84957504\n",
      "Epoch  1335: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  42.41455078\n",
      "################################  1500  ################################\n",
      "Training Loss:  42.449543\n",
      "################################  1600  ################################\n",
      "Training Loss:  42.6563797\n",
      "################################  1700  ################################\n",
      "Training Loss:  41.67673111\n",
      "################################  1800  ################################\n",
      "Training Loss:  41.51168442\n",
      "Epoch  1894: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.20911026\n",
      "Final training Loss:  40.79118347\n",
      "\n",
      "Running model (trial=4, mod=138, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1168265.75\n",
      "################################  100  ################################\n",
      "Training Loss:  72.14858246\n",
      "################################  200  ################################\n",
      "Training Loss:  53.3730278\n",
      "################################  300  ################################\n",
      "Training Loss:  48.13641357\n",
      "################################  400  ################################\n",
      "Training Loss:  46.84659195\n",
      "################################  500  ################################\n",
      "Training Loss:  46.15093231\n",
      "################################  600  ################################\n",
      "Training Loss:  44.63639832\n",
      "################################  700  ################################\n",
      "Training Loss:  44.40462112\n",
      "################################  800  ################################\n",
      "Training Loss:  42.77333832\n",
      "################################  900  ################################\n",
      "Training Loss:  42.95703506\n",
      "################################  1000  ################################\n",
      "Training Loss:  41.03076935\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.65846252\n",
      "################################  1200  ################################\n",
      "Training Loss:  39.98461914\n",
      "################################  1300  ################################\n",
      "Training Loss:  50.87466431\n",
      "################################  1400  ################################\n",
      "Training Loss:  37.65501022\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.07654572\n",
      "################################  1600  ################################\n",
      "Training Loss:  65.63397217\n",
      "Epoch  1636: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  44.42067337\n",
      "################################  1800  ################################\n",
      "Training Loss:  46.79839706\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.71364212\n",
      "Epoch  1953: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  44.42801666\n",
      "\n",
      "Running model (trial=4, mod=139, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  21397.7109375\n",
      "################################  100  ################################\n",
      "Training Loss:  117.960495\n",
      "################################  200  ################################\n",
      "Training Loss:  116.86252594\n",
      "Epoch   259: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  116.14078522\n",
      "################################  400  ################################\n",
      "Training Loss:  116.84482574\n",
      "Epoch   469: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  115.83071899\n",
      "################################  600  ################################\n",
      "Training Loss:  116.24967194\n",
      "Epoch   670: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  117.48052216\n",
      "################################  800  ################################\n",
      "Training Loss:  117.12492371\n",
      "Epoch   871: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  117.24626923\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.60527039\n",
      "Epoch  1072: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.32575989\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.86373901\n",
      "Epoch  1273: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  117.07691193\n",
      "################################  1400  ################################\n",
      "Training Loss:  117.05483246\n",
      "Epoch  1474: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.94811249\n",
      "################################  1600  ################################\n",
      "Training Loss:  117.53100586\n",
      "Epoch  1675: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  117.05697632\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.05506897\n",
      "Epoch  1876: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.35301208\n",
      "Final training Loss:  115.40953827\n",
      "\n",
      "Running model (trial=4, mod=140, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1606352.75\n",
      "################################  100  ################################\n",
      "Training Loss:  121.96359253\n",
      "################################  200  ################################\n",
      "Training Loss:  89.50132751\n",
      "################################  300  ################################\n",
      "Training Loss:  62.79314423\n",
      "################################  400  ################################\n",
      "Training Loss:  59.30407333\n",
      "################################  500  ################################\n",
      "Training Loss:  56.23023605\n",
      "################################  600  ################################\n",
      "Training Loss:  56.7336235\n",
      "################################  700  ################################\n",
      "Training Loss:  52.02526855\n",
      "################################  800  ################################\n",
      "Training Loss:  54.37456512\n",
      "################################  900  ################################\n",
      "Training Loss:  51.43386841\n",
      "################################  1000  ################################\n",
      "Training Loss:  60.72356415\n",
      "################################  1100  ################################\n",
      "Training Loss:  59.59288406\n",
      "################################  1200  ################################\n",
      "Training Loss:  45.70734787\n",
      "################################  1300  ################################\n",
      "Training Loss:  48.1459198\n",
      "################################  1400  ################################\n",
      "Training Loss:  46.44681931\n",
      "################################  1500  ################################\n",
      "Training Loss:  43.95057297\n",
      "################################  1600  ################################\n",
      "Training Loss:  46.03782272\n",
      "################################  1700  ################################\n",
      "Training Loss:  44.52096176\n",
      "################################  1800  ################################\n",
      "Training Loss:  60.96917725\n",
      "################################  1900  ################################\n",
      "Training Loss:  49.37567139\n",
      "Final training Loss:  42.66453934\n",
      "\n",
      "Running model (trial=4, mod=141, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4462.75878906\n",
      "################################  100  ################################\n",
      "Training Loss:  180.59819031\n",
      "################################  200  ################################\n",
      "Training Loss:  87.097435\n",
      "################################  300  ################################\n",
      "Training Loss:  88.82627869\n",
      "################################  400  ################################\n",
      "Training Loss:  94.43606567\n",
      "################################  500  ################################\n",
      "Training Loss:  470.76879883\n",
      "################################  600  ################################\n",
      "Training Loss:  54.84947586\n",
      "################################  700  ################################\n",
      "Training Loss:  50.82751846\n",
      "################################  800  ################################\n",
      "Training Loss:  49.55826187\n",
      "################################  900  ################################\n",
      "Training Loss:  47.72687912\n",
      "################################  1000  ################################\n",
      "Training Loss:  47.42021179\n",
      "################################  1100  ################################\n",
      "Training Loss:  45.47393417\n",
      "################################  1200  ################################\n",
      "Training Loss:  44.2560997\n",
      "################################  1300  ################################\n",
      "Training Loss:  43.47852325\n",
      "################################  1400  ################################\n",
      "Training Loss:  42.82440948\n",
      "################################  1500  ################################\n",
      "Training Loss:  42.73564148\n",
      "################################  1600  ################################\n",
      "Training Loss:  42.6739502\n",
      "################################  1700  ################################\n",
      "Training Loss:  41.3611908\n",
      "################################  1800  ################################\n",
      "Training Loss:  44.78144455\n",
      "################################  1900  ################################\n",
      "Training Loss:  40.56603241\n",
      "Final training Loss:  41.12955093\n",
      "\n",
      "Running model (trial=4, mod=142, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  171086.25\n",
      "################################  100  ################################\n",
      "Training Loss:  114.07602692\n",
      "################################  200  ################################\n",
      "Training Loss:  205.16387939\n",
      "################################  300  ################################\n",
      "Training Loss:  95.34497833\n",
      "################################  400  ################################\n",
      "Training Loss:  180.2902832\n",
      "################################  500  ################################\n",
      "Training Loss:  71.71756744\n",
      "Epoch   574: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  100.6631012\n",
      "################################  700  ################################\n",
      "Training Loss:  69.05545044\n",
      "################################  800  ################################\n",
      "Training Loss:  322.64569092\n",
      "################################  900  ################################\n",
      "Training Loss:  96.41164398\n",
      "Epoch   970: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  67.15459442\n",
      "################################  1100  ################################\n",
      "Training Loss:  147.35229492\n",
      "################################  1200  ################################\n",
      "Training Loss:  103.99040222\n",
      "################################  1300  ################################\n",
      "Training Loss:  253.56347656\n",
      "################################  1400  ################################\n",
      "Training Loss:  88.7942276\n",
      "Epoch  1404: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  100.0690155\n",
      "################################  1600  ################################\n",
      "Training Loss:  64.77365112\n",
      "################################  1700  ################################\n",
      "Training Loss:  77.05101013\n",
      "################################  1800  ################################\n",
      "Training Loss:  47.35065079\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.31542587\n",
      "Final training Loss:  98.39047241\n",
      "\n",
      "Running model (trial=4, mod=143, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  20183.015625\n",
      "################################  100  ################################\n",
      "Training Loss:  106.5976944\n",
      "################################  200  ################################\n",
      "Training Loss:  63.44530869\n",
      "################################  300  ################################\n",
      "Training Loss:  57.96396255\n",
      "################################  400  ################################\n",
      "Training Loss:  55.42568207\n",
      "################################  500  ################################\n",
      "Training Loss:  54.81478119\n",
      "################################  600  ################################\n",
      "Training Loss:  63.30739975\n",
      "################################  700  ################################\n",
      "Training Loss:  57.12900162\n",
      "Epoch   727: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  61.01988983\n",
      "################################  900  ################################\n",
      "Training Loss:  57.49315643\n",
      "Epoch   928: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.69404602\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.18972778\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.90145111\n",
      "Epoch  1231: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.94013977\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.80604935\n",
      "Epoch  1432: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.98403931\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.9325676\n",
      "Epoch  1633: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.71141815\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.25855255\n",
      "Epoch  1834: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.54795837\n",
      "Final training Loss:  52.64165878\n",
      "\n",
      "Running model (trial=4, mod=144, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  25089.40234375\n",
      "################################  100  ################################\n",
      "Training Loss:  61.5438385\n",
      "################################  200  ################################\n",
      "Training Loss:  51.08699417\n",
      "################################  300  ################################\n",
      "Training Loss:  47.15871048\n",
      "################################  400  ################################\n",
      "Training Loss:  46.18225479\n",
      "################################  500  ################################\n",
      "Training Loss:  44.38005447\n",
      "################################  600  ################################\n",
      "Training Loss:  54.40800476\n",
      "################################  700  ################################\n",
      "Training Loss:  42.0965538\n",
      "################################  800  ################################\n",
      "Training Loss:  40.06175613\n",
      "################################  900  ################################\n",
      "Training Loss:  54.96559906\n",
      "################################  1000  ################################\n",
      "Training Loss:  59.68925095\n",
      "Epoch  1008: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  44.35644531\n",
      "################################  1200  ################################\n",
      "Training Loss:  43.93703079\n",
      "Epoch  1209: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  40.88697052\n",
      "################################  1400  ################################\n",
      "Training Loss:  50.65038681\n",
      "Epoch  1410: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  48.28090668\n",
      "################################  1600  ################################\n",
      "Training Loss:  48.84719086\n",
      "Epoch  1611: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  40.59999847\n",
      "################################  1800  ################################\n",
      "Training Loss:  41.70337677\n",
      "Epoch  1812: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  39.01131439\n",
      "Final training Loss:  38.84159851\n",
      "\n",
      "Running model (trial=4, mod=145, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1250715.625\n",
      "################################  100  ################################\n",
      "Training Loss:  114.02088928\n",
      "################################  200  ################################\n",
      "Training Loss:  73.19760132\n",
      "################################  300  ################################\n",
      "Training Loss:  63.85806656\n",
      "################################  400  ################################\n",
      "Training Loss:  67.27919006\n",
      "################################  500  ################################\n",
      "Training Loss:  56.9353981\n",
      "################################  600  ################################\n",
      "Training Loss:  55.53931046\n",
      "################################  700  ################################\n",
      "Training Loss:  55.51361084\n",
      "################################  800  ################################\n",
      "Training Loss:  62.31410217\n",
      "################################  900  ################################\n",
      "Training Loss:  55.83551025\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.439991\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.75413132\n",
      "################################  1200  ################################\n",
      "Training Loss:  55.1014328\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.82767868\n",
      "################################  1400  ################################\n",
      "Training Loss:  47.45441437\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.46450806\n",
      "################################  1600  ################################\n",
      "Training Loss:  45.98265457\n",
      "################################  1700  ################################\n",
      "Training Loss:  42.0766983\n",
      "################################  1800  ################################\n",
      "Training Loss:  57.14952087\n",
      "################################  1900  ################################\n",
      "Training Loss:  43.07735062\n",
      "Epoch  1907: reducing learning rate of group 0 to 4.0000e-02.\n",
      "Final training Loss:  46.37979889\n",
      "\n",
      "Running model (trial=4, mod=146, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  570137.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  89.62339783\n",
      "################################  200  ################################\n",
      "Training Loss:  71.34367371\n",
      "################################  300  ################################\n",
      "Training Loss:  51.75073624\n",
      "################################  400  ################################\n",
      "Training Loss:  51.95462799\n",
      "################################  500  ################################\n",
      "Training Loss:  54.12149811\n",
      "Epoch   539: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  52.8621254\n",
      "################################  700  ################################\n",
      "Training Loss:  51.83381271\n",
      "Epoch   740: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  58.65151596\n",
      "################################  900  ################################\n",
      "Training Loss:  49.39352417\n",
      "Epoch   941: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  49.06222534\n",
      "################################  1100  ################################\n",
      "Training Loss:  50.97058105\n",
      "Epoch  1142: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  55.38985443\n",
      "################################  1300  ################################\n",
      "Training Loss:  49.24151993\n",
      "################################  1400  ################################\n",
      "Training Loss:  49.44631577\n",
      "Epoch  1425: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.68262863\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.69271088\n",
      "Epoch  1626: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  48.25574112\n",
      "################################  1800  ################################\n",
      "Training Loss:  48.40121078\n",
      "Epoch  1827: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  48.06162643\n",
      "Final training Loss:  48.9534874\n",
      "\n",
      "Running model (trial=4, mod=147, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1103869.125\n",
      "################################  100  ################################\n",
      "Training Loss:  56.0952301\n",
      "################################  200  ################################\n",
      "Training Loss:  52.44074249\n",
      "################################  300  ################################\n",
      "Training Loss:  51.2400589\n",
      "################################  400  ################################\n",
      "Training Loss:  48.81568909\n",
      "Epoch   452: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  48.2414093\n",
      "################################  600  ################################\n",
      "Training Loss:  47.96459198\n",
      "Epoch   653: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  47.14190292\n",
      "################################  800  ################################\n",
      "Training Loss:  46.28975296\n",
      "Epoch   854: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  46.22619629\n",
      "################################  1000  ################################\n",
      "Training Loss:  45.70365143\n",
      "################################  1100  ################################\n",
      "Training Loss:  45.56522369\n",
      "Epoch  1178: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.84037781\n",
      "################################  1300  ################################\n",
      "Training Loss:  45.12504578\n",
      "################################  1400  ################################\n",
      "Training Loss:  44.85751343\n",
      "Epoch  1476: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  44.82865524\n",
      "################################  1600  ################################\n",
      "Training Loss:  44.76753235\n",
      "################################  1700  ################################\n",
      "Training Loss:  46.2770195\n",
      "################################  1800  ################################\n",
      "Training Loss:  44.57598114\n",
      "################################  1900  ################################\n",
      "Training Loss:  44.47823715\n",
      "Final training Loss:  44.52830505\n",
      "\n",
      "Running model (trial=4, mod=148, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  90644992.0\n",
      "################################  100  ################################\n",
      "Training Loss:  135.99508667\n",
      "################################  200  ################################\n",
      "Training Loss:  44.28147125\n",
      "################################  300  ################################\n",
      "Training Loss:  40.84176636\n",
      "################################  400  ################################\n",
      "Training Loss:  39.68341446\n",
      "################################  500  ################################\n",
      "Training Loss:  41.19620514\n",
      "################################  600  ################################\n",
      "Training Loss:  40.24928665\n",
      "################################  700  ################################\n",
      "Training Loss:  40.94242859\n",
      "################################  800  ################################\n",
      "Training Loss:  39.63532257\n",
      "################################  900  ################################\n",
      "Training Loss:  39.65379333\n",
      "################################  1000  ################################\n",
      "Training Loss:  41.50404358\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.59425735\n",
      "################################  1200  ################################\n",
      "Training Loss:  39.95615768\n",
      "################################  1300  ################################\n",
      "Training Loss:  38.25681305\n",
      "################################  1400  ################################\n",
      "Training Loss:  197.35748291\n",
      "Epoch  1482: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.60119247\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.52323151\n",
      "Epoch  1683: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.81642151\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.35663223\n",
      "Epoch  1884: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.29037094\n",
      "Final training Loss:  52.26776123\n",
      "\n",
      "Running model (trial=4, mod=149, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.477876505758925e+16\n",
      "################################  100  ################################\n",
      "Training Loss:  224814.03125\n",
      "################################  200  ################################\n",
      "Training Loss:  136431.765625\n",
      "################################  300  ################################\n",
      "Training Loss:  310773.28125\n",
      "################################  400  ################################\n",
      "Training Loss:  583130.875\n",
      "################################  500  ################################\n",
      "Training Loss:  879130.1875\n",
      "################################  600  ################################\n",
      "Training Loss:  1242424.375\n",
      "Epoch   668: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  116629.6171875\n",
      "################################  800  ################################\n",
      "Training Loss:  77440.96875\n",
      "################################  900  ################################\n",
      "Training Loss:  81128.4921875\n",
      "################################  1000  ################################\n",
      "Training Loss:  87085.625\n",
      "Epoch  1014: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  770420.0\n",
      "################################  1200  ################################\n",
      "Training Loss:  2827.56469727\n",
      "################################  1300  ################################\n",
      "Training Loss:  1730108.625\n",
      "################################  1400  ################################\n",
      "Training Loss:  3313.6796875\n",
      "Epoch  1478: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  910735.375\n",
      "################################  1600  ################################\n",
      "Training Loss:  188168.390625\n",
      "Epoch  1679: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  546692.9375\n",
      "################################  1800  ################################\n",
      "Training Loss:  1281.28833008\n",
      "################################  1900  ################################\n",
      "Training Loss:  1273.9041748\n",
      "Final training Loss:  1274.01025391\n",
      "\n",
      "Running model (trial=5, mod=150, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  29031.89453125\n",
      "################################  100  ################################\n",
      "Training Loss:  114.10622406\n",
      "################################  200  ################################\n",
      "Training Loss:  109.50132751\n",
      "################################  300  ################################\n",
      "Training Loss:  96.64211273\n",
      "################################  400  ################################\n",
      "Training Loss:  82.83927155\n",
      "################################  500  ################################\n",
      "Training Loss:  68.32798767\n",
      "################################  600  ################################\n",
      "Training Loss:  60.18511581\n",
      "################################  700  ################################\n",
      "Training Loss:  64.55859375\n",
      "################################  800  ################################\n",
      "Training Loss:  54.82003021\n",
      "Epoch   831: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  53.67613983\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.60761261\n",
      "Epoch  1076: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.33643723\n",
      "################################  1200  ################################\n",
      "Training Loss:  56.57055283\n",
      "Epoch  1277: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.972435\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.87146759\n",
      "Epoch  1478: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.06307983\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.93844986\n",
      "Epoch  1679: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.28997421\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.12821198\n",
      "Epoch  1880: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.15949631\n",
      "Final training Loss:  52.89577103\n",
      "\n",
      "Running model (trial=5, mod=151, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  670446.875\n",
      "################################  100  ################################\n",
      "Training Loss:  106.93223572\n",
      "################################  200  ################################\n",
      "Training Loss:  96.15814209\n",
      "################################  300  ################################\n",
      "Training Loss:  75.20333862\n",
      "################################  400  ################################\n",
      "Training Loss:  54.72737503\n",
      "################################  500  ################################\n",
      "Training Loss:  54.68840408\n",
      "################################  600  ################################\n",
      "Training Loss:  56.61352921\n",
      "Epoch   640: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  55.16039658\n",
      "################################  800  ################################\n",
      "Training Loss:  55.10344696\n",
      "Epoch   841: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  55.77841568\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.61912155\n",
      "Epoch  1042: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.45133591\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.12353897\n",
      "Epoch  1243: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  55.58283234\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.46492767\n",
      "Epoch  1444: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.55157089\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.12308121\n",
      "Epoch  1645: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.55131531\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.35429382\n",
      "Epoch  1846: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.74860382\n",
      "Final training Loss:  55.05328369\n",
      "\n",
      "Running model (trial=5, mod=152, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2688.50195312\n",
      "################################  100  ################################\n",
      "Training Loss:  60.43517303\n",
      "################################  200  ################################\n",
      "Training Loss:  57.04932022\n",
      "################################  300  ################################\n",
      "Training Loss:  61.7458992\n",
      "################################  400  ################################\n",
      "Training Loss:  51.79319382\n",
      "################################  500  ################################\n",
      "Training Loss:  187.87245178\n",
      "################################  600  ################################\n",
      "Training Loss:  54.15006256\n",
      "Epoch   696: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  53.67945862\n",
      "################################  800  ################################\n",
      "Training Loss:  63.34153366\n",
      "Epoch   897: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  56.24757004\n",
      "################################  1000  ################################\n",
      "Training Loss:  52.66366196\n",
      "Epoch  1098: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  68.79405975\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.84354401\n",
      "Epoch  1299: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.94066238\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.46491623\n",
      "Epoch  1500: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.85121918\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.64041138\n",
      "################################  1700  ################################\n",
      "Epoch  1701: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Training Loss:  55.14394379\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.86793518\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.20650101\n",
      "Epoch  1902: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  52.78543091\n",
      "\n",
      "Running model (trial=5, mod=153, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  725574.25\n",
      "################################  100  ################################\n",
      "Training Loss:  123.09915161\n",
      "################################  200  ################################\n",
      "Training Loss:  114.15638733\n",
      "Epoch   213: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  114.06607819\n",
      "################################  400  ################################\n",
      "Training Loss:  114.1013031\n",
      "Epoch   414: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  114.03794861\n",
      "################################  600  ################################\n",
      "Training Loss:  114.2228775\n",
      "Epoch   615: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  114.69035339\n",
      "################################  800  ################################\n",
      "Training Loss:  114.04694366\n",
      "Epoch   816: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  114.51661682\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.11466217\n",
      "Epoch  1017: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.0841217\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.13747406\n",
      "Epoch  1218: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.0247345\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.06177521\n",
      "Epoch  1419: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  114.169487\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.03203583\n",
      "Epoch  1620: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.14424896\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.08937836\n",
      "Epoch  1821: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.09498596\n",
      "Final training Loss:  113.97879791\n",
      "\n",
      "Running model (trial=5, mod=154, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  443531.125\n",
      "################################  100  ################################\n",
      "Training Loss:  127.69658661\n",
      "################################  200  ################################\n",
      "Training Loss:  116.47376251\n",
      "Epoch   268: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  116.18692017\n",
      "################################  400  ################################\n",
      "Training Loss:  115.10710907\n",
      "################################  500  ################################\n",
      "Training Loss:  115.00611877\n",
      "################################  600  ################################\n",
      "Training Loss:  114.24793243\n",
      "################################  700  ################################\n",
      "Training Loss:  114.06452942\n",
      "################################  800  ################################\n",
      "Training Loss:  114.57627106\n",
      "################################  900  ################################\n",
      "Training Loss:  116.16233826\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.49898529\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.25982666\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.478302\n",
      "Epoch  1226: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.29533386\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.80348969\n",
      "Epoch  1427: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  114.20549011\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.63394165\n",
      "Epoch  1628: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  113.62660217\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.25959778\n",
      "Epoch  1829: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  113.98351288\n",
      "Final training Loss:  113.72665405\n",
      "\n",
      "Running model (trial=5, mod=155, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  398572.25\n",
      "################################  100  ################################\n",
      "Training Loss:  113.58625793\n",
      "################################  200  ################################\n",
      "Training Loss:  84.17575073\n",
      "################################  300  ################################\n",
      "Training Loss:  63.03762817\n",
      "################################  400  ################################\n",
      "Training Loss:  57.57673264\n",
      "################################  500  ################################\n",
      "Training Loss:  55.91057205\n",
      "################################  600  ################################\n",
      "Training Loss:  54.60186386\n",
      "################################  700  ################################\n",
      "Training Loss:  55.69521713\n",
      "################################  800  ################################\n",
      "Training Loss:  59.44402695\n",
      "################################  900  ################################\n",
      "Training Loss:  53.14922333\n",
      "################################  1000  ################################\n",
      "Training Loss:  65.61551666\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.57424164\n",
      "################################  1200  ################################\n",
      "Training Loss:  59.90224457\n",
      "################################  1300  ################################\n",
      "Training Loss:  58.73934555\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.64013672\n",
      "################################  1500  ################################\n",
      "Training Loss:  57.31930542\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.14543152\n",
      "################################  1700  ################################\n",
      "Training Loss:  68.7208252\n",
      "################################  1800  ################################\n",
      "Training Loss:  75.62805176\n",
      "################################  1900  ################################\n",
      "Training Loss:  50.49337006\n",
      "Final training Loss:  50.64894485\n",
      "\n",
      "Running model (trial=5, mod=156, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8465.55371094\n",
      "################################  100  ################################\n",
      "Training Loss:  135.46998596\n",
      "################################  200  ################################\n",
      "Training Loss:  119.06336975\n",
      "################################  300  ################################\n",
      "Training Loss:  119.34246063\n",
      "################################  400  ################################\n",
      "Training Loss:  119.08010101\n",
      "################################  500  ################################\n",
      "Training Loss:  122.68522644\n",
      "################################  600  ################################\n",
      "Training Loss:  119.07138062\n",
      "################################  700  ################################\n",
      "Training Loss:  117.59784698\n",
      "################################  800  ################################\n",
      "Training Loss:  115.45635986\n",
      "################################  900  ################################\n",
      "Training Loss:  136.87376404\n",
      "################################  1000  ################################\n",
      "Training Loss:  121.10774994\n",
      "Epoch  1011: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  125.94485474\n",
      "################################  1200  ################################\n",
      "Training Loss:  115.85495758\n",
      "Epoch  1271: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  117.45632172\n",
      "################################  1400  ################################\n",
      "Training Loss:  185.05697632\n",
      "################################  1500  ################################\n",
      "Training Loss:  120.24134064\n",
      "################################  1600  ################################\n",
      "Training Loss:  128.10339355\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.78646088\n",
      "Epoch  1762: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  136.03102112\n",
      "################################  1900  ################################\n",
      "Training Loss:  119.01075745\n",
      "Final training Loss:  129.90063477\n",
      "\n",
      "Running model (trial=5, mod=157, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54857.27734375\n",
      "################################  100  ################################\n",
      "Training Loss:  193.7568512\n",
      "################################  200  ################################\n",
      "Training Loss:  95.82432556\n",
      "################################  300  ################################\n",
      "Training Loss:  81.09661102\n",
      "################################  400  ################################\n",
      "Training Loss:  60.06419754\n",
      "################################  500  ################################\n",
      "Training Loss:  77.46664429\n",
      "Epoch   564: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  96.07949066\n",
      "################################  700  ################################\n",
      "Training Loss:  86.98716736\n",
      "Epoch   765: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  72.59490204\n",
      "################################  900  ################################\n",
      "Training Loss:  65.04600525\n",
      "Epoch   966: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  59.35739517\n",
      "################################  1100  ################################\n",
      "Training Loss:  69.36786652\n",
      "Epoch  1167: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  70.8709259\n",
      "################################  1300  ################################\n",
      "Training Loss:  59.87542725\n",
      "Epoch  1368: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  61.02355576\n",
      "################################  1500  ################################\n",
      "Training Loss:  62.40449142\n",
      "Epoch  1569: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  62.07556534\n",
      "################################  1700  ################################\n",
      "Training Loss:  67.2330246\n",
      "Epoch  1770: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  58.48944855\n",
      "################################  1900  ################################\n",
      "Training Loss:  72.79157257\n",
      "Epoch  1971: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Final training Loss:  62.68764114\n",
      "\n",
      "Running model (trial=5, mod=158, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1247707.0\n",
      "################################  100  ################################\n",
      "Training Loss:  118.65529633\n",
      "################################  200  ################################\n",
      "Training Loss:  115.21342468\n",
      "################################  300  ################################\n",
      "Training Loss:  113.50827789\n",
      "################################  400  ################################\n",
      "Training Loss:  107.81650543\n",
      "################################  500  ################################\n",
      "Training Loss:  99.24098206\n",
      "################################  600  ################################\n",
      "Training Loss:  88.35102081\n",
      "################################  700  ################################\n",
      "Training Loss:  64.48033905\n",
      "################################  800  ################################\n",
      "Training Loss:  54.79768372\n",
      "################################  900  ################################\n",
      "Training Loss:  53.69789886\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.36834335\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.62562561\n",
      "################################  1200  ################################\n",
      "Training Loss:  55.0932579\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.93054962\n",
      "Epoch  1331: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.27096558\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.52391052\n",
      "Epoch  1532: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.28807449\n",
      "################################  1700  ################################\n",
      "Training Loss:  55.6541214\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.64012909\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.54304504\n",
      "Epoch  1972: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  53.15387344\n",
      "\n",
      "Running model (trial=5, mod=159, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  696394.25\n",
      "################################  100  ################################\n",
      "Training Loss:  82.74720001\n",
      "################################  200  ################################\n",
      "Training Loss:  69.66612244\n",
      "################################  300  ################################\n",
      "Training Loss:  60.91140747\n",
      "################################  400  ################################\n",
      "Training Loss:  54.79471588\n",
      "################################  500  ################################\n",
      "Training Loss:  53.16459274\n",
      "Epoch   587: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  54.41539764\n",
      "################################  700  ################################\n",
      "Training Loss:  61.08451462\n",
      "Epoch   788: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  54.59538269\n",
      "################################  900  ################################\n",
      "Training Loss:  54.15756226\n",
      "Epoch   989: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.67617035\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.64281082\n",
      "Epoch  1190: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.92345047\n",
      "################################  1300  ################################\n",
      "Training Loss:  58.41174698\n",
      "Epoch  1391: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.60405731\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.52021027\n",
      "Epoch  1592: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  52.21817017\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.80752945\n",
      "Epoch  1793: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.1090889\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.91796875\n",
      "Epoch  1994: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Final training Loss:  51.91788864\n",
      "\n",
      "Running model (trial=5, mod=160, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4873.44287109\n",
      "################################  100  ################################\n",
      "Training Loss:  55.13125229\n",
      "################################  200  ################################\n",
      "Training Loss:  55.22026443\n",
      "################################  300  ################################\n",
      "Training Loss:  55.40826416\n",
      "################################  400  ################################\n",
      "Training Loss:  72.19205475\n",
      "################################  500  ################################\n",
      "Training Loss:  69.32149506\n",
      "################################  600  ################################\n",
      "Training Loss:  10854.21777344\n",
      "################################  700  ################################\n",
      "Training Loss:  122.08329773\n",
      "Epoch   778: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  117.15763092\n",
      "################################  900  ################################\n",
      "Training Loss:  116.2400589\n",
      "Epoch   979: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.85912323\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.71012878\n",
      "Epoch  1180: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  115.301651\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.46656036\n",
      "Epoch  1381: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.92933655\n",
      "################################  1500  ################################\n",
      "Training Loss:  115.06764221\n",
      "Epoch  1582: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.72972107\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.68569183\n",
      "Epoch  1783: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.62445831\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.5987854\n",
      "Epoch  1984: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  114.64440918\n",
      "\n",
      "Running model (trial=5, mod=161, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  723525.875\n",
      "################################  100  ################################\n",
      "Training Loss:  57.95843124\n",
      "################################  200  ################################\n",
      "Training Loss:  55.8200798\n",
      "################################  300  ################################\n",
      "Training Loss:  59.60226059\n",
      "################################  400  ################################\n",
      "Training Loss:  74.5743103\n",
      "################################  500  ################################\n",
      "Training Loss:  64.54531097\n",
      "################################  600  ################################\n",
      "Training Loss:  59.28524399\n",
      "################################  700  ################################\n",
      "Training Loss:  53.06309891\n",
      "################################  800  ################################\n",
      "Training Loss:  64.92121887\n",
      "Epoch   807: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  55.85368347\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.65457535\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.11161041\n",
      "################################  1200  ################################\n",
      "Training Loss:  90.1539917\n",
      "Epoch  1220: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  62.55421448\n",
      "################################  1400  ################################\n",
      "Training Loss:  60.80966187\n",
      "################################  1500  ################################\n",
      "Training Loss:  62.91255569\n",
      "################################  1600  ################################\n",
      "Training Loss:  68.61140442\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.80241013\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.76638031\n",
      "################################  1900  ################################\n",
      "Training Loss:  56.35580444\n",
      "Final training Loss:  50.90341187\n",
      "\n",
      "Running model (trial=5, mod=162, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3126550.25\n",
      "################################  100  ################################\n",
      "Training Loss:  122.63293457\n",
      "################################  200  ################################\n",
      "Training Loss:  120.04093933\n",
      "Epoch   253: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  119.24639893\n",
      "################################  400  ################################\n",
      "Training Loss:  118.43855286\n",
      "Epoch   454: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  117.45065308\n",
      "################################  600  ################################\n",
      "Training Loss:  116.84819794\n",
      "Epoch   655: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  114.52427673\n",
      "################################  800  ################################\n",
      "Training Loss:  112.55976868\n",
      "################################  900  ################################\n",
      "Training Loss:  111.94010162\n",
      "################################  1000  ################################\n",
      "Training Loss:  100.52039337\n",
      "################################  1100  ################################\n",
      "Training Loss:  81.36387634\n",
      "################################  1200  ################################\n",
      "Training Loss:  78.5103302\n",
      "################################  1300  ################################\n",
      "Training Loss:  59.07055283\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.84352875\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.33567429\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.97909164\n",
      "Epoch  1688: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.35064697\n",
      "################################  1800  ################################\n",
      "Training Loss:  62.49423981\n",
      "################################  1900  ################################\n",
      "Training Loss:  56.36447906\n",
      "Final training Loss:  51.86743546\n",
      "\n",
      "Running model (trial=5, mod=163, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  31965.77148438\n",
      "################################  100  ################################\n",
      "Training Loss:  117.84902191\n",
      "################################  200  ################################\n",
      "Training Loss:  240.66676331\n",
      "################################  300  ################################\n",
      "Training Loss:  136.76957703\n",
      "################################  400  ################################\n",
      "Training Loss:  167.93621826\n",
      "################################  500  ################################\n",
      "Training Loss:  189.06802368\n",
      "################################  600  ################################\n",
      "Training Loss:  156.16424561\n",
      "################################  700  ################################\n",
      "Training Loss:  125.47392273\n",
      "Epoch   750: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  117.88463593\n",
      "################################  900  ################################\n",
      "Training Loss:  159.38484192\n",
      "################################  1000  ################################\n",
      "Training Loss:  131.32531738\n",
      "Epoch  1029: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.7403717\n",
      "################################  1200  ################################\n",
      "Training Loss:  124.87778473\n",
      "Epoch  1230: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  119.54111481\n",
      "################################  1400  ################################\n",
      "Training Loss:  113.31299591\n",
      "################################  1500  ################################\n",
      "Training Loss:  118.39485168\n",
      "################################  1600  ################################\n",
      "Training Loss:  122.32839203\n",
      "Epoch  1606: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  118.72595978\n",
      "################################  1800  ################################\n",
      "Training Loss:  119.2538147\n",
      "Epoch  1807: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.68282318\n",
      "Final training Loss:  116.91845703\n",
      "\n",
      "Running model (trial=5, mod=164, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6870.29248047\n",
      "################################  100  ################################\n",
      "Training Loss:  117.75733948\n",
      "################################  200  ################################\n",
      "Training Loss:  117.9792099\n",
      "Epoch   253: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  120.55674744\n",
      "################################  400  ################################\n",
      "Training Loss:  122.22092438\n",
      "Epoch   454: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  118.90962219\n",
      "################################  600  ################################\n",
      "Training Loss:  115.66947937\n",
      "Epoch   655: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  117.16381836\n",
      "################################  800  ################################\n",
      "Training Loss:  115.33731079\n",
      "Epoch   856: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  120.42160034\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.86228943\n",
      "Epoch  1057: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.8090744\n",
      "################################  1200  ################################\n",
      "Training Loss:  126.2729187\n",
      "Epoch  1258: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.50326538\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.73622894\n",
      "Epoch  1459: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.63209534\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.96612549\n",
      "Epoch  1660: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.20595551\n",
      "################################  1800  ################################\n",
      "Training Loss:  113.77256775\n",
      "Epoch  1861: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  113.70627594\n",
      "Final training Loss:  114.08634186\n",
      "\n",
      "Running model (trial=5, mod=165, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4914.47460938\n",
      "################################  100  ################################\n",
      "Training Loss:  105.16474915\n",
      "################################  200  ################################\n",
      "Training Loss:  104.49884796\n",
      "################################  300  ################################\n",
      "Training Loss:  109.71092987\n",
      "################################  400  ################################\n",
      "Training Loss:  120.65848541\n",
      "Epoch   416: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  103.41632843\n",
      "################################  600  ################################\n",
      "Training Loss:  101.88473511\n",
      "Epoch   617: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  104.60975647\n",
      "################################  800  ################################\n",
      "Training Loss:  103.50881195\n",
      "Epoch   818: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  119.07637024\n",
      "################################  1000  ################################\n",
      "Training Loss:  104.75582886\n",
      "Epoch  1019: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  102.99612427\n",
      "################################  1200  ################################\n",
      "Training Loss:  125.36514282\n",
      "Epoch  1220: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  103.62055206\n",
      "################################  1400  ################################\n",
      "Training Loss:  103.51609802\n",
      "Epoch  1421: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  103.39816284\n",
      "################################  1600  ################################\n",
      "Training Loss:  103.3476181\n",
      "Epoch  1622: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  103.84962463\n",
      "################################  1800  ################################\n",
      "Training Loss:  103.21411133\n",
      "Epoch  1823: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  103.39148712\n",
      "Final training Loss:  103.20850372\n",
      "\n",
      "Running model (trial=5, mod=166, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  527590.5\n",
      "################################  100  ################################\n",
      "Training Loss:  104.02746582\n",
      "################################  200  ################################\n",
      "Training Loss:  101.71146393\n",
      "Epoch   242: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  104.62670898\n",
      "################################  400  ################################\n",
      "Training Loss:  104.00907898\n",
      "Epoch   443: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  102.30057526\n",
      "################################  600  ################################\n",
      "Training Loss:  111.21543121\n",
      "Epoch   644: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  103.3273468\n",
      "################################  800  ################################\n",
      "Training Loss:  103.2268219\n",
      "Epoch   845: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  103.37434387\n",
      "################################  1000  ################################\n",
      "Training Loss:  103.37583923\n",
      "Epoch  1046: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  103.2183075\n",
      "################################  1200  ################################\n",
      "Training Loss:  103.20801544\n",
      "Epoch  1247: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  103.2666626\n",
      "################################  1400  ################################\n",
      "Training Loss:  103.20906067\n",
      "Epoch  1448: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  103.22272491\n",
      "################################  1600  ################################\n",
      "Training Loss:  103.21743011\n",
      "Epoch  1649: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  103.20996094\n",
      "################################  1800  ################################\n",
      "Training Loss:  103.2201767\n",
      "Epoch  1850: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  103.2063446\n",
      "Final training Loss:  103.21754456\n",
      "\n",
      "Running model (trial=5, mod=167, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  541799.9375\n",
      "################################  100  ################################\n",
      "Training Loss:  61.13550186\n",
      "################################  200  ################################\n",
      "Training Loss:  40.27499771\n",
      "################################  300  ################################\n",
      "Training Loss:  39.25598145\n",
      "################################  400  ################################\n",
      "Training Loss:  44.34481812\n",
      "################################  500  ################################\n",
      "Training Loss:  44.11635971\n",
      "################################  600  ################################\n",
      "Training Loss:  41.60631943\n",
      "Epoch   675: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  38.55136871\n",
      "################################  800  ################################\n",
      "Training Loss:  43.12548065\n",
      "################################  900  ################################\n",
      "Training Loss:  40.10280991\n",
      "################################  1000  ################################\n",
      "Training Loss:  38.05976105\n",
      "################################  1100  ################################\n",
      "Training Loss:  38.54493332\n",
      "Epoch  1136: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  37.21704865\n",
      "################################  1300  ################################\n",
      "Training Loss:  41.69244385\n",
      "Epoch  1337: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  39.2102356\n",
      "################################  1500  ################################\n",
      "Training Loss:  37.86580276\n",
      "Epoch  1538: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  37.78177643\n",
      "################################  1700  ################################\n",
      "Training Loss:  42.87386703\n",
      "Epoch  1739: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  37.55331802\n",
      "################################  1900  ################################\n",
      "Training Loss:  37.59878159\n",
      "Epoch  1940: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  37.80085373\n",
      "\n",
      "Running model (trial=5, mod=168, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8916.91113281\n",
      "################################  100  ################################\n",
      "Training Loss:  67.45426941\n",
      "################################  200  ################################\n",
      "Training Loss:  56.72351837\n",
      "################################  300  ################################\n",
      "Training Loss:  48.28429031\n",
      "################################  400  ################################\n",
      "Training Loss:  46.29410934\n",
      "################################  500  ################################\n",
      "Training Loss:  44.23409271\n",
      "################################  600  ################################\n",
      "Training Loss:  42.97404099\n",
      "################################  700  ################################\n",
      "Training Loss:  42.00579071\n",
      "################################  800  ################################\n",
      "Training Loss:  43.74515915\n",
      "################################  900  ################################\n",
      "Training Loss:  42.1717453\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.97360229\n",
      "Epoch  1067: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  125.34924316\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.86481476\n",
      "Epoch  1268: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  118.1150589\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.47850037\n",
      "Epoch  1469: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  125.42051697\n",
      "################################  1600  ################################\n",
      "Training Loss:  109.35762024\n",
      "Epoch  1670: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  106.23137665\n",
      "################################  1800  ################################\n",
      "Training Loss:  103.29068756\n",
      "Epoch  1871: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  104.52832031\n",
      "Final training Loss:  107.35372925\n",
      "\n",
      "Running model (trial=5, mod=169, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5267.56787109\n",
      "################################  100  ################################\n",
      "Training Loss:  118.72058868\n",
      "################################  200  ################################\n",
      "Training Loss:  116.47715759\n",
      "################################  300  ################################\n",
      "Training Loss:  131.82824707\n",
      "Epoch   393: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  134.20632935\n",
      "################################  500  ################################\n",
      "Training Loss:  122.609375\n",
      "################################  600  ################################\n",
      "Training Loss:  271.64859009\n",
      "################################  700  ################################\n",
      "Training Loss:  66.09663391\n",
      "################################  800  ################################\n",
      "Training Loss:  45.83435059\n",
      "################################  900  ################################\n",
      "Training Loss:  45.86871719\n",
      "################################  1000  ################################\n",
      "Training Loss:  44.17807007\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.30179596\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.71227264\n",
      "Epoch  1218: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.70130539\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.2469635\n",
      "Epoch  1419: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.54785538\n",
      "################################  1600  ################################\n",
      "Training Loss:  52.34624863\n",
      "Epoch  1620: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.5946312\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.57408142\n",
      "Epoch  1821: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.85298538\n",
      "Final training Loss:  52.64376831\n",
      "\n",
      "Running model (trial=5, mod=170, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  362052.1875\n",
      "################################  100  ################################\n",
      "Training Loss:  60.41140366\n",
      "################################  200  ################################\n",
      "Training Loss:  108.68281555\n",
      "################################  300  ################################\n",
      "Training Loss:  158.58163452\n",
      "Epoch   394: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  116.14331055\n",
      "################################  500  ################################\n",
      "Training Loss:  131.81307983\n",
      "################################  600  ################################\n",
      "Training Loss:  61.5398941\n",
      "################################  700  ################################\n",
      "Training Loss:  103.67382812\n",
      "################################  800  ################################\n",
      "Training Loss:  298.07073975\n",
      "Epoch   887: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  44.63979721\n",
      "################################  1000  ################################\n",
      "Training Loss:  42.15725708\n",
      "################################  1100  ################################\n",
      "Training Loss:  66.88302612\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.00889969\n",
      "################################  1300  ################################\n",
      "Training Loss:  39.59034348\n",
      "Epoch  1366: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  84.77105713\n",
      "################################  1500  ################################\n",
      "Training Loss:  51.19083023\n",
      "Epoch  1567: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  49.18686295\n",
      "################################  1700  ################################\n",
      "Training Loss:  43.66434479\n",
      "Epoch  1768: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  42.54115677\n",
      "################################  1900  ################################\n",
      "Training Loss:  40.38248062\n",
      "Epoch  1987: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  41.00992203\n",
      "\n",
      "Running model (trial=5, mod=171, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  960729.1875\n",
      "################################  100  ################################\n",
      "Training Loss:  1149.91748047\n",
      "################################  200  ################################\n",
      "Training Loss:  113.64998627\n",
      "################################  300  ################################\n",
      "Training Loss:  103.91796875\n",
      "################################  400  ################################\n",
      "Training Loss:  107.26602936\n",
      "Epoch   466: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  569.90582275\n",
      "################################  600  ################################\n",
      "Training Loss:  131.69786072\n",
      "Epoch   667: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  209.90837097\n",
      "################################  800  ################################\n",
      "Training Loss:  130.09062195\n",
      "Epoch   868: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  570.65057373\n",
      "################################  1000  ################################\n",
      "Training Loss:  133.4463501\n",
      "Epoch  1069: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  103.74126434\n",
      "################################  1200  ################################\n",
      "Training Loss:  224.78993225\n",
      "Epoch  1270: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  133.27406311\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.31648254\n",
      "Epoch  1471: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  106.66131592\n",
      "################################  1600  ################################\n",
      "Training Loss:  141.69563293\n",
      "Epoch  1672: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  119.9182663\n",
      "################################  1800  ################################\n",
      "Training Loss:  108.46469116\n",
      "Epoch  1873: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  117.00793457\n",
      "Final training Loss:  103.63098907\n",
      "\n",
      "Running model (trial=5, mod=172, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  550697.625\n",
      "################################  100  ################################\n",
      "Training Loss:  115.01940155\n",
      "################################  200  ################################\n",
      "Training Loss:  120.06237793\n",
      "################################  300  ################################\n",
      "Training Loss:  121.20940399\n",
      "################################  400  ################################\n",
      "Training Loss:  113.17111969\n",
      "Epoch   491: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  113.98506927\n",
      "################################  600  ################################\n",
      "Training Loss:  112.50563049\n",
      "################################  700  ################################\n",
      "Training Loss:  137.87886047\n",
      "################################  800  ################################\n",
      "Training Loss:  123.78803253\n",
      "Epoch   851: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  192.33917236\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.80376434\n",
      "Epoch  1052: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.46129608\n",
      "################################  1200  ################################\n",
      "Training Loss:  119.46067047\n",
      "Epoch  1253: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.61687469\n",
      "################################  1400  ################################\n",
      "Training Loss:  116.74958038\n",
      "Epoch  1454: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.84649658\n",
      "################################  1600  ################################\n",
      "Training Loss:  183.27073669\n",
      "Epoch  1655: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  153.64985657\n",
      "################################  1800  ################################\n",
      "Training Loss:  133.93731689\n",
      "Epoch  1856: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  134.19995117\n",
      "Final training Loss:  118.59909058\n",
      "\n",
      "Running model (trial=5, mod=173, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  218753.40625\n",
      "################################  100  ################################\n",
      "Training Loss:  114.07260132\n",
      "################################  200  ################################\n",
      "Training Loss:  113.61679077\n",
      "################################  300  ################################\n",
      "Training Loss:  114.0009079\n",
      "################################  400  ################################\n",
      "Training Loss:  115.37906647\n",
      "################################  500  ################################\n",
      "Training Loss:  116.45362854\n",
      "Epoch   509: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  115.69966888\n",
      "################################  700  ################################\n",
      "Training Loss:  122.26552582\n",
      "Epoch   710: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  114.93473816\n",
      "################################  900  ################################\n",
      "Training Loss:  117.94084167\n",
      "Epoch   911: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.7902298\n",
      "################################  1100  ################################\n",
      "Training Loss:  116.20352173\n",
      "Epoch  1112: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.75856018\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.35272217\n",
      "Epoch  1313: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.64350128\n",
      "################################  1500  ################################\n",
      "Training Loss:  119.33953094\n",
      "Epoch  1514: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  116.05636597\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.7808075\n",
      "Epoch  1715: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.5104599\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.72437286\n",
      "Epoch  1916: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Final training Loss:  115.18451691\n",
      "\n",
      "Running model (trial=5, mod=174, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  271614.3125\n",
      "################################  100  ################################\n",
      "Training Loss:  69.29994202\n",
      "################################  200  ################################\n",
      "Training Loss:  63.51771927\n",
      "################################  300  ################################\n",
      "Training Loss:  59.07328796\n",
      "################################  400  ################################\n",
      "Training Loss:  58.36185837\n",
      "################################  500  ################################\n",
      "Training Loss:  57.86572647\n",
      "################################  600  ################################\n",
      "Training Loss:  53.84027863\n",
      "################################  700  ################################\n",
      "Training Loss:  50.28621292\n",
      "################################  800  ################################\n",
      "Training Loss:  47.83088303\n",
      "################################  900  ################################\n",
      "Training Loss:  46.73683548\n",
      "################################  1000  ################################\n",
      "Training Loss:  47.14238358\n",
      "################################  1100  ################################\n",
      "Training Loss:  46.53299713\n",
      "################################  1200  ################################\n",
      "Training Loss:  50.06944275\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.5177002\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.02123642\n",
      "################################  1500  ################################\n",
      "Training Loss:  45.27778625\n",
      "################################  1600  ################################\n",
      "Training Loss:  46.65537643\n",
      "################################  1700  ################################\n",
      "Training Loss:  45.16609955\n",
      "################################  1800  ################################\n",
      "Training Loss:  44.04143524\n",
      "################################  1900  ################################\n",
      "Training Loss:  43.10380554\n",
      "Final training Loss:  41.63685989\n",
      "\n",
      "Running model (trial=5, mod=175, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1539831.0\n",
      "################################  100  ################################\n",
      "Training Loss:  74.69812012\n",
      "################################  200  ################################\n",
      "Training Loss:  66.23982239\n",
      "################################  300  ################################\n",
      "Training Loss:  123.88483429\n",
      "################################  400  ################################\n",
      "Training Loss:  114.38389587\n",
      "Epoch   443: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  114.45349884\n",
      "################################  600  ################################\n",
      "Training Loss:  114.75460815\n",
      "Epoch   644: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  114.58315277\n",
      "################################  800  ################################\n",
      "Training Loss:  114.74729156\n",
      "Epoch   845: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  115.10251617\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.70617676\n",
      "Epoch  1046: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.45401764\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.79718781\n",
      "Epoch  1247: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.69781494\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.4010849\n",
      "Epoch  1448: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  114.42118835\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.4302063\n",
      "Epoch  1649: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.69686127\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.41181183\n",
      "Epoch  1850: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.38645172\n",
      "Final training Loss:  114.32145691\n",
      "\n",
      "Running model (trial=5, mod=176, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3909.52490234\n",
      "################################  100  ################################\n",
      "Training Loss:  53.22699738\n",
      "################################  200  ################################\n",
      "Training Loss:  53.08898163\n",
      "################################  300  ################################\n",
      "Training Loss:  48.97443008\n",
      "################################  400  ################################\n",
      "Training Loss:  50.43921661\n",
      "################################  500  ################################\n",
      "Training Loss:  56.92246246\n",
      "################################  600  ################################\n",
      "Training Loss:  53.31700516\n",
      "################################  700  ################################\n",
      "Training Loss:  44.15798569\n",
      "################################  800  ################################\n",
      "Training Loss:  50.30565643\n",
      "################################  900  ################################\n",
      "Training Loss:  41.31560898\n",
      "################################  1000  ################################\n",
      "Training Loss:  51.52344131\n",
      "Epoch  1051: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.41597366\n",
      "################################  1200  ################################\n",
      "Training Loss:  61.43812561\n",
      "Epoch  1252: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  39.01630402\n",
      "################################  1400  ################################\n",
      "Training Loss:  41.93911362\n",
      "################################  1500  ################################\n",
      "Training Loss:  39.00830078\n",
      "################################  1600  ################################\n",
      "Training Loss:  37.9004097\n",
      "################################  1700  ################################\n",
      "Training Loss:  38.57601547\n",
      "################################  1800  ################################\n",
      "Training Loss:  59.15509033\n",
      "################################  1900  ################################\n",
      "Training Loss:  45.59457779\n",
      "Epoch  1908: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  45.3944397\n",
      "\n",
      "Running model (trial=5, mod=177, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4800130.0\n",
      "################################  100  ################################\n",
      "Training Loss:  83.93624115\n",
      "################################  200  ################################\n",
      "Training Loss:  78.28469849\n",
      "################################  300  ################################\n",
      "Training Loss:  50.21510696\n",
      "################################  400  ################################\n",
      "Training Loss:  60.45121002\n",
      "################################  500  ################################\n",
      "Training Loss:  43.63994598\n",
      "################################  600  ################################\n",
      "Training Loss:  96.58061981\n",
      "################################  700  ################################\n",
      "Training Loss:  53.21636963\n",
      "################################  800  ################################\n",
      "Training Loss:  42.72969055\n",
      "################################  900  ################################\n",
      "Training Loss:  41.2755394\n",
      "################################  1000  ################################\n",
      "Training Loss:  50.4229393\n",
      "################################  1100  ################################\n",
      "Training Loss:  57.43450546\n",
      "################################  1200  ################################\n",
      "Training Loss:  43.4883728\n",
      "Epoch  1210: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  68.38806152\n",
      "################################  1400  ################################\n",
      "Training Loss:  42.95521545\n",
      "################################  1500  ################################\n",
      "Training Loss:  44.72478867\n",
      "################################  1600  ################################\n",
      "Training Loss:  42.29524994\n",
      "Epoch  1669: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  48.38823318\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.26063538\n",
      "################################  1900  ################################\n",
      "Training Loss:  42.34597015\n",
      "Final training Loss:  45.14609146\n",
      "\n",
      "Running model (trial=5, mod=178, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  18125438976.0\n",
      "################################  100  ################################\n",
      "Training Loss:  91359.421875\n",
      "################################  200  ################################\n",
      "Training Loss:  12534.44726562\n",
      "################################  300  ################################\n",
      "Training Loss:  3156.2734375\n",
      "################################  400  ################################\n",
      "Training Loss:  1816.70544434\n",
      "################################  500  ################################\n",
      "Training Loss:  1367.72583008\n",
      "################################  600  ################################\n",
      "Training Loss:  1097.61401367\n",
      "################################  700  ################################\n",
      "Training Loss:  934.50091553\n",
      "################################  800  ################################\n",
      "Training Loss:  817.50909424\n",
      "################################  900  ################################\n",
      "Training Loss:  694.61102295\n",
      "################################  1000  ################################\n",
      "Training Loss:  501.61303711\n",
      "################################  1100  ################################\n",
      "Training Loss:  368.33737183\n",
      "################################  1200  ################################\n",
      "Training Loss:  273.71505737\n",
      "################################  1300  ################################\n",
      "Training Loss:  204.64492798\n",
      "################################  1400  ################################\n",
      "Training Loss:  156.43109131\n",
      "################################  1500  ################################\n",
      "Training Loss:  122.77082825\n",
      "################################  1600  ################################\n",
      "Training Loss:  105.01083374\n",
      "################################  1700  ################################\n",
      "Training Loss:  91.82458496\n",
      "################################  1800  ################################\n",
      "Training Loss:  83.12522888\n",
      "################################  1900  ################################\n",
      "Training Loss:  77.56070709\n",
      "Final training Loss:  75.01699829\n",
      "\n",
      "Running model (trial=5, mod=179, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.3904052802748416e+16\n",
      "################################  100  ################################\n",
      "Training Loss:  4199.4453125\n",
      "################################  200  ################################\n",
      "Training Loss:  21837.48828125\n",
      "################################  300  ################################\n",
      "Training Loss:  28046.5703125\n",
      "################################  400  ################################\n",
      "Training Loss:  11684.29882812\n",
      "################################  500  ################################\n",
      "Training Loss:  20396.0234375\n",
      "################################  600  ################################\n",
      "Training Loss:  3706.65429688\n",
      "################################  700  ################################\n",
      "Training Loss:  33680.50390625\n",
      "################################  800  ################################\n",
      "Training Loss:  3701.85864258\n",
      "################################  900  ################################\n",
      "Training Loss:  43724.13671875\n",
      "################################  1000  ################################\n",
      "Training Loss:  4110.69335938\n",
      "################################  1100  ################################\n",
      "Training Loss:  2871.33959961\n",
      "################################  1200  ################################\n",
      "Training Loss:  2845.26513672\n",
      "################################  1300  ################################\n",
      "Training Loss:  2812.97167969\n",
      "################################  1400  ################################\n",
      "Training Loss:  2781.01416016\n",
      "################################  1500  ################################\n",
      "Training Loss:  2742.36987305\n",
      "################################  1600  ################################\n",
      "Training Loss:  2708.81298828\n",
      "################################  1700  ################################\n",
      "Training Loss:  2666.78833008\n",
      "################################  1800  ################################\n",
      "Training Loss:  2627.6640625\n",
      "################################  1900  ################################\n",
      "Training Loss:  2586.23193359\n",
      "Final training Loss:  2532.96875\n",
      "\n",
      "Running model (trial=6, mod=180, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  25025.36328125\n",
      "################################  100  ################################\n",
      "Training Loss:  102.51106262\n",
      "################################  200  ################################\n",
      "Training Loss:  56.54277802\n",
      "################################  300  ################################\n",
      "Training Loss:  54.11058044\n",
      "Epoch   394: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  55.52684784\n",
      "################################  500  ################################\n",
      "Training Loss:  54.41783142\n",
      "Epoch   595: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  54.38531494\n",
      "################################  700  ################################\n",
      "Training Loss:  58.1035881\n",
      "Epoch   796: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  54.25335312\n",
      "################################  900  ################################\n",
      "Training Loss:  54.13363647\n",
      "Epoch   997: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.58682632\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.99243164\n",
      "Epoch  1198: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.21469498\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.1241684\n",
      "Epoch  1399: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.04809189\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.54850388\n",
      "Epoch  1600: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.56170654\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.67690277\n",
      "################################  1800  ################################\n",
      "Epoch  1801: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Training Loss:  53.91460037\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.01038361\n",
      "Final training Loss:  53.98720169\n",
      "\n",
      "Running model (trial=6, mod=181, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  457238.59375\n",
      "################################  100  ################################\n",
      "Training Loss:  92.47787476\n",
      "################################  200  ################################\n",
      "Training Loss:  75.3839798\n",
      "################################  300  ################################\n",
      "Training Loss:  56.46739197\n",
      "################################  400  ################################\n",
      "Training Loss:  53.4432869\n",
      "Epoch   489: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  54.55243683\n",
      "################################  600  ################################\n",
      "Training Loss:  55.85941315\n",
      "################################  700  ################################\n",
      "Training Loss:  56.22600555\n",
      "################################  800  ################################\n",
      "Training Loss:  63.22663116\n",
      "Epoch   873: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  53.37607574\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.8204689\n",
      "Epoch  1074: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.5490799\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.76965332\n",
      "Epoch  1275: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.85771942\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.31212234\n",
      "Epoch  1476: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.42445755\n",
      "################################  1600  ################################\n",
      "Training Loss:  56.46700287\n",
      "Epoch  1677: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.02391815\n",
      "################################  1800  ################################\n",
      "Training Loss:  54.87906265\n",
      "Epoch  1878: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.6207962\n",
      "Final training Loss:  52.42160797\n",
      "\n",
      "Running model (trial=6, mod=182, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  674638.25\n",
      "################################  100  ################################\n",
      "Training Loss:  116.98072815\n",
      "################################  200  ################################\n",
      "Training Loss:  114.78239441\n",
      "################################  300  ################################\n",
      "Training Loss:  117.15860748\n",
      "################################  400  ################################\n",
      "Training Loss:  119.78125\n",
      "################################  500  ################################\n",
      "Training Loss:  139.51985168\n",
      "################################  600  ################################\n",
      "Training Loss:  123.38942719\n",
      "################################  700  ################################\n",
      "Training Loss:  122.94178009\n",
      "Epoch   757: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  117.44300079\n",
      "################################  900  ################################\n",
      "Training Loss:  132.10339355\n",
      "################################  1000  ################################\n",
      "Training Loss:  122.3112793\n",
      "################################  1100  ################################\n",
      "Training Loss:  118.33573151\n",
      "Epoch  1129: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  115.30149841\n",
      "################################  1300  ################################\n",
      "Training Loss:  120.62512207\n",
      "Epoch  1330: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  118.58989716\n",
      "################################  1500  ################################\n",
      "Training Loss:  129.08883667\n",
      "################################  1600  ################################\n",
      "Training Loss:  135.67955017\n",
      "Epoch  1635: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  117.41350555\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.38549042\n",
      "Epoch  1836: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.22185516\n",
      "Final training Loss:  115.53591919\n",
      "\n",
      "Running model (trial=6, mod=183, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  68523.390625\n",
      "################################  100  ################################\n",
      "Training Loss:  120.55361938\n",
      "################################  200  ################################\n",
      "Training Loss:  102.54624939\n",
      "Epoch   215: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  58.73830414\n",
      "################################  400  ################################\n",
      "Training Loss:  58.14725494\n",
      "Epoch   492: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  62.11701965\n",
      "################################  600  ################################\n",
      "Training Loss:  54.23314667\n",
      "Epoch   693: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  55.66433716\n",
      "################################  800  ################################\n",
      "Training Loss:  60.04455185\n",
      "################################  900  ################################\n",
      "Training Loss:  54.35648727\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.08230209\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.3631897\n",
      "Epoch  1199: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  53.63928986\n",
      "################################  1300  ################################\n",
      "Training Loss:  55.64701462\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.35662079\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.86200714\n",
      "Epoch  1568: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  52.54099655\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.79416275\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.9776268\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.13536835\n",
      "Final training Loss:  51.51128769\n",
      "\n",
      "Running model (trial=6, mod=184, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  202898.03125\n",
      "################################  100  ################################\n",
      "Training Loss:  125.14221191\n",
      "################################  200  ################################\n",
      "Training Loss:  123.52062225\n",
      "Epoch   291: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  123.99507141\n",
      "################################  400  ################################\n",
      "Training Loss:  123.53295898\n",
      "Epoch   492: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  123.67003632\n",
      "################################  600  ################################\n",
      "Training Loss:  123.52818298\n",
      "Epoch   693: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  123.67784119\n",
      "################################  800  ################################\n",
      "Training Loss:  123.58853912\n",
      "Epoch   894: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  123.60208893\n",
      "################################  1000  ################################\n",
      "Training Loss:  123.63645172\n",
      "Epoch  1095: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  123.46230316\n",
      "################################  1200  ################################\n",
      "Training Loss:  123.48736572\n",
      "Epoch  1296: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  123.48485565\n",
      "################################  1400  ################################\n",
      "Training Loss:  123.50196838\n",
      "Epoch  1497: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  123.65579224\n",
      "################################  1600  ################################\n",
      "Training Loss:  123.51184845\n",
      "Epoch  1698: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  123.52368927\n",
      "################################  1800  ################################\n",
      "Training Loss:  123.41854095\n",
      "Epoch  1899: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  123.57498169\n",
      "Final training Loss:  123.59586334\n",
      "\n",
      "Running model (trial=6, mod=185, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1367102.5\n",
      "################################  100  ################################\n",
      "Training Loss:  67.5585022\n",
      "################################  200  ################################\n",
      "Training Loss:  53.4731636\n",
      "################################  300  ################################\n",
      "Training Loss:  54.1973877\n",
      "################################  400  ################################\n",
      "Training Loss:  59.89703751\n",
      "################################  500  ################################\n",
      "Training Loss:  56.43319321\n",
      "################################  600  ################################\n",
      "Training Loss:  52.96546173\n",
      "################################  700  ################################\n",
      "Training Loss:  54.55583954\n",
      "################################  800  ################################\n",
      "Epoch   801: reducing learning rate of group 0 to 4.0000e-02.\n",
      "Training Loss:  56.25225067\n",
      "################################  900  ################################\n",
      "Training Loss:  57.73810196\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.38145828\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.90458298\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.66121292\n",
      "################################  1300  ################################\n",
      "Training Loss:  65.54502869\n",
      "################################  1400  ################################\n",
      "Training Loss:  55.77677536\n",
      "################################  1500  ################################\n",
      "Training Loss:  63.49181366\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.7720108\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.42406464\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.87292099\n",
      "################################  1900  ################################\n",
      "Training Loss:  49.00342178\n",
      "Final training Loss:  83.16764069\n",
      "\n",
      "Running model (trial=6, mod=186, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1086473.0\n",
      "################################  100  ################################\n",
      "Training Loss:  120.38594818\n",
      "################################  200  ################################\n",
      "Training Loss:  119.84416199\n",
      "################################  300  ################################\n",
      "Training Loss:  119.65789795\n",
      "################################  400  ################################\n",
      "Training Loss:  119.13691711\n",
      "################################  500  ################################\n",
      "Training Loss:  118.6137085\n",
      "################################  600  ################################\n",
      "Training Loss:  116.84022522\n",
      "################################  700  ################################\n",
      "Training Loss:  116.86702728\n",
      "################################  800  ################################\n",
      "Training Loss:  117.87125397\n",
      "################################  900  ################################\n",
      "Training Loss:  116.61859894\n",
      "Epoch   999: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.41017914\n",
      "################################  1100  ################################\n",
      "Training Loss:  116.02835083\n",
      "################################  1200  ################################\n",
      "Training Loss:  116.49909973\n",
      "################################  1300  ################################\n",
      "Training Loss:  117.01364899\n",
      "################################  1400  ################################\n",
      "Training Loss:  120.28388214\n",
      "################################  1500  ################################\n",
      "Training Loss:  116.78197479\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.61956024\n",
      "################################  1700  ################################\n",
      "Training Loss:  115.95864868\n",
      "################################  1800  ################################\n",
      "Training Loss:  152.11410522\n",
      "################################  1900  ################################\n",
      "Training Loss:  128.27220154\n",
      "Final training Loss:  134.53125\n",
      "\n",
      "Running model (trial=6, mod=187, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  773561.6875\n",
      "################################  100  ################################\n",
      "Training Loss:  134.52682495\n",
      "################################  200  ################################\n",
      "Training Loss:  120.32390594\n",
      "################################  300  ################################\n",
      "Training Loss:  122.07369232\n",
      "################################  400  ################################\n",
      "Training Loss:  161.866745\n",
      "################################  500  ################################\n",
      "Training Loss:  133.48825073\n",
      "################################  600  ################################\n",
      "Training Loss:  132.14208984\n",
      "################################  700  ################################\n",
      "Training Loss:  207.66421509\n",
      "################################  800  ################################\n",
      "Training Loss:  228.0763855\n",
      "################################  900  ################################\n",
      "Training Loss:  336.63119507\n",
      "################################  1000  ################################\n",
      "Training Loss:  72.02646637\n",
      "################################  1100  ################################\n",
      "Training Loss:  64.85891724\n",
      "################################  1200  ################################\n",
      "Training Loss:  76.62133789\n",
      "################################  1300  ################################\n",
      "Training Loss:  96.5741272\n",
      "################################  1400  ################################\n",
      "Training Loss:  7169.87792969\n",
      "Epoch  1403: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  79.50400543\n",
      "################################  1600  ################################\n",
      "Training Loss:  62.43639755\n",
      "################################  1700  ################################\n",
      "Training Loss:  1095.73754883\n",
      "################################  1800  ################################\n",
      "Training Loss:  1031.89208984\n",
      "Epoch  1884: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  512.70159912\n",
      "Final training Loss:  1155.27734375\n",
      "\n",
      "Running model (trial=6, mod=188, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3753.22924805\n",
      "################################  100  ################################\n",
      "Training Loss:  123.18317413\n",
      "################################  200  ################################\n",
      "Training Loss:  111.01561737\n",
      "################################  300  ################################\n",
      "Training Loss:  82.15349579\n",
      "################################  400  ################################\n",
      "Training Loss:  54.21871567\n",
      "Epoch   445: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  56.28776169\n",
      "################################  600  ################################\n",
      "Training Loss:  59.72132111\n",
      "Epoch   646: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  56.51053238\n",
      "################################  800  ################################\n",
      "Training Loss:  56.86010361\n",
      "Epoch   847: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  62.05967712\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.28432083\n",
      "Epoch  1048: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.83735275\n",
      "################################  1200  ################################\n",
      "Training Loss:  53.43455505\n",
      "Epoch  1249: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.26791763\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.58117676\n",
      "Epoch  1450: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.71580124\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.65298843\n",
      "Epoch  1651: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.69421768\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.81860352\n",
      "Epoch  1852: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.0149765\n",
      "Final training Loss:  54.79197311\n",
      "\n",
      "Running model (trial=6, mod=189, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10653.86816406\n",
      "################################  100  ################################\n",
      "Training Loss:  117.69416809\n",
      "################################  200  ################################\n",
      "Training Loss:  115.80419159\n",
      "################################  300  ################################\n",
      "Training Loss:  116.49964142\n",
      "################################  400  ################################\n",
      "Training Loss:  115.7489624\n",
      "################################  500  ################################\n",
      "Training Loss:  115.76612091\n",
      "################################  600  ################################\n",
      "Training Loss:  118.04238892\n",
      "################################  700  ################################\n",
      "Training Loss:  114.54961395\n",
      "Epoch   790: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  114.41182709\n",
      "################################  900  ################################\n",
      "Training Loss:  121.42694092\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.87369537\n",
      "################################  1100  ################################\n",
      "Training Loss:  116.54489136\n",
      "################################  1200  ################################\n",
      "Training Loss:  109.89154053\n",
      "################################  1300  ################################\n",
      "Training Loss:  89.11773682\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.00370026\n",
      "################################  1500  ################################\n",
      "Training Loss:  48.69185638\n",
      "################################  1600  ################################\n",
      "Training Loss:  58.54953766\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.86711884\n",
      "################################  1800  ################################\n",
      "Training Loss:  55.875103\n",
      "################################  1900  ################################\n",
      "Training Loss:  45.88720322\n",
      "Epoch  1915: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  43.78570557\n",
      "\n",
      "Running model (trial=6, mod=190, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2006580.125\n",
      "################################  100  ################################\n",
      "Training Loss:  124.30568695\n",
      "################################  200  ################################\n",
      "Training Loss:  123.87609863\n",
      "Epoch   250: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  121.32318878\n",
      "################################  400  ################################\n",
      "Training Loss:  117.71170807\n",
      "################################  500  ################################\n",
      "Training Loss:  117.71038818\n",
      "################################  600  ################################\n",
      "Training Loss:  116.85954285\n",
      "################################  700  ################################\n",
      "Training Loss:  114.96822357\n",
      "################################  800  ################################\n",
      "Training Loss:  123.65444946\n",
      "################################  900  ################################\n",
      "Training Loss:  116.82538605\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.38103485\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.20021057\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.39316559\n",
      "################################  1300  ################################\n",
      "Training Loss:  112.96421051\n",
      "################################  1400  ################################\n",
      "Training Loss:  116.27500916\n",
      "################################  1500  ################################\n",
      "Training Loss:  111.83337402\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.16117096\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.15287781\n",
      "Epoch  1728: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  112.60643005\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.69609833\n",
      "Final training Loss:  129.52404785\n",
      "\n",
      "Running model (trial=6, mod=191, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1919.02429199\n",
      "################################  100  ################################\n",
      "Training Loss:  132.22479248\n",
      "################################  200  ################################\n",
      "Training Loss:  56.70679855\n",
      "################################  300  ################################\n",
      "Training Loss:  54.28275299\n",
      "################################  400  ################################\n",
      "Training Loss:  53.14552307\n",
      "################################  500  ################################\n",
      "Training Loss:  53.87595749\n",
      "################################  600  ################################\n",
      "Training Loss:  50.35028839\n",
      "################################  700  ################################\n",
      "Training Loss:  49.63497543\n",
      "################################  800  ################################\n",
      "Training Loss:  48.17803574\n",
      "################################  900  ################################\n",
      "Training Loss:  48.99311447\n",
      "################################  1000  ################################\n",
      "Training Loss:  52.15088272\n",
      "################################  1100  ################################\n",
      "Training Loss:  44.41284943\n",
      "################################  1200  ################################\n",
      "Training Loss:  50.65335083\n",
      "################################  1300  ################################\n",
      "Training Loss:  42.86980438\n",
      "################################  1400  ################################\n",
      "Training Loss:  43.91613388\n",
      "################################  1500  ################################\n",
      "Training Loss:  39.9088707\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.56864166\n",
      "################################  1700  ################################\n",
      "Training Loss:  58.31100464\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.06258011\n",
      "Epoch  1889: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  43.83107758\n",
      "Final training Loss:  40.69025803\n",
      "\n",
      "Running model (trial=6, mod=192, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3466.10620117\n",
      "################################  100  ################################\n",
      "Training Loss:  116.62166595\n",
      "################################  200  ################################\n",
      "Training Loss:  116.00895691\n",
      "################################  300  ################################\n",
      "Training Loss:  115.65060425\n",
      "################################  400  ################################\n",
      "Training Loss:  117.47598267\n",
      "################################  500  ################################\n",
      "Training Loss:  127.74134064\n",
      "################################  600  ################################\n",
      "Training Loss:  116.31295776\n",
      "################################  700  ################################\n",
      "Training Loss:  178.290802\n",
      "Epoch   771: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  116.30397034\n",
      "################################  900  ################################\n",
      "Training Loss:  115.98933411\n",
      "################################  1000  ################################\n",
      "Training Loss:  113.80010986\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.40827179\n",
      "################################  1200  ################################\n",
      "Training Loss:  112.98152924\n",
      "Epoch  1202: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.84087372\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.36174011\n",
      "Epoch  1403: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.82377625\n",
      "################################  1600  ################################\n",
      "Training Loss:  160.59234619\n",
      "Epoch  1604: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.13551331\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.5377655\n",
      "Epoch  1805: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.26455688\n",
      "Final training Loss:  113.72268677\n",
      "\n",
      "Running model (trial=6, mod=193, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  355288.84375\n",
      "################################  100  ################################\n",
      "Training Loss:  365.56399536\n",
      "################################  200  ################################\n",
      "Training Loss:  129.3119812\n",
      "################################  300  ################################\n",
      "Training Loss:  128.2673645\n",
      "Epoch   387: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  130.50001526\n",
      "################################  500  ################################\n",
      "Training Loss:  122.1668396\n",
      "Epoch   588: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  120.54522705\n",
      "################################  700  ################################\n",
      "Training Loss:  119.72737885\n",
      "################################  800  ################################\n",
      "Training Loss:  121.64588928\n",
      "################################  900  ################################\n",
      "Training Loss:  123.45557404\n",
      "################################  1000  ################################\n",
      "Training Loss:  146.67785645\n",
      "################################  1100  ################################\n",
      "Training Loss:  124.60055542\n",
      "################################  1200  ################################\n",
      "Training Loss:  121.65782928\n",
      "################################  1300  ################################\n",
      "Training Loss:  136.42602539\n",
      "################################  1400  ################################\n",
      "Training Loss:  124.92373657\n",
      "################################  1500  ################################\n",
      "Training Loss:  117.47224426\n",
      "Epoch  1583: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  118.43623352\n",
      "################################  1700  ################################\n",
      "Training Loss:  127.77946472\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.96710205\n",
      "################################  1900  ################################\n",
      "Training Loss:  129.25360107\n",
      "Epoch  1989: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  116.61934662\n",
      "\n",
      "Running model (trial=6, mod=194, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  431758.625\n",
      "################################  100  ################################\n",
      "Training Loss:  118.90470886\n",
      "################################  200  ################################\n",
      "Training Loss:  121.26681519\n",
      "Epoch   228: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  120.62532806\n",
      "################################  400  ################################\n",
      "Training Loss:  120.79567719\n",
      "Epoch   429: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  113.88236237\n",
      "################################  600  ################################\n",
      "Training Loss:  115.31852722\n",
      "Epoch   630: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  113.78852081\n",
      "################################  800  ################################\n",
      "Training Loss:  123.61865997\n",
      "Epoch   831: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  136.30563354\n",
      "################################  1000  ################################\n",
      "Training Loss:  135.71539307\n",
      "Epoch  1032: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  129.37986755\n",
      "################################  1200  ################################\n",
      "Training Loss:  118.66036987\n",
      "Epoch  1233: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  113.13689423\n",
      "################################  1400  ################################\n",
      "Training Loss:  113.60353851\n",
      "Epoch  1434: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  122.76058197\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.01293182\n",
      "Epoch  1635: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  127.90403748\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.21395111\n",
      "Epoch  1836: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  119.81969452\n",
      "Final training Loss:  112.22275543\n",
      "\n",
      "Running model (trial=6, mod=195, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  986798.125\n",
      "################################  100  ################################\n",
      "Training Loss:  4203.93457031\n",
      "################################  200  ################################\n",
      "Training Loss:  2972.734375\n",
      "################################  300  ################################\n",
      "Training Loss:  1452.18371582\n",
      "################################  400  ################################\n",
      "Training Loss:  352.27441406\n",
      "################################  500  ################################\n",
      "Training Loss:  117.09452057\n",
      "################################  600  ################################\n",
      "Training Loss:  104.46578979\n",
      "################################  700  ################################\n",
      "Training Loss:  108.01365662\n",
      "################################  800  ################################\n",
      "Training Loss:  104.86167145\n",
      "Epoch   844: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  102.72174072\n",
      "################################  1000  ################################\n",
      "Training Loss:  105.45526886\n",
      "Epoch  1060: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  101.96781921\n",
      "################################  1200  ################################\n",
      "Training Loss:  106.93756104\n",
      "################################  1300  ################################\n",
      "Training Loss:  117.27568054\n",
      "Epoch  1361: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  106.4936676\n",
      "################################  1500  ################################\n",
      "Training Loss:  109.30508423\n",
      "Epoch  1562: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  102.09300232\n",
      "################################  1700  ################################\n",
      "Training Loss:  104.14632416\n",
      "Epoch  1763: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  104.58306885\n",
      "################################  1900  ################################\n",
      "Training Loss:  102.15145874\n",
      "Epoch  1964: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  106.01704407\n",
      "\n",
      "Running model (trial=6, mod=196, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  282622.625\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=197, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  164545.109375\n",
      "################################  100  ################################\n",
      "Training Loss:  51.79534149\n",
      "################################  200  ################################\n",
      "Training Loss:  49.12995911\n",
      "################################  300  ################################\n",
      "Training Loss:  49.45059586\n",
      "################################  400  ################################\n",
      "Training Loss:  52.62628174\n",
      "################################  500  ################################\n",
      "Training Loss:  47.92922592\n",
      "################################  600  ################################\n",
      "Training Loss:  46.98522568\n",
      "################################  700  ################################\n",
      "Training Loss:  47.55620956\n",
      "################################  800  ################################\n",
      "Training Loss:  96.58311462\n",
      "################################  900  ################################\n",
      "Training Loss:  46.86800385\n",
      "################################  1000  ################################\n",
      "Training Loss:  47.36685181\n",
      "################################  1100  ################################\n",
      "Training Loss:  45.68491364\n",
      "################################  1200  ################################\n",
      "Training Loss:  49.98209381\n",
      "################################  1300  ################################\n",
      "Training Loss:  44.01841354\n",
      "################################  1400  ################################\n",
      "Training Loss:  45.60580826\n",
      "################################  1500  ################################\n",
      "Training Loss:  61.10169601\n",
      "################################  1600  ################################\n",
      "Training Loss:  49.51506424\n",
      "################################  1700  ################################\n",
      "Training Loss:  46.48700333\n",
      "################################  1800  ################################\n",
      "Training Loss:  47.49513626\n",
      "################################  1900  ################################\n",
      "Training Loss:  56.73643875\n",
      "Epoch  1951: reducing learning rate of group 0 to 4.0000e-02.\n",
      "Final training Loss:  53.19229126\n",
      "\n",
      "Running model (trial=6, mod=198, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  58289.3828125\n",
      "################################  100  ################################\n",
      "Training Loss:  54.15605545\n",
      "################################  200  ################################\n",
      "Training Loss:  656.6307373\n",
      "################################  300  ################################\n",
      "Training Loss:  54.75288391\n",
      "Epoch   383: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  55.43048096\n",
      "################################  500  ################################\n",
      "Training Loss:  51.86253357\n",
      "Epoch   584: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  45.88988876\n",
      "################################  700  ################################\n",
      "Training Loss:  42.82174683\n",
      "Epoch   785: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  44.08750534\n",
      "################################  900  ################################\n",
      "Training Loss:  42.03646469\n",
      "Epoch   986: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  46.80325317\n",
      "################################  1100  ################################\n",
      "Training Loss:  48.63482666\n",
      "Epoch  1187: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  44.79697037\n",
      "################################  1300  ################################\n",
      "Training Loss:  41.07126999\n",
      "Epoch  1388: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  40.16836548\n",
      "################################  1500  ################################\n",
      "Training Loss:  42.6919136\n",
      "Epoch  1589: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  39.40259552\n",
      "################################  1700  ################################\n",
      "Training Loss:  42.98361588\n",
      "Epoch  1790: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.73105621\n",
      "################################  1900  ################################\n",
      "Training Loss:  39.55682755\n",
      "Final training Loss:  48.10508728\n",
      "\n",
      "Running model (trial=6, mod=199, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  334028.96875\n",
      "################################  100  ################################\n",
      "Training Loss:  66.06678772\n",
      "################################  200  ################################\n",
      "Training Loss:  75.14543915\n",
      "################################  300  ################################\n",
      "Training Loss:  51.76346588\n",
      "################################  400  ################################\n",
      "Training Loss:  45.08446503\n",
      "################################  500  ################################\n",
      "Training Loss:  64.84857941\n",
      "################################  600  ################################\n",
      "Training Loss:  55.40063858\n",
      "################################  700  ################################\n",
      "Training Loss:  43.23657227\n",
      "################################  800  ################################\n",
      "Training Loss:  47.84521103\n",
      "################################  900  ################################\n",
      "Training Loss:  42.20532227\n",
      "################################  1000  ################################\n",
      "Training Loss:  41.13039017\n",
      "Epoch  1087: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.99845505\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.76982117\n",
      "################################  1300  ################################\n",
      "Training Loss:  42.12722397\n",
      "################################  1400  ################################\n",
      "Training Loss:  40.01804733\n",
      "################################  1500  ################################\n",
      "Training Loss:  40.56876755\n",
      "################################  1600  ################################\n",
      "Training Loss:  1969.34033203\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.82128906\n",
      "Epoch  1751: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  86.35557556\n",
      "################################  1900  ################################\n",
      "Training Loss:  86.28159332\n",
      "Epoch  1952: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  82.53366089\n",
      "\n",
      "Running model (trial=6, mod=200, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  199088.984375\n",
      "################################  100  ################################\n",
      "Training Loss:  132.85189819\n",
      "################################  200  ################################\n",
      "Training Loss:  119.09237671\n",
      "Epoch   279: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  102.76876831\n",
      "################################  400  ################################\n",
      "Training Loss:  89.87809753\n",
      "################################  500  ################################\n",
      "Training Loss:  77.71060181\n",
      "################################  600  ################################\n",
      "Training Loss:  70.86213684\n",
      "################################  700  ################################\n",
      "Training Loss:  63.5018959\n",
      "################################  800  ################################\n",
      "Training Loss:  85.6497345\n",
      "################################  900  ################################\n",
      "Training Loss:  66.95619202\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.21967316\n",
      "################################  1100  ################################\n",
      "Training Loss:  62.09971619\n",
      "################################  1200  ################################\n",
      "Training Loss:  96.76035309\n",
      "################################  1300  ################################\n",
      "Training Loss:  64.10020447\n",
      "################################  1400  ################################\n",
      "Training Loss:  67.88768768\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.28046799\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.48280334\n",
      "################################  1700  ################################\n",
      "Training Loss:  115.5869751\n",
      "################################  1800  ################################\n",
      "Training Loss:  64.8181839\n",
      "################################  1900  ################################\n",
      "Training Loss:  57.15054321\n",
      "Final training Loss:  57.47430038\n",
      "\n",
      "Running model (trial=6, mod=201, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  113908.0703125\n",
      "################################  100  ################################\n",
      "Training Loss:  6966.62060547\n",
      "################################  200  ################################\n",
      "Training Loss:  111.35839081\n",
      "################################  300  ################################\n",
      "Training Loss:  111.51992035\n",
      "################################  400  ################################\n",
      "Training Loss:  137.65261841\n",
      "################################  500  ################################\n",
      "Training Loss:  139.71186829\n",
      "################################  600  ################################\n",
      "Training Loss:  10802.94238281\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=202, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3167.87988281\n",
      "################################  100  ################################\n",
      "Training Loss:  276018.84375\n",
      "################################  200  ################################\n",
      "Training Loss:  173.94204712\n",
      "################################  300  ################################\n",
      "Training Loss:  117.71367645\n",
      "################################  400  ################################\n",
      "Training Loss:  117.47975922\n",
      "################################  500  ################################\n",
      "Training Loss:  116.05657959\n",
      "################################  600  ################################\n",
      "Training Loss:  115.94773865\n",
      "################################  700  ################################\n",
      "Training Loss:  118.35326385\n",
      "################################  800  ################################\n",
      "Training Loss:  118.62002563\n",
      "################################  900  ################################\n",
      "Training Loss:  114.16576385\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.82377625\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.55782318\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.05335999\n",
      "Epoch  1232: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.28992462\n",
      "################################  1400  ################################\n",
      "Training Loss:  112.31513214\n",
      "################################  1500  ################################\n",
      "Training Loss:  112.5240097\n",
      "Epoch  1554: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.89578247\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.56738281\n",
      "################################  1800  ################################\n",
      "Training Loss:  112.41584015\n",
      "Epoch  1809: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.40561676\n",
      "Final training Loss:  112.74241638\n",
      "\n",
      "Running model (trial=6, mod=203, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3634.88745117\n",
      "################################  100  ################################\n",
      "Training Loss:  127.77374268\n",
      "################################  200  ################################\n",
      "Training Loss:  45.48006058\n",
      "################################  300  ################################\n",
      "Training Loss:  42.69375229\n",
      "################################  400  ################################\n",
      "Training Loss:  41.16158295\n",
      "################################  500  ################################\n",
      "Training Loss:  42.72227097\n",
      "################################  600  ################################\n",
      "Training Loss:  43.37940216\n",
      "Epoch   668: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  39.23711395\n",
      "################################  800  ################################\n",
      "Training Loss:  49.25357056\n",
      "################################  900  ################################\n",
      "Training Loss:  47.25249481\n",
      "################################  1000  ################################\n",
      "Training Loss:  42.78245926\n",
      "################################  1100  ################################\n",
      "Training Loss:  38.37973785\n",
      "################################  1200  ################################\n",
      "Training Loss:  45.79496002\n",
      "Epoch  1254: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  40.7347641\n",
      "################################  1400  ################################\n",
      "Training Loss:  39.67795563\n",
      "Epoch  1455: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  40.56882095\n",
      "################################  1600  ################################\n",
      "Training Loss:  39.40598297\n",
      "Epoch  1656: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  37.29180145\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.60973358\n",
      "################################  1900  ################################\n",
      "Training Loss:  56.03781509\n",
      "Final training Loss:  37.77376556\n",
      "\n",
      "Running model (trial=6, mod=204, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  657087.5\n",
      "################################  100  ################################\n",
      "Training Loss:  104.01172638\n",
      "################################  200  ################################\n",
      "Training Loss:  103.00972748\n",
      "################################  300  ################################\n",
      "Training Loss:  102.42626953\n",
      "Epoch   359: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  104.48654938\n",
      "################################  500  ################################\n",
      "Training Loss:  102.45303345\n",
      "Epoch   560: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  116.90058899\n",
      "################################  700  ################################\n",
      "Training Loss:  104.12415314\n",
      "Epoch   761: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  103.31536865\n",
      "################################  900  ################################\n",
      "Training Loss:  103.36080933\n",
      "Epoch   962: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  110.55878448\n",
      "################################  1100  ################################\n",
      "Training Loss:  102.29043579\n",
      "Epoch  1163: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  104.52233124\n",
      "################################  1300  ################################\n",
      "Training Loss:  103.55949402\n",
      "Epoch  1364: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  103.45591736\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.99523926\n",
      "Epoch  1565: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  103.05812073\n",
      "################################  1700  ################################\n",
      "Training Loss:  103.21382141\n",
      "Epoch  1766: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  103.25520325\n",
      "################################  1900  ################################\n",
      "Training Loss:  103.30215454\n",
      "Epoch  1967: reducing learning rate of group 0 to 6.7109e-03.\n",
      "Final training Loss:  103.21965027\n",
      "\n",
      "Running model (trial=6, mod=205, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  57032.671875\n",
      "################################  100  ################################\n",
      "Training Loss:  46.3809433\n",
      "################################  200  ################################\n",
      "Training Loss:  53.80968857\n",
      "################################  300  ################################\n",
      "Training Loss:  46.33410645\n",
      "################################  400  ################################\n",
      "Training Loss:  268.01501465\n",
      "################################  500  ################################\n",
      "Training Loss:  52.52024841\n",
      "Epoch   565: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  41.03931808\n",
      "################################  700  ################################\n",
      "Training Loss:  43.44989014\n",
      "################################  800  ################################\n",
      "Training Loss:  38.23329163\n",
      "################################  900  ################################\n",
      "Training Loss:  39.71297455\n",
      "################################  1000  ################################\n",
      "Training Loss:  40.7250061\n",
      "################################  1100  ################################\n",
      "Training Loss:  39.332798\n",
      "################################  1200  ################################\n",
      "Training Loss:  41.17272186\n",
      "Epoch  1236: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  41.6230545\n",
      "################################  1400  ################################\n",
      "Training Loss:  38.29764938\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.51416397\n",
      "################################  1600  ################################\n",
      "Training Loss:  39.9066124\n",
      "Epoch  1617: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  39.68677902\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.74546432\n",
      "################################  1900  ################################\n",
      "Training Loss:  38.02587128\n",
      "Epoch  1990: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  39.82693481\n",
      "\n",
      "Running model (trial=6, mod=206, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  25125.80078125\n",
      "################################  100  ################################\n",
      "Training Loss:  117.17974854\n",
      "################################  200  ################################\n",
      "Training Loss:  51.18292236\n",
      "################################  300  ################################\n",
      "Training Loss:  73.32841492\n",
      "################################  400  ################################\n",
      "Training Loss:  48.84543991\n",
      "################################  500  ################################\n",
      "Training Loss:  43.75897217\n",
      "Epoch   535: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  48.41726303\n",
      "################################  700  ################################\n",
      "Training Loss:  54.1325531\n",
      "################################  800  ################################\n",
      "Training Loss:  38.37833023\n",
      "################################  900  ################################\n",
      "Training Loss:  44.90425873\n",
      "################################  1000  ################################\n",
      "Training Loss:  46.76135254\n",
      "Epoch  1004: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  41.41551971\n",
      "################################  1200  ################################\n",
      "Training Loss:  43.17793655\n",
      "Epoch  1277: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  48.55952454\n",
      "################################  1400  ################################\n",
      "Training Loss:  49.77735901\n",
      "Epoch  1478: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  38.51813889\n",
      "################################  1600  ################################\n",
      "Training Loss:  46.68175125\n",
      "Epoch  1679: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  38.75650406\n",
      "################################  1800  ################################\n",
      "Training Loss:  40.93387604\n",
      "################################  1900  ################################\n",
      "Training Loss:  38.4273262\n",
      "Final training Loss:  38.50728607\n",
      "\n",
      "Running model (trial=6, mod=207, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  44548132.0\n",
      "################################  100  ################################\n",
      "Training Loss:  156.22113037\n",
      "################################  200  ################################\n",
      "Training Loss:  110.96109009\n",
      "################################  300  ################################\n",
      "Training Loss:  108.2543869\n",
      "Epoch   303: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  106.35038757\n",
      "################################  500  ################################\n",
      "Training Loss:  129.95095825\n",
      "################################  600  ################################\n",
      "Training Loss:  92.39985657\n",
      "################################  700  ################################\n",
      "Training Loss:  85.71642303\n",
      "################################  800  ################################\n",
      "Training Loss:  72.4725647\n",
      "################################  900  ################################\n",
      "Training Loss:  63.93977737\n",
      "################################  1000  ################################\n",
      "Training Loss:  51.7831955\n",
      "################################  1100  ################################\n",
      "Training Loss:  49.18590164\n",
      "Epoch  1173: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  48.26972198\n",
      "################################  1300  ################################\n",
      "Training Loss:  48.26279068\n",
      "Epoch  1374: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.25809097\n",
      "################################  1500  ################################\n",
      "Training Loss:  48.97480011\n",
      "Epoch  1575: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  48.2322197\n",
      "################################  1700  ################################\n",
      "Training Loss:  49.34909058\n",
      "################################  1800  ################################\n",
      "Training Loss:  49.01800156\n",
      "################################  1900  ################################\n",
      "Training Loss:  47.39914703\n",
      "Final training Loss:  46.53533936\n",
      "\n",
      "Running model (trial=6, mod=208, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  239031680.0\n",
      "################################  100  ################################\n",
      "Training Loss:  133667.640625\n",
      "################################  200  ################################\n",
      "Training Loss:  114.50514221\n",
      "################################  300  ################################\n",
      "Training Loss:  73.137146\n",
      "################################  400  ################################\n",
      "Training Loss:  8457.49414062\n",
      "################################  500  ################################\n",
      "Training Loss:  47.84017181\n",
      "################################  600  ################################\n",
      "Training Loss:  40.43168259\n",
      "################################  700  ################################\n",
      "Training Loss:  39.79771042\n",
      "################################  800  ################################\n",
      "Training Loss:  40.30434418\n",
      "################################  900  ################################\n",
      "Training Loss:  43.13043976\n",
      "################################  1000  ################################\n",
      "Training Loss:  37.95604324\n",
      "################################  1100  ################################\n",
      "Training Loss:  38.45964432\n",
      "################################  1200  ################################\n",
      "Training Loss:  37.93883133\n",
      "################################  1300  ################################\n",
      "Training Loss:  40.2028656\n",
      "################################  1400  ################################\n",
      "Training Loss:  39.38563538\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.95673752\n",
      "Epoch  1561: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  37.28855133\n",
      "################################  1700  ################################\n",
      "Training Loss:  37.66749954\n",
      "Epoch  1762: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.77642441\n",
      "################################  1900  ################################\n",
      "Training Loss:  37.77618027\n",
      "Epoch  1963: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  37.77497482\n",
      "\n",
      "Running model (trial=6, mod=209, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  78101270757376.0\n",
      "################################  100  ################################\n",
      "Training Loss:  4861214.0\n",
      "################################  200  ################################\n",
      "Training Loss:  4824972.0\n",
      "################################  300  ################################\n",
      "Training Loss:  4784269.0\n",
      "################################  400  ################################\n",
      "Training Loss:  4743826.0\n",
      "################################  500  ################################\n",
      "Training Loss:  4634973.5\n",
      "################################  600  ################################\n",
      "Training Loss:  4340270.5\n",
      "################################  700  ################################\n",
      "Training Loss:  3997314.5\n",
      "################################  800  ################################\n",
      "Training Loss:  3561770.25\n",
      "################################  900  ################################\n",
      "Training Loss:  3392373.75\n",
      "################################  1000  ################################\n",
      "Training Loss:  3197143.5\n",
      "################################  1100  ################################\n",
      "Training Loss:  2835988.5\n",
      "################################  1200  ################################\n",
      "Training Loss:  2538598.25\n",
      "################################  1300  ################################\n",
      "Training Loss:  2315233.75\n",
      "################################  1400  ################################\n",
      "Training Loss:  2090258.625\n",
      "################################  1500  ################################\n",
      "Training Loss:  1859068.75\n",
      "################################  1600  ################################\n",
      "Training Loss:  1571179.75\n",
      "################################  1700  ################################\n",
      "Training Loss:  1298067.625\n",
      "################################  1800  ################################\n",
      "Training Loss:  1205727.375\n",
      "################################  1900  ################################\n",
      "Training Loss:  1159971.75\n",
      "Final training Loss:  1114407.375\n",
      "\n",
      "Running model (trial=7, mod=210, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5095.05517578\n",
      "################################  100  ################################\n",
      "Training Loss:  97.86611176\n",
      "################################  200  ################################\n",
      "Training Loss:  53.95969772\n",
      "################################  300  ################################\n",
      "Training Loss:  61.29779434\n",
      "Epoch   382: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  61.88918686\n",
      "################################  500  ################################\n",
      "Training Loss:  54.79333115\n",
      "Epoch   583: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  54.15416718\n",
      "################################  700  ################################\n",
      "Training Loss:  54.88497543\n",
      "Epoch   784: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  54.81549072\n",
      "################################  900  ################################\n",
      "Training Loss:  54.72242737\n",
      "Epoch   985: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  65.22452545\n",
      "################################  1100  ################################\n",
      "Training Loss:  59.62913513\n",
      "Epoch  1186: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.50443268\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.4729805\n",
      "Epoch  1387: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  54.82599258\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.60484695\n",
      "Epoch  1588: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.42954254\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.50405884\n",
      "Epoch  1789: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.55183792\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.62428665\n",
      "Epoch  1990: reducing learning rate of group 0 to 6.7109e-03.\n",
      "Final training Loss:  54.19072342\n",
      "\n",
      "Running model (trial=7, mod=211, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  137296.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  123.76409149\n",
      "################################  200  ################################\n",
      "Training Loss:  123.52098083\n",
      "Epoch   242: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  123.82084656\n",
      "################################  400  ################################\n",
      "Training Loss:  122.42235565\n",
      "Epoch   443: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  121.39217377\n",
      "################################  600  ################################\n",
      "Training Loss:  121.41047668\n",
      "################################  700  ################################\n",
      "Training Loss:  120.76924133\n",
      "################################  800  ################################\n",
      "Training Loss:  122.54199982\n",
      "################################  900  ################################\n",
      "Training Loss:  117.81204987\n",
      "################################  1000  ################################\n",
      "Training Loss:  114.71121216\n",
      "################################  1100  ################################\n",
      "Training Loss:  107.96496582\n",
      "################################  1200  ################################\n",
      "Training Loss:  95.84405518\n",
      "################################  1300  ################################\n",
      "Training Loss:  80.58270264\n",
      "################################  1400  ################################\n",
      "Training Loss:  56.75889587\n",
      "################################  1500  ################################\n",
      "Training Loss:  71.58221436\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.38343811\n",
      "################################  1700  ################################\n",
      "Training Loss:  59.69638062\n",
      "Epoch  1737: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.08564377\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.23886108\n",
      "Epoch  1938: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  52.39354324\n",
      "\n",
      "Running model (trial=7, mod=212, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7884.17871094\n",
      "################################  100  ################################\n",
      "Training Loss:  78.15460205\n",
      "################################  200  ################################\n",
      "Training Loss:  57.44653702\n",
      "################################  300  ################################\n",
      "Training Loss:  55.40644836\n",
      "################################  400  ################################\n",
      "Training Loss:  66.71624756\n",
      "################################  500  ################################\n",
      "Training Loss:  56.21362686\n",
      "Epoch   587: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  55.40591812\n",
      "################################  700  ################################\n",
      "Training Loss:  54.5405159\n",
      "################################  800  ################################\n",
      "Training Loss:  53.1867485\n",
      "################################  900  ################################\n",
      "Training Loss:  48.4044838\n",
      "################################  1000  ################################\n",
      "Training Loss:  43.01603317\n",
      "################################  1100  ################################\n",
      "Training Loss:  44.41989517\n",
      "################################  1200  ################################\n",
      "Training Loss:  47.08124542\n",
      "################################  1300  ################################\n",
      "Training Loss:  205.21452332\n",
      "################################  1400  ################################\n",
      "Training Loss:  87.53205109\n",
      "Epoch  1470: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  56.2989502\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.87149429\n",
      "Epoch  1671: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  59.61353683\n",
      "################################  1800  ################################\n",
      "Training Loss:  67.02606964\n",
      "Epoch  1872: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.61903\n",
      "Final training Loss:  53.23794556\n",
      "\n",
      "Running model (trial=7, mod=213, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  53767.38671875\n",
      "################################  100  ################################\n",
      "Training Loss:  111.60405731\n",
      "################################  200  ################################\n",
      "Training Loss:  109.09165192\n",
      "Epoch   277: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  103.05382538\n",
      "################################  400  ################################\n",
      "Training Loss:  97.56044006\n",
      "################################  500  ################################\n",
      "Training Loss:  89.98593903\n",
      "################################  600  ################################\n",
      "Training Loss:  76.47576904\n",
      "################################  700  ################################\n",
      "Training Loss:  59.3694191\n",
      "################################  800  ################################\n",
      "Training Loss:  53.44277191\n",
      "################################  900  ################################\n",
      "Training Loss:  52.93331528\n",
      "################################  1000  ################################\n",
      "Training Loss:  52.28453445\n",
      "Epoch  1018: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  51.96699524\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.08338165\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.35767746\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.12107086\n",
      "################################  1500  ################################\n",
      "Training Loss:  51.46734619\n",
      "Epoch  1577: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.51815414\n",
      "################################  1700  ################################\n",
      "Training Loss:  51.43018723\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.1631012\n",
      "Epoch  1885: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.11736679\n",
      "Final training Loss:  52.4955864\n",
      "\n",
      "Running model (trial=7, mod=214, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  316413.34375\n",
      "################################  100  ################################\n",
      "Training Loss:  128.40272522\n",
      "################################  200  ################################\n",
      "Training Loss:  123.83586121\n",
      "################################  300  ################################\n",
      "Training Loss:  124.04387665\n",
      "Epoch   302: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  122.69297028\n",
      "################################  500  ################################\n",
      "Training Loss:  121.90035248\n",
      "Epoch   503: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  124.1750946\n",
      "################################  700  ################################\n",
      "Training Loss:  121.28775024\n",
      "Epoch   704: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  121.23612976\n",
      "################################  900  ################################\n",
      "Training Loss:  121.04290009\n",
      "################################  1000  ################################\n",
      "Training Loss:  120.93225861\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.39796448\n",
      "################################  1200  ################################\n",
      "Training Loss:  118.81647491\n",
      "################################  1300  ################################\n",
      "Training Loss:  118.57029724\n",
      "################################  1400  ################################\n",
      "Training Loss:  118.42160034\n",
      "Epoch  1478: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  118.24046326\n",
      "################################  1600  ################################\n",
      "Training Loss:  119.3975296\n",
      "################################  1700  ################################\n",
      "Training Loss:  117.48770142\n",
      "################################  1800  ################################\n",
      "Training Loss:  116.63585663\n",
      "################################  1900  ################################\n",
      "Training Loss:  116.53424835\n",
      "Final training Loss:  116.92813873\n",
      "\n",
      "Running model (trial=7, mod=215, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  11026.53125\n",
      "################################  100  ################################\n",
      "Training Loss:  135.47839355\n",
      "################################  200  ################################\n",
      "Training Loss:  119.51278687\n",
      "Epoch   288: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  118.63358307\n",
      "################################  400  ################################\n",
      "Training Loss:  118.44050598\n",
      "################################  500  ################################\n",
      "Training Loss:  118.80477905\n",
      "################################  600  ################################\n",
      "Training Loss:  118.01613617\n",
      "################################  700  ################################\n",
      "Training Loss:  117.55206299\n",
      "################################  800  ################################\n",
      "Training Loss:  117.04371643\n",
      "################################  900  ################################\n",
      "Training Loss:  118.9391098\n",
      "################################  1000  ################################\n",
      "Training Loss:  119.45033264\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.06820679\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.57952881\n",
      "Epoch  1295: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.4539032\n",
      "################################  1400  ################################\n",
      "Training Loss:  115.6799469\n",
      "################################  1500  ################################\n",
      "Training Loss:  115.2492981\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.16365814\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.20091248\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.68818665\n",
      "################################  1900  ################################\n",
      "Training Loss:  113.3560257\n",
      "Final training Loss:  118.08428955\n",
      "\n",
      "Running model (trial=7, mod=216, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5241.14794922\n",
      "################################  100  ################################\n",
      "Training Loss:  129.94284058\n",
      "################################  200  ################################\n",
      "Training Loss:  115.39076996\n",
      "Epoch   278: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  115.59870148\n",
      "################################  400  ################################\n",
      "Training Loss:  115.65488434\n",
      "Epoch   479: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  115.53170013\n",
      "################################  600  ################################\n",
      "Training Loss:  115.91256714\n",
      "Epoch   680: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  115.53347778\n",
      "################################  800  ################################\n",
      "Training Loss:  115.43661499\n",
      "Epoch   881: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  115.55960083\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.33184052\n",
      "Epoch  1082: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  117.98441315\n",
      "################################  1200  ################################\n",
      "Training Loss:  115.32564545\n",
      "Epoch  1283: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.96529388\n",
      "################################  1400  ################################\n",
      "Training Loss:  117.72433472\n",
      "Epoch  1484: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  115.34572601\n",
      "################################  1600  ################################\n",
      "Training Loss:  116.08094788\n",
      "Epoch  1685: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  115.00261688\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.80524445\n",
      "Epoch  1886: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.16069031\n",
      "Final training Loss:  117.66561127\n",
      "\n",
      "Running model (trial=7, mod=217, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1909.96984863\n",
      "################################  100  ################################\n",
      "Training Loss:  133.81167603\n",
      "################################  200  ################################\n",
      "Training Loss:  129.91288757\n",
      "Epoch   297: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  123.15921783\n",
      "################################  400  ################################\n",
      "Training Loss:  121.54517365\n",
      "Epoch   498: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  121.60254669\n",
      "################################  600  ################################\n",
      "Training Loss:  120.08414459\n",
      "################################  700  ################################\n",
      "Training Loss:  127.10721588\n",
      "################################  800  ################################\n",
      "Training Loss:  129.38798523\n",
      "################################  900  ################################\n",
      "Training Loss:  95.84794617\n",
      "################################  1000  ################################\n",
      "Training Loss:  82.39975739\n",
      "################################  1100  ################################\n",
      "Training Loss:  63.57619858\n",
      "################################  1200  ################################\n",
      "Training Loss:  71.87006378\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.81381607\n",
      "################################  1400  ################################\n",
      "Training Loss:  72.58448792\n",
      "Epoch  1463: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.50873947\n",
      "################################  1600  ################################\n",
      "Training Loss:  62.90312576\n",
      "Epoch  1664: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  86.93614197\n",
      "################################  1800  ################################\n",
      "Training Loss:  109.67085266\n",
      "Epoch  1865: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  79.04572296\n",
      "Final training Loss:  76.80207062\n",
      "\n",
      "Running model (trial=7, mod=218, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  197687.265625\n",
      "################################  100  ################################\n",
      "Training Loss:  124.10372925\n",
      "################################  200  ################################\n",
      "Training Loss:  127.03268433\n",
      "################################  300  ################################\n",
      "Training Loss:  118.5579834\n",
      "################################  400  ################################\n",
      "Training Loss:  120.60617828\n",
      "################################  500  ################################\n",
      "Training Loss:  119.63921356\n",
      "################################  600  ################################\n",
      "Training Loss:  116.58598328\n",
      "################################  700  ################################\n",
      "Training Loss:  117.0276413\n",
      "################################  800  ################################\n",
      "Training Loss:  116.82709503\n",
      "################################  900  ################################\n",
      "Training Loss:  115.2554245\n",
      "################################  1000  ################################\n",
      "Training Loss:  119.03356171\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.56407166\n",
      "################################  1200  ################################\n",
      "Training Loss:  115.09470367\n",
      "Epoch  1282: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.19469452\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.34947205\n",
      "################################  1500  ################################\n",
      "Training Loss:  119.66073608\n",
      "################################  1600  ################################\n",
      "Training Loss:  117.41358185\n",
      "################################  1700  ################################\n",
      "Training Loss:  113.54666138\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.27829742\n",
      "################################  1900  ################################\n",
      "Training Loss:  131.63754272\n",
      "Epoch  1995: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  113.61974335\n",
      "\n",
      "Running model (trial=7, mod=219, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  729814.125\n",
      "################################  100  ################################\n",
      "Training Loss:  55.97947693\n",
      "################################  200  ################################\n",
      "Training Loss:  59.11780548\n",
      "################################  300  ################################\n",
      "Training Loss:  51.40436172\n",
      "################################  400  ################################\n",
      "Training Loss:  60.28624725\n",
      "################################  500  ################################\n",
      "Training Loss:  48.55867386\n",
      "################################  600  ################################\n",
      "Training Loss:  46.07574844\n",
      "################################  700  ################################\n",
      "Training Loss:  45.19271851\n",
      "################################  800  ################################\n",
      "Training Loss:  43.18011475\n",
      "################################  900  ################################\n",
      "Training Loss:  41.57343292\n",
      "################################  1000  ################################\n",
      "Training Loss:  41.20113373\n",
      "Epoch  1034: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.93632126\n",
      "################################  1200  ################################\n",
      "Training Loss:  41.82236099\n",
      "Epoch  1270: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  41.2481041\n",
      "################################  1400  ################################\n",
      "Training Loss:  40.80365753\n",
      "Epoch  1471: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  43.95887375\n",
      "################################  1600  ################################\n",
      "Training Loss:  42.26466751\n",
      "Epoch  1672: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  40.78455734\n",
      "################################  1800  ################################\n",
      "Training Loss:  42.10422516\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.50387573\n",
      "Epoch  1975: reducing learning rate of group 0 to 1.6384e-02.\n",
      "Final training Loss:  42.3970108\n",
      "\n",
      "Running model (trial=7, mod=220, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7756.61083984\n",
      "################################  100  ################################\n",
      "Training Loss:  118.19065857\n",
      "################################  200  ################################\n",
      "Training Loss:  112.44724274\n",
      "################################  300  ################################\n",
      "Training Loss:  85.01625061\n",
      "################################  400  ################################\n",
      "Training Loss:  62.09053802\n",
      "################################  500  ################################\n",
      "Training Loss:  47.72005081\n",
      "################################  600  ################################\n",
      "Training Loss:  50.77809143\n",
      "Epoch   691: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  64.72064209\n",
      "################################  800  ################################\n",
      "Training Loss:  93.09111786\n",
      "################################  900  ################################\n",
      "Training Loss:  44.27937317\n",
      "################################  1000  ################################\n",
      "Training Loss:  45.26791382\n",
      "################################  1100  ################################\n",
      "Training Loss:  165.5164032\n",
      "Epoch  1136: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  120.56845093\n",
      "################################  1300  ################################\n",
      "Training Loss:  112.31552887\n",
      "Epoch  1337: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  85.35816956\n",
      "################################  1500  ################################\n",
      "Training Loss:  56.83975601\n",
      "Epoch  1538: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  60.74740601\n",
      "################################  1700  ################################\n",
      "Training Loss:  55.45835114\n",
      "Epoch  1739: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  55.08538818\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.95345688\n",
      "Epoch  1940: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  62.4200325\n",
      "\n",
      "Running model (trial=7, mod=221, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2094177.25\n",
      "################################  100  ################################\n",
      "Training Loss:  76.07603455\n",
      "################################  200  ################################\n",
      "Training Loss:  65.02761841\n",
      "################################  300  ################################\n",
      "Training Loss:  56.42988586\n",
      "################################  400  ################################\n",
      "Training Loss:  56.71593094\n",
      "################################  500  ################################\n",
      "Training Loss:  49.0297966\n",
      "################################  600  ################################\n",
      "Training Loss:  46.89516449\n",
      "################################  700  ################################\n",
      "Training Loss:  51.90670776\n",
      "################################  800  ################################\n",
      "Training Loss:  45.46895981\n",
      "################################  900  ################################\n",
      "Training Loss:  46.628582\n",
      "################################  1000  ################################\n",
      "Training Loss:  43.85170364\n",
      "################################  1100  ################################\n",
      "Training Loss:  45.33518219\n",
      "################################  1200  ################################\n",
      "Training Loss:  49.20888901\n",
      "Epoch  1297: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.85826874\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.45757294\n",
      "Epoch  1498: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.14370346\n",
      "################################  1600  ################################\n",
      "Training Loss:  46.22578812\n",
      "Epoch  1699: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  48.93913651\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.28895569\n",
      "Epoch  1900: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.36831284\n",
      "Final training Loss:  46.6776619\n",
      "\n",
      "Running model (trial=7, mod=222, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  73663.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  127.06123352\n",
      "################################  200  ################################\n",
      "Training Loss:  125.65356445\n",
      "################################  300  ################################\n",
      "Training Loss:  159.42790222\n",
      "################################  400  ################################\n",
      "Training Loss:  126.08416748\n",
      "################################  500  ################################\n",
      "Training Loss:  129.29089355\n",
      "Epoch   530: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  122.96723175\n",
      "################################  700  ################################\n",
      "Training Loss:  119.34560394\n",
      "################################  800  ################################\n",
      "Training Loss:  118.7911377\n",
      "################################  900  ################################\n",
      "Training Loss:  118.33129883\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.53056335\n",
      "################################  1100  ################################\n",
      "Training Loss:  125.88678741\n",
      "################################  1200  ################################\n",
      "Training Loss:  117.35717773\n",
      "Epoch  1226: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  119.20115662\n",
      "################################  1400  ################################\n",
      "Training Loss:  125.84745789\n",
      "################################  1500  ################################\n",
      "Training Loss:  136.17657471\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.40801239\n",
      "Epoch  1602: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.8729248\n",
      "################################  1800  ################################\n",
      "Training Loss:  117.52670288\n",
      "Epoch  1851: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  120.85140991\n",
      "Final training Loss:  115.15071106\n",
      "\n",
      "Running model (trial=7, mod=223, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1871201.5\n",
      "################################  100  ################################\n",
      "Training Loss:  181.75976562\n",
      "################################  200  ################################\n",
      "Training Loss:  159.09925842\n",
      "################################  300  ################################\n",
      "Training Loss:  196.87771606\n",
      "################################  400  ################################\n",
      "Training Loss:  155.32009888\n",
      "################################  500  ################################\n",
      "Training Loss:  116.81308746\n",
      "################################  600  ################################\n",
      "Training Loss:  192.55194092\n",
      "Epoch   618: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  109.33158112\n",
      "################################  800  ################################\n",
      "Training Loss:  110.28318024\n",
      "Epoch   876: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  76.33079529\n",
      "################################  1000  ################################\n",
      "Training Loss:  105.65129852\n",
      "Epoch  1077: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  71.24739838\n",
      "################################  1200  ################################\n",
      "Training Loss:  88.29582214\n",
      "Epoch  1278: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  69.30067444\n",
      "################################  1400  ################################\n",
      "Training Loss:  71.91201782\n",
      "################################  1500  ################################\n",
      "Training Loss:  78.72618103\n",
      "################################  1600  ################################\n",
      "Training Loss:  96.96678925\n",
      "################################  1700  ################################\n",
      "Training Loss:  68.91346741\n",
      "################################  1800  ################################\n",
      "Training Loss:  79.92072296\n",
      "Epoch  1877: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  132.04776001\n",
      "Final training Loss:  73.5763092\n",
      "\n",
      "Running model (trial=7, mod=224, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1016340.25\n",
      "################################  100  ################################\n",
      "Training Loss:  108.50484467\n",
      "################################  200  ################################\n",
      "Training Loss:  116.6197052\n",
      "################################  300  ################################\n",
      "Training Loss:  82.57907867\n",
      "################################  400  ################################\n",
      "Training Loss:  86.69216156\n",
      "Epoch   452: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  81.75330353\n",
      "################################  600  ################################\n",
      "Training Loss:  136.52664185\n",
      "Epoch   653: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  61.53046417\n",
      "################################  800  ################################\n",
      "Training Loss:  76.94037628\n",
      "Epoch   854: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  85.93634033\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.24028778\n",
      "Epoch  1055: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  80.96757507\n",
      "################################  1200  ################################\n",
      "Training Loss:  68.06624603\n",
      "Epoch  1256: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.82216644\n",
      "################################  1400  ################################\n",
      "Training Loss:  58.43634415\n",
      "Epoch  1457: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  56.39145279\n",
      "################################  1600  ################################\n",
      "Training Loss:  56.65333557\n",
      "Epoch  1658: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  58.09077835\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.30384064\n",
      "Epoch  1859: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  59.47278595\n",
      "Final training Loss:  59.19963837\n",
      "\n",
      "Running model (trial=7, mod=225, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  36003.71484375\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=226, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1051000.5\n",
      "################################  100  ################################\n",
      "Training Loss:  4129.36279297\n",
      "################################  200  ################################\n",
      "Training Loss:  3675.73950195\n",
      "################################  300  ################################\n",
      "Training Loss:  2931.86547852\n",
      "################################  400  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=7, mod=227, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  86780.234375\n",
      "################################  100  ################################\n",
      "Training Loss:  104.48203278\n",
      "################################  200  ################################\n",
      "Training Loss:  104.43351746\n",
      "################################  300  ################################\n",
      "Training Loss:  141.29675293\n",
      "################################  400  ################################\n",
      "Training Loss:  103.04637146\n",
      "Epoch   462: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  105.84077454\n",
      "################################  600  ################################\n",
      "Training Loss:  111.87649536\n",
      "Epoch   663: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  103.53012085\n",
      "################################  800  ################################\n",
      "Training Loss:  112.608078\n",
      "################################  900  ################################\n",
      "Training Loss:  103.17658997\n",
      "Epoch   929: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  106.61001587\n",
      "################################  1100  ################################\n",
      "Training Loss:  105.65808105\n",
      "Epoch  1130: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  104.35190582\n",
      "################################  1300  ################################\n",
      "Training Loss:  101.78601837\n",
      "Epoch  1331: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  105.61296844\n",
      "################################  1500  ################################\n",
      "Training Loss:  136.83166504\n",
      "Epoch  1532: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  105.15412903\n",
      "################################  1700  ################################\n",
      "Training Loss:  102.30213165\n",
      "Epoch  1733: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  103.70546722\n",
      "################################  1900  ################################\n",
      "Training Loss:  103.81012726\n",
      "Epoch  1934: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Final training Loss:  111.60583496\n",
      "\n",
      "Running model (trial=7, mod=228, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  531196.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  63.14056015\n",
      "################################  200  ################################\n",
      "Training Loss:  44.49060822\n",
      "################################  300  ################################\n",
      "Training Loss:  48.9556694\n",
      "################################  400  ################################\n",
      "Training Loss:  45.59617233\n",
      "################################  500  ################################\n",
      "Training Loss:  69.41109467\n",
      "################################  600  ################################\n",
      "Training Loss:  43.97120285\n",
      "################################  700  ################################\n",
      "Training Loss:  53.18491364\n",
      "################################  800  ################################\n",
      "Training Loss:  42.23719788\n",
      "################################  900  ################################\n",
      "Training Loss:  51.56993103\n",
      "################################  1000  ################################\n",
      "Training Loss:  39.22743988\n",
      "################################  1100  ################################\n",
      "Training Loss:  87.2494812\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.0711441\n",
      "Epoch  1231: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  49.87787628\n",
      "################################  1400  ################################\n",
      "Training Loss:  43.16611099\n",
      "Epoch  1432: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  42.74499512\n",
      "################################  1600  ################################\n",
      "Training Loss:  41.85131073\n",
      "Epoch  1633: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  44.17304993\n",
      "################################  1800  ################################\n",
      "Training Loss:  41.58541489\n",
      "Epoch  1834: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  40.64646149\n",
      "Final training Loss:  46.49798584\n",
      "\n",
      "Running model (trial=7, mod=229, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  195048.46875\n",
      "################################  100  ################################\n",
      "Training Loss:  121.09468079\n",
      "################################  200  ################################\n",
      "Training Loss:  121.49635315\n",
      "Epoch   236: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  121.99588776\n",
      "################################  400  ################################\n",
      "Training Loss:  123.54824066\n",
      "Epoch   437: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  103.26582336\n",
      "################################  600  ################################\n",
      "Training Loss:  102.28077698\n",
      "################################  700  ################################\n",
      "Training Loss:  104.33057404\n",
      "Epoch   793: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  108.95474243\n",
      "################################  900  ################################\n",
      "Training Loss:  106.4125824\n",
      "Epoch   994: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  104.56276703\n",
      "################################  1100  ################################\n",
      "Training Loss:  103.69874573\n",
      "Epoch  1195: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  123.66606903\n",
      "################################  1300  ################################\n",
      "Training Loss:  105.32576752\n",
      "################################  1400  ################################\n",
      "Training Loss:  132.89575195\n",
      "################################  1500  ################################\n",
      "Training Loss:  100.66230774\n",
      "Epoch  1589: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  103.56237793\n",
      "################################  1700  ################################\n",
      "Training Loss:  99.78141785\n",
      "Epoch  1790: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  99.3028183\n",
      "################################  1900  ################################\n",
      "Training Loss:  99.32932281\n",
      "Epoch  1991: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Final training Loss:  98.57647705\n",
      "\n",
      "Running model (trial=7, mod=230, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  943771.25\n",
      "################################  100  ################################\n",
      "Training Loss:  110.6362915\n",
      "################################  200  ################################\n",
      "Training Loss:  109.12397766\n",
      "################################  300  ################################\n",
      "Training Loss:  125.3635788\n",
      "################################  400  ################################\n",
      "Training Loss:  108.26573181\n",
      "################################  500  ################################\n",
      "Training Loss:  143.82296753\n",
      "################################  600  ################################\n",
      "Training Loss:  55.90684509\n",
      "################################  700  ################################\n",
      "Training Loss:  58.50556564\n",
      "Epoch   719: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  55.40575409\n",
      "################################  900  ################################\n",
      "Training Loss:  66.95048523\n",
      "Epoch   920: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  58.36371231\n",
      "################################  1100  ################################\n",
      "Training Loss:  72.40853882\n",
      "Epoch  1121: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  69.94659424\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.86346054\n",
      "Epoch  1322: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  64.17458344\n",
      "################################  1500  ################################\n",
      "Training Loss:  71.22206879\n",
      "Epoch  1523: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.53129959\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.83132172\n",
      "Epoch  1742: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  59.27100754\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.72842789\n",
      "Final training Loss:  62.41916275\n",
      "\n",
      "Running model (trial=7, mod=231, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  16946.77929688\n",
      "################################  100  ################################\n",
      "Training Loss:  140.54669189\n",
      "################################  200  ################################\n",
      "Training Loss:  73.55073547\n",
      "################################  300  ################################\n",
      "Training Loss:  64.50128937\n",
      "################################  400  ################################\n",
      "Training Loss:  66.97618103\n",
      "################################  500  ################################\n",
      "Training Loss:  58.53155518\n",
      "################################  600  ################################\n",
      "Training Loss:  57.35269928\n",
      "################################  700  ################################\n",
      "Training Loss:  54.58628464\n",
      "################################  800  ################################\n",
      "Training Loss:  52.09182739\n",
      "################################  900  ################################\n",
      "Training Loss:  53.51060104\n",
      "################################  1000  ################################\n",
      "Training Loss:  48.63130569\n",
      "################################  1100  ################################\n",
      "Training Loss:  47.37737274\n",
      "################################  1200  ################################\n",
      "Training Loss:  424182.9375\n",
      "################################  1300  ################################\n",
      "Training Loss:  144.45016479\n",
      "Epoch  1371: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  112.51114655\n",
      "################################  1500  ################################\n",
      "Training Loss:  111.64193726\n",
      "Epoch  1572: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  110.98420715\n",
      "################################  1700  ################################\n",
      "Training Loss:  111.50455475\n",
      "Epoch  1773: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  110.36062622\n",
      "################################  1900  ################################\n",
      "Training Loss:  110.49889374\n",
      "Epoch  1974: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  110.07300568\n",
      "\n",
      "Running model (trial=7, mod=232, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  506557.90625\n",
      "################################  100  ################################\n",
      "Training Loss:  219.3793335\n",
      "################################  200  ################################\n",
      "Training Loss:  246.55136108\n",
      "################################  300  ################################\n",
      "Training Loss:  177.13308716\n",
      "Epoch   330: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  180.81033325\n",
      "################################  500  ################################\n",
      "Training Loss:  133.1651001\n",
      "Epoch   531: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  166.21961975\n",
      "################################  700  ################################\n",
      "Training Loss:  143.09094238\n",
      "Epoch   732: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  176.19039917\n",
      "################################  900  ################################\n",
      "Training Loss:  127.47682953\n",
      "################################  1000  ################################\n",
      "Training Loss:  147.35488892\n",
      "################################  1100  ################################\n",
      "Training Loss:  128.5584259\n",
      "################################  1200  ################################\n",
      "Training Loss:  210.95767212\n",
      "################################  1300  ################################\n",
      "Training Loss:  95.56408691\n",
      "################################  1400  ################################\n",
      "Training Loss:  155.40141296\n",
      "################################  1500  ################################\n",
      "Training Loss:  406.51885986\n",
      "################################  1600  ################################\n",
      "Training Loss:  133.38711548\n",
      "################################  1700  ################################\n",
      "Training Loss:  118.0798645\n",
      "Epoch  1722: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  68.43456268\n",
      "################################  1900  ################################\n",
      "Training Loss:  57.31657791\n",
      "Final training Loss:  74.28397369\n",
      "\n",
      "Running model (trial=7, mod=233, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1452253.75\n",
      "################################  100  ################################\n",
      "Training Loss:  57.39165878\n",
      "################################  200  ################################\n",
      "Training Loss:  52.2821312\n",
      "################################  300  ################################\n",
      "Training Loss:  52.075634\n",
      "################################  400  ################################\n",
      "Training Loss:  51.65307999\n",
      "################################  500  ################################\n",
      "Training Loss:  50.15480804\n",
      "################################  600  ################################\n",
      "Training Loss:  47.71749496\n",
      "################################  700  ################################\n",
      "Training Loss:  43.75259399\n",
      "################################  800  ################################\n",
      "Training Loss:  42.42399216\n",
      "################################  900  ################################\n",
      "Training Loss:  43.7875824\n",
      "################################  1000  ################################\n",
      "Training Loss:  43.90432739\n",
      "################################  1100  ################################\n",
      "Training Loss:  43.66003799\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.26862717\n",
      "################################  1300  ################################\n",
      "Training Loss:  41.34794617\n",
      "Epoch  1397: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  50.98869705\n",
      "################################  1500  ################################\n",
      "Training Loss:  43.39430618\n",
      "################################  1600  ################################\n",
      "Training Loss:  43.9561348\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.26165009\n",
      "Epoch  1795: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  41.39105225\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.34794998\n",
      "Epoch  1996: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  42.04347992\n",
      "\n",
      "Running model (trial=7, mod=234, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  111773.375\n",
      "################################  100  ################################\n",
      "Training Loss:  116.48070526\n",
      "################################  200  ################################\n",
      "Training Loss:  116.05085754\n",
      "Epoch   204: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  124.63343811\n",
      "################################  400  ################################\n",
      "Training Loss:  116.81115723\n",
      "Epoch   405: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  117.39718628\n",
      "################################  600  ################################\n",
      "Training Loss:  121.07291412\n",
      "Epoch   606: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  125.25888824\n",
      "################################  800  ################################\n",
      "Training Loss:  118.67125702\n",
      "Epoch   807: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  119.13550568\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.54722595\n",
      "Epoch  1008: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.66564178\n",
      "################################  1200  ################################\n",
      "Training Loss:  115.57244873\n",
      "Epoch  1209: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  290.94787598\n",
      "################################  1400  ################################\n",
      "Training Loss:  57.23136902\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.59985733\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.73371887\n",
      "################################  1700  ################################\n",
      "Training Loss:  59.28605652\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.58067322\n",
      "################################  1900  ################################\n",
      "Training Loss:  50.7959137\n",
      "Final training Loss:  49.98631668\n",
      "\n",
      "Running model (trial=7, mod=235, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  653163.9375\n",
      "################################  100  ################################\n",
      "Training Loss:  67.41194916\n",
      "################################  200  ################################\n",
      "Training Loss:  50.21156311\n",
      "################################  300  ################################\n",
      "Training Loss:  64.25032043\n",
      "################################  400  ################################\n",
      "Training Loss:  55.0825386\n",
      "################################  500  ################################\n",
      "Training Loss:  56.46389771\n",
      "################################  600  ################################\n",
      "Training Loss:  95.02218628\n",
      "################################  700  ################################\n",
      "Training Loss:  43.34556961\n",
      "################################  800  ################################\n",
      "Training Loss:  44.5437088\n",
      "################################  900  ################################\n",
      "Training Loss:  78.70141602\n",
      "################################  1000  ################################\n",
      "Training Loss:  44.50558472\n",
      "Epoch  1023: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  47.56629562\n",
      "################################  1200  ################################\n",
      "Training Loss:  46.92617798\n",
      "################################  1300  ################################\n",
      "Training Loss:  40.23677826\n",
      "################################  1400  ################################\n",
      "Training Loss:  43.17648697\n",
      "################################  1500  ################################\n",
      "Training Loss:  71.64735413\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.05913544\n",
      "################################  1700  ################################\n",
      "Training Loss:  38.4594841\n",
      "################################  1800  ################################\n",
      "Training Loss:  44.97480011\n",
      "################################  1900  ################################\n",
      "Training Loss:  43.69433594\n",
      "Epoch  1951: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  40.80773163\n",
      "\n",
      "Running model (trial=7, mod=236, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3304859.5\n",
      "################################  100  ################################\n",
      "Training Loss:  60.25525284\n",
      "################################  200  ################################\n",
      "Training Loss:  49.07847214\n",
      "################################  300  ################################\n",
      "Training Loss:  41.85765076\n",
      "################################  400  ################################\n",
      "Training Loss:  58.6024437\n",
      "################################  500  ################################\n",
      "Training Loss:  41.58823013\n",
      "################################  600  ################################\n",
      "Training Loss:  42.61428833\n",
      "################################  700  ################################\n",
      "Training Loss:  40.26654053\n",
      "Epoch   711: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  39.38405609\n",
      "################################  900  ################################\n",
      "Training Loss:  40.54353333\n",
      "################################  1000  ################################\n",
      "Training Loss:  39.31062317\n",
      "################################  1100  ################################\n",
      "Training Loss:  49.84685135\n",
      "################################  1200  ################################\n",
      "Training Loss:  42.647789\n",
      "################################  1300  ################################\n",
      "Training Loss:  42.25411606\n",
      "################################  1400  ################################\n",
      "Training Loss:  40.75313568\n",
      "Epoch  1446: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  38.55673599\n",
      "################################  1600  ################################\n",
      "Training Loss:  39.27991104\n",
      "Epoch  1650: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  42.18499756\n",
      "################################  1800  ################################\n",
      "Training Loss:  40.44621277\n",
      "Epoch  1851: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  37.70334625\n",
      "Final training Loss:  40.61515808\n",
      "\n",
      "Running model (trial=7, mod=237, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4453614.0\n",
      "################################  100  ################################\n",
      "Training Loss:  117.40498352\n",
      "################################  200  ################################\n",
      "Training Loss:  114.92537689\n",
      "################################  300  ################################\n",
      "Training Loss:  114.87690735\n",
      "################################  400  ################################\n",
      "Training Loss:  115.37569427\n",
      "Epoch   432: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  121.43492889\n",
      "################################  600  ################################\n",
      "Training Loss:  114.24908447\n",
      "Epoch   633: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  115.97359467\n",
      "################################  800  ################################\n",
      "Training Loss:  113.85739136\n",
      "Epoch   834: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  114.57644653\n",
      "################################  1000  ################################\n",
      "Training Loss:  113.95088196\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.96092987\n",
      "Epoch  1177: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.14598846\n",
      "################################  1300  ################################\n",
      "Training Loss:  116.35993195\n",
      "Epoch  1378: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  113.77799988\n",
      "################################  1500  ################################\n",
      "Training Loss:  113.85327911\n",
      "Epoch  1579: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.90596771\n",
      "################################  1700  ################################\n",
      "Training Loss:  114.04060364\n",
      "Epoch  1780: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  113.62870789\n",
      "################################  1900  ################################\n",
      "Training Loss:  118.48905182\n",
      "Epoch  1981: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Final training Loss:  114.28417206\n",
      "\n",
      "Running model (trial=7, mod=238, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  686915776.0\n",
      "################################  100  ################################\n",
      "Training Loss:  4753.0625\n",
      "################################  200  ################################\n",
      "Training Loss:  3888.09301758\n",
      "Epoch   222: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  2993.25292969\n",
      "################################  400  ################################\n",
      "Training Loss:  2277.77124023\n",
      "################################  500  ################################\n",
      "Training Loss:  1593.16040039\n",
      "################################  600  ################################\n",
      "Training Loss:  714.08209229\n",
      "################################  700  ################################\n",
      "Training Loss:  391.17810059\n",
      "################################  800  ################################\n",
      "Training Loss:  107.20863342\n",
      "################################  900  ################################\n",
      "Training Loss:  53.89124298\n",
      "################################  1000  ################################\n",
      "Training Loss:  51.39425659\n",
      "################################  1100  ################################\n",
      "Training Loss:  50.59121704\n",
      "################################  1200  ################################\n",
      "Training Loss:  50.82932281\n",
      "################################  1300  ################################\n",
      "Training Loss:  49.87060165\n",
      "################################  1400  ################################\n",
      "Training Loss:  49.79264832\n",
      "################################  1500  ################################\n",
      "Training Loss:  49.81246567\n",
      "################################  1600  ################################\n",
      "Training Loss:  49.70886612\n",
      "Epoch  1631: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  49.48233795\n",
      "################################  1800  ################################\n",
      "Training Loss:  49.53142929\n",
      "################################  1900  ################################\n",
      "Training Loss:  49.52054977\n",
      "Final training Loss:  49.2652092\n",
      "\n",
      "Running model (trial=7, mod=239, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3705234199085056e+16\n",
      "################################  100  ################################\n",
      "Training Loss:  417967136.0\n",
      "################################  200  ################################\n",
      "Training Loss:  386370560.0\n",
      "Epoch   204: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  355025184.0\n",
      "################################  400  ################################\n",
      "Training Loss:  321724256.0\n",
      "Epoch   405: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  292049408.0\n",
      "################################  600  ################################\n",
      "Training Loss:  261770720.0\n",
      "Epoch   606: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  237966288.0\n",
      "################################  800  ################################\n",
      "Training Loss:  214780160.0\n",
      "Epoch   807: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  195989568.0\n",
      "################################  1000  ################################\n",
      "Training Loss:  178270128.0\n",
      "Epoch  1008: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  163833392.0\n",
      "################################  1200  ################################\n",
      "Training Loss:  150388512.0\n",
      "Epoch  1209: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  139005840.0\n",
      "################################  1400  ################################\n",
      "Training Loss:  128629592.0\n",
      "Epoch  1410: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  120158088.0\n",
      "################################  1600  ################################\n",
      "Training Loss:  112124184.0\n",
      "Epoch  1611: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  105481432.0\n",
      "################################  1800  ################################\n",
      "Training Loss:  99181664.0\n",
      "Epoch  1812: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  94163672.0\n",
      "Final training Loss:  89407152.0\n",
      "\n",
      "Running model (trial=8, mod=240, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  906936.4375\n",
      "################################  100  ################################\n",
      "Training Loss:  124.51813507\n",
      "################################  200  ################################\n",
      "Training Loss:  123.56537628\n",
      "Epoch   278: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  123.55950928\n",
      "################################  400  ################################\n",
      "Training Loss:  123.56291962\n",
      "Epoch   479: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  123.56385803\n",
      "################################  600  ################################\n",
      "Training Loss:  123.56365204\n",
      "Epoch   680: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  123.60831451\n",
      "################################  800  ################################\n",
      "Training Loss:  123.60704041\n",
      "Epoch   881: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  123.55690765\n",
      "################################  1000  ################################\n",
      "Training Loss:  123.55706787\n",
      "Epoch  1082: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  123.55630493\n",
      "################################  1200  ################################\n",
      "Training Loss:  123.55626678\n",
      "Epoch  1283: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  123.55569458\n",
      "################################  1400  ################################\n",
      "Training Loss:  123.55858612\n",
      "Epoch  1484: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  123.55632019\n",
      "################################  1600  ################################\n",
      "Training Loss:  123.55540466\n",
      "Epoch  1685: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  123.55530548\n",
      "################################  1800  ################################\n",
      "Training Loss:  123.55525208\n",
      "Epoch  1886: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  123.55512238\n",
      "Final training Loss:  123.55510712\n",
      "\n",
      "Running model (trial=8, mod=241, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  28321.93554688\n",
      "################################  100  ################################\n",
      "Training Loss:  120.32247162\n",
      "################################  200  ################################\n",
      "Training Loss:  119.08942413\n",
      "################################  300  ################################\n",
      "Training Loss:  116.75263214\n",
      "################################  400  ################################\n",
      "Training Loss:  116.92622375\n",
      "################################  500  ################################\n",
      "Training Loss:  115.48480225\n",
      "################################  600  ################################\n",
      "Training Loss:  114.08657074\n",
      "################################  700  ################################\n",
      "Training Loss:  118.66284943\n",
      "################################  800  ################################\n",
      "Training Loss:  113.04727173\n",
      "################################  900  ################################\n",
      "Training Loss:  114.78610229\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.47516632\n",
      "Epoch  1062: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  113.21134949\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.20420837\n",
      "Epoch  1263: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  113.2880249\n",
      "################################  1400  ################################\n",
      "Training Loss:  112.37309265\n",
      "Epoch  1464: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  119.31743622\n",
      "################################  1600  ################################\n",
      "Training Loss:  116.64652252\n",
      "Epoch  1665: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  115.82905579\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.20168304\n",
      "Epoch  1866: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.91059875\n",
      "Final training Loss:  112.9292984\n",
      "\n",
      "Running model (trial=8, mod=242, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  29380.14648438\n",
      "################################  100  ################################\n",
      "Training Loss:  73.87546539\n",
      "################################  200  ################################\n",
      "Training Loss:  63.55665207\n",
      "################################  300  ################################\n",
      "Training Loss:  58.87810516\n",
      "################################  400  ################################\n",
      "Training Loss:  58.00542068\n",
      "################################  500  ################################\n",
      "Training Loss:  58.4657135\n",
      "################################  600  ################################\n",
      "Training Loss:  53.3898468\n",
      "################################  700  ################################\n",
      "Training Loss:  59.93630981\n",
      "################################  800  ################################\n",
      "Training Loss:  53.83763123\n",
      "################################  900  ################################\n",
      "Training Loss:  60.85319519\n",
      "################################  1000  ################################\n",
      "Training Loss:  58.53249359\n",
      "################################  1100  ################################\n",
      "Training Loss:  341.31347656\n",
      "################################  1200  ################################\n",
      "Training Loss:  68.9597702\n",
      "Epoch  1296: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  68.58700562\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.37459564\n",
      "Epoch  1497: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  61.83532715\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.74995804\n",
      "Epoch  1698: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.6348877\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.62363052\n",
      "Epoch  1899: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.01413727\n",
      "Final training Loss:  55.1557312\n",
      "\n",
      "Running model (trial=8, mod=243, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  40604.17578125\n",
      "################################  100  ################################\n",
      "Training Loss:  138.0383606\n",
      "################################  200  ################################\n",
      "Training Loss:  117.22074127\n",
      "################################  300  ################################\n",
      "Training Loss:  122.13330078\n",
      "################################  400  ################################\n",
      "Training Loss:  122.92456055\n",
      "################################  500  ################################\n",
      "Training Loss:  122.36196899\n",
      "################################  600  ################################\n",
      "Training Loss:  124.1798172\n",
      "Epoch   643: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  113.90534973\n",
      "################################  800  ################################\n",
      "Training Loss:  116.99040985\n",
      "################################  900  ################################\n",
      "Training Loss:  122.54971313\n",
      "################################  1000  ################################\n",
      "Training Loss:  127.5472641\n",
      "Epoch  1015: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  123.91340637\n",
      "################################  1200  ################################\n",
      "Training Loss:  124.30431366\n",
      "Epoch  1216: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  114.14134216\n",
      "################################  1400  ################################\n",
      "Training Loss:  113.86641693\n",
      "################################  1500  ################################\n",
      "Training Loss:  120.85286713\n",
      "Epoch  1532: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  112.77509308\n",
      "################################  1700  ################################\n",
      "Training Loss:  121.52967072\n",
      "Epoch  1733: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  114.62435913\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.3221283\n",
      "Epoch  1934: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  113.92809296\n",
      "\n",
      "Running model (trial=8, mod=244, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12684.99609375\n",
      "################################  100  ################################\n",
      "Training Loss:  129.07266235\n",
      "################################  200  ################################\n",
      "Training Loss:  119.54276276\n",
      "Epoch   290: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  119.95323944\n",
      "################################  400  ################################\n",
      "Training Loss:  119.46372986\n",
      "Epoch   491: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  119.02256012\n",
      "################################  600  ################################\n",
      "Training Loss:  121.13378143\n",
      "################################  700  ################################\n",
      "Training Loss:  119.54463196\n",
      "################################  800  ################################\n",
      "Training Loss:  118.93265533\n",
      "################################  900  ################################\n",
      "Training Loss:  119.59046173\n",
      "################################  1000  ################################\n",
      "Training Loss:  118.98446655\n",
      "################################  1100  ################################\n",
      "Training Loss:  118.95122528\n",
      "################################  1200  ################################\n",
      "Training Loss:  118.46104431\n",
      "################################  1300  ################################\n",
      "Training Loss:  118.36564636\n",
      "Epoch  1315: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  120.53938293\n",
      "################################  1500  ################################\n",
      "Training Loss:  118.0396347\n",
      "################################  1600  ################################\n",
      "Training Loss:  121.00004578\n",
      "################################  1700  ################################\n",
      "Training Loss:  117.93666077\n",
      "################################  1800  ################################\n",
      "Training Loss:  117.34146881\n",
      "################################  1900  ################################\n",
      "Training Loss:  123.771698\n",
      "Final training Loss:  116.35884094\n",
      "\n",
      "Running model (trial=8, mod=245, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1273398.375\n",
      "################################  100  ################################\n",
      "Training Loss:  122.42733002\n",
      "################################  200  ################################\n",
      "Training Loss:  122.39402008\n",
      "Epoch   280: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  122.78522491\n",
      "################################  400  ################################\n",
      "Training Loss:  122.48790741\n",
      "Epoch   481: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  122.3936615\n",
      "################################  600  ################################\n",
      "Training Loss:  122.20015717\n",
      "################################  700  ################################\n",
      "Training Loss:  122.02867126\n",
      "################################  800  ################################\n",
      "Training Loss:  122.02741241\n",
      "################################  900  ################################\n",
      "Training Loss:  122.08159637\n",
      "################################  1000  ################################\n",
      "Training Loss:  122.07562256\n",
      "################################  1100  ################################\n",
      "Training Loss:  122.54813385\n",
      "################################  1200  ################################\n",
      "Training Loss:  121.35704041\n",
      "################################  1300  ################################\n",
      "Training Loss:  121.63214874\n",
      "################################  1400  ################################\n",
      "Training Loss:  121.1038208\n",
      "################################  1500  ################################\n",
      "Training Loss:  121.08979034\n",
      "################################  1600  ################################\n",
      "Training Loss:  120.98840332\n",
      "Epoch  1628: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  121.22114563\n",
      "################################  1800  ################################\n",
      "Training Loss:  120.50600433\n",
      "Epoch  1829: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  120.46112061\n",
      "Final training Loss:  120.45672607\n",
      "\n",
      "Running model (trial=8, mod=246, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  137966.5\n",
      "################################  100  ################################\n",
      "Training Loss:  178.95002747\n",
      "################################  200  ################################\n",
      "Training Loss:  118.259552\n",
      "################################  300  ################################\n",
      "Training Loss:  113.11545563\n",
      "################################  400  ################################\n",
      "Training Loss:  108.35552979\n",
      "################################  500  ################################\n",
      "Training Loss:  102.68627167\n",
      "################################  600  ################################\n",
      "Training Loss:  96.40968323\n",
      "################################  700  ################################\n",
      "Training Loss:  90.19262695\n",
      "################################  800  ################################\n",
      "Training Loss:  72.45983124\n",
      "################################  900  ################################\n",
      "Training Loss:  66.71012115\n",
      "################################  1000  ################################\n",
      "Training Loss:  75.1986084\n",
      "################################  1100  ################################\n",
      "Training Loss:  72.91204834\n",
      "################################  1200  ################################\n",
      "Training Loss:  61.8846817\n",
      "Epoch  1222: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.85102081\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.97834778\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.53559113\n",
      "################################  1600  ################################\n",
      "Training Loss:  68.1857605\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.73053741\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.01009369\n",
      "################################  1900  ################################\n",
      "Training Loss:  85.9618988\n",
      "Final training Loss:  68.1650238\n",
      "\n",
      "Running model (trial=8, mod=247, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  19276.78710938\n",
      "################################  100  ################################\n",
      "Training Loss:  112.00794983\n",
      "################################  200  ################################\n",
      "Training Loss:  87.40681458\n",
      "################################  300  ################################\n",
      "Training Loss:  69.51023865\n",
      "################################  400  ################################\n",
      "Training Loss:  52.71286011\n",
      "################################  500  ################################\n",
      "Training Loss:  52.39397049\n",
      "################################  600  ################################\n",
      "Training Loss:  50.87115097\n",
      "################################  700  ################################\n",
      "Training Loss:  57.73524094\n",
      "################################  800  ################################\n",
      "Training Loss:  57.70022964\n",
      "################################  900  ################################\n",
      "Training Loss:  57.86156845\n",
      "################################  1000  ################################\n",
      "Training Loss:  79.27119446\n",
      "################################  1100  ################################\n",
      "Training Loss:  45.51018524\n",
      "################################  1200  ################################\n",
      "Training Loss:  47.8601532\n",
      "################################  1300  ################################\n",
      "Training Loss:  51.57509232\n",
      "################################  1400  ################################\n",
      "Training Loss:  71.00234222\n",
      "################################  1500  ################################\n",
      "Training Loss:  839.78015137\n",
      "Epoch  1569: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  119.0524826\n",
      "################################  1700  ################################\n",
      "Training Loss:  119.01207733\n",
      "Epoch  1770: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  135.58139038\n",
      "################################  1900  ################################\n",
      "Training Loss:  127.86799622\n",
      "Epoch  1971: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  117.70439148\n",
      "\n",
      "Running model (trial=8, mod=248, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  7719.60107422\n",
      "################################  100  ################################\n",
      "Training Loss:  58.93554688\n",
      "################################  200  ################################\n",
      "Training Loss:  63.04669189\n",
      "Epoch   290: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  60.34345245\n",
      "################################  400  ################################\n",
      "Training Loss:  60.4840889\n",
      "################################  500  ################################\n",
      "Training Loss:  55.02897644\n",
      "Epoch   586: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  54.19725418\n",
      "################################  700  ################################\n",
      "Training Loss:  59.28312302\n",
      "################################  800  ################################\n",
      "Training Loss:  83.34036255\n",
      "################################  900  ################################\n",
      "Training Loss:  64.9524765\n",
      "Epoch   956: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  59.3682785\n",
      "################################  1100  ################################\n",
      "Training Loss:  55.2394104\n",
      "Epoch  1157: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  59.79959488\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.40398407\n",
      "################################  1400  ################################\n",
      "Training Loss:  64.07939911\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.98577118\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.36335373\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.82455063\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.99674606\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.68711853\n",
      "Final training Loss:  57.33686829\n",
      "\n",
      "Running model (trial=8, mod=249, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  441471.0\n",
      "################################  100  ################################\n",
      "Training Loss:  62.51795197\n",
      "################################  200  ################################\n",
      "Training Loss:  53.62386703\n",
      "################################  300  ################################\n",
      "Training Loss:  53.11667633\n",
      "################################  400  ################################\n",
      "Training Loss:  53.03581238\n",
      "################################  500  ################################\n",
      "Training Loss:  54.67917252\n",
      "################################  600  ################################\n",
      "Training Loss:  54.38108826\n",
      "################################  700  ################################\n",
      "Training Loss:  54.26488495\n",
      "################################  800  ################################\n",
      "Training Loss:  61.8976059\n",
      "################################  900  ################################\n",
      "Training Loss:  51.1686821\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.24803925\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.9779892\n",
      "################################  1200  ################################\n",
      "Training Loss:  51.85456085\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.33626938\n",
      "################################  1400  ################################\n",
      "Training Loss:  50.33397675\n",
      "################################  1500  ################################\n",
      "Training Loss:  54.4464798\n",
      "################################  1600  ################################\n",
      "Training Loss:  48.453022\n",
      "################################  1700  ################################\n",
      "Training Loss:  76.31480408\n",
      "################################  1800  ################################\n",
      "Training Loss:  42.29279327\n",
      "################################  1900  ################################\n",
      "Training Loss:  43.88940048\n",
      "Final training Loss:  56.28059006\n",
      "\n",
      "Running model (trial=8, mod=250, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1613341.5\n",
      "################################  100  ################################\n",
      "Training Loss:  207.26731873\n",
      "################################  200  ################################\n",
      "Training Loss:  93.28253937\n",
      "################################  300  ################################\n",
      "Training Loss:  54.9872551\n",
      "################################  400  ################################\n",
      "Training Loss:  55.93313599\n",
      "################################  500  ################################\n",
      "Training Loss:  50.02460098\n",
      "################################  600  ################################\n",
      "Training Loss:  50.45492554\n",
      "################################  700  ################################\n",
      "Training Loss:  48.88366699\n",
      "################################  800  ################################\n",
      "Training Loss:  45.51486588\n",
      "################################  900  ################################\n",
      "Training Loss:  44.13231659\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.77199554\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.03021622\n",
      "################################  1200  ################################\n",
      "Training Loss:  40.5255661\n",
      "################################  1300  ################################\n",
      "Training Loss:  48.70126343\n",
      "################################  1400  ################################\n",
      "Training Loss:  62.06850433\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.82110596\n",
      "################################  1600  ################################\n",
      "Training Loss:  41.29538727\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.41977692\n",
      "################################  1800  ################################\n",
      "Training Loss:  40.20976639\n",
      "################################  1900  ################################\n",
      "Training Loss:  42.0656662\n",
      "Final training Loss:  42.88616562\n",
      "\n",
      "Running model (trial=8, mod=251, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1346266.0\n",
      "################################  100  ################################\n",
      "Training Loss:  100.33146667\n",
      "################################  200  ################################\n",
      "Training Loss:  60.96369553\n",
      "################################  300  ################################\n",
      "Training Loss:  56.57862854\n",
      "################################  400  ################################\n",
      "Training Loss:  52.17047119\n",
      "################################  500  ################################\n",
      "Training Loss:  50.2564621\n",
      "################################  600  ################################\n",
      "Training Loss:  89.62699127\n",
      "################################  700  ################################\n",
      "Training Loss:  47.24882889\n",
      "################################  800  ################################\n",
      "Training Loss:  46.68561554\n",
      "################################  900  ################################\n",
      "Training Loss:  61.40763092\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.80976105\n",
      "################################  1100  ################################\n",
      "Training Loss:  55.81915283\n",
      "################################  1200  ################################\n",
      "Training Loss:  136.1865387\n",
      "Epoch  1300: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  123.8894043\n",
      "################################  1400  ################################\n",
      "Training Loss:  123.34172058\n",
      "################################  1500  ################################\n",
      "Epoch  1501: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Training Loss:  117.35959625\n",
      "################################  1600  ################################\n",
      "Training Loss:  116.5289917\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.59947205\n",
      "Epoch  1702: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  117.24694824\n",
      "################################  1900  ################################\n",
      "Training Loss:  117.78131866\n",
      "Epoch  1903: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  120.23686218\n",
      "\n",
      "Running model (trial=8, mod=252, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  448502.21875\n",
      "################################  100  ################################\n",
      "Training Loss:  115.07134247\n",
      "################################  200  ################################\n",
      "Training Loss:  113.63734436\n",
      "################################  300  ################################\n",
      "Training Loss:  114.40982819\n",
      "################################  400  ################################\n",
      "Training Loss:  117.01778412\n",
      "################################  500  ################################\n",
      "Training Loss:  116.53791046\n",
      "################################  600  ################################\n",
      "Training Loss:  112.43669891\n",
      "################################  700  ################################\n",
      "Training Loss:  120.72797394\n",
      "################################  800  ################################\n",
      "Training Loss:  118.75886536\n",
      "################################  900  ################################\n",
      "Training Loss:  116.66748047\n",
      "################################  1000  ################################\n",
      "Training Loss:  121.07873535\n",
      "Epoch  1068: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  119.87203979\n",
      "################################  1200  ################################\n",
      "Training Loss:  111.98391724\n",
      "Epoch  1269: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  111.77961731\n",
      "################################  1400  ################################\n",
      "Training Loss:  112.38286591\n",
      "################################  1500  ################################\n",
      "Training Loss:  119.71202087\n",
      "Epoch  1566: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  111.40778351\n",
      "################################  1700  ################################\n",
      "Training Loss:  131.02726746\n",
      "Epoch  1767: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  117.34541321\n",
      "################################  1900  ################################\n",
      "Training Loss:  112.28044891\n",
      "Final training Loss:  111.70381927\n",
      "\n",
      "Running model (trial=8, mod=253, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  64200.77734375\n",
      "################################  100  ################################\n",
      "Training Loss:  130.48695374\n",
      "################################  200  ################################\n",
      "Training Loss:  57.37851715\n",
      "################################  300  ################################\n",
      "Training Loss:  73.8886261\n",
      "################################  400  ################################\n",
      "Training Loss:  81.65859985\n",
      "################################  500  ################################\n",
      "Training Loss:  95.33222961\n",
      "################################  600  ################################\n",
      "Training Loss:  56.12773895\n",
      "################################  700  ################################\n",
      "Training Loss:  57.83256531\n",
      "Epoch   794: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  55.57778549\n",
      "################################  900  ################################\n",
      "Training Loss:  72.9262085\n",
      "################################  1000  ################################\n",
      "Training Loss:  62.93950653\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.43062973\n",
      "################################  1200  ################################\n",
      "Training Loss:  55.11086273\n",
      "Epoch  1243: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  60.18891144\n",
      "################################  1400  ################################\n",
      "Training Loss:  66.74893188\n",
      "################################  1500  ################################\n",
      "Training Loss:  63.47693253\n",
      "################################  1600  ################################\n",
      "Training Loss:  80.78962708\n",
      "Epoch  1604: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.5225563\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.54136276\n",
      "Epoch  1805: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  56.68011856\n",
      "Final training Loss:  53.83351898\n",
      "\n",
      "Running model (trial=8, mod=254, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4311558.5\n",
      "################################  100  ################################\n",
      "Training Loss:  117.13205719\n",
      "################################  200  ################################\n",
      "Training Loss:  117.55252838\n",
      "Epoch   226: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  122.06311035\n",
      "################################  400  ################################\n",
      "Training Loss:  116.20041656\n",
      "Epoch   427: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  134.02685547\n",
      "################################  600  ################################\n",
      "Training Loss:  115.69736481\n",
      "################################  700  ################################\n",
      "Training Loss:  112.67614746\n",
      "################################  800  ################################\n",
      "Training Loss:  119.66634369\n",
      "################################  900  ################################\n",
      "Training Loss:  114.19910431\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.58822632\n",
      "Epoch  1090: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  120.69500732\n",
      "################################  1200  ################################\n",
      "Training Loss:  118.52366638\n",
      "################################  1300  ################################\n",
      "Training Loss:  145.67756653\n",
      "################################  1400  ################################\n",
      "Training Loss:  121.43473816\n",
      "Epoch  1435: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  140.04516602\n",
      "################################  1600  ################################\n",
      "Training Loss:  120.0628891\n",
      "Epoch  1636: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  116.34365845\n",
      "################################  1800  ################################\n",
      "Training Loss:  117.95110321\n",
      "Epoch  1837: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  123.96044159\n",
      "Final training Loss:  122.11418152\n",
      "\n",
      "Running model (trial=8, mod=255, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1472630.75\n",
      "################################  100  ################################\n",
      "Training Loss:  4490.86474609\n",
      "################################  200  ################################\n",
      "Training Loss:  4229.61328125\n",
      "################################  300  ################################\n",
      "Training Loss:  3838.6940918\n",
      "################################  400  ################################\n",
      "Training Loss:  3282.93383789\n",
      "################################  500  ################################\n",
      "Training Loss:  2532.7421875\n",
      "################################  600  ################################\n",
      "Training Loss:  1653.84265137\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=256, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1396658.25\n",
      "################################  100  ################################\n",
      "Training Loss:  73.03975677\n",
      "################################  200  ################################\n",
      "Training Loss:  52.72212601\n",
      "################################  300  ################################\n",
      "Training Loss:  52.69886398\n",
      "################################  400  ################################\n",
      "Training Loss:  52.40285492\n",
      "Epoch   426: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  52.59495926\n",
      "################################  600  ################################\n",
      "Training Loss:  52.67975616\n",
      "Epoch   650: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  52.64060974\n",
      "################################  800  ################################\n",
      "Training Loss:  53.9248085\n",
      "Epoch   851: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  52.44002151\n",
      "################################  1000  ################################\n",
      "Training Loss:  52.70065308\n",
      "Epoch  1052: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.61501694\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.46661377\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.45642853\n",
      "Epoch  1322: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.84597397\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.02947235\n",
      "Epoch  1523: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  52.46025848\n",
      "################################  1700  ################################\n",
      "Training Loss:  55.21987152\n",
      "Epoch  1724: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.58722687\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.39283752\n",
      "Epoch  1925: reducing learning rate of group 0 to 8.3886e-03.\n",
      "Final training Loss:  52.44774246\n",
      "\n",
      "Running model (trial=8, mod=257, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  68178.1640625\n",
      "################################  100  ################################\n",
      "Training Loss:  55.73530579\n",
      "################################  200  ################################\n",
      "Training Loss:  53.74215317\n",
      "################################  300  ################################\n",
      "Training Loss:  48.87723541\n",
      "################################  400  ################################\n",
      "Training Loss:  48.87179947\n",
      "################################  500  ################################\n",
      "Training Loss:  46.62784958\n",
      "################################  600  ################################\n",
      "Training Loss:  43.82203674\n",
      "################################  700  ################################\n",
      "Training Loss:  46.50375748\n",
      "################################  800  ################################\n",
      "Training Loss:  42.17596054\n",
      "################################  900  ################################\n",
      "Training Loss:  41.10294342\n",
      "################################  1000  ################################\n",
      "Training Loss:  39.69673538\n",
      "################################  1100  ################################\n",
      "Training Loss:  41.04793549\n",
      "################################  1200  ################################\n",
      "Training Loss:  39.86417007\n",
      "################################  1300  ################################\n",
      "Training Loss:  40.35010529\n",
      "################################  1400  ################################\n",
      "Training Loss:  44.87516785\n",
      "Epoch  1437: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  38.21226501\n",
      "################################  1600  ################################\n",
      "Training Loss:  50.10270691\n",
      "################################  1700  ################################\n",
      "Training Loss:  37.6387825\n",
      "################################  1800  ################################\n",
      "Training Loss:  48.20634079\n",
      "################################  1900  ################################\n",
      "Training Loss:  39.16448593\n",
      "Epoch  1988: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  37.12892151\n",
      "\n",
      "Running model (trial=8, mod=258, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5083.88085938\n",
      "################################  100  ################################\n",
      "Training Loss:  62.91119003\n",
      "################################  200  ################################\n",
      "Training Loss:  50.78610229\n",
      "################################  300  ################################\n",
      "Training Loss:  46.21699524\n",
      "################################  400  ################################\n",
      "Training Loss:  44.43706512\n",
      "################################  500  ################################\n",
      "Training Loss:  42.58243179\n",
      "################################  600  ################################\n",
      "Training Loss:  47.5553894\n",
      "################################  700  ################################\n",
      "Training Loss:  42.91938782\n",
      "################################  800  ################################\n",
      "Training Loss:  43.83021545\n",
      "################################  900  ################################\n",
      "Training Loss:  52.02364349\n",
      "################################  1000  ################################\n",
      "Training Loss:  41.42219925\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.97317505\n",
      "Epoch  1123: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  41.59880066\n",
      "################################  1300  ################################\n",
      "Training Loss:  38.48884583\n",
      "################################  1400  ################################\n",
      "Training Loss:  39.74742508\n",
      "################################  1500  ################################\n",
      "Training Loss:  38.27947617\n",
      "################################  1600  ################################\n",
      "Training Loss:  40.09372711\n",
      "################################  1700  ################################\n",
      "Training Loss:  40.2876091\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.74778748\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.00426483\n",
      "Epoch  1957: reducing learning rate of group 0 to 3.2000e-02.\n",
      "Final training Loss:  38.68083954\n",
      "\n",
      "Running model (trial=8, mod=259, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  976782.3125\n",
      "################################  100  ################################\n",
      "Training Loss:  50.26032257\n",
      "################################  200  ################################\n",
      "Training Loss:  44.60102463\n",
      "################################  300  ################################\n",
      "Training Loss:  104.77244568\n",
      "################################  400  ################################\n",
      "Training Loss:  70.30130005\n",
      "################################  500  ################################\n",
      "Training Loss:  43.55106354\n",
      "################################  600  ################################\n",
      "Training Loss:  90.02069092\n",
      "################################  700  ################################\n",
      "Training Loss:  49.03911972\n",
      "Epoch   734: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  51.28236389\n",
      "################################  900  ################################\n",
      "Training Loss:  42.14842224\n",
      "################################  1000  ################################\n",
      "Training Loss:  70.67140961\n",
      "################################  1100  ################################\n",
      "Training Loss:  68.59480286\n",
      "Epoch  1128: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  50.08724213\n",
      "################################  1300  ################################\n",
      "Training Loss:  45.90817261\n",
      "Epoch  1329: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  45.14776611\n",
      "################################  1500  ################################\n",
      "Training Loss:  44.27753067\n",
      "Epoch  1530: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  43.1348381\n",
      "################################  1700  ################################\n",
      "Training Loss:  42.61170959\n",
      "Epoch  1731: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  42.59346008\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.94108963\n",
      "Epoch  1932: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  41.95225143\n",
      "\n",
      "Running model (trial=8, mod=260, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  199191.953125\n",
      "################################  100  ################################\n",
      "Training Loss:  47.7562294\n",
      "################################  200  ################################\n",
      "Training Loss:  91.14598846\n",
      "################################  300  ################################\n",
      "Training Loss:  129.79826355\n",
      "Epoch   364: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  70.90211487\n",
      "################################  500  ################################\n",
      "Training Loss:  84.75834656\n",
      "Epoch   565: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  50.85821533\n",
      "################################  700  ################################\n",
      "Training Loss:  47.32112122\n",
      "Epoch   766: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  46.44645309\n",
      "################################  900  ################################\n",
      "Training Loss:  76.72193146\n",
      "Epoch   967: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  43.58740234\n",
      "################################  1100  ################################\n",
      "Training Loss:  70.01944733\n",
      "Epoch  1168: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  58.97574615\n",
      "################################  1300  ################################\n",
      "Training Loss:  41.05977249\n",
      "Epoch  1369: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  41.5938797\n",
      "################################  1500  ################################\n",
      "Training Loss:  41.58955383\n",
      "################################  1600  ################################\n",
      "Training Loss:  42.16292191\n",
      "################################  1700  ################################\n",
      "Training Loss:  38.65066528\n",
      "################################  1800  ################################\n",
      "Training Loss:  42.29798889\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.08594894\n",
      "Final training Loss:  69.88049316\n",
      "\n",
      "Running model (trial=8, mod=261, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  113652.8671875\n",
      "################################  100  ################################\n",
      "Training Loss:  122.74279022\n",
      "################################  200  ################################\n",
      "Training Loss:  119.24504089\n",
      "Epoch   269: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  119.14243317\n",
      "################################  400  ################################\n",
      "Training Loss:  119.04429626\n",
      "Epoch   470: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  118.38230896\n",
      "################################  600  ################################\n",
      "Training Loss:  118.13933563\n",
      "Epoch   671: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  110.79048157\n",
      "################################  800  ################################\n",
      "Training Loss:  88.29997253\n",
      "################################  900  ################################\n",
      "Training Loss:  53.90472412\n",
      "################################  1000  ################################\n",
      "Training Loss:  51.48450851\n",
      "################################  1100  ################################\n",
      "Training Loss:  49.57584763\n",
      "################################  1200  ################################\n",
      "Training Loss:  49.97259903\n",
      "################################  1300  ################################\n",
      "Training Loss:  47.78859711\n",
      "################################  1400  ################################\n",
      "Training Loss:  46.37836838\n",
      "################################  1500  ################################\n",
      "Training Loss:  44.3972435\n",
      "################################  1600  ################################\n",
      "Training Loss:  120.48120117\n",
      "Epoch  1694: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  119.22316742\n",
      "################################  1800  ################################\n",
      "Training Loss:  119.23514557\n",
      "Epoch  1895: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  118.97872162\n",
      "Final training Loss:  118.89043427\n",
      "\n",
      "Running model (trial=8, mod=262, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  261446.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  5781.92822266\n",
      "################################  200  ################################\n",
      "Training Loss:  109.43206024\n",
      "################################  300  ################################\n",
      "Training Loss:  198.08673096\n",
      "################################  400  ################################\n",
      "Training Loss:  505.48840332\n",
      "################################  500  ################################\n",
      "Training Loss:  138.08053589\n",
      "################################  600  ################################\n",
      "Training Loss:  102.01405334\n",
      "################################  700  ################################\n",
      "Training Loss:  117.35583496\n",
      "################################  800  ################################\n",
      "Training Loss:  635.32940674\n",
      "################################  900  ################################\n",
      "Training Loss:  674.80548096\n",
      "################################  1000  ################################\n",
      "Training Loss:  144.09387207\n",
      "Epoch  1009: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  104.67254639\n",
      "################################  1200  ################################\n",
      "Training Loss:  238.5736084\n",
      "Epoch  1210: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  126.31744385\n",
      "################################  1400  ################################\n",
      "Training Loss:  111.10315704\n",
      "Epoch  1411: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  122.0380249\n",
      "################################  1600  ################################\n",
      "Training Loss:  113.32917786\n",
      "Epoch  1612: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  108.49421692\n",
      "################################  1800  ################################\n",
      "Training Loss:  109.21856689\n",
      "Epoch  1813: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  127.77510834\n",
      "Final training Loss:  106.23686218\n",
      "\n",
      "Running model (trial=8, mod=263, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2719.96972656\n",
      "################################  100  ################################\n",
      "Training Loss:  108.35299683\n",
      "################################  200  ################################\n",
      "Training Loss:  103.87914276\n",
      "Epoch   216: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  104.01914978\n",
      "################################  400  ################################\n",
      "Training Loss:  103.13544464\n",
      "Epoch   417: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  87.36032867\n",
      "################################  600  ################################\n",
      "Training Loss:  67.19915771\n",
      "################################  700  ################################\n",
      "Training Loss:  53.1206131\n",
      "################################  800  ################################\n",
      "Training Loss:  48.14287949\n",
      "################################  900  ################################\n",
      "Training Loss:  45.56880569\n",
      "################################  1000  ################################\n",
      "Training Loss:  49.14438629\n",
      "################################  1100  ################################\n",
      "Training Loss:  46.53530502\n",
      "################################  1200  ################################\n",
      "Training Loss:  48.01406097\n",
      "Epoch  1277: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  41.99061966\n",
      "################################  1400  ################################\n",
      "Training Loss:  45.47508621\n",
      "################################  1500  ################################\n",
      "Training Loss:  46.55405426\n",
      "################################  1600  ################################\n",
      "Training Loss:  75.3971405\n",
      "Epoch  1644: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.03877258\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.54700089\n",
      "Epoch  1845: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  47.51363373\n",
      "Final training Loss:  45.38549805\n",
      "\n",
      "Running model (trial=8, mod=264, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1744896.125\n",
      "################################  100  ################################\n",
      "Training Loss:  52.29671097\n",
      "################################  200  ################################\n",
      "Training Loss:  42.06552505\n",
      "################################  300  ################################\n",
      "Training Loss:  41.09000397\n",
      "################################  400  ################################\n",
      "Training Loss:  42.88962936\n",
      "################################  500  ################################\n",
      "Training Loss:  40.10399628\n",
      "Epoch   530: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  40.30236053\n",
      "################################  700  ################################\n",
      "Training Loss:  40.72836685\n",
      "Epoch   731: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  40.47829819\n",
      "################################  900  ################################\n",
      "Training Loss:  39.98549652\n",
      "################################  1000  ################################\n",
      "Training Loss:  39.62986374\n",
      "################################  1100  ################################\n",
      "Training Loss:  42.10686874\n",
      "################################  1200  ################################\n",
      "Training Loss:  40.17293549\n",
      "Epoch  1213: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  40.38733673\n",
      "################################  1400  ################################\n",
      "Training Loss:  38.1843338\n",
      "################################  1500  ################################\n",
      "Training Loss:  45.54332352\n",
      "Epoch  1552: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  40.07844925\n",
      "################################  1700  ################################\n",
      "Training Loss:  38.41697311\n",
      "Epoch  1753: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  39.48035431\n",
      "################################  1900  ################################\n",
      "Training Loss:  38.27071762\n",
      "Epoch  1954: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  38.68086243\n",
      "\n",
      "Running model (trial=8, mod=265, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  15265.09765625\n",
      "################################  100  ################################\n",
      "Training Loss:  50.31100464\n",
      "################################  200  ################################\n",
      "Training Loss:  46.25151443\n",
      "################################  300  ################################\n",
      "Training Loss:  49.65052414\n",
      "################################  400  ################################\n",
      "Training Loss:  42.60574722\n",
      "################################  500  ################################\n",
      "Training Loss:  41.55507278\n",
      "################################  600  ################################\n",
      "Training Loss:  46.78431702\n",
      "################################  700  ################################\n",
      "Training Loss:  42.14232254\n",
      "################################  800  ################################\n",
      "Training Loss:  80.74131775\n",
      "################################  900  ################################\n",
      "Training Loss:  45.10622025\n",
      "################################  1000  ################################\n",
      "Training Loss:  78.42097473\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.3764801\n",
      "Epoch  1169: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  39.21065903\n",
      "################################  1300  ################################\n",
      "Training Loss:  58.73242188\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.21013641\n",
      "################################  1500  ################################\n",
      "Training Loss:  95.08191681\n",
      "Epoch  1546: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  40.15713501\n",
      "################################  1700  ################################\n",
      "Training Loss:  41.90910721\n",
      "################################  1800  ################################\n",
      "Training Loss:  38.16677856\n",
      "################################  1900  ################################\n",
      "Training Loss:  42.18665314\n",
      "Final training Loss:  40.51783752\n",
      "\n",
      "Running model (trial=8, mod=266, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  172542.890625\n",
      "################################  100  ################################\n",
      "Training Loss:  110.09008789\n",
      "################################  200  ################################\n",
      "Training Loss:  117.43396759\n",
      "Epoch   207: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  112.69729614\n",
      "################################  400  ################################\n",
      "Training Loss:  115.63082886\n",
      "Epoch   408: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=8, mod=267, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2388632.75\n",
      "################################  100  ################################\n",
      "Training Loss:  2045.5802002\n",
      "################################  200  ################################\n",
      "Training Loss:  70.28836823\n",
      "################################  300  ################################\n",
      "Training Loss:  54.89273071\n",
      "################################  400  ################################\n",
      "Training Loss:  54.19866943\n",
      "################################  500  ################################\n",
      "Training Loss:  57.29888535\n",
      "################################  600  ################################\n",
      "Training Loss:  57.19384384\n",
      "Epoch   670: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  54.43257904\n",
      "################################  800  ################################\n",
      "Training Loss:  54.73206711\n",
      "Epoch   871: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  53.09770203\n",
      "################################  1000  ################################\n",
      "Training Loss:  55.02721405\n",
      "Epoch  1072: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.10044479\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.85487747\n",
      "Epoch  1273: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.31048965\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.92368698\n",
      "Epoch  1474: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.78651047\n",
      "################################  1600  ################################\n",
      "Training Loss:  52.67049789\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.96604919\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.81038666\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.64632416\n",
      "Epoch  1933: reducing learning rate of group 0 to 1.3107e-02.\n",
      "Final training Loss:  52.59391022\n",
      "\n",
      "Running model (trial=8, mod=268, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1854944000.0\n",
      "################################  100  ################################\n",
      "Training Loss:  78917.4296875\n",
      "################################  200  ################################\n",
      "Training Loss:  58351.6328125\n",
      "################################  300  ################################\n",
      "Training Loss:  35352.2421875\n",
      "################################  400  ################################\n",
      "Training Loss:  12377.6171875\n",
      "################################  500  ################################\n",
      "Training Loss:  5733.70117188\n",
      "################################  600  ################################\n",
      "Training Loss:  4941.06396484\n",
      "################################  700  ################################\n",
      "Training Loss:  4598.59423828\n",
      "################################  800  ################################\n",
      "Training Loss:  4468.77294922\n",
      "################################  900  ################################\n",
      "Training Loss:  4423.32470703\n",
      "################################  1000  ################################\n",
      "Training Loss:  4405.89941406\n",
      "################################  1100  ################################\n",
      "Training Loss:  4394.69335938\n",
      "################################  1200  ################################\n",
      "Training Loss:  4386.88037109\n",
      "################################  1300  ################################\n",
      "Training Loss:  4378.65332031\n",
      "################################  1400  ################################\n",
      "Training Loss:  4369.64160156\n",
      "################################  1500  ################################\n",
      "Training Loss:  4360.39160156\n",
      "################################  1600  ################################\n",
      "Training Loss:  4350.57177734\n",
      "################################  1700  ################################\n",
      "Training Loss:  4340.15527344\n",
      "################################  1800  ################################\n",
      "Training Loss:  4329.04736328\n",
      "################################  1900  ################################\n",
      "Training Loss:  4317.31933594\n",
      "Final training Loss:  4305.00585938\n",
      "\n",
      "Running model (trial=8, mod=269, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12751424.0\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=270, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  833506.0\n",
      "################################  100  ################################\n",
      "Training Loss:  123.72997284\n",
      "################################  200  ################################\n",
      "Training Loss:  117.60058594\n",
      "################################  300  ################################\n",
      "Training Loss:  116.10926819\n",
      "Epoch   307: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  114.54420471\n",
      "################################  500  ################################\n",
      "Training Loss:  112.23391724\n",
      "################################  600  ################################\n",
      "Training Loss:  108.79950714\n",
      "################################  700  ################################\n",
      "Training Loss:  106.22345734\n",
      "################################  800  ################################\n",
      "Training Loss:  102.09152222\n",
      "################################  900  ################################\n",
      "Training Loss:  97.03889465\n",
      "################################  1000  ################################\n",
      "Training Loss:  93.46472168\n",
      "################################  1100  ################################\n",
      "Training Loss:  86.90626526\n",
      "################################  1200  ################################\n",
      "Training Loss:  70.58307648\n",
      "################################  1300  ################################\n",
      "Training Loss:  59.39994812\n",
      "################################  1400  ################################\n",
      "Training Loss:  56.57449341\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.38822556\n",
      "################################  1600  ################################\n",
      "Training Loss:  54.44169998\n",
      "Epoch  1603: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.02512741\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.9695015\n",
      "Epoch  1870: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.11699677\n",
      "Final training Loss:  53.01057053\n",
      "\n",
      "Running model (trial=9, mod=271, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1051675.875\n",
      "################################  100  ################################\n",
      "Training Loss:  122.03318787\n",
      "################################  200  ################################\n",
      "Training Loss:  112.68347931\n",
      "################################  300  ################################\n",
      "Training Loss:  95.95275879\n",
      "################################  400  ################################\n",
      "Training Loss:  55.66058731\n",
      "################################  500  ################################\n",
      "Training Loss:  53.21893311\n",
      "################################  600  ################################\n",
      "Training Loss:  53.65173721\n",
      "Epoch   619: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  53.13437271\n",
      "################################  800  ################################\n",
      "Training Loss:  53.30371475\n",
      "Epoch   820: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  52.73183441\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.5531044\n",
      "Epoch  1021: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  52.88825607\n",
      "################################  1200  ################################\n",
      "Training Loss:  53.06328583\n",
      "Epoch  1222: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.44056702\n",
      "################################  1400  ################################\n",
      "Training Loss:  52.35104752\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.47324371\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.47528458\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.27390289\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.14822388\n",
      "Epoch  1824: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.56454086\n",
      "Final training Loss:  52.33279037\n",
      "\n",
      "Running model (trial=9, mod=272, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8205.984375\n",
      "################################  100  ################################\n",
      "Training Loss:  115.79029846\n",
      "################################  200  ################################\n",
      "Training Loss:  57.17582703\n",
      "################################  300  ################################\n",
      "Training Loss:  57.50092316\n",
      "################################  400  ################################\n",
      "Training Loss:  65.50795746\n",
      "################################  500  ################################\n",
      "Training Loss:  58.98690796\n",
      "################################  600  ################################\n",
      "Training Loss:  69.43456268\n",
      "################################  700  ################################\n",
      "Training Loss:  65.47533417\n",
      "Epoch   741: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  57.90432358\n",
      "################################  900  ################################\n",
      "Training Loss:  73.84060669\n",
      "################################  1000  ################################\n",
      "Training Loss:  71.16484833\n",
      "################################  1100  ################################\n",
      "Training Loss:  85.72794342\n",
      "Epoch  1132: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  52.22327423\n",
      "################################  1300  ################################\n",
      "Training Loss:  52.98988342\n",
      "################################  1400  ################################\n",
      "Training Loss:  58.54483795\n",
      "Epoch  1444: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  57.66686249\n",
      "################################  1600  ################################\n",
      "Training Loss:  61.54920197\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.44730377\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.64729309\n",
      "Epoch  1833: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  52.5520401\n",
      "Final training Loss:  54.37917709\n",
      "\n",
      "Running model (trial=9, mod=273, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  777896.4375\n",
      "################################  100  ################################\n",
      "Training Loss:  62.21496964\n",
      "################################  200  ################################\n",
      "Training Loss:  60.12652969\n",
      "################################  300  ################################\n",
      "Training Loss:  75.22805786\n",
      "################################  400  ################################\n",
      "Training Loss:  49.46220016\n",
      "################################  500  ################################\n",
      "Training Loss:  49.4467926\n",
      "################################  600  ################################\n",
      "Training Loss:  159.86833191\n",
      "################################  700  ################################\n",
      "Training Loss:  122.64580536\n",
      "Epoch   754: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  120.16017151\n",
      "################################  900  ################################\n",
      "Training Loss:  119.16796112\n",
      "Epoch   955: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  117.36923981\n",
      "################################  1100  ################################\n",
      "Training Loss:  121.16915894\n",
      "Epoch  1156: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  117.17485809\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.91855621\n",
      "Epoch  1357: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.60411072\n",
      "################################  1500  ################################\n",
      "Training Loss:  123.95953369\n",
      "Epoch  1558: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.93800354\n",
      "################################  1700  ################################\n",
      "Training Loss:  121.86107635\n",
      "Epoch  1759: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.66377258\n",
      "################################  1900  ################################\n",
      "Training Loss:  114.78327179\n",
      "Epoch  1960: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  114.25871277\n",
      "\n",
      "Running model (trial=9, mod=274, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12694.05273438\n",
      "################################  100  ################################\n",
      "Training Loss:  106.31661224\n",
      "################################  200  ################################\n",
      "Training Loss:  103.08307648\n",
      "################################  300  ################################\n",
      "Training Loss:  97.01013184\n",
      "################################  400  ################################\n",
      "Training Loss:  89.71762848\n",
      "################################  500  ################################\n",
      "Training Loss:  65.74610138\n",
      "################################  600  ################################\n",
      "Training Loss:  65.09560394\n",
      "################################  700  ################################\n",
      "Training Loss:  64.28271484\n",
      "Epoch   765: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  58.54388809\n",
      "################################  900  ################################\n",
      "Training Loss:  54.68229675\n",
      "Epoch   966: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  53.87859344\n",
      "################################  1100  ################################\n",
      "Training Loss:  65.53757477\n",
      "Epoch  1167: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  57.33480072\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.49927902\n",
      "Epoch  1368: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  58.44120407\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.98999786\n",
      "Epoch  1569: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  56.79728317\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.42435837\n",
      "Epoch  1770: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.80437469\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.44770813\n",
      "Epoch  1971: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  53.77536774\n",
      "\n",
      "Running model (trial=9, mod=275, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1093.79772949\n",
      "################################  100  ################################\n",
      "Training Loss:  11136.20507812\n",
      "################################  200  ################################\n",
      "Training Loss:  87.43086243\n",
      "################################  300  ################################\n",
      "Training Loss:  85.51843262\n",
      "Epoch   334: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  83.48562622\n",
      "################################  500  ################################\n",
      "Training Loss:  80.74134827\n",
      "Epoch   535: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  77.84226227\n",
      "################################  700  ################################\n",
      "Training Loss:  75.32253265\n",
      "Epoch   740: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  77.31383514\n",
      "################################  900  ################################\n",
      "Training Loss:  70.41046906\n",
      "################################  1000  ################################\n",
      "Training Loss:  67.92015839\n",
      "################################  1100  ################################\n",
      "Training Loss:  67.25362396\n",
      "################################  1200  ################################\n",
      "Training Loss:  65.02547455\n",
      "################################  1300  ################################\n",
      "Training Loss:  60.77862549\n",
      "################################  1400  ################################\n",
      "Training Loss:  58.75776672\n",
      "################################  1500  ################################\n",
      "Training Loss:  59.27758026\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.39925766\n",
      "Epoch  1615: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  56.55305481\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.86421204\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.25589752\n",
      "Final training Loss:  54.26791\n",
      "\n",
      "Running model (trial=9, mod=276, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1831.25268555\n",
      "################################  100  ################################\n",
      "Training Loss:  142.28800964\n",
      "################################  200  ################################\n",
      "Training Loss:  128.0806427\n",
      "Epoch   289: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  124.71182251\n",
      "################################  400  ################################\n",
      "Training Loss:  124.38275909\n",
      "Epoch   490: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  123.24420929\n",
      "################################  600  ################################\n",
      "Training Loss:  122.20051575\n",
      "################################  700  ################################\n",
      "Training Loss:  122.14438629\n",
      "Epoch   776: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  121.35686493\n",
      "################################  900  ################################\n",
      "Training Loss:  121.28832245\n",
      "################################  1000  ################################\n",
      "Training Loss:  120.75357819\n",
      "################################  1100  ################################\n",
      "Training Loss:  121.10581207\n",
      "################################  1200  ################################\n",
      "Training Loss:  119.10271454\n",
      "Epoch  1240: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  120.03692627\n",
      "################################  1400  ################################\n",
      "Training Loss:  121.34899902\n",
      "################################  1500  ################################\n",
      "Training Loss:  125.52786255\n",
      "################################  1600  ################################\n",
      "Training Loss:  118.3961792\n",
      "################################  1700  ################################\n",
      "Training Loss:  120.53065491\n",
      "################################  1800  ################################\n",
      "Training Loss:  122.64626312\n",
      "################################  1900  ################################\n",
      "Training Loss:  118.14562988\n",
      "Final training Loss:  119.64827728\n",
      "\n",
      "Running model (trial=9, mod=277, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  698928.4375\n",
      "################################  100  ################################\n",
      "Training Loss:  303.21707153\n",
      "################################  200  ################################\n",
      "Training Loss:  120.85827637\n",
      "################################  300  ################################\n",
      "Training Loss:  120.90917969\n",
      "################################  400  ################################\n",
      "Training Loss:  127.79501343\n",
      "################################  500  ################################\n",
      "Training Loss:  123.09823608\n",
      "################################  600  ################################\n",
      "Training Loss:  123.58253479\n",
      "################################  700  ################################\n",
      "Training Loss:  118.93752289\n",
      "################################  800  ################################\n",
      "Training Loss:  122.0508194\n",
      "################################  900  ################################\n",
      "Training Loss:  120.39625549\n",
      "################################  1000  ################################\n",
      "Training Loss:  123.68053436\n",
      "################################  1100  ################################\n",
      "Training Loss:  118.38891602\n",
      "################################  1200  ################################\n",
      "Training Loss:  138.47000122\n",
      "################################  1300  ################################\n",
      "Training Loss:  117.51647949\n",
      "################################  1400  ################################\n",
      "Training Loss:  120.55040741\n",
      "################################  1500  ################################\n",
      "Training Loss:  226.46066284\n",
      "################################  1600  ################################\n",
      "Training Loss:  184.17262268\n",
      "################################  1700  ################################\n",
      "Training Loss:  126.64941406\n",
      "################################  1800  ################################\n",
      "Training Loss:  176.03607178\n",
      "################################  1900  ################################\n",
      "Training Loss:  199.59907532\n",
      "Epoch  1926: reducing learning rate of group 0 to 4.0000e-02.\n",
      "Final training Loss:  116.28400421\n",
      "\n",
      "Running model (trial=9, mod=278, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  783948.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  108.97074127\n",
      "################################  200  ################################\n",
      "Training Loss:  105.69392395\n",
      "Epoch   243: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  100.66822052\n",
      "################################  400  ################################\n",
      "Training Loss:  96.80224609\n",
      "Epoch   444: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  90.14633942\n",
      "################################  600  ################################\n",
      "Training Loss:  82.65525818\n",
      "################################  700  ################################\n",
      "Training Loss:  65.66483307\n",
      "################################  800  ################################\n",
      "Training Loss:  61.90725708\n",
      "################################  900  ################################\n",
      "Training Loss:  64.57318115\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.83909988\n",
      "################################  1100  ################################\n",
      "Training Loss:  53.19705963\n",
      "Epoch  1106: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  54.79553604\n",
      "################################  1300  ################################\n",
      "Training Loss:  53.57952118\n",
      "Epoch  1307: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  53.80203629\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.13085938\n",
      "Epoch  1508: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.19815445\n",
      "################################  1700  ################################\n",
      "Training Loss:  53.80547714\n",
      "Epoch  1709: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.63897324\n",
      "################################  1900  ################################\n",
      "Training Loss:  54.62938309\n",
      "Epoch  1910: reducing learning rate of group 0 to 1.0486e-02.\n",
      "Final training Loss:  53.10456467\n",
      "\n",
      "Running model (trial=9, mod=279, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  28258.94140625\n",
      "################################  100  ################################\n",
      "Training Loss:  122.66160583\n",
      "################################  200  ################################\n",
      "Training Loss:  123.3458252\n",
      "################################  300  ################################\n",
      "Training Loss:  120.68730164\n",
      "################################  400  ################################\n",
      "Training Loss:  119.10415649\n",
      "################################  500  ################################\n",
      "Training Loss:  116.51478577\n",
      "################################  600  ################################\n",
      "Training Loss:  128.92398071\n",
      "################################  700  ################################\n",
      "Training Loss:  118.03044128\n",
      "################################  800  ################################\n",
      "Training Loss:  115.6279068\n",
      "################################  900  ################################\n",
      "Training Loss:  127.28695679\n",
      "Epoch   977: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.82479858\n",
      "################################  1100  ################################\n",
      "Training Loss:  118.52439117\n",
      "################################  1200  ################################\n",
      "Training Loss:  113.94564056\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.50273132\n",
      "Epoch  1355: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  114.57061768\n",
      "################################  1500  ################################\n",
      "Training Loss:  125.94340515\n",
      "Epoch  1556: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.29351044\n",
      "################################  1700  ################################\n",
      "Training Loss:  126.6679306\n",
      "################################  1800  ################################\n",
      "Training Loss:  117.48226929\n",
      "################################  1900  ################################\n",
      "Training Loss:  117.12554169\n",
      "Final training Loss:  114.29922485\n",
      "\n",
      "Running model (trial=9, mod=280, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  16142.15527344\n",
      "################################  100  ################################\n",
      "Training Loss:  114.45487976\n",
      "################################  200  ################################\n",
      "Training Loss:  138.76200867\n",
      "################################  300  ################################\n",
      "Training Loss:  114.52062988\n",
      "################################  400  ################################\n",
      "Training Loss:  113.37135315\n",
      "################################  500  ################################\n",
      "Training Loss:  114.30723572\n",
      "################################  600  ################################\n",
      "Training Loss:  114.22383881\n",
      "Epoch   688: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  113.32671356\n",
      "################################  800  ################################\n",
      "Training Loss:  112.71855164\n",
      "Epoch   889: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  111.63998413\n",
      "################################  1000  ################################\n",
      "Training Loss:  112.85883331\n",
      "################################  1100  ################################\n",
      "Training Loss:  114.2675705\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.87246704\n",
      "################################  1300  ################################\n",
      "Training Loss:  118.4814682\n",
      "################################  1400  ################################\n",
      "Training Loss:  111.83318329\n",
      "################################  1500  ################################\n",
      "Training Loss:  117.0343399\n",
      "Epoch  1596: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  114.89242554\n",
      "################################  1700  ################################\n",
      "Training Loss:  111.90589905\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.13214111\n",
      "################################  1900  ################################\n",
      "Training Loss:  110.88861084\n",
      "Final training Loss:  118.84841919\n",
      "\n",
      "Running model (trial=9, mod=281, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  434696.0625\n",
      "################################  100  ################################\n",
      "Training Loss:  54.54346848\n",
      "################################  200  ################################\n",
      "Training Loss:  52.24657822\n",
      "Epoch   275: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  52.85443115\n",
      "################################  400  ################################\n",
      "Training Loss:  51.31964493\n",
      "Epoch   476: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  52.67966461\n",
      "################################  600  ################################\n",
      "Training Loss:  53.67409134\n",
      "Epoch   677: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  54.94233704\n",
      "################################  800  ################################\n",
      "Training Loss:  51.38111877\n",
      "################################  900  ################################\n",
      "Training Loss:  63.04058075\n",
      "################################  1000  ################################\n",
      "Training Loss:  49.85626984\n",
      "################################  1100  ################################\n",
      "Training Loss:  55.92518234\n",
      "################################  1200  ################################\n",
      "Training Loss:  53.02484131\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.33673859\n",
      "################################  1400  ################################\n",
      "Training Loss:  48.76332092\n",
      "################################  1500  ################################\n",
      "Training Loss:  48.65290833\n",
      "################################  1600  ################################\n",
      "Training Loss:  53.52124405\n",
      "################################  1700  ################################\n",
      "Training Loss:  48.25211716\n",
      "################################  1800  ################################\n",
      "Training Loss:  82.04188538\n",
      "################################  1900  ################################\n",
      "Training Loss:  49.02196503\n",
      "Final training Loss:  45.75333786\n",
      "\n",
      "Running model (trial=9, mod=282, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54656.76171875\n",
      "################################  100  ################################\n",
      "Training Loss:  138.99349976\n",
      "################################  200  ################################\n",
      "Training Loss:  114.62297821\n",
      "################################  300  ################################\n",
      "Training Loss:  114.28655243\n",
      "################################  400  ################################\n",
      "Training Loss:  113.81199646\n",
      "################################  500  ################################\n",
      "Training Loss:  114.16568756\n",
      "################################  600  ################################\n",
      "Training Loss:  113.77996826\n",
      "################################  700  ################################\n",
      "Training Loss:  114.11215973\n",
      "################################  800  ################################\n",
      "Training Loss:  114.68882751\n",
      "################################  900  ################################\n",
      "Training Loss:  115.08899689\n",
      "################################  1000  ################################\n",
      "Training Loss:  83.31616974\n",
      "################################  1100  ################################\n",
      "Training Loss:  56.73809433\n",
      "################################  1200  ################################\n",
      "Training Loss:  50.02947235\n",
      "################################  1300  ################################\n",
      "Training Loss:  47.97681046\n",
      "################################  1400  ################################\n",
      "Training Loss:  46.55033493\n",
      "################################  1500  ################################\n",
      "Training Loss:  44.15713882\n",
      "################################  1600  ################################\n",
      "Training Loss:  45.10007095\n",
      "################################  1700  ################################\n",
      "Training Loss:  43.3035965\n",
      "################################  1800  ################################\n",
      "Training Loss:  41.97312164\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.65009689\n",
      "Final training Loss:  51.7086792\n",
      "\n",
      "Running model (trial=9, mod=283, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9665838.0\n",
      "################################  100  ################################\n",
      "Training Loss:  110.77816772\n",
      "################################  200  ################################\n",
      "Training Loss:  97.89941406\n",
      "################################  300  ################################\n",
      "Training Loss:  72.67774963\n",
      "################################  400  ################################\n",
      "Training Loss:  63.85153198\n",
      "Epoch   414: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  59.8849678\n",
      "################################  600  ################################\n",
      "Training Loss:  98.61787415\n",
      "Epoch   615: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  57.98618317\n",
      "################################  800  ################################\n",
      "Training Loss:  59.87611008\n",
      "################################  900  ################################\n",
      "Training Loss:  60.2396431\n",
      "################################  1000  ################################\n",
      "Training Loss:  70.57762909\n",
      "################################  1100  ################################\n",
      "Training Loss:  59.48919678\n",
      "################################  1200  ################################\n",
      "Training Loss:  58.56621552\n",
      "################################  1300  ################################\n",
      "Training Loss:  59.05128479\n",
      "################################  1400  ################################\n",
      "Training Loss:  101.02371216\n",
      "################################  1500  ################################\n",
      "Training Loss:  59.71098328\n",
      "Epoch  1572: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  64.32157898\n",
      "################################  1700  ################################\n",
      "Training Loss:  51.52081299\n",
      "################################  1800  ################################\n",
      "Training Loss:  64.03452301\n",
      "################################  1900  ################################\n",
      "Training Loss:  50.33226776\n",
      "Final training Loss:  56.34281921\n",
      "\n",
      "Running model (trial=9, mod=284, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'tanh'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4335396.0\n",
      "################################  100  ################################\n",
      "Training Loss:  122.28231049\n",
      "################################  200  ################################\n",
      "Training Loss:  123.1410141\n",
      "################################  300  ################################\n",
      "Training Loss:  509.23626709\n",
      "Epoch   306: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  400  ################################\n",
      "Training Loss:  120.90956879\n",
      "################################  500  ################################\n",
      "Training Loss:  190.6312561\n",
      "Epoch   507: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  139.53833008\n",
      "################################  700  ################################\n",
      "Training Loss:  140.20059204\n",
      "Epoch   708: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  128.02008057\n",
      "################################  900  ################################\n",
      "Training Loss:  118.44026947\n",
      "################################  1000  ################################\n",
      "Training Loss:  116.53751373\n",
      "Epoch  1070: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  121.59536743\n",
      "################################  1200  ################################\n",
      "Training Loss:  131.74707031\n",
      "Epoch  1271: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  138.38548279\n",
      "################################  1400  ################################\n",
      "Training Loss:  126.27557373\n",
      "################################  1500  ################################\n",
      "Training Loss:  176.91767883\n",
      "Epoch  1531: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  122.22123718\n",
      "################################  1700  ################################\n",
      "Training Loss:  115.76678467\n",
      "Epoch  1732: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  122.79190826\n",
      "################################  1900  ################################\n",
      "Training Loss:  131.3081665\n",
      "Final training Loss:  135.84182739\n",
      "\n",
      "Running model (trial=9, mod=285, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 2, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  498737.25\n",
      "################################  100  ################################\n",
      "Training Loss:  1994.08508301\n",
      "################################  200  ################################\n",
      "Training Loss:  777.36395264\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=286, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 4, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  187745.015625\n",
      "################################  100  ################################\n",
      "Training Loss:  52.9004097\n",
      "################################  200  ################################\n",
      "Training Loss:  52.72755432\n",
      "Epoch   248: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  53.59488297\n",
      "################################  400  ################################\n",
      "Training Loss:  52.74824905\n",
      "Epoch   449: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  55.08969498\n",
      "################################  600  ################################\n",
      "Training Loss:  52.32068634\n",
      "Epoch   650: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  52.10788345\n",
      "################################  800  ################################\n",
      "Training Loss:  53.84936523\n",
      "Epoch   851: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  53.2234993\n",
      "################################  1000  ################################\n",
      "Training Loss:  52.28815079\n",
      "Epoch  1052: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  51.67733765\n",
      "################################  1200  ################################\n",
      "Training Loss:  51.6429863\n",
      "Epoch  1253: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  51.91500473\n",
      "################################  1400  ################################\n",
      "Training Loss:  51.76152802\n",
      "Epoch  1454: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  52.15037537\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.95838928\n",
      "Epoch  1655: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.298069\n",
      "################################  1800  ################################\n",
      "Training Loss:  52.37147522\n",
      "Epoch  1856: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  51.75164795\n",
      "Final training Loss:  51.62978745\n",
      "\n",
      "Running model (trial=9, mod=287, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  193274.296875\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=288, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 16, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  852255.5\n",
      "################################  100  ################################\n",
      "Training Loss:  49.57889175\n",
      "################################  200  ################################\n",
      "Training Loss:  41.27383041\n",
      "################################  300  ################################\n",
      "Training Loss:  45.92794037\n",
      "################################  400  ################################\n",
      "Training Loss:  49.38264847\n",
      "################################  500  ################################\n",
      "Training Loss:  53.69253159\n",
      "Epoch   509: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  40.55279541\n",
      "################################  700  ################################\n",
      "Training Loss:  48.92158127\n",
      "################################  800  ################################\n",
      "Training Loss:  40.64392853\n",
      "################################  900  ################################\n",
      "Training Loss:  40.08647537\n",
      "################################  1000  ################################\n",
      "Training Loss:  43.68079758\n",
      "################################  1100  ################################\n",
      "Training Loss:  54.69411087\n",
      "################################  1200  ################################\n",
      "Training Loss:  44.30843735\n",
      "Epoch  1221: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  42.18887329\n",
      "################################  1400  ################################\n",
      "Training Loss:  39.39965057\n",
      "################################  1500  ################################\n",
      "Training Loss:  38.80313873\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.59965134\n",
      "Epoch  1691: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  39.49965668\n",
      "################################  1800  ################################\n",
      "Training Loss:  47.52861404\n",
      "################################  1900  ################################\n",
      "Training Loss:  44.06267166\n",
      "Epoch  1902: reducing learning rate of group 0 to 2.0480e-02.\n",
      "Final training Loss:  37.11473465\n",
      "\n",
      "Running model (trial=9, mod=289, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 32, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3063.63916016\n",
      "################################  100  ################################\n",
      "Training Loss:  55.95059967\n",
      "################################  200  ################################\n",
      "Training Loss:  47.59387207\n",
      "################################  300  ################################\n",
      "Training Loss:  105.09431458\n",
      "################################  400  ################################\n",
      "Training Loss:  104.21801758\n",
      "Epoch   418: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  104.72631836\n",
      "################################  600  ################################\n",
      "Training Loss:  105.33032227\n",
      "Epoch   619: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  104.82640839\n",
      "################################  800  ################################\n",
      "Training Loss:  114.6338501\n",
      "Epoch   820: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  107.57010651\n",
      "################################  1000  ################################\n",
      "Training Loss:  102.62158203\n",
      "Epoch  1021: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  109.16346741\n",
      "################################  1200  ################################\n",
      "Training Loss:  103.14156342\n",
      "Epoch  1222: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  104.84159851\n",
      "################################  1400  ################################\n",
      "Training Loss:  104.2181778\n",
      "Epoch  1423: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  103.42742157\n",
      "################################  1600  ################################\n",
      "Training Loss:  103.76367188\n",
      "Epoch  1624: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  115.31615448\n",
      "################################  1800  ################################\n",
      "Training Loss:  104.87153625\n",
      "Epoch  1825: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  102.0690918\n",
      "Final training Loss:  103.56453705\n",
      "\n",
      "Running model (trial=9, mod=290, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 64, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  586998.5\n",
      "################################  100  ################################\n",
      "Training Loss:  231.90759277\n",
      "################################  200  ################################\n",
      "Training Loss:  57.81856918\n",
      "################################  300  ################################\n",
      "Training Loss:  51.37363052\n",
      "################################  400  ################################\n",
      "Training Loss:  112.31152344\n",
      "################################  500  ################################\n",
      "Training Loss:  49.1889534\n",
      "################################  600  ################################\n",
      "Training Loss:  75.91960907\n",
      "################################  700  ################################\n",
      "Training Loss:  55.89850616\n",
      "################################  800  ################################\n",
      "Training Loss:  48.27362442\n",
      "################################  900  ################################\n",
      "Training Loss:  44.63648605\n",
      "################################  1000  ################################\n",
      "Training Loss:  60.83607101\n",
      "################################  1100  ################################\n",
      "Training Loss:  96.96805573\n",
      "Epoch  1189: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1200  ################################\n",
      "Training Loss:  87.80601501\n",
      "################################  1300  ################################\n",
      "Training Loss:  68.6593399\n",
      "Epoch  1390: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1400  ################################\n",
      "Training Loss:  59.87549591\n",
      "################################  1500  ################################\n",
      "Training Loss:  55.3699379\n",
      "Epoch  1591: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1600  ################################\n",
      "Training Loss:  51.90052414\n",
      "################################  1700  ################################\n",
      "Training Loss:  52.63055801\n",
      "Epoch  1792: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1800  ################################\n",
      "Training Loss:  51.8388176\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.33457184\n",
      "Epoch  1993: reducing learning rate of group 0 to 1.6384e-02.\n",
      "Final training Loss:  47.6857872\n",
      "\n",
      "Running model (trial=9, mod=291, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 128, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  126002.15625\n",
      "################################  100  ################################\n",
      "Training Loss:  110.81086731\n",
      "################################  200  ################################\n",
      "Training Loss:  298.14450073\n",
      "################################  300  ################################\n",
      "Training Loss:  211.907547\n",
      "################################  400  ################################\n",
      "Training Loss:  148.28361511\n",
      "################################  500  ################################\n",
      "Training Loss:  104.51753998\n",
      "################################  600  ################################\n",
      "Training Loss:  210.99023438\n",
      "################################  700  ################################\n",
      "Training Loss:  275.35845947\n",
      "################################  800  ################################\n",
      "Training Loss:  132.3145752\n",
      "################################  900  ################################\n",
      "Training Loss:  163.82444763\n",
      "Epoch   959: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1000  ################################\n",
      "Training Loss:  57.67717361\n",
      "################################  1100  ################################\n",
      "Training Loss:  57.24756622\n",
      "################################  1200  ################################\n",
      "Training Loss:  119.60520172\n",
      "################################  1300  ################################\n",
      "Training Loss:  55.19539642\n",
      "################################  1400  ################################\n",
      "Training Loss:  62.95881653\n",
      "Epoch  1477: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  51.968853\n",
      "################################  1600  ################################\n",
      "Training Loss:  46.75919724\n",
      "################################  1700  ################################\n",
      "Training Loss:  46.66709518\n",
      "################################  1800  ################################\n",
      "Training Loss:  71.10728455\n",
      "################################  1900  ################################\n",
      "Training Loss:  64.10990143\n",
      "Epoch  1921: reducing learning rate of group 0 to 2.5600e-02.\n",
      "Final training Loss:  45.64141083\n",
      "\n",
      "Running model (trial=9, mod=292, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'n_hidden_layers': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'neurons': 256, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  108050.5859375\n",
      "################################  100  ################################\n",
      "Training Loss:  456.06225586\n",
      "################################  200  ################################\n",
      "Training Loss:  198.06668091\n",
      "################################  300  ################################\n",
      "Training Loss:  138.96105957\n",
      "################################  400  ################################\n",
      "Training Loss:  120.02390289\n",
      "Epoch   413: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  118.47114563\n",
      "################################  600  ################################\n",
      "Training Loss:  130.28186035\n",
      "Epoch   614: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  114.9624939\n",
      "################################  800  ################################\n",
      "Training Loss:  144.37400818\n",
      "Epoch   815: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  115.36220551\n",
      "################################  1000  ################################\n",
      "Training Loss:  115.07624817\n",
      "Epoch  1016: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  120.14692688\n",
      "################################  1200  ################################\n",
      "Training Loss:  114.51180267\n",
      "Epoch  1217: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  127.22666931\n",
      "################################  1400  ################################\n",
      "Training Loss:  118.55684662\n",
      "Epoch  1418: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  114.79934692\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.93930054\n",
      "Epoch  1619: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  121.93842316\n",
      "################################  1800  ################################\n",
      "Training Loss:  156.40948486\n",
      "Epoch  1820: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  113.45036316\n",
      "Final training Loss:  114.72338867\n",
      "\n",
      "Running model (trial=9, mod=293, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 1, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3183.70825195\n",
      "################################  100  ################################\n",
      "Training Loss:  62.01325607\n",
      "################################  200  ################################\n",
      "Training Loss:  59.46312332\n",
      "################################  300  ################################\n",
      "Training Loss:  82.87991333\n",
      "################################  400  ################################\n",
      "Training Loss:  53.97024918\n",
      "################################  500  ################################\n",
      "Training Loss:  64.70806122\n",
      "################################  600  ################################\n",
      "Training Loss:  54.4148407\n",
      "################################  700  ################################\n",
      "Training Loss:  56.74563599\n",
      "################################  800  ################################\n",
      "Training Loss:  56.70877075\n",
      "################################  900  ################################\n",
      "Training Loss:  57.35048294\n",
      "################################  1000  ################################\n",
      "Training Loss:  137.07821655\n",
      "################################  1100  ################################\n",
      "Training Loss:  49.57186127\n",
      "################################  1200  ################################\n",
      "Training Loss:  57.61155701\n",
      "################################  1300  ################################\n",
      "Training Loss:  46.25345993\n",
      "################################  1400  ################################\n",
      "Training Loss:  44.3319931\n",
      "################################  1500  ################################\n",
      "Training Loss:  53.81764221\n",
      "################################  1600  ################################\n",
      "Training Loss:  49.66599655\n",
      "Epoch  1642: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  57.62724686\n",
      "################################  1800  ################################\n",
      "Training Loss:  41.36313248\n",
      "################################  1900  ################################\n",
      "Training Loss:  41.35677719\n",
      "Final training Loss:  39.29966736\n",
      "\n",
      "Running model (trial=9, mod=294, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 2, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  436260.96875\n",
      "################################  100  ################################\n",
      "Training Loss:  122.75987244\n",
      "################################  200  ################################\n",
      "Training Loss:  121.50326538\n",
      "Epoch   229: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  128.71548462\n",
      "################################  400  ################################\n",
      "Training Loss:  128.37533569\n",
      "################################  500  ################################\n",
      "Training Loss:  106.17702484\n",
      "################################  600  ################################\n",
      "Training Loss:  103.18750763\n",
      "################################  700  ################################\n",
      "Training Loss:  103.44681549\n",
      "################################  800  ################################\n",
      "Training Loss:  97.45592499\n",
      "################################  900  ################################\n",
      "Training Loss:  98.47084045\n",
      "################################  1000  ################################\n",
      "Training Loss:  95.82977295\n",
      "################################  1100  ################################\n",
      "Training Loss:  72.88246155\n",
      "################################  1200  ################################\n",
      "Training Loss:  60.92913818\n",
      "################################  1300  ################################\n",
      "Training Loss:  72.17562866\n",
      "################################  1400  ################################\n",
      "Training Loss:  60.6785965\n",
      "################################  1500  ################################\n",
      "Training Loss:  118.67042542\n",
      "################################  1600  ################################\n",
      "Training Loss:  57.84749603\n",
      "################################  1700  ################################\n",
      "Training Loss:  59.29584122\n",
      "################################  1800  ################################\n",
      "Training Loss:  56.29933929\n",
      "################################  1900  ################################\n",
      "Training Loss:  55.29459\n",
      "Final training Loss:  54.12958527\n",
      "\n",
      "Running model (trial=9, mod=295, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 4, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1047234.75\n",
      "################################  100  ################################\n",
      "Training Loss:  60.27257538\n",
      "################################  200  ################################\n",
      "Training Loss:  61.0093956\n",
      "################################  300  ################################\n",
      "Training Loss:  63.45798111\n",
      "################################  400  ################################\n",
      "Training Loss:  58.48927307\n",
      "################################  500  ################################\n",
      "Training Loss:  83.96337891\n",
      "################################  600  ################################\n",
      "Training Loss:  61.76524734\n",
      "################################  700  ################################\n",
      "Training Loss:  68.81080627\n",
      "Epoch   778: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  800  ################################\n",
      "Training Loss:  55.60557938\n",
      "################################  900  ################################\n",
      "Training Loss:  73.56177521\n",
      "################################  1000  ################################\n",
      "Training Loss:  54.88108444\n",
      "Epoch  1095: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  58.87553406\n",
      "################################  1200  ################################\n",
      "Training Loss:  56.14279175\n",
      "Epoch  1296: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  54.40926743\n",
      "################################  1400  ################################\n",
      "Training Loss:  63.23194504\n",
      "Epoch  1497: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  59.44614792\n",
      "################################  1600  ################################\n",
      "Training Loss:  55.31689072\n",
      "Epoch  1698: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  54.10168076\n",
      "################################  1800  ################################\n",
      "Training Loss:  53.7313385\n",
      "Epoch  1899: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  53.72053528\n",
      "Final training Loss:  61.55466843\n",
      "\n",
      "Running model (trial=9, mod=296, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 8, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1789165.75\n",
      "################################  100  ################################\n",
      "Training Loss:  1566.94445801\n",
      "################################  200  ################################\n",
      "Training Loss:  47.43623734\n",
      "################################  300  ################################\n",
      "Training Loss:  47.7543602\n",
      "################################  400  ################################\n",
      "Training Loss:  46.07110596\n",
      "################################  500  ################################\n",
      "Training Loss:  75.17919159\n",
      "Epoch   594: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  600  ################################\n",
      "Training Loss:  42.3062706\n",
      "################################  700  ################################\n",
      "Training Loss:  42.17806244\n",
      "################################  800  ################################\n",
      "Training Loss:  41.26861572\n",
      "################################  900  ################################\n",
      "Training Loss:  77.42389679\n",
      "################################  1000  ################################\n",
      "Training Loss:  75.87765503\n",
      "Epoch  1051: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  43.11242294\n",
      "################################  1200  ################################\n",
      "Training Loss:  40.39419556\n",
      "################################  1300  ################################\n",
      "Training Loss:  48.41584778\n",
      "################################  1400  ################################\n",
      "Training Loss:  43.59954834\n",
      "Epoch  1428: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  39.18533707\n",
      "################################  1600  ################################\n",
      "Training Loss:  38.47312164\n",
      "################################  1700  ################################\n",
      "Training Loss:  64.20652771\n",
      "################################  1800  ################################\n",
      "Training Loss:  44.26093674\n",
      "Epoch  1854: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  42.00204468\n",
      "Final training Loss:  38.74105072\n",
      "\n",
      "Running model (trial=9, mod=297, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 16, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  30246.93554688\n",
      "################################  100  ################################\n",
      "Training Loss:  123.13961029\n",
      "################################  200  ################################\n",
      "Training Loss:  119.60088348\n",
      "Epoch   277: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  300  ################################\n",
      "Training Loss:  120.4184494\n",
      "################################  400  ################################\n",
      "Training Loss:  119.82280731\n",
      "Epoch   478: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  500  ################################\n",
      "Training Loss:  118.81484222\n",
      "################################  600  ################################\n",
      "Training Loss:  118.20449066\n",
      "Epoch   679: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  700  ################################\n",
      "Training Loss:  122.29177856\n",
      "################################  800  ################################\n",
      "Training Loss:  117.22843933\n",
      "Epoch   880: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  116.06958008\n",
      "################################  1000  ################################\n",
      "Training Loss:  117.5671463\n",
      "Epoch  1081: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  115.94314575\n",
      "################################  1200  ################################\n",
      "Training Loss:  115.10873413\n",
      "Epoch  1282: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  115.94760895\n",
      "################################  1400  ################################\n",
      "Training Loss:  117.22112274\n",
      "Epoch  1483: reducing learning rate of group 0 to 1.0486e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  115.3809433\n",
      "################################  1600  ################################\n",
      "Training Loss:  115.16168213\n",
      "Epoch  1684: reducing learning rate of group 0 to 8.3886e-03.\n",
      "################################  1700  ################################\n",
      "Training Loss:  115.0743866\n",
      "################################  1800  ################################\n",
      "Training Loss:  115.28670502\n",
      "Epoch  1885: reducing learning rate of group 0 to 6.7109e-03.\n",
      "################################  1900  ################################\n",
      "Training Loss:  115.33130646\n",
      "Final training Loss:  115.81928253\n",
      "\n",
      "Running model (trial=9, mod=298, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 32, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  6109677568.0\n",
      "################################  100  ################################\n",
      "Training Loss:  436736.9375\n",
      "################################  200  ################################\n",
      "Training Loss:  45254.94140625\n",
      "################################  300  ################################\n",
      "Training Loss:  6595.97705078\n",
      "################################  400  ################################\n",
      "Training Loss:  1030.30993652\n",
      "################################  500  ################################\n",
      "Training Loss:  218.34393311\n",
      "################################  600  ################################\n",
      "Training Loss:  101.12506104\n",
      "################################  700  ################################\n",
      "Training Loss:  89.91312408\n",
      "################################  800  ################################\n",
      "Training Loss:  89.59936523\n",
      "Epoch   847: reducing learning rate of group 0 to 4.0000e-02.\n",
      "################################  900  ################################\n",
      "Training Loss:  89.15275574\n",
      "################################  1000  ################################\n",
      "Training Loss:  88.94804382\n",
      "Epoch  1048: reducing learning rate of group 0 to 3.2000e-02.\n",
      "################################  1100  ################################\n",
      "Training Loss:  88.75867462\n",
      "################################  1200  ################################\n",
      "Training Loss:  88.54525757\n",
      "Epoch  1249: reducing learning rate of group 0 to 2.5600e-02.\n",
      "################################  1300  ################################\n",
      "Training Loss:  88.11371613\n",
      "################################  1400  ################################\n",
      "Training Loss:  88.01436615\n",
      "Epoch  1450: reducing learning rate of group 0 to 2.0480e-02.\n",
      "################################  1500  ################################\n",
      "Training Loss:  87.95475006\n",
      "################################  1600  ################################\n",
      "Training Loss:  87.89582062\n",
      "Epoch  1651: reducing learning rate of group 0 to 1.6384e-02.\n",
      "################################  1700  ################################\n",
      "Training Loss:  87.85348511\n",
      "################################  1800  ################################\n",
      "Training Loss:  87.77827454\n",
      "Epoch  1852: reducing learning rate of group 0 to 1.3107e-02.\n",
      "################################  1900  ################################\n",
      "Training Loss:  87.75190735\n",
      "Final training Loss:  87.58525848\n",
      "\n",
      "Running model (trial=9, mod=299, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'model': <class 'deep_reparametrization.ResNET.ResNET'>, 'n_hidden_layers': 64, 'neurons': 8, 'activation': 'relu'}, {'batch_size': 289, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12de80e50>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12e1dcee0>, 'optimizer': 'ADAM', 'num_epochs': 2000, 'learning_rate': 0.05, 'lr_scheduler': <function <lambda> at 0x12e06c430>})\n",
      "################################  0  ################################\n",
      "Training Loss:  103769345884160.0\n",
      "Final training Loss:  nan\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# plotting\n",
    "disc_points = x_train.detach()\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": disc_points,\n",
    "    \"x_train\": disc_points,\n",
    "    \"y_train\": I1_new,\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"DP solution\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: overflow encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: invalid value encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "activation = np.vectorize(lambda model: model.activation)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: penalty_free_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "loss_array -= d_01 ** 2\n",
    "# make data frame\n",
    "# optims = [\"line search\", \"ADAM\", \"LBFGS\"]*(len(loss_array)//3)\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type,\n",
    "     \"activation\": activation})\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-31T00:28:10.650287</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m6fdb46ca6c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.654947\" xlink:href=\"#m6fdb46ca6c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(152.854947 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"306.094106\" xlink:href=\"#m6fdb46ca6c\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(297.294106 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"me361ec1236\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"60.696307\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"86.13078\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"104.176826\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"118.174427\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"129.6113\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"139.281038\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"147.657346\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"155.045773\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"205.135466\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"230.569939\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"248.615986\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"262.613586\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"274.050459\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"283.720197\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"292.096505\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"299.484932\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"349.574625\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"375.009099\" xlink:href=\"#me361ec1236\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Neurons -->\n     <g transform=\"translate(192.116406 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"199.707031\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"238.570312\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"299.751953\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"363.130859\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m3b0c40e588\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m3b0c40e588\" y=\"190.623056\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 194.422275)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_22\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"mcf3ee5084e\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"220.625157\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"209.393019\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"199.485571\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"132.318227\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"98.212088\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"74.013398\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"55.243435\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"39.907259\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"26.940707\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mcf3ee5084e\" y=\"15.708569\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_5\">\n     <!-- $\\Delta E$ -->\n     <g transform=\"translate(14.798438 122.52)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 906 525 \nL 3472 525 \nL 2188 4044 \nz\nM 50 0 \nL 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 50 0 \nz\n\" id=\"DejaVuSans-394\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-394\"/>\n      <use transform=\"translate(68.408203 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 60.696307 110.049079 \nL 60.696307 52.302014 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 104.176826 144.047214 \nL 104.176826 96.843147 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 147.657346 98.745753 \nL 147.657346 59.412068 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 191.137865 55.045527 \nL 191.137865 21.311877 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 234.618385 42.537355 \nL 234.618385 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 278.098904 94.514202 \nL 278.098904 43.129099 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 321.579424 58.47535 \nL 321.579424 23.497692 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 365.059943 75.187569 \nL 365.059943 34.557837 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 60.696307 79.833736 \nL 60.696307 36.299885 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 104.176826 176.061275 \nL 104.176826 74.032853 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 147.657346 161.906934 \nL 147.657346 100.179346 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 191.137865 214.756364 \nL 191.137865 113.554669 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 234.618385 80.479469 \nL 234.618385 36.563566 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 278.098904 146.654832 \nL 278.098904 81.651859 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 321.579424 93.641185 \nL 321.579424 42.891915 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 365.059943 46.814664 \nL 365.059943 20.987718 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_32\">\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 60.696307 76.725164 \nL 104.176826 117.405389 \nL 147.657346 75.711066 \nL 191.137865 36.270057 \nL 234.618385 27.241678 \nL 278.098904 64.820259 \nL 321.579424 38.97516 \nL 365.059943 51.837848 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_33\">\n    <path clip-path=\"url(#p48fb9d416a)\" d=\"M 60.696307 55.483853 \nL 104.176826 110.44224 \nL 147.657346 126.456706 \nL 191.137865 150.071016 \nL 234.618385 55.388333 \nL 278.098904 110.035332 \nL 321.579424 64.634143 \nL 365.059943 32.650467 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_34\"/>\n   <g id=\"line2d_35\"/>\n   <g id=\"line2d_36\"/>\n   <g id=\"line2d_37\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 52.478125 219.64 \nL 107.201563 219.64 \nQ 109.201563 219.64 109.201563 217.64 \nL 109.201563 174.605625 \nQ 109.201563 172.605625 107.201563 172.605625 \nL 52.478125 172.605625 \nQ 50.478125 172.605625 50.478125 174.605625 \nL 50.478125 217.64 \nQ 50.478125 219.64 52.478125 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_6\">\n     <!-- activation -->\n     <g transform=\"translate(55.076563 184.204062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"155.46875\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"183.251953\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"242.431641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"303.710938\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"342.919922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"370.703125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"431.884766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_38\">\n     <path d=\"M 54.478125 195.382187 \nL 74.478125 195.382187 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_39\"/>\n    <g id=\"text_7\">\n     <!-- tanh -->\n     <g transform=\"translate(82.478125 198.882187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"100.488281\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"163.867188\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n    <g id=\"line2d_40\">\n     <path d=\"M 54.478125 210.060312 \nL 74.478125 210.060312 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_41\"/>\n    <g id=\"text_8\">\n     <!-- relu -->\n     <g transform=\"translate(82.478125 213.560312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"38.863281\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"100.386719\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"128.169922\" xlink:href=\"#DejaVuSans-75\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p48fb9d416a\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"activation\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "140 180\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 10]), len(d_results_neurons))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-31T00:28:11.598465</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mac318d08a9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.696307\" xlink:href=\"#mac318d08a9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(51.896307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"229.208659\" xlink:href=\"#mac318d08a9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(220.408659 239.238437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m1d182606ff\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"52.985604\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"111.42358\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"141.097132\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"162.150852\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"178.481387\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"191.824405\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"203.105766\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"212.878125\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"221.497957\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"279.935932\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"309.609484\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"330.663205\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"346.993739\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"360.336757\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"371.618118\" xlink:href=\"#m1d182606ff\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Layers -->\n     <g transform=\"translate(196.332031 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"55.712891\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"116.992188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"176.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"237.695312\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"278.808594\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_18\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"me592cd8d21\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#me592cd8d21\" y=\"192.122527\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 195.921745)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#me592cd8d21\" y=\"83.733165\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 87.532384)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"me21f87fc8f\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"216.168571\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"208.912251\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"202.626541\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"197.082152\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"159.494077\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"140.407658\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"126.865628\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"116.361614\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"107.779209\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"100.522889\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"94.237179\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"88.69279\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"51.104716\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"32.018297\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"18.476267\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#me21f87fc8f\" y=\"7.972252\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_6\">\n     <!-- $\\Delta E$ -->\n     <g transform=\"translate(14.798438 122.52)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2188 4044 \nL 906 525 \nL 3472 525 \nL 2188 4044 \nz\nM 50 0 \nL 1831 4666 \nL 2547 4666 \nL 4325 0 \nL 50 0 \nz\n\" id=\"DejaVuSans-394\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-394\"/>\n      <use transform=\"translate(68.408203 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 60.696307 163.913536 \nL 60.696307 124.616557 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 111.42358 140.384109 \nL 111.42358 118.455187 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 162.150852 123.789594 \nL 162.150852 102.40993 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 212.878125 154.328916 \nL 212.878125 118.376757 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 263.605398 126.938479 \nL 263.605398 102.886143 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 314.33267 126.949376 \nL 314.33267 103.036312 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 365.059943 134.398028 \nL 365.059943 109.999844 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 60.696307 195.552789 \nL 60.696307 140.809199 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 111.42358 177.448443 \nL 111.42358 141.062954 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 162.150852 205.068813 \nL 162.150852 150.688428 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 212.878125 214.756364 \nL 212.878125 189.003213 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 263.605398 149.97535 \nL 263.605398 115.340554 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 314.33267 139.878021 \nL 314.33267 116.995079 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 365.059943 87.121509 \nL 365.059943 17.083636 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_36\">\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 60.696307 141.03711 \nL 111.42358 127.815675 \nL 162.150852 112.019987 \nL 212.878125 132.961972 \nL 263.605398 113.147842 \nL 314.33267 113.165945 \nL 365.059943 120.084427 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_37\">\n    <path clip-path=\"url(#p2dce417729)\" d=\"M 60.696307 162.593872 \nL 111.42358 156.213863 \nL 162.150852 170.789217 \nL 212.878125 200.792727 \nL 263.605398 130.459298 \nL 314.33267 126.809067 \nL 365.059943 43.223535 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_38\"/>\n   <g id=\"line2d_39\"/>\n   <g id=\"line2d_40\"/>\n   <g id=\"line2d_41\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 52.478125 59.234375 \nL 107.201563 59.234375 \nQ 109.201563 59.234375 109.201563 57.234375 \nL 109.201563 14.2 \nQ 109.201563 12.2 107.201563 12.2 \nL 52.478125 12.2 \nQ 50.478125 12.2 50.478125 14.2 \nL 50.478125 57.234375 \nQ 50.478125 59.234375 52.478125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_7\">\n     <!-- activation -->\n     <g transform=\"translate(55.076563 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3122 3366 \nL 3122 2828 \nQ 2878 2963 2633 3030 \nQ 2388 3097 2138 3097 \nQ 1578 3097 1268 2742 \nQ 959 2388 959 1747 \nQ 959 1106 1268 751 \nQ 1578 397 2138 397 \nQ 2388 397 2633 464 \nQ 2878 531 3122 666 \nL 3122 134 \nQ 2881 22 2623 -34 \nQ 2366 -91 2075 -91 \nQ 1284 -91 818 406 \nQ 353 903 353 1747 \nQ 353 2603 823 3093 \nQ 1294 3584 2113 3584 \nQ 2378 3584 2631 3529 \nQ 2884 3475 3122 3366 \nz\n\" id=\"DejaVuSans-63\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 191 3500 \nL 800 3500 \nL 1894 563 \nL 2988 3500 \nL 3597 3500 \nL 2284 0 \nL 1503 0 \nL 191 3500 \nz\n\" id=\"DejaVuSans-76\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"61.279297\" xlink:href=\"#DejaVuSans-63\"/>\n      <use x=\"116.259766\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"155.46875\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"183.251953\" xlink:href=\"#DejaVuSans-76\"/>\n      <use x=\"242.431641\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"303.710938\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"342.919922\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"370.703125\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"431.884766\" xlink:href=\"#DejaVuSans-6e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_42\">\n     <path d=\"M 54.478125 34.976562 \nL 74.478125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_43\"/>\n    <g id=\"text_8\">\n     <!-- tanh -->\n     <g transform=\"translate(82.478125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 4863 \nL 1159 4863 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-68\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"39.208984\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"100.488281\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"163.867188\" xlink:href=\"#DejaVuSans-68\"/>\n     </g>\n    </g>\n    <g id=\"line2d_44\">\n     <path d=\"M 54.478125 49.654687 \nL 74.478125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_45\"/>\n    <g id=\"text_9\">\n     <!-- relu -->\n     <g transform=\"translate(82.478125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"38.863281\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"100.386719\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"128.169922\" xlink:href=\"#DejaVuSans-75\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p2dce417729\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"activation\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Layers\", ylabel=\"Error\")\n",
    "fig_layers.set(ylabel=\"$\\\\Delta E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "118 160\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 10]), len(d_results_layer))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 387.478125 262.19625\" width=\"387.478125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-31T00:28:12.171136</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 387.478125 262.19625 \nL 387.478125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \nL 380.278125 7.2 \nL 45.478125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 105.774363 \nC 61.491916 105.774363 62.255046 105.458264 62.817627 104.895683 \nC 63.380208 104.333102 63.696307 103.569972 63.696307 102.774363 \nC 63.696307 101.978753 63.380208 101.215623 62.817627 100.653042 \nC 62.255046 100.090462 61.491916 99.774363 60.696307 99.774363 \nC 59.900698 99.774363 59.137567 100.090462 58.574986 100.653042 \nC 58.012406 101.215623 57.696307 101.978753 57.696307 102.774363 \nC 57.696307 103.569972 58.012406 104.333102 58.574986 104.895683 \nC 59.137567 105.458264 59.900698 105.774363 60.696307 105.774363 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 157.029052 \nC 61.570115 157.029052 62.333245 156.712953 62.895826 156.150372 \nC 63.458407 155.587791 63.774506 154.824661 63.774506 154.029052 \nC 63.774506 153.233442 63.458407 152.470312 62.895826 151.907731 \nC 62.333245 151.345151 61.570115 151.029052 60.774506 151.029052 \nC 59.978896 151.029052 59.215766 151.345151 58.653185 151.907731 \nC 58.090605 152.470312 57.774506 153.233442 57.774506 154.029052 \nC 57.774506 154.824661 58.090605 155.587791 58.653185 156.150372 \nC 59.215766 156.712953 59.978896 157.029052 60.774506 157.029052 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 105.842231 \nC 61.836911 105.842231 62.600041 105.526132 63.162622 104.963551 \nC 63.725203 104.400971 64.041302 103.63784 64.041302 102.842231 \nC 64.041302 102.046622 63.725203 101.283491 63.162622 100.720911 \nC 62.600041 100.15833 61.836911 99.842231 61.041302 99.842231 \nC 60.245692 99.842231 59.482562 100.15833 58.919981 100.720911 \nC 58.357401 101.283491 58.041302 102.046622 58.041302 102.842231 \nC 58.041302 103.63784 58.357401 104.400971 58.919981 104.963551 \nC 59.482562 105.526132 60.245692 105.842231 61.041302 105.842231 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 108.41484 \nC 62.812096 108.41484 63.575227 108.098741 64.137807 107.53616 \nC 64.700388 106.973579 65.016487 106.210449 65.016487 105.41484 \nC 65.016487 104.61923 64.700388 103.8561 64.137807 103.293519 \nC 63.575227 102.730938 62.812096 102.41484 62.016487 102.41484 \nC 61.220878 102.41484 60.457747 102.730938 59.895167 103.293519 \nC 59.332586 103.8561 59.016487 104.61923 59.016487 105.41484 \nC 59.016487 106.210449 59.332586 106.973579 59.895167 107.53616 \nC 60.457747 108.098741 61.220878 108.41484 62.016487 108.41484 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 106.445537 \nC 66.528841 106.445537 67.291971 106.129438 67.854552 105.566858 \nC 68.417132 105.004277 68.733231 104.241147 68.733231 103.445537 \nC 68.733231 102.649928 68.417132 101.886798 67.854552 101.324217 \nC 67.291971 100.761636 66.528841 100.445537 65.733231 100.445537 \nC 64.937622 100.445537 64.174492 100.761636 63.611911 101.324217 \nC 63.04933 101.886798 62.733231 102.649928 62.733231 103.445537 \nC 62.733231 104.241147 63.04933 105.004277 63.611911 105.566858 \nC 64.174492 106.129438 64.937622 106.445537 65.733231 106.445537 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 151.469723 \nC 81.027823 151.469723 81.790954 151.153625 82.353534 150.591044 \nC 82.916115 150.028463 83.232214 149.265333 83.232214 148.469723 \nC 83.232214 147.674114 82.916115 146.910984 82.353534 146.348403 \nC 81.790954 145.785822 81.027823 145.469723 80.232214 145.469723 \nC 79.436605 145.469723 78.673474 145.785822 78.110894 146.348403 \nC 77.548313 146.910984 77.232214 147.674114 77.232214 148.469723 \nC 77.232214 149.265333 77.548313 150.028463 78.110894 150.591044 \nC 78.673474 151.153625 79.436605 151.469723 80.232214 151.469723 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 156.062483 \nC 138.287765 156.062483 139.050895 155.746384 139.613476 155.183803 \nC 140.176056 154.621222 140.492155 153.858092 140.492155 153.062483 \nC 140.492155 152.266873 140.176056 151.503743 139.613476 150.941162 \nC 139.050895 150.378582 138.287765 150.062483 137.492155 150.062483 \nC 136.696546 150.062483 135.933416 150.378582 135.370835 150.941162 \nC 134.808254 151.503743 134.492155 152.266873 134.492155 153.062483 \nC 134.492155 153.858092 134.808254 154.621222 135.370835 155.183803 \nC 135.933416 155.746384 136.696546 156.062483 137.492155 156.062483 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 157.285423 \nC 365.855552 157.285423 366.618683 156.969324 367.181264 156.406743 \nC 367.743844 155.844162 368.059943 155.081032 368.059943 154.285423 \nC 368.059943 153.489813 367.743844 152.726683 367.181264 152.164102 \nC 366.618683 151.601522 365.855552 151.285423 365.059943 151.285423 \nC 364.264334 151.285423 363.501204 151.601522 362.938623 152.164102 \nC 362.376042 152.726683 362.059943 153.489813 362.059943 154.285423 \nC 362.059943 155.081032 362.376042 155.844162 362.938623 156.406743 \nC 363.501204 156.969324 364.264334 157.285423 365.059943 157.285423 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 159.007009 \nC 61.671313 159.007009 62.434444 158.69091 62.997024 158.128329 \nC 63.559605 157.565748 63.875704 156.802618 63.875704 156.007009 \nC 63.875704 155.211399 63.559605 154.448269 62.997024 153.885688 \nC 62.434444 153.323108 61.671313 153.007009 60.875704 153.007009 \nC 60.080095 153.007009 59.316965 153.323108 58.754384 153.885688 \nC 58.191803 154.448269 57.875704 155.211399 57.875704 156.007009 \nC 57.875704 156.802618 58.191803 157.565748 58.754384 158.128329 \nC 59.316965 158.69091 60.080095 159.007009 60.875704 159.007009 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 155.194252 \nC 61.836911 155.194252 62.600041 154.878153 63.162622 154.315573 \nC 63.725203 153.752992 64.041302 152.989862 64.041302 152.194252 \nC 64.041302 151.398643 63.725203 150.635513 63.162622 150.072932 \nC 62.600041 149.510351 61.836911 149.194252 61.041302 149.194252 \nC 60.245692 149.194252 59.482562 149.510351 58.919981 150.072932 \nC 58.357401 150.635513 58.041302 151.398643 58.041302 152.194252 \nC 58.041302 152.989862 58.357401 153.752992 58.919981 154.315573 \nC 59.482562 154.878153 60.245692 155.194252 61.041302 155.194252 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 108.137682 \nC 62.168106 108.137682 62.931236 107.821583 63.493817 107.259003 \nC 64.056398 106.696422 64.372497 105.933291 64.372497 105.137682 \nC 64.372497 104.342073 64.056398 103.578943 63.493817 103.016362 \nC 62.931236 102.453781 62.168106 102.137682 61.372497 102.137682 \nC 60.576887 102.137682 59.813757 102.453781 59.251176 103.016362 \nC 58.688596 103.578943 58.372497 104.342073 58.372497 105.137682 \nC 58.372497 105.933291 58.688596 106.696422 59.251176 107.259003 \nC 59.813757 107.821583 60.576887 108.137682 61.372497 108.137682 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 154.040016 \nC 62.830496 154.040016 63.593626 153.723917 64.156207 153.161337 \nC 64.718788 152.598756 65.034887 151.835626 65.034887 151.040016 \nC 65.034887 150.244407 64.718788 149.481277 64.156207 148.918696 \nC 63.593626 148.356115 62.830496 148.040016 62.034887 148.040016 \nC 61.239277 148.040016 60.476147 148.356115 59.913566 148.918696 \nC 59.350986 149.481277 59.034887 150.244407 59.034887 151.040016 \nC 59.034887 151.835626 59.350986 152.598756 59.913566 153.161337 \nC 60.476147 153.723917 61.239277 154.040016 62.034887 154.040016 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 105.981424 \nC 64.155276 105.981424 64.918406 105.665325 65.480987 105.102744 \nC 66.043568 104.540163 66.359667 103.777033 66.359667 102.981424 \nC 66.359667 102.185815 66.043568 101.422684 65.480987 100.860104 \nC 64.918406 100.297523 64.155276 99.981424 63.359667 99.981424 \nC 62.564058 99.981424 61.800927 100.297523 61.238347 100.860104 \nC 60.675766 101.422684 60.359667 102.185815 60.359667 102.981424 \nC 60.359667 103.777033 60.675766 104.540163 61.238347 105.102744 \nC 61.800927 105.665325 62.564058 105.981424 63.359667 105.981424 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 105.263551 \nC 66.804836 105.263551 67.567967 104.947452 68.130547 104.384872 \nC 68.693128 103.822291 69.009227 103.05916 69.009227 102.263551 \nC 69.009227 101.467942 68.693128 100.704812 68.130547 100.142231 \nC 67.567967 99.57965 66.804836 99.263551 66.009227 99.263551 \nC 65.213618 99.263551 64.450488 99.57965 63.887907 100.142231 \nC 63.325326 100.704812 63.009227 101.467942 63.009227 102.263551 \nC 63.009227 103.05916 63.325326 103.822291 63.887907 104.384872 \nC 64.450488 104.947452 65.213618 105.263551 66.009227 105.263551 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 152.780746 \nC 72.103957 152.780746 72.867087 152.464647 73.429668 151.902067 \nC 73.992249 151.339486 74.308348 150.576355 74.308348 149.780746 \nC 74.308348 148.985137 73.992249 148.222007 73.429668 147.659426 \nC 72.867087 147.096845 72.103957 146.780746 71.308348 146.780746 \nC 70.512738 146.780746 69.749608 147.096845 69.187027 147.659426 \nC 68.624447 148.222007 68.308348 148.985137 68.308348 149.780746 \nC 68.308348 150.576355 68.624447 151.339486 69.187027 151.902067 \nC 69.749608 152.464647 70.512738 152.780746 71.308348 152.780746 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 161.413861 \nC 61.836911 161.413861 62.600041 161.097762 63.162622 160.535181 \nC 63.725203 159.972601 64.041302 159.20947 64.041302 158.413861 \nC 64.041302 157.618252 63.725203 156.855121 63.162622 156.292541 \nC 62.600041 155.72996 61.836911 155.413861 61.041302 155.413861 \nC 60.245692 155.413861 59.482562 155.72996 58.919981 156.292541 \nC 58.357401 156.855121 58.041302 157.618252 58.041302 158.413861 \nC 58.041302 159.20947 58.357401 159.972601 58.919981 160.535181 \nC 59.482562 161.097762 60.245692 161.413861 61.041302 161.413861 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 167.374218 \nC 62.812096 167.374218 63.575227 167.058119 64.137807 166.495539 \nC 64.700388 165.932958 65.016487 165.169827 65.016487 164.374218 \nC 65.016487 163.578609 64.700388 162.815479 64.137807 162.252898 \nC 63.575227 161.690317 62.812096 161.374218 62.016487 161.374218 \nC 61.220878 161.374218 60.457747 161.690317 59.895167 162.252898 \nC 59.332586 162.815479 59.016487 163.578609 59.016487 164.374218 \nC 59.016487 165.169827 59.332586 165.932958 59.895167 166.495539 \nC 60.457747 167.058119 61.220878 167.374218 62.016487 167.374218 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 191.208367 \nC 66.528841 191.208367 67.291971 190.892268 67.854552 190.329687 \nC 68.417132 189.767107 68.733231 189.003976 68.733231 188.208367 \nC 68.733231 187.412758 68.417132 186.649628 67.854552 186.087047 \nC 67.291971 185.524466 66.528841 185.208367 65.733231 185.208367 \nC 64.937622 185.208367 64.174492 185.524466 63.611911 186.087047 \nC 63.04933 186.649628 62.733231 187.412758 62.733231 188.208367 \nC 62.733231 189.003976 63.04933 189.767107 63.611911 190.329687 \nC 64.174492 190.892268 64.937622 191.208367 65.733231 191.208367 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 107.916303 \nC 81.027823 107.916303 81.790954 107.600204 82.353534 107.037623 \nC 82.916115 106.475043 83.232214 105.711912 83.232214 104.916303 \nC 83.232214 104.120694 82.916115 103.357563 82.353534 102.794983 \nC 81.790954 102.232402 81.027823 101.916303 80.232214 101.916303 \nC 79.436605 101.916303 78.673474 102.232402 78.110894 102.794983 \nC 77.548313 103.357563 77.232214 104.120694 77.232214 104.916303 \nC 77.232214 105.711912 77.548313 106.475043 78.110894 107.037623 \nC 78.673474 107.600204 79.436605 107.916303 80.232214 107.916303 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 103.788341 \nC 138.287765 103.788341 139.050895 103.472242 139.613476 102.909661 \nC 140.176056 102.347081 140.492155 101.58395 140.492155 100.788341 \nC 140.492155 99.992732 140.176056 99.229601 139.613476 98.667021 \nC 139.050895 98.10444 138.287765 97.788341 137.492155 97.788341 \nC 136.696546 97.788341 135.933416 98.10444 135.370835 98.667021 \nC 134.808254 99.229601 134.492155 99.992732 134.492155 100.788341 \nC 134.492155 101.58395 134.808254 102.347081 135.370835 102.909661 \nC 135.933416 103.472242 136.696546 103.788341 137.492155 103.788341 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 105.703353 \nC 365.855552 105.703353 366.618683 105.387254 367.181264 104.824673 \nC 367.743844 104.262093 368.059943 103.498962 368.059943 102.703353 \nC 368.059943 101.907744 367.743844 101.144613 367.181264 100.582033 \nC 366.618683 100.019452 365.855552 99.703353 365.059943 99.703353 \nC 364.264334 99.703353 363.501204 100.019452 362.938623 100.582033 \nC 362.376042 101.144613 362.059943 101.907744 362.059943 102.703353 \nC 362.059943 103.498962 362.376042 104.262093 362.938623 104.824673 \nC 363.501204 105.387254 364.264334 105.703353 365.059943 105.703353 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 204.131234 \nC 61.671313 204.131234 62.434444 203.815135 62.997024 203.252554 \nC 63.559605 202.689973 63.875704 201.926843 63.875704 201.131234 \nC 63.875704 200.335624 63.559605 199.572494 62.997024 199.009913 \nC 62.434444 198.447333 61.671313 198.131234 60.875704 198.131234 \nC 60.080095 198.131234 59.316965 198.447333 58.754384 199.009913 \nC 58.191803 199.572494 57.875704 200.335624 57.875704 201.131234 \nC 57.875704 201.926843 58.191803 202.689973 58.754384 203.252554 \nC 59.316965 203.815135 60.080095 204.131234 60.875704 204.131234 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 178.236282 \nC 62.168106 178.236282 62.931236 177.920183 63.493817 177.357603 \nC 64.056398 176.795022 64.372497 176.031892 64.372497 175.236282 \nC 64.372497 174.440673 64.056398 173.677543 63.493817 173.114962 \nC 62.931236 172.552381 62.168106 172.236282 61.372497 172.236282 \nC 60.576887 172.236282 59.813757 172.552381 59.251176 173.114962 \nC 58.688596 173.677543 58.372497 174.440673 58.372497 175.236282 \nC 58.372497 176.031892 58.688596 176.795022 59.251176 177.357603 \nC 59.813757 177.920183 60.576887 178.236282 61.372497 178.236282 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 194.57986 \nC 62.830496 194.57986 63.593626 194.263761 64.156207 193.70118 \nC 64.718788 193.1386 65.034887 192.375469 65.034887 191.57986 \nC 65.034887 190.784251 64.718788 190.02112 64.156207 189.45854 \nC 63.593626 188.895959 62.830496 188.57986 62.034887 188.57986 \nC 61.239277 188.57986 60.476147 188.895959 59.913566 189.45854 \nC 59.350986 190.02112 59.034887 190.784251 59.034887 191.57986 \nC 59.034887 192.375469 59.350986 193.1386 59.913566 193.70118 \nC 60.476147 194.263761 61.239277 194.57986 62.034887 194.57986 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 170.413068 \nC 64.155276 170.413068 64.918406 170.096969 65.480987 169.534388 \nC 66.043568 168.971808 66.359667 168.208677 66.359667 167.413068 \nC 66.359667 166.617459 66.043568 165.854328 65.480987 165.291748 \nC 64.918406 164.729167 64.155276 164.413068 63.359667 164.413068 \nC 62.564058 164.413068 61.800927 164.729167 61.238347 165.291748 \nC 60.675766 165.854328 60.359667 166.617459 60.359667 167.413068 \nC 60.359667 168.208677 60.675766 168.971808 61.238347 169.534388 \nC 61.800927 170.096969 62.564058 170.413068 63.359667 170.413068 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 104.441184 \nC 66.804836 104.441184 67.567967 104.125085 68.130547 103.562504 \nC 68.693128 102.999924 69.009227 102.236793 69.009227 101.441184 \nC 69.009227 100.645575 68.693128 99.882444 68.130547 99.319864 \nC 67.567967 98.757283 66.804836 98.441184 66.009227 98.441184 \nC 65.213618 98.441184 64.450488 98.757283 63.887907 99.319864 \nC 63.325326 99.882444 63.009227 100.645575 63.009227 101.441184 \nC 63.009227 102.236793 63.325326 102.999924 63.887907 103.562504 \nC 64.450488 104.125085 65.213618 104.441184 66.009227 104.441184 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 85.278457 \nC 72.103957 85.278457 72.867087 84.962358 73.429668 84.399778 \nC 73.992249 83.837197 74.308348 83.074066 74.308348 82.278457 \nC 74.308348 81.482848 73.992249 80.719718 73.429668 80.157137 \nC 72.867087 79.594556 72.103957 79.278457 71.308348 79.278457 \nC 70.512738 79.278457 69.749608 79.594556 69.187027 80.157137 \nC 68.624447 80.719718 68.308348 81.482848 68.308348 82.278457 \nC 68.308348 83.074066 68.624447 83.837197 69.187027 84.399778 \nC 69.749608 84.962358 70.512738 85.278457 71.308348 85.278457 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 156.475802 \nC 61.491916 156.475802 62.255046 156.159703 62.817627 155.597122 \nC 63.380208 155.034541 63.696307 154.271411 63.696307 153.475802 \nC 63.696307 152.680192 63.380208 151.917062 62.817627 151.354481 \nC 62.255046 150.7919 61.491916 150.475802 60.696307 150.475802 \nC 59.900698 150.475802 59.137567 150.7919 58.574986 151.354481 \nC 58.012406 151.917062 57.696307 152.680192 57.696307 153.475802 \nC 57.696307 154.271411 58.012406 155.034541 58.574986 155.597122 \nC 59.137567 156.159703 59.900698 156.475802 60.696307 156.475802 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 156.314098 \nC 61.570115 156.314098 62.333245 155.997999 62.895826 155.435418 \nC 63.458407 154.872838 63.774506 154.109707 63.774506 153.314098 \nC 63.774506 152.518489 63.458407 151.755359 62.895826 151.192778 \nC 62.333245 150.630197 61.570115 150.314098 60.774506 150.314098 \nC 59.978896 150.314098 59.215766 150.630197 58.653185 151.192778 \nC 58.090605 151.755359 57.774506 152.518489 57.774506 153.314098 \nC 57.774506 154.109707 58.090605 154.872838 58.653185 155.435418 \nC 59.215766 155.997999 59.978896 156.314098 60.774506 156.314098 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 180.894831 \nC 61.836911 180.894831 62.600041 180.578732 63.162622 180.016151 \nC 63.725203 179.45357 64.041302 178.69044 64.041302 177.894831 \nC 64.041302 177.099221 63.725203 176.336091 63.162622 175.77351 \nC 62.600041 175.210929 61.836911 174.894831 61.041302 174.894831 \nC 60.245692 174.894831 59.482562 175.210929 58.919981 175.77351 \nC 58.357401 176.336091 58.041302 177.099221 58.041302 177.894831 \nC 58.041302 178.69044 58.357401 179.45357 58.919981 180.016151 \nC 59.482562 180.578732 60.245692 180.894831 61.041302 180.894831 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 147.272426 \nC 62.812096 147.272426 63.575227 146.956327 64.137807 146.393747 \nC 64.700388 145.831166 65.016487 145.068036 65.016487 144.272426 \nC 65.016487 143.476817 64.700388 142.713687 64.137807 142.151106 \nC 63.575227 141.588525 62.812096 141.272426 62.016487 141.272426 \nC 61.220878 141.272426 60.457747 141.588525 59.895167 142.151106 \nC 59.332586 142.713687 59.016487 143.476817 59.016487 144.272426 \nC 59.016487 145.068036 59.332586 145.831166 59.895167 146.393747 \nC 60.457747 146.956327 61.220878 147.272426 62.016487 147.272426 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 102.94357 \nC 66.528841 102.94357 67.291971 102.627471 67.854552 102.064891 \nC 68.417132 101.50231 68.733231 100.739179 68.733231 99.94357 \nC 68.733231 99.147961 68.417132 98.384831 67.854552 97.82225 \nC 67.291971 97.259669 66.528841 96.94357 65.733231 96.94357 \nC 64.937622 96.94357 64.174492 97.259669 63.611911 97.82225 \nC 63.04933 98.384831 62.733231 99.147961 62.733231 99.94357 \nC 62.733231 100.739179 63.04933 101.50231 63.611911 102.064891 \nC 64.174492 102.627471 64.937622 102.94357 65.733231 102.94357 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 151.71646 \nC 81.027823 151.71646 81.790954 151.400361 82.353534 150.83778 \nC 82.916115 150.275199 83.232214 149.512069 83.232214 148.71646 \nC 83.232214 147.92085 82.916115 147.15772 82.353534 146.595139 \nC 81.790954 146.032559 81.027823 145.71646 80.232214 145.71646 \nC 79.436605 145.71646 78.673474 146.032559 78.110894 146.595139 \nC 77.548313 147.15772 77.232214 147.92085 77.232214 148.71646 \nC 77.232214 149.512069 77.548313 150.275199 78.110894 150.83778 \nC 78.673474 151.400361 79.436605 151.71646 80.232214 151.71646 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 157.423987 \nC 138.287765 157.423987 139.050895 157.107888 139.613476 156.545307 \nC 140.176056 155.982727 140.492155 155.219596 140.492155 154.423987 \nC 140.492155 153.628378 140.176056 152.865247 139.613476 152.302667 \nC 139.050895 151.740086 138.287765 151.423987 137.492155 151.423987 \nC 136.696546 151.423987 135.933416 151.740086 135.370835 152.302667 \nC 134.808254 152.865247 134.492155 153.628378 134.492155 154.423987 \nC 134.492155 155.219596 134.808254 155.982727 135.370835 156.545307 \nC 135.933416 157.107888 136.696546 157.423987 137.492155 157.423987 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 105.139489 \nC 365.855552 105.139489 366.618683 104.82339 367.181264 104.260809 \nC 367.743844 103.698228 368.059943 102.935098 368.059943 102.139489 \nC 368.059943 101.34388 367.743844 100.580749 367.181264 100.018169 \nC 366.618683 99.455588 365.855552 99.139489 365.059943 99.139489 \nC 364.264334 99.139489 363.501204 99.455588 362.938623 100.018169 \nC 362.376042 100.580749 362.059943 101.34388 362.059943 102.139489 \nC 362.059943 102.935098 362.376042 103.698228 362.938623 104.260809 \nC 363.501204 104.82339 364.264334 105.139489 365.059943 105.139489 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 170.299055 \nC 61.671313 170.299055 62.434444 169.982956 62.997024 169.420376 \nC 63.559605 168.857795 63.875704 168.094665 63.875704 167.299055 \nC 63.875704 166.503446 63.559605 165.740316 62.997024 165.177735 \nC 62.434444 164.615154 61.671313 164.299055 60.875704 164.299055 \nC 60.080095 164.299055 59.316965 164.615154 58.754384 165.177735 \nC 58.191803 165.740316 57.875704 166.503446 57.875704 167.299055 \nC 57.875704 168.094665 58.191803 168.857795 58.754384 169.420376 \nC 59.316965 169.982956 60.080095 170.299055 60.875704 170.299055 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 104.625717 \nC 61.836911 104.625717 62.600041 104.309618 63.162622 103.747038 \nC 63.725203 103.184457 64.041302 102.421327 64.041302 101.625717 \nC 64.041302 100.830108 63.725203 100.066978 63.162622 99.504397 \nC 62.600041 98.941816 61.836911 98.625717 61.041302 98.625717 \nC 60.245692 98.625717 59.482562 98.941816 58.919981 99.504397 \nC 58.357401 100.066978 58.041302 100.830108 58.041302 101.625717 \nC 58.041302 102.421327 58.357401 103.184457 58.919981 103.747038 \nC 59.482562 104.309618 60.245692 104.625717 61.041302 104.625717 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 148.891095 \nC 62.168106 148.891095 62.931236 148.574996 63.493817 148.012416 \nC 64.056398 147.449835 64.372497 146.686704 64.372497 145.891095 \nC 64.372497 145.095486 64.056398 144.332356 63.493817 143.769775 \nC 62.931236 143.207194 62.168106 142.891095 61.372497 142.891095 \nC 60.576887 142.891095 59.813757 143.207194 59.251176 143.769775 \nC 58.688596 144.332356 58.372497 145.095486 58.372497 145.891095 \nC 58.372497 146.686704 58.688596 147.449835 59.251176 148.012416 \nC 59.813757 148.574996 60.576887 148.891095 61.372497 148.891095 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 153.636974 \nC 62.830496 153.636974 63.593626 153.320875 64.156207 152.758294 \nC 64.718788 152.195713 65.034887 151.432583 65.034887 150.636974 \nC 65.034887 149.841365 64.718788 149.078234 64.156207 148.515654 \nC 63.593626 147.953073 62.830496 147.636974 62.034887 147.636974 \nC 61.239277 147.636974 60.476147 147.953073 59.913566 148.515654 \nC 59.350986 149.078234 59.034887 149.841365 59.034887 150.636974 \nC 59.034887 151.432583 59.350986 152.195713 59.913566 152.758294 \nC 60.476147 153.320875 61.239277 153.636974 62.034887 153.636974 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 107.005467 \nC 64.155276 107.005467 64.918406 106.689368 65.480987 106.126787 \nC 66.043568 105.564206 66.359667 104.801076 66.359667 104.005467 \nC 66.359667 103.209857 66.043568 102.446727 65.480987 101.884146 \nC 64.918406 101.321566 64.155276 101.005467 63.359667 101.005467 \nC 62.564058 101.005467 61.800927 101.321566 61.238347 101.884146 \nC 60.675766 102.446727 60.359667 103.209857 60.359667 104.005467 \nC 60.359667 104.801076 60.675766 105.564206 61.238347 106.126787 \nC 61.800927 106.689368 62.564058 107.005467 63.359667 107.005467 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 155.501658 \nC 66.804836 155.501658 67.567967 155.185559 68.130547 154.622978 \nC 68.693128 154.060397 69.009227 153.297267 69.009227 152.501658 \nC 69.009227 151.706048 68.693128 150.942918 68.130547 150.380337 \nC 67.567967 149.817757 66.804836 149.501658 66.009227 149.501658 \nC 65.213618 149.501658 64.450488 149.817757 63.887907 150.380337 \nC 63.325326 150.942918 63.009227 151.706048 63.009227 152.501658 \nC 63.009227 153.297267 63.325326 154.060397 63.887907 154.622978 \nC 64.450488 155.185559 65.213618 155.501658 66.009227 155.501658 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 143.711398 \nC 72.103957 143.711398 72.867087 143.395299 73.429668 142.832719 \nC 73.992249 142.270138 74.308348 141.507008 74.308348 140.711398 \nC 74.308348 139.915789 73.992249 139.152659 73.429668 138.590078 \nC 72.867087 138.027497 72.103957 137.711398 71.308348 137.711398 \nC 70.512738 137.711398 69.749608 138.027497 69.187027 138.590078 \nC 68.624447 139.152659 68.308348 139.915789 68.308348 140.711398 \nC 68.308348 141.507008 68.624447 142.270138 69.187027 142.832719 \nC 69.749608 143.395299 70.512738 143.711398 71.308348 143.711398 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 105.31549 \nC 61.491916 105.31549 62.255046 104.999391 62.817627 104.43681 \nC 63.380208 103.87423 63.696307 103.111099 63.696307 102.31549 \nC 63.696307 101.519881 63.380208 100.756751 62.817627 100.19417 \nC 62.255046 99.631589 61.491916 99.31549 60.696307 99.31549 \nC 59.900698 99.31549 59.137567 99.631589 58.574986 100.19417 \nC 58.012406 100.756751 57.696307 101.519881 57.696307 102.31549 \nC 57.696307 103.111099 58.012406 103.87423 58.574986 104.43681 \nC 59.137567 104.999391 59.900698 105.31549 60.696307 105.31549 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 183.224592 \nC 61.570115 183.224592 62.333245 182.908494 62.895826 182.345913 \nC 63.458407 181.783332 63.774506 181.020202 63.774506 180.224592 \nC 63.774506 179.428983 63.458407 178.665853 62.895826 178.103272 \nC 62.333245 177.540691 61.570115 177.224592 60.774506 177.224592 \nC 59.978896 177.224592 59.215766 177.540691 58.653185 178.103272 \nC 58.090605 178.665853 57.774506 179.428983 57.774506 180.224592 \nC 57.774506 181.020202 58.090605 181.783332 58.653185 182.345913 \nC 59.215766 182.908494 59.978896 183.224592 60.774506 183.224592 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 159.124299 \nC 61.836911 159.124299 62.600041 158.8082 63.162622 158.245619 \nC 63.725203 157.683039 64.041302 156.919908 64.041302 156.124299 \nC 64.041302 155.32869 63.725203 154.565559 63.162622 154.002979 \nC 62.600041 153.440398 61.836911 153.124299 61.041302 153.124299 \nC 60.245692 153.124299 59.482562 153.440398 58.919981 154.002979 \nC 58.357401 154.565559 58.041302 155.32869 58.041302 156.124299 \nC 58.041302 156.919908 58.357401 157.683039 58.919981 158.245619 \nC 59.482562 158.8082 60.245692 159.124299 61.041302 159.124299 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 158.896591 \nC 62.812096 158.896591 63.575227 158.580492 64.137807 158.017911 \nC 64.700388 157.455331 65.016487 156.6922 65.016487 155.896591 \nC 65.016487 155.100982 64.700388 154.337851 64.137807 153.775271 \nC 63.575227 153.21269 62.812096 152.896591 62.016487 152.896591 \nC 61.220878 152.896591 60.457747 153.21269 59.895167 153.775271 \nC 59.332586 154.337851 59.016487 155.100982 59.016487 155.896591 \nC 59.016487 156.6922 59.332586 157.455331 59.895167 158.017911 \nC 60.457747 158.580492 61.220878 158.896591 62.016487 158.896591 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 103.998102 \nC 66.528841 103.998102 67.291971 103.682003 67.854552 103.119422 \nC 68.417132 102.556842 68.733231 101.793711 68.733231 100.998102 \nC 68.733231 100.202493 68.417132 99.439362 67.854552 98.876782 \nC 67.291971 98.314201 66.528841 97.998102 65.733231 97.998102 \nC 64.937622 97.998102 64.174492 98.314201 63.611911 98.876782 \nC 63.04933 99.439362 62.733231 100.202493 62.733231 100.998102 \nC 62.733231 101.793711 63.04933 102.556842 63.611911 103.119422 \nC 64.174492 103.682003 64.937622 103.998102 65.733231 103.998102 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 112.187067 \nC 81.027823 112.187067 81.790954 111.870968 82.353534 111.308387 \nC 82.916115 110.745806 83.232214 109.982676 83.232214 109.187067 \nC 83.232214 108.391457 82.916115 107.628327 82.353534 107.065746 \nC 81.790954 106.503166 81.027823 106.187067 80.232214 106.187067 \nC 79.436605 106.187067 78.673474 106.503166 78.110894 107.065746 \nC 77.548313 107.628327 77.232214 108.391457 77.232214 109.187067 \nC 77.232214 109.982676 77.548313 110.745806 78.110894 111.308387 \nC 78.673474 111.870968 79.436605 112.187067 80.232214 112.187067 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 122.962911 \nC 138.287765 122.962911 139.050895 122.646812 139.613476 122.084231 \nC 140.176056 121.52165 140.492155 120.75852 140.492155 119.962911 \nC 140.492155 119.167301 140.176056 118.404171 139.613476 117.84159 \nC 139.050895 117.27901 138.287765 116.962911 137.492155 116.962911 \nC 136.696546 116.962911 135.933416 117.27901 135.370835 117.84159 \nC 134.808254 118.404171 134.492155 119.167301 134.492155 119.962911 \nC 134.492155 120.75852 134.808254 121.52165 135.370835 122.084231 \nC 135.933416 122.646812 136.696546 122.962911 137.492155 122.962911 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 102.474784 \nC 365.855552 102.474784 366.618683 102.158685 367.181264 101.596104 \nC 367.743844 101.033523 368.059943 100.270393 368.059943 99.474784 \nC 368.059943 98.679174 367.743844 97.916044 367.181264 97.353463 \nC 366.618683 96.790883 365.855552 96.474784 365.059943 96.474784 \nC 364.264334 96.474784 363.501204 96.790883 362.938623 97.353463 \nC 362.376042 97.916044 362.059943 98.679174 362.059943 99.474784 \nC 362.059943 100.270393 362.376042 101.033523 362.938623 101.596104 \nC 363.501204 102.158685 364.264334 102.474784 365.059943 102.474784 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 194.79001 \nC 61.671313 194.79001 62.434444 194.473911 62.997024 193.911331 \nC 63.559605 193.34875 63.875704 192.58562 63.875704 191.79001 \nC 63.875704 190.994401 63.559605 190.231271 62.997024 189.66869 \nC 62.434444 189.106109 61.671313 188.79001 60.875704 188.79001 \nC 60.080095 188.79001 59.316965 189.106109 58.754384 189.66869 \nC 58.191803 190.231271 57.875704 190.994401 57.875704 191.79001 \nC 57.875704 192.58562 58.191803 193.34875 58.754384 193.911331 \nC 59.316965 194.473911 60.080095 194.79001 60.875704 194.79001 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 105.553191 \nC 61.836911 105.553191 62.600041 105.237092 63.162622 104.674511 \nC 63.725203 104.11193 64.041302 103.3488 64.041302 102.553191 \nC 64.041302 101.757581 63.725203 100.994451 63.162622 100.43187 \nC 62.600041 99.86929 61.836911 99.553191 61.041302 99.553191 \nC 60.245692 99.553191 59.482562 99.86929 58.919981 100.43187 \nC 58.357401 100.994451 58.041302 101.757581 58.041302 102.553191 \nC 58.041302 103.3488 58.357401 104.11193 58.919981 104.674511 \nC 59.482562 105.237092 60.245692 105.553191 61.041302 105.553191 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 160.350869 \nC 62.168106 160.350869 62.931236 160.03477 63.493817 159.472189 \nC 64.056398 158.909608 64.372497 158.146478 64.372497 157.350869 \nC 64.372497 156.555259 64.056398 155.792129 63.493817 155.229548 \nC 62.931236 154.666968 62.168106 154.350869 61.372497 154.350869 \nC 60.576887 154.350869 59.813757 154.666968 59.251176 155.229548 \nC 58.688596 155.792129 58.372497 156.555259 58.372497 157.350869 \nC 58.372497 158.146478 58.688596 158.909608 59.251176 159.472189 \nC 59.813757 160.03477 60.576887 160.350869 61.372497 160.350869 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 202.502335 \nC 62.830496 202.502335 63.593626 202.186237 64.156207 201.623656 \nC 64.718788 201.061075 65.034887 200.297945 65.034887 199.502335 \nC 65.034887 198.706726 64.718788 197.943596 64.156207 197.381015 \nC 63.593626 196.818434 62.830496 196.502335 62.034887 196.502335 \nC 61.239277 196.502335 60.476147 196.818434 59.913566 197.381015 \nC 59.350986 197.943596 59.034887 198.706726 59.034887 199.502335 \nC 59.034887 200.297945 59.350986 201.061075 59.913566 201.623656 \nC 60.476147 202.186237 61.239277 202.502335 62.034887 202.502335 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 177.735375 \nC 64.155276 177.735375 64.918406 177.419276 65.480987 176.856696 \nC 66.043568 176.294115 66.359667 175.530985 66.359667 174.735375 \nC 66.359667 173.939766 66.043568 173.176636 65.480987 172.614055 \nC 64.918406 172.051474 64.155276 171.735375 63.359667 171.735375 \nC 62.564058 171.735375 61.800927 172.051474 61.238347 172.614055 \nC 60.675766 173.176636 60.359667 173.939766 60.359667 174.735375 \nC 60.359667 175.530985 60.675766 176.294115 61.238347 176.856696 \nC 61.800927 177.419276 62.564058 177.735375 63.359667 177.735375 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 121.389702 \nC 66.804836 121.389702 67.567967 121.073603 68.130547 120.511022 \nC 68.693128 119.948442 69.009227 119.185311 69.009227 118.389702 \nC 69.009227 117.594093 68.693128 116.830962 68.130547 116.268382 \nC 67.567967 115.705801 66.804836 115.389702 66.009227 115.389702 \nC 65.213618 115.389702 64.450488 115.705801 63.887907 116.268382 \nC 63.325326 116.830962 63.009227 117.594093 63.009227 118.389702 \nC 63.009227 119.185311 63.325326 119.948442 63.887907 120.511022 \nC 64.450488 121.073603 65.213618 121.389702 66.009227 121.389702 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 153.643908 \nC 72.103957 153.643908 72.867087 153.327809 73.429668 152.765228 \nC 73.992249 152.202647 74.308348 151.439517 74.308348 150.643908 \nC 74.308348 149.848298 73.992249 149.085168 73.429668 148.522587 \nC 72.867087 147.960007 72.103957 147.643908 71.308348 147.643908 \nC 70.512738 147.643908 69.749608 147.960007 69.187027 148.522587 \nC 68.624447 149.085168 68.308348 149.848298 68.308348 150.643908 \nC 68.308348 151.439517 68.624447 152.202647 69.187027 152.765228 \nC 69.749608 153.327809 70.512738 153.643908 71.308348 153.643908 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 149.050586 \nC 61.491916 149.050586 62.255046 148.734487 62.817627 148.171906 \nC 63.380208 147.609326 63.696307 146.846195 63.696307 146.050586 \nC 63.696307 145.254977 63.380208 144.491846 62.817627 143.929266 \nC 62.255046 143.366685 61.491916 143.050586 60.696307 143.050586 \nC 59.900698 143.050586 59.137567 143.366685 58.574986 143.929266 \nC 58.012406 144.491846 57.696307 145.254977 57.696307 146.050586 \nC 57.696307 146.846195 58.012406 147.609326 58.574986 148.171906 \nC 59.137567 148.734487 59.900698 149.050586 60.696307 149.050586 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 158.328323 \nC 61.570115 158.328323 62.333245 158.012225 62.895826 157.449644 \nC 63.458407 156.887063 63.774506 156.123933 63.774506 155.328323 \nC 63.774506 154.532714 63.458407 153.769584 62.895826 153.207003 \nC 62.333245 152.644422 61.570115 152.328323 60.774506 152.328323 \nC 59.978896 152.328323 59.215766 152.644422 58.653185 153.207003 \nC 58.090605 153.769584 57.774506 154.532714 57.774506 155.328323 \nC 57.774506 156.123933 58.090605 156.887063 58.653185 157.449644 \nC 59.215766 158.012225 59.978896 158.328323 60.774506 158.328323 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 105.58078 \nC 61.836911 105.58078 62.600041 105.264681 63.162622 104.7021 \nC 63.725203 104.139519 64.041302 103.376389 64.041302 102.58078 \nC 64.041302 101.78517 63.725203 101.02204 63.162622 100.459459 \nC 62.600041 99.896879 61.836911 99.58078 61.041302 99.58078 \nC 60.245692 99.58078 59.482562 99.896879 58.919981 100.459459 \nC 58.357401 101.02204 58.041302 101.78517 58.041302 102.58078 \nC 58.041302 103.376389 58.357401 104.139519 58.919981 104.7021 \nC 59.482562 105.264681 60.245692 105.58078 61.041302 105.58078 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 101.766169 \nC 62.812096 101.766169 63.575227 101.45007 64.137807 100.887489 \nC 64.700388 100.324909 65.016487 99.561778 65.016487 98.766169 \nC 65.016487 97.97056 64.700388 97.207429 64.137807 96.644849 \nC 63.575227 96.082268 62.812096 95.766169 62.016487 95.766169 \nC 61.220878 95.766169 60.457747 96.082268 59.895167 96.644849 \nC 59.332586 97.207429 59.016487 97.97056 59.016487 98.766169 \nC 59.016487 99.561778 59.332586 100.324909 59.895167 100.887489 \nC 60.457747 101.45007 61.220878 101.766169 62.016487 101.766169 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 157.519788 \nC 66.528841 157.519788 67.291971 157.203689 67.854552 156.641108 \nC 68.417132 156.078528 68.733231 155.315397 68.733231 154.519788 \nC 68.733231 153.724179 68.417132 152.961048 67.854552 152.398468 \nC 67.291971 151.835887 66.528841 151.519788 65.733231 151.519788 \nC 64.937622 151.519788 64.174492 151.835887 63.611911 152.398468 \nC 63.04933 152.961048 62.733231 153.724179 62.733231 154.519788 \nC 62.733231 155.315397 63.04933 156.078528 63.611911 156.641108 \nC 64.174492 157.203689 64.937622 157.519788 65.733231 157.519788 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 158.710191 \nC 81.027823 158.710191 81.790954 158.394093 82.353534 157.831512 \nC 82.916115 157.268931 83.232214 156.505801 83.232214 155.710191 \nC 83.232214 154.914582 82.916115 154.151452 82.353534 153.588871 \nC 81.790954 153.02629 81.027823 152.710191 80.232214 152.710191 \nC 79.436605 152.710191 78.673474 153.02629 78.110894 153.588871 \nC 77.548313 154.151452 77.232214 154.914582 77.232214 155.710191 \nC 77.232214 156.505801 77.548313 157.268931 78.110894 157.831512 \nC 78.673474 158.394093 79.436605 158.710191 80.232214 158.710191 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 104.900325 \nC 138.287765 104.900325 139.050895 104.584226 139.613476 104.021646 \nC 140.176056 103.459065 140.492155 102.695935 140.492155 101.900325 \nC 140.492155 101.104716 140.176056 100.341586 139.613476 99.779005 \nC 139.050895 99.216424 138.287765 98.900325 137.492155 98.900325 \nC 136.696546 98.900325 135.933416 99.216424 135.370835 99.779005 \nC 134.808254 100.341586 134.492155 101.104716 134.492155 101.900325 \nC 134.492155 102.695935 134.808254 103.459065 135.370835 104.021646 \nC 135.933416 104.584226 136.696546 104.900325 137.492155 104.900325 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 164.091798 \nC 365.855552 164.091798 366.618683 163.775699 367.181264 163.213119 \nC 367.743844 162.650538 368.059943 161.887408 368.059943 161.091798 \nC 368.059943 160.296189 367.743844 159.533059 367.181264 158.970478 \nC 366.618683 158.407897 365.855552 158.091798 365.059943 158.091798 \nC 364.264334 158.091798 363.501204 158.407897 362.938623 158.970478 \nC 362.376042 159.533059 362.059943 160.296189 362.059943 161.091798 \nC 362.059943 161.887408 362.376042 162.650538 362.938623 163.213119 \nC 363.501204 163.775699 364.264334 164.091798 365.059943 164.091798 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 154.49053 \nC 61.671313 154.49053 62.434444 154.174431 62.997024 153.61185 \nC 63.559605 153.049269 63.875704 152.286139 63.875704 151.49053 \nC 63.875704 150.69492 63.559605 149.93179 62.997024 149.369209 \nC 62.434444 148.806629 61.671313 148.49053 60.875704 148.49053 \nC 60.080095 148.49053 59.316965 148.806629 58.754384 149.369209 \nC 58.191803 149.93179 57.875704 150.69492 57.875704 151.49053 \nC 57.875704 152.286139 58.191803 153.049269 58.754384 153.61185 \nC 59.316965 154.174431 60.080095 154.49053 60.875704 154.49053 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 156.153815 \nC 61.836911 156.153815 62.600041 155.837716 63.162622 155.275135 \nC 63.725203 154.712555 64.041302 153.949424 64.041302 153.153815 \nC 64.041302 152.358206 63.725203 151.595075 63.162622 151.032495 \nC 62.600041 150.469914 61.836911 150.153815 61.041302 150.153815 \nC 60.245692 150.153815 59.482562 150.469914 58.919981 151.032495 \nC 58.357401 151.595075 58.041302 152.358206 58.041302 153.153815 \nC 58.041302 153.949424 58.357401 154.712555 58.919981 155.275135 \nC 59.482562 155.837716 60.245692 156.153815 61.041302 156.153815 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 152.896835 \nC 62.168106 152.896835 62.931236 152.580736 63.493817 152.018156 \nC 64.056398 151.455575 64.372497 150.692444 64.372497 149.896835 \nC 64.372497 149.101226 64.056398 148.338096 63.493817 147.775515 \nC 62.931236 147.212934 62.168106 146.896835 61.372497 146.896835 \nC 60.576887 146.896835 59.813757 147.212934 59.251176 147.775515 \nC 58.688596 148.338096 58.372497 149.101226 58.372497 149.896835 \nC 58.372497 150.692444 58.688596 151.455575 59.251176 152.018156 \nC 59.813757 152.580736 60.576887 152.896835 61.372497 152.896835 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 149.660493 \nC 62.830496 149.660493 63.593626 149.344394 64.156207 148.781814 \nC 64.718788 148.219233 65.034887 147.456103 65.034887 146.660493 \nC 65.034887 145.864884 64.718788 145.101754 64.156207 144.539173 \nC 63.593626 143.976592 62.830496 143.660493 62.034887 143.660493 \nC 61.239277 143.660493 60.476147 143.976592 59.913566 144.539173 \nC 59.350986 145.101754 59.034887 145.864884 59.034887 146.660493 \nC 59.034887 147.456103 59.350986 148.219233 59.913566 148.781814 \nC 60.476147 149.344394 61.239277 149.660493 62.034887 149.660493 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 105.899951 \nC 64.155276 105.899951 64.918406 105.583852 65.480987 105.021272 \nC 66.043568 104.458691 66.359667 103.695561 66.359667 102.899951 \nC 66.359667 102.104342 66.043568 101.341212 65.480987 100.778631 \nC 64.918406 100.21605 64.155276 99.899951 63.359667 99.899951 \nC 62.564058 99.899951 61.800927 100.21605 61.238347 100.778631 \nC 60.675766 101.341212 60.359667 102.104342 60.359667 102.899951 \nC 60.359667 103.695561 60.675766 104.458691 61.238347 105.021272 \nC 61.800927 105.583852 62.564058 105.899951 63.359667 105.899951 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 150.699286 \nC 66.804836 150.699286 67.567967 150.383187 68.130547 149.820606 \nC 68.693128 149.258026 69.009227 148.494895 69.009227 147.699286 \nC 69.009227 146.903677 68.693128 146.140547 68.130547 145.577966 \nC 67.567967 145.015385 66.804836 144.699286 66.009227 144.699286 \nC 65.213618 144.699286 64.450488 145.015385 63.887907 145.577966 \nC 63.325326 146.140547 63.009227 146.903677 63.009227 147.699286 \nC 63.009227 148.494895 63.325326 149.258026 63.887907 149.820606 \nC 64.450488 150.383187 65.213618 150.699286 66.009227 150.699286 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 153.909813 \nC 72.103957 153.909813 72.867087 153.593714 73.429668 153.031133 \nC 73.992249 152.468553 74.308348 151.705422 74.308348 150.909813 \nC 74.308348 150.114204 73.992249 149.351073 73.429668 148.788493 \nC 72.867087 148.225912 72.103957 147.909813 71.308348 147.909813 \nC 70.512738 147.909813 69.749608 148.225912 69.187027 148.788493 \nC 68.624447 149.351073 68.308348 150.114204 68.308348 150.909813 \nC 68.308348 151.705422 68.624447 152.468553 69.187027 153.031133 \nC 69.749608 153.593714 70.512738 153.909813 71.308348 153.909813 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 155.0807 \nC 61.491916 155.0807 62.255046 154.764601 62.817627 154.20202 \nC 63.380208 153.63944 63.696307 152.876309 63.696307 152.0807 \nC 63.696307 151.285091 63.380208 150.521961 62.817627 149.95938 \nC 62.255046 149.396799 61.491916 149.0807 60.696307 149.0807 \nC 59.900698 149.0807 59.137567 149.396799 58.574986 149.95938 \nC 58.012406 150.521961 57.696307 151.285091 57.696307 152.0807 \nC 57.696307 152.876309 58.012406 153.63944 58.574986 154.20202 \nC 59.137567 154.764601 59.900698 155.0807 60.696307 155.0807 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 172.108423 \nC 61.836911 172.108423 62.600041 171.792324 63.162622 171.229744 \nC 63.725203 170.667163 64.041302 169.904033 64.041302 169.108423 \nC 64.041302 168.312814 63.725203 167.549684 63.162622 166.987103 \nC 62.600041 166.424522 61.836911 166.108423 61.041302 166.108423 \nC 60.245692 166.108423 59.482562 166.424522 58.919981 166.987103 \nC 58.357401 167.549684 58.041302 168.312814 58.041302 169.108423 \nC 58.041302 169.904033 58.357401 170.667163 58.919981 171.229744 \nC 59.482562 171.792324 60.245692 172.108423 61.041302 172.108423 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 127.870796 \nC 66.528841 127.870796 67.291971 127.554697 67.854552 126.992116 \nC 68.417132 126.429535 68.733231 125.666405 68.733231 124.870796 \nC 68.733231 124.075186 68.417132 123.312056 67.854552 122.749475 \nC 67.291971 122.186895 66.528841 121.870796 65.733231 121.870796 \nC 64.937622 121.870796 64.174492 122.186895 63.611911 122.749475 \nC 63.04933 123.312056 62.733231 124.075186 62.733231 124.870796 \nC 62.733231 125.666405 63.04933 126.429535 63.611911 126.992116 \nC 64.174492 127.554697 64.937622 127.870796 65.733231 127.870796 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 174.547204 \nC 138.287765 174.547204 139.050895 174.231105 139.613476 173.668524 \nC 140.176056 173.105943 140.492155 172.342813 140.492155 171.547204 \nC 140.492155 170.751594 140.176056 169.988464 139.613476 169.425883 \nC 139.050895 168.863302 138.287765 168.547204 137.492155 168.547204 \nC 136.696546 168.547204 135.933416 168.863302 135.370835 169.425883 \nC 134.808254 169.988464 134.492155 170.751594 134.492155 171.547204 \nC 134.492155 172.342813 134.808254 173.105943 135.370835 173.668524 \nC 135.933416 174.231105 136.696546 174.547204 137.492155 174.547204 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 160.712748 \nC 365.855552 160.712748 366.618683 160.396649 367.181264 159.834068 \nC 367.743844 159.271487 368.059943 158.508357 368.059943 157.712748 \nC 368.059943 156.917138 367.743844 156.154008 367.181264 155.591427 \nC 366.618683 155.028847 365.855552 154.712748 365.059943 154.712748 \nC 364.264334 154.712748 363.501204 155.028847 362.938623 155.591427 \nC 362.376042 156.154008 362.059943 156.917138 362.059943 157.712748 \nC 362.059943 158.508357 362.376042 159.271487 362.938623 159.834068 \nC 363.501204 160.396649 364.264334 160.712748 365.059943 160.712748 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 187.58138 \nC 61.671313 187.58138 62.434444 187.265281 62.997024 186.7027 \nC 63.559605 186.14012 63.875704 185.376989 63.875704 184.58138 \nC 63.875704 183.785771 63.559605 183.02264 62.997024 182.46006 \nC 62.434444 181.897479 61.671313 181.58138 60.875704 181.58138 \nC 60.080095 181.58138 59.316965 181.897479 58.754384 182.46006 \nC 58.191803 183.02264 57.875704 183.785771 57.875704 184.58138 \nC 57.875704 185.376989 58.191803 186.14012 58.754384 186.7027 \nC 59.316965 187.265281 60.080095 187.58138 60.875704 187.58138 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 172.280967 \nC 61.836911 172.280967 62.600041 171.964868 63.162622 171.402287 \nC 63.725203 170.839706 64.041302 170.076576 64.041302 169.280967 \nC 64.041302 168.485357 63.725203 167.722227 63.162622 167.159646 \nC 62.600041 166.597066 61.836911 166.280967 61.041302 166.280967 \nC 60.245692 166.280967 59.482562 166.597066 58.919981 167.159646 \nC 58.357401 167.722227 58.041302 168.485357 58.041302 169.280967 \nC 58.041302 170.076576 58.357401 170.839706 58.919981 171.402287 \nC 59.482562 171.964868 60.245692 172.280967 61.041302 172.280967 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 203.651027 \nC 62.168106 203.651027 62.931236 203.334928 63.493817 202.772347 \nC 64.056398 202.209766 64.372497 201.446636 64.372497 200.651027 \nC 64.372497 199.855417 64.056398 199.092287 63.493817 198.529706 \nC 62.931236 197.967126 62.168106 197.651027 61.372497 197.651027 \nC 60.576887 197.651027 59.813757 197.967126 59.251176 198.529706 \nC 58.688596 199.092287 58.372497 199.855417 58.372497 200.651027 \nC 58.372497 201.446636 58.688596 202.209766 59.251176 202.772347 \nC 59.813757 203.334928 60.576887 203.651027 61.372497 203.651027 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 154.762527 \nC 62.830496 154.762527 63.593626 154.446428 64.156207 153.883847 \nC 64.718788 153.321266 65.034887 152.558136 65.034887 151.762527 \nC 65.034887 150.966917 64.718788 150.203787 64.156207 149.641206 \nC 63.593626 149.078625 62.830496 148.762527 62.034887 148.762527 \nC 61.239277 148.762527 60.476147 149.078625 59.913566 149.641206 \nC 59.350986 150.203787 59.034887 150.966917 59.034887 151.762527 \nC 59.034887 152.558136 59.350986 153.321266 59.913566 153.883847 \nC 60.476147 154.446428 61.239277 154.762527 62.034887 154.762527 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 102.767156 \nC 64.155276 102.767156 64.918406 102.451057 65.480987 101.888476 \nC 66.043568 101.325896 66.359667 100.562765 66.359667 99.767156 \nC 66.359667 98.971547 66.043568 98.208416 65.480987 97.645836 \nC 64.918406 97.083255 64.155276 96.767156 63.359667 96.767156 \nC 62.564058 96.767156 61.800927 97.083255 61.238347 97.645836 \nC 60.675766 98.208416 60.359667 98.971547 60.359667 99.767156 \nC 60.359667 100.562765 60.675766 101.325896 61.238347 101.888476 \nC 61.800927 102.451057 62.564058 102.767156 63.359667 102.767156 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 117.329278 \nC 66.804836 117.329278 67.567967 117.013179 68.130547 116.450598 \nC 68.693128 115.888017 69.009227 115.124887 69.009227 114.329278 \nC 69.009227 113.533668 68.693128 112.770538 68.130547 112.207957 \nC 67.567967 111.645377 66.804836 111.329278 66.009227 111.329278 \nC 65.213618 111.329278 64.450488 111.645377 63.887907 112.207957 \nC 63.325326 112.770538 63.009227 113.533668 63.009227 114.329278 \nC 63.009227 115.124887 63.325326 115.888017 63.887907 116.450598 \nC 64.450488 117.013179 65.213618 117.329278 66.009227 117.329278 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 38.611618 \nC 72.103957 38.611618 72.867087 38.295519 73.429668 37.732939 \nC 73.992249 37.170358 74.308348 36.407228 74.308348 35.611618 \nC 74.308348 34.816009 73.992249 34.052879 73.429668 33.490298 \nC 72.867087 32.927717 72.103957 32.611618 71.308348 32.611618 \nC 70.512738 32.611618 69.749608 32.927717 69.187027 33.490298 \nC 68.624447 34.052879 68.308348 34.816009 68.308348 35.611618 \nC 68.308348 36.407228 68.624447 37.170358 69.187027 37.732939 \nC 69.749608 38.295519 70.512738 38.611618 71.308348 38.611618 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 155.719689 \nC 61.491916 155.719689 62.255046 155.40359 62.817627 154.84101 \nC 63.380208 154.278429 63.696307 153.515299 63.696307 152.719689 \nC 63.696307 151.92408 63.380208 151.16095 62.817627 150.598369 \nC 62.255046 150.035788 61.491916 149.719689 60.696307 149.719689 \nC 59.900698 149.719689 59.137567 150.035788 58.574986 150.598369 \nC 58.012406 151.16095 57.696307 151.92408 57.696307 152.719689 \nC 57.696307 153.515299 58.012406 154.278429 58.574986 154.84101 \nC 59.137567 155.40359 59.900698 155.719689 60.696307 155.719689 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 155.912837 \nC 61.570115 155.912837 62.333245 155.596738 62.895826 155.034157 \nC 63.458407 154.471576 63.774506 153.708446 63.774506 152.912837 \nC 63.774506 152.117227 63.458407 151.354097 62.895826 150.791516 \nC 62.333245 150.228936 61.570115 149.912837 60.774506 149.912837 \nC 59.978896 149.912837 59.215766 150.228936 58.653185 150.791516 \nC 58.090605 151.354097 57.774506 152.117227 57.774506 152.912837 \nC 57.774506 153.708446 58.090605 154.471576 58.653185 155.034157 \nC 59.215766 155.596738 59.978896 155.912837 60.774506 155.912837 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 105.337518 \nC 61.836911 105.337518 62.600041 105.021419 63.162622 104.458838 \nC 63.725203 103.896257 64.041302 103.133127 64.041302 102.337518 \nC 64.041302 101.541908 63.725203 100.778778 63.162622 100.216197 \nC 62.600041 99.653617 61.836911 99.337518 61.041302 99.337518 \nC 60.245692 99.337518 59.482562 99.653617 58.919981 100.216197 \nC 58.357401 100.778778 58.041302 101.541908 58.041302 102.337518 \nC 58.041302 103.133127 58.357401 103.896257 58.919981 104.458838 \nC 59.482562 105.021419 60.245692 105.337518 61.041302 105.337518 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 105.977615 \nC 62.812096 105.977615 63.575227 105.661516 64.137807 105.098936 \nC 64.700388 104.536355 65.016487 103.773224 65.016487 102.977615 \nC 65.016487 102.182006 64.700388 101.418876 64.137807 100.856295 \nC 63.575227 100.293714 62.812096 99.977615 62.016487 99.977615 \nC 61.220878 99.977615 60.457747 100.293714 59.895167 100.856295 \nC 59.332586 101.418876 59.016487 102.182006 59.016487 102.977615 \nC 59.016487 103.773224 59.332586 104.536355 59.895167 105.098936 \nC 60.457747 105.661516 61.220878 105.977615 62.016487 105.977615 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 105.287737 \nC 66.528841 105.287737 67.291971 104.971638 67.854552 104.409057 \nC 68.417132 103.846476 68.733231 103.083346 68.733231 102.287737 \nC 68.733231 101.492127 68.417132 100.728997 67.854552 100.166416 \nC 67.291971 99.603836 66.528841 99.287737 65.733231 99.287737 \nC 64.937622 99.287737 64.174492 99.603836 63.611911 100.166416 \nC 63.04933 100.728997 62.733231 101.492127 62.733231 102.287737 \nC 62.733231 103.083346 63.04933 103.846476 63.611911 104.409057 \nC 64.174492 104.971638 64.937622 105.287737 65.733231 105.287737 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 102.95916 \nC 81.027823 102.95916 81.790954 102.643061 82.353534 102.08048 \nC 82.916115 101.517899 83.232214 100.754769 83.232214 99.95916 \nC 83.232214 99.16355 82.916115 98.40042 82.353534 97.837839 \nC 81.790954 97.275259 81.027823 96.95916 80.232214 96.95916 \nC 79.436605 96.95916 78.673474 97.275259 78.110894 97.837839 \nC 77.548313 98.40042 77.232214 99.16355 77.232214 99.95916 \nC 77.232214 100.754769 77.548313 101.517899 78.110894 102.08048 \nC 78.673474 102.643061 79.436605 102.95916 80.232214 102.95916 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 103.499577 \nC 138.287765 103.499577 139.050895 103.183478 139.613476 102.620898 \nC 140.176056 102.058317 140.492155 101.295187 140.492155 100.499577 \nC 140.492155 99.703968 140.176056 98.940838 139.613476 98.378257 \nC 139.050895 97.815676 138.287765 97.499577 137.492155 97.499577 \nC 136.696546 97.499577 135.933416 97.815676 135.370835 98.378257 \nC 134.808254 98.940838 134.492155 99.703968 134.492155 100.499577 \nC 134.492155 101.295187 134.808254 102.058317 135.370835 102.620898 \nC 135.933416 103.183478 136.696546 103.499577 137.492155 103.499577 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 105.363896 \nC 365.855552 105.363896 366.618683 105.047797 367.181264 104.485216 \nC 367.743844 103.922636 368.059943 103.159505 368.059943 102.363896 \nC 368.059943 101.568287 367.743844 100.805156 367.181264 100.242576 \nC 366.618683 99.679995 365.855552 99.363896 365.059943 99.363896 \nC 364.264334 99.363896 363.501204 99.679995 362.938623 100.242576 \nC 362.376042 100.805156 362.059943 101.568287 362.059943 102.363896 \nC 362.059943 103.159505 362.376042 103.922636 362.938623 104.485216 \nC 363.501204 105.047797 364.264334 105.363896 365.059943 105.363896 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 105.950269 \nC 61.671313 105.950269 62.434444 105.63417 62.997024 105.071589 \nC 63.559605 104.509008 63.875704 103.745878 63.875704 102.950269 \nC 63.875704 102.154659 63.559605 101.391529 62.997024 100.828948 \nC 62.434444 100.266368 61.671313 99.950269 60.875704 99.950269 \nC 60.080095 99.950269 59.316965 100.266368 58.754384 100.828948 \nC 58.191803 101.391529 57.875704 102.154659 57.875704 102.950269 \nC 57.875704 103.745878 58.191803 104.509008 58.754384 105.071589 \nC 59.316965 105.63417 60.080095 105.950269 60.875704 105.950269 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 105.343634 \nC 61.836911 105.343634 62.600041 105.027535 63.162622 104.464955 \nC 63.725203 103.902374 64.041302 103.139244 64.041302 102.343634 \nC 64.041302 101.548025 63.725203 100.784895 63.162622 100.222314 \nC 62.600041 99.659733 61.836911 99.343634 61.041302 99.343634 \nC 60.245692 99.343634 59.482562 99.659733 58.919981 100.222314 \nC 58.357401 100.784895 58.041302 101.548025 58.041302 102.343634 \nC 58.041302 103.139244 58.357401 103.902374 58.919981 104.464955 \nC 59.482562 105.027535 60.245692 105.343634 61.041302 105.343634 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 105.52935 \nC 62.168106 105.52935 62.931236 105.213251 63.493817 104.650671 \nC 64.056398 104.08809 64.372497 103.32496 64.372497 102.52935 \nC 64.372497 101.733741 64.056398 100.970611 63.493817 100.40803 \nC 62.931236 99.845449 62.168106 99.52935 61.372497 99.52935 \nC 60.576887 99.52935 59.813757 99.845449 59.251176 100.40803 \nC 58.688596 100.970611 58.372497 101.733741 58.372497 102.52935 \nC 58.372497 103.32496 58.688596 104.08809 59.251176 104.650671 \nC 59.813757 105.213251 60.576887 105.52935 61.372497 105.52935 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 105.453862 \nC 62.830496 105.453862 63.593626 105.137763 64.156207 104.575182 \nC 64.718788 104.012601 65.034887 103.249471 65.034887 102.453862 \nC 65.034887 101.658252 64.718788 100.895122 64.156207 100.332541 \nC 63.593626 99.76996 62.830496 99.453862 62.034887 99.453862 \nC 61.239277 99.453862 60.476147 99.76996 59.913566 100.332541 \nC 59.350986 100.895122 59.034887 101.658252 59.034887 102.453862 \nC 59.034887 103.249471 59.350986 104.012601 59.913566 104.575182 \nC 60.476147 105.137763 61.239277 105.453862 62.034887 105.453862 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 151.034371 \nC 64.155276 151.034371 64.918406 150.718272 65.480987 150.155691 \nC 66.043568 149.59311 66.359667 148.82998 66.359667 148.034371 \nC 66.359667 147.238762 66.043568 146.475631 65.480987 145.91305 \nC 64.918406 145.35047 64.155276 145.034371 63.359667 145.034371 \nC 62.564058 145.034371 61.800927 145.35047 61.238347 145.91305 \nC 60.675766 146.475631 60.359667 147.238762 60.359667 148.034371 \nC 60.359667 148.82998 60.675766 149.59311 61.238347 150.155691 \nC 61.800927 150.718272 62.564058 151.034371 63.359667 151.034371 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 102.999322 \nC 66.804836 102.999322 67.567967 102.683223 68.130547 102.120642 \nC 68.693128 101.558062 69.009227 100.794931 69.009227 99.999322 \nC 69.009227 99.203713 68.693128 98.440582 68.130547 97.878002 \nC 67.567967 97.315421 66.804836 96.999322 66.009227 96.999322 \nC 65.213618 96.999322 64.450488 97.315421 63.887907 97.878002 \nC 63.325326 98.440582 63.009227 99.203713 63.009227 99.999322 \nC 63.009227 100.794931 63.325326 101.558062 63.887907 102.120642 \nC 64.450488 102.683223 65.213618 102.999322 66.009227 102.999322 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 132.797096 \nC 72.103957 132.797096 72.867087 132.480997 73.429668 131.918416 \nC 73.992249 131.355836 74.308348 130.592705 74.308348 129.797096 \nC 74.308348 129.001487 73.992249 128.238356 73.429668 127.675776 \nC 72.867087 127.113195 72.103957 126.797096 71.308348 126.797096 \nC 70.512738 126.797096 69.749608 127.113195 69.187027 127.675776 \nC 68.624447 128.238356 68.308348 129.001487 68.308348 129.797096 \nC 68.308348 130.592705 68.624447 131.355836 69.187027 131.918416 \nC 69.749608 132.480997 70.512738 132.797096 71.308348 132.797096 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 139.766594 \nC 61.491916 139.766594 62.255046 139.450495 62.817627 138.887915 \nC 63.380208 138.325334 63.696307 137.562204 63.696307 136.766594 \nC 63.696307 135.970985 63.380208 135.207855 62.817627 134.645274 \nC 62.255046 134.082693 61.491916 133.766594 60.696307 133.766594 \nC 59.900698 133.766594 59.137567 134.082693 58.574986 134.645274 \nC 58.012406 135.207855 57.696307 135.970985 57.696307 136.766594 \nC 57.696307 137.562204 58.012406 138.325334 58.574986 138.887915 \nC 59.137567 139.450495 59.900698 139.766594 60.696307 139.766594 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 192.264747 \nC 62.812096 192.264747 63.575227 191.948648 64.137807 191.386067 \nC 64.700388 190.823487 65.016487 190.060356 65.016487 189.264747 \nC 65.016487 188.469138 64.700388 187.706007 64.137807 187.143427 \nC 63.575227 186.580846 62.812096 186.264747 62.016487 186.264747 \nC 61.220878 186.264747 60.457747 186.580846 59.895167 187.143427 \nC 59.332586 187.706007 59.016487 188.469138 59.016487 189.264747 \nC 59.016487 190.060356 59.332586 190.823487 59.895167 191.386067 \nC 60.457747 191.948648 61.220878 192.264747 62.016487 192.264747 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 102.684038 \nC 66.528841 102.684038 67.291971 102.367939 67.854552 101.805358 \nC 68.417132 101.242777 68.733231 100.479647 68.733231 99.684038 \nC 68.733231 98.888428 68.417132 98.125298 67.854552 97.562717 \nC 67.291971 97.000137 66.528841 96.684038 65.733231 96.684038 \nC 64.937622 96.684038 64.174492 97.000137 63.611911 97.562717 \nC 63.04933 98.125298 62.733231 98.888428 62.733231 99.684038 \nC 62.733231 100.479647 63.04933 101.242777 63.611911 101.805358 \nC 64.174492 102.367939 64.937622 102.684038 65.733231 102.684038 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 190.621228 \nC 81.027823 190.621228 81.790954 190.305129 82.353534 189.742548 \nC 82.916115 189.179967 83.232214 188.416837 83.232214 187.621228 \nC 83.232214 186.825618 82.916115 186.062488 82.353534 185.499907 \nC 81.790954 184.937326 81.027823 184.621228 80.232214 184.621228 \nC 79.436605 184.621228 78.673474 184.937326 78.110894 185.499907 \nC 77.548313 186.062488 77.232214 186.825618 77.232214 187.621228 \nC 77.232214 188.416837 77.548313 189.179967 78.110894 189.742548 \nC 78.673474 190.305129 79.436605 190.621228 80.232214 190.621228 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 161.332849 \nC 138.287765 161.332849 139.050895 161.016751 139.613476 160.45417 \nC 140.176056 159.891589 140.492155 159.128459 140.492155 158.332849 \nC 140.492155 157.53724 140.176056 156.77411 139.613476 156.211529 \nC 139.050895 155.648948 138.287765 155.332849 137.492155 155.332849 \nC 136.696546 155.332849 135.933416 155.648948 135.370835 156.211529 \nC 134.808254 156.77411 134.492155 157.53724 134.492155 158.332849 \nC 134.492155 159.128459 134.808254 159.891589 135.370835 160.45417 \nC 135.933416 161.016751 136.696546 161.332849 137.492155 161.332849 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 109.228123 \nC 365.855552 109.228123 366.618683 108.912025 367.181264 108.349444 \nC 367.743844 107.786863 368.059943 107.023733 368.059943 106.228123 \nC 368.059943 105.432514 367.743844 104.669384 367.181264 104.106803 \nC 366.618683 103.544222 365.855552 103.228123 365.059943 103.228123 \nC 364.264334 103.228123 363.501204 103.544222 362.938623 104.106803 \nC 362.376042 104.669384 362.059943 105.432514 362.059943 106.228123 \nC 362.059943 107.023733 362.376042 107.786863 362.938623 108.349444 \nC 363.501204 108.912025 364.264334 109.228123 365.059943 109.228123 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 125.317996 \nC 61.671313 125.317996 62.434444 125.001897 62.997024 124.439317 \nC 63.559605 123.876736 63.875704 123.113606 63.875704 122.317996 \nC 63.875704 121.522387 63.559605 120.759257 62.997024 120.196676 \nC 62.434444 119.634095 61.671313 119.317996 60.875704 119.317996 \nC 60.080095 119.317996 59.316965 119.634095 58.754384 120.196676 \nC 58.191803 120.759257 57.875704 121.522387 57.875704 122.317996 \nC 57.875704 123.113606 58.191803 123.876736 58.754384 124.439317 \nC 59.316965 125.001897 60.080095 125.317996 60.875704 125.317996 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 197.200377 \nC 61.836911 197.200377 62.600041 196.884278 63.162622 196.321697 \nC 63.725203 195.759116 64.041302 194.995986 64.041302 194.200377 \nC 64.041302 193.404767 63.725203 192.641637 63.162622 192.079056 \nC 62.600041 191.516476 61.836911 191.200377 61.041302 191.200377 \nC 60.245692 191.200377 59.482562 191.516476 58.919981 192.079056 \nC 58.357401 192.641637 58.041302 193.404767 58.041302 194.200377 \nC 58.041302 194.995986 58.357401 195.759116 58.919981 196.321697 \nC 59.482562 196.884278 60.245692 197.200377 61.041302 197.200377 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 204.412409 \nC 62.168106 204.412409 62.931236 204.09631 63.493817 203.533729 \nC 64.056398 202.971149 64.372497 202.208018 64.372497 201.412409 \nC 64.372497 200.6168 64.056398 199.853669 63.493817 199.291089 \nC 62.931236 198.728508 62.168106 198.412409 61.372497 198.412409 \nC 60.576887 198.412409 59.813757 198.728508 59.251176 199.291089 \nC 58.688596 199.853669 58.372497 200.6168 58.372497 201.412409 \nC 58.372497 202.208018 58.688596 202.971149 59.251176 203.533729 \nC 59.813757 204.09631 60.576887 204.412409 61.372497 204.412409 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 121.164725 \nC 64.155276 121.164725 64.918406 120.848626 65.480987 120.286046 \nC 66.043568 119.723465 66.359667 118.960335 66.359667 118.164725 \nC 66.359667 117.369116 66.043568 116.605986 65.480987 116.043405 \nC 64.918406 115.480824 64.155276 115.164725 63.359667 115.164725 \nC 62.564058 115.164725 61.800927 115.480824 61.238347 116.043405 \nC 60.675766 116.605986 60.359667 117.369116 60.359667 118.164725 \nC 60.359667 118.960335 60.675766 119.723465 61.238347 120.286046 \nC 61.800927 120.848626 62.564058 121.164725 63.359667 121.164725 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 115.874089 \nC 66.804836 115.874089 67.567967 115.55799 68.130547 114.995409 \nC 68.693128 114.432828 69.009227 113.669698 69.009227 112.874089 \nC 69.009227 112.078479 68.693128 111.315349 68.130547 110.752768 \nC 67.567967 110.190188 66.804836 109.874089 66.009227 109.874089 \nC 65.213618 109.874089 64.450488 110.190188 63.887907 110.752768 \nC 63.325326 111.315349 63.009227 112.078479 63.009227 112.874089 \nC 63.009227 113.669698 63.325326 114.432828 63.887907 114.995409 \nC 64.450488 115.55799 65.213618 115.874089 66.009227 115.874089 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 20.083636 \nC 72.103957 20.083636 72.867087 19.767537 73.429668 19.204957 \nC 73.992249 18.642376 74.308348 17.879246 74.308348 17.083636 \nC 74.308348 16.288027 73.992249 15.524897 73.429668 14.962316 \nC 72.867087 14.399735 72.103957 14.083636 71.308348 14.083636 \nC 70.512738 14.083636 69.749608 14.399735 69.187027 14.962316 \nC 68.624447 15.524897 68.308348 16.288027 68.308348 17.083636 \nC 68.308348 17.879246 68.624447 18.642376 69.187027 19.204957 \nC 69.749608 19.767537 70.512738 20.083636 71.308348 20.083636 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 106.327693 \nC 61.491916 106.327693 62.255046 106.011594 62.817627 105.449013 \nC 63.380208 104.886432 63.696307 104.123302 63.696307 103.327693 \nC 63.696307 102.532083 63.380208 101.768953 62.817627 101.206372 \nC 62.255046 100.643792 61.491916 100.327693 60.696307 100.327693 \nC 59.900698 100.327693 59.137567 100.643792 58.574986 101.206372 \nC 58.012406 101.768953 57.696307 102.532083 57.696307 103.327693 \nC 57.696307 104.123302 58.012406 104.886432 58.574986 105.449013 \nC 59.137567 106.011594 59.900698 106.327693 60.696307 106.327693 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 165.903662 \nC 61.570115 165.903662 62.333245 165.587563 62.895826 165.024983 \nC 63.458407 164.462402 63.774506 163.699271 63.774506 162.903662 \nC 63.774506 162.108053 63.458407 161.344923 62.895826 160.782342 \nC 62.333245 160.219761 61.570115 159.903662 60.774506 159.903662 \nC 59.978896 159.903662 59.215766 160.219761 58.653185 160.782342 \nC 58.090605 161.344923 57.774506 162.108053 57.774506 162.903662 \nC 57.774506 163.699271 58.090605 164.462402 58.653185 165.024983 \nC 59.215766 165.587563 59.978896 165.903662 60.774506 165.903662 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 155.462679 \nC 61.836911 155.462679 62.600041 155.14658 63.162622 154.583999 \nC 63.725203 154.021419 64.041302 153.258288 64.041302 152.462679 \nC 64.041302 151.66707 63.725203 150.903939 63.162622 150.341359 \nC 62.600041 149.778778 61.836911 149.462679 61.041302 149.462679 \nC 60.245692 149.462679 59.482562 149.778778 58.919981 150.341359 \nC 58.357401 150.903939 58.041302 151.66707 58.041302 152.462679 \nC 58.041302 153.258288 58.357401 154.021419 58.919981 154.583999 \nC 59.482562 155.14658 60.245692 155.462679 61.041302 155.462679 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 105.288244 \nC 62.812096 105.288244 63.575227 104.972145 64.137807 104.409564 \nC 64.700388 103.846983 65.016487 103.083853 65.016487 102.288244 \nC 65.016487 101.492634 64.700388 100.729504 64.137807 100.166923 \nC 63.575227 99.604343 62.812096 99.288244 62.016487 99.288244 \nC 61.220878 99.288244 60.457747 99.604343 59.895167 100.166923 \nC 59.332586 100.729504 59.016487 101.492634 59.016487 102.288244 \nC 59.016487 103.083853 59.332586 103.846983 59.895167 104.409564 \nC 60.457747 104.972145 61.220878 105.288244 62.016487 105.288244 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 106.320272 \nC 66.528841 106.320272 67.291971 106.004173 67.854552 105.441592 \nC 68.417132 104.879011 68.733231 104.115881 68.733231 103.320272 \nC 68.733231 102.524662 68.417132 101.761532 67.854552 101.198951 \nC 67.291971 100.636371 66.528841 100.320272 65.733231 100.320272 \nC 64.937622 100.320272 64.174492 100.636371 63.611911 101.198951 \nC 63.04933 101.761532 62.733231 102.524662 62.733231 103.320272 \nC 62.733231 104.115881 63.04933 104.879011 63.611911 105.441592 \nC 64.174492 106.004173 64.937622 106.320272 65.733231 106.320272 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 105.694612 \nC 81.027823 105.694612 81.790954 105.378513 82.353534 104.815933 \nC 82.916115 104.253352 83.232214 103.490222 83.232214 102.694612 \nC 83.232214 101.899003 82.916115 101.135873 82.353534 100.573292 \nC 81.790954 100.010711 81.027823 99.694612 80.232214 99.694612 \nC 79.436605 99.694612 78.673474 100.010711 78.110894 100.573292 \nC 77.548313 101.135873 77.232214 101.899003 77.232214 102.694612 \nC 77.232214 103.490222 77.548313 104.253352 78.110894 104.815933 \nC 78.673474 105.378513 79.436605 105.694612 80.232214 105.694612 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 106.44302 \nC 138.287765 106.44302 139.050895 106.126921 139.613476 105.56434 \nC 140.176056 105.001759 140.492155 104.238629 140.492155 103.44302 \nC 140.492155 102.64741 140.176056 101.88428 139.613476 101.321699 \nC 139.050895 100.759119 138.287765 100.44302 137.492155 100.44302 \nC 136.696546 100.44302 135.933416 100.759119 135.370835 101.321699 \nC 134.808254 101.88428 134.492155 102.64741 134.492155 103.44302 \nC 134.492155 104.238629 134.808254 105.001759 135.370835 105.56434 \nC 135.933416 106.126921 136.696546 106.44302 137.492155 106.44302 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 133.820647 \nC 365.855552 133.820647 366.618683 133.504548 367.181264 132.941967 \nC 367.743844 132.379386 368.059943 131.616256 368.059943 130.820647 \nC 368.059943 130.025038 367.743844 129.261907 367.181264 128.699326 \nC 366.618683 128.136746 365.855552 127.820647 365.059943 127.820647 \nC 364.264334 127.820647 363.501204 128.136746 362.938623 128.699326 \nC 362.376042 129.261907 362.059943 130.025038 362.059943 130.820647 \nC 362.059943 131.616256 362.376042 132.379386 362.938623 132.941967 \nC 363.501204 133.504548 364.264334 133.820647 365.059943 133.820647 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 158.215134 \nC 61.671313 158.215134 62.434444 157.899035 62.997024 157.336455 \nC 63.559605 156.773874 63.875704 156.010744 63.875704 155.215134 \nC 63.875704 154.419525 63.559605 153.656395 62.997024 153.093814 \nC 62.434444 152.531233 61.671313 152.215134 60.875704 152.215134 \nC 60.080095 152.215134 59.316965 152.531233 58.754384 153.093814 \nC 58.191803 153.656395 57.875704 154.419525 57.875704 155.215134 \nC 57.875704 156.010744 58.191803 156.773874 58.754384 157.336455 \nC 59.316965 157.899035 60.080095 158.215134 60.875704 158.215134 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 149.592508 \nC 61.836911 149.592508 62.600041 149.276409 63.162622 148.713828 \nC 63.725203 148.151248 64.041302 147.388117 64.041302 146.592508 \nC 64.041302 145.796899 63.725203 145.033768 63.162622 144.471188 \nC 62.600041 143.908607 61.836911 143.592508 61.041302 143.592508 \nC 60.245692 143.592508 59.482562 143.908607 58.919981 144.471188 \nC 58.357401 145.033768 58.041302 145.796899 58.041302 146.592508 \nC 58.041302 147.388117 58.357401 148.151248 58.919981 148.713828 \nC 59.482562 149.276409 60.245692 149.592508 61.041302 149.592508 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 105.820039 \nC 62.168106 105.820039 62.931236 105.50394 63.493817 104.94136 \nC 64.056398 104.378779 64.372497 103.615649 64.372497 102.820039 \nC 64.372497 102.02443 64.056398 101.2613 63.493817 100.698719 \nC 62.931236 100.136138 62.168106 99.820039 61.372497 99.820039 \nC 60.576887 99.820039 59.813757 100.136138 59.251176 100.698719 \nC 58.688596 101.2613 58.372497 102.02443 58.372497 102.820039 \nC 58.372497 103.615649 58.688596 104.378779 59.251176 104.94136 \nC 59.813757 105.50394 60.576887 105.820039 61.372497 105.820039 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 104.288964 \nC 62.830496 104.288964 63.593626 103.972865 64.156207 103.410284 \nC 64.718788 102.847704 65.034887 102.084573 65.034887 101.288964 \nC 65.034887 100.493355 64.718788 99.730224 64.156207 99.167644 \nC 63.593626 98.605063 62.830496 98.288964 62.034887 98.288964 \nC 61.239277 98.288964 60.476147 98.605063 59.913566 99.167644 \nC 59.350986 99.730224 59.034887 100.493355 59.034887 101.288964 \nC 59.034887 102.084573 59.350986 102.847704 59.913566 103.410284 \nC 60.476147 103.972865 61.239277 104.288964 62.034887 104.288964 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 172.488084 \nC 64.155276 172.488084 64.918406 172.171985 65.480987 171.609404 \nC 66.043568 171.046824 66.359667 170.283693 66.359667 169.488084 \nC 66.359667 168.692475 66.043568 167.929344 65.480987 167.366764 \nC 64.918406 166.804183 64.155276 166.488084 63.359667 166.488084 \nC 62.564058 166.488084 61.800927 166.804183 61.238347 167.366764 \nC 60.675766 167.929344 60.359667 168.692475 60.359667 169.488084 \nC 60.359667 170.283693 60.675766 171.046824 61.238347 171.609404 \nC 61.800927 172.171985 62.564058 172.488084 63.359667 172.488084 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 104.819654 \nC 66.804836 104.819654 67.567967 104.503556 68.130547 103.940975 \nC 68.693128 103.378394 69.009227 102.615264 69.009227 101.819654 \nC 69.009227 101.024045 68.693128 100.260915 68.130547 99.698334 \nC 67.567967 99.135753 66.804836 98.819654 66.009227 98.819654 \nC 65.213618 98.819654 64.450488 99.135753 63.887907 99.698334 \nC 63.325326 100.260915 63.009227 101.024045 63.009227 101.819654 \nC 63.009227 102.615264 63.325326 103.378394 63.887907 103.940975 \nC 64.450488 104.503556 65.213618 104.819654 66.009227 104.819654 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 150.759737 \nC 72.103957 150.759737 72.867087 150.443638 73.429668 149.881057 \nC 73.992249 149.318477 74.308348 148.555346 74.308348 147.759737 \nC 74.308348 146.964128 73.992249 146.200997 73.429668 145.638417 \nC 72.867087 145.075836 72.103957 144.759737 71.308348 144.759737 \nC 70.512738 144.759737 69.749608 145.075836 69.187027 145.638417 \nC 68.624447 146.200997 68.308348 146.964128 68.308348 147.759737 \nC 68.308348 148.555346 68.624447 149.318477 69.187027 149.881057 \nC 69.749608 150.443638 70.512738 150.759737 71.308348 150.759737 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 186.746668 \nC 61.836911 186.746668 62.600041 186.430569 63.162622 185.867988 \nC 63.725203 185.305407 64.041302 184.542277 64.041302 183.746668 \nC 64.041302 182.951058 63.725203 182.187928 63.162622 181.625347 \nC 62.600041 181.062767 61.836911 180.746668 61.041302 180.746668 \nC 60.245692 180.746668 59.482562 181.062767 58.919981 181.625347 \nC 58.357401 182.187928 58.041302 182.951058 58.041302 183.746668 \nC 58.041302 184.542277 58.357401 185.305407 58.919981 185.867988 \nC 59.482562 186.430569 60.245692 186.746668 61.041302 186.746668 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 186.102053 \nC 62.812096 186.102053 63.575227 185.785954 64.137807 185.223373 \nC 64.700388 184.660793 65.016487 183.897662 65.016487 183.102053 \nC 65.016487 182.306444 64.700388 181.543313 64.137807 180.980733 \nC 63.575227 180.418152 62.812096 180.102053 62.016487 180.102053 \nC 61.220878 180.102053 60.457747 180.418152 59.895167 180.980733 \nC 59.332586 181.543313 59.016487 182.306444 59.016487 183.102053 \nC 59.016487 183.897662 59.332586 184.660793 59.895167 185.223373 \nC 60.457747 185.785954 61.220878 186.102053 62.016487 186.102053 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 104.877423 \nC 66.528841 104.877423 67.291971 104.561324 67.854552 103.998744 \nC 68.417132 103.436163 68.733231 102.673032 68.733231 101.877423 \nC 68.733231 101.081814 68.417132 100.318684 67.854552 99.756103 \nC 67.291971 99.193522 66.528841 98.877423 65.733231 98.877423 \nC 64.937622 98.877423 64.174492 99.193522 63.611911 99.756103 \nC 63.04933 100.318684 62.733231 101.081814 62.733231 101.877423 \nC 62.733231 102.673032 63.04933 103.436163 63.611911 103.998744 \nC 64.174492 104.561324 64.937622 104.877423 65.733231 104.877423 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 181.629527 \nC 81.027823 181.629527 81.790954 181.313428 82.353534 180.750847 \nC 82.916115 180.188266 83.232214 179.425136 83.232214 178.629527 \nC 83.232214 177.833917 82.916115 177.070787 82.353534 176.508206 \nC 81.790954 175.945626 81.027823 175.629527 80.232214 175.629527 \nC 79.436605 175.629527 78.673474 175.945626 78.110894 176.508206 \nC 77.548313 177.070787 77.232214 177.833917 77.232214 178.629527 \nC 77.232214 179.425136 77.548313 180.188266 78.110894 180.750847 \nC 78.673474 181.313428 79.436605 181.629527 80.232214 181.629527 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 188.258896 \nC 138.287765 188.258896 139.050895 187.942797 139.613476 187.380216 \nC 140.176056 186.817636 140.492155 186.054505 140.492155 185.258896 \nC 140.492155 184.463287 140.176056 183.700156 139.613476 183.137576 \nC 139.050895 182.574995 138.287765 182.258896 137.492155 182.258896 \nC 136.696546 182.258896 135.933416 182.574995 135.370835 183.137576 \nC 134.808254 183.700156 134.492155 184.463287 134.492155 185.258896 \nC 134.492155 186.054505 134.808254 186.817636 135.370835 187.380216 \nC 135.933416 187.942797 136.696546 188.258896 137.492155 188.258896 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 140.48377 \nC 365.855552 140.48377 366.618683 140.167671 367.181264 139.60509 \nC 367.743844 139.042509 368.059943 138.279379 368.059943 137.48377 \nC 368.059943 136.688161 367.743844 135.92503 367.181264 135.362449 \nC 366.618683 134.799869 365.855552 134.48377 365.059943 134.48377 \nC 364.264334 134.48377 363.501204 134.799869 362.938623 135.362449 \nC 362.376042 135.92503 362.059943 136.688161 362.059943 137.48377 \nC 362.059943 138.279379 362.376042 139.042509 362.938623 139.60509 \nC 363.501204 140.167671 364.264334 140.48377 365.059943 140.48377 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 160.987044 \nC 61.671313 160.987044 62.434444 160.670945 62.997024 160.108364 \nC 63.559605 159.545783 63.875704 158.782653 63.875704 157.987044 \nC 63.875704 157.191434 63.559605 156.428304 62.997024 155.865723 \nC 62.434444 155.303143 61.671313 154.987044 60.875704 154.987044 \nC 60.080095 154.987044 59.316965 155.303143 58.754384 155.865723 \nC 58.191803 156.428304 57.875704 157.191434 57.875704 157.987044 \nC 57.875704 158.782653 58.191803 159.545783 58.754384 160.108364 \nC 59.316965 160.670945 60.080095 160.987044 60.875704 160.987044 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 202.150289 \nC 61.836911 202.150289 62.600041 201.83419 63.162622 201.271609 \nC 63.725203 200.709028 64.041302 199.945898 64.041302 199.150289 \nC 64.041302 198.354679 63.725203 197.591549 63.162622 197.028968 \nC 62.600041 196.466387 61.836911 196.150289 61.041302 196.150289 \nC 60.245692 196.150289 59.482562 196.466387 58.919981 197.028968 \nC 58.357401 197.591549 58.041302 198.354679 58.041302 199.150289 \nC 58.041302 199.945898 58.357401 200.709028 58.919981 201.271609 \nC 59.482562 201.83419 60.245692 202.150289 61.041302 202.150289 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 189.326955 \nC 62.168106 189.326955 62.931236 189.010856 63.493817 188.448275 \nC 64.056398 187.885695 64.372497 187.122564 64.372497 186.326955 \nC 64.372497 185.531346 64.056398 184.768215 63.493817 184.205635 \nC 62.931236 183.643054 62.168106 183.326955 61.372497 183.326955 \nC 60.576887 183.326955 59.813757 183.643054 59.251176 184.205635 \nC 58.688596 184.768215 58.372497 185.531346 58.372497 186.326955 \nC 58.372497 187.122564 58.688596 187.885695 59.251176 188.448275 \nC 59.813757 189.010856 60.576887 189.326955 61.372497 189.326955 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 170.096815 \nC 62.830496 170.096815 63.593626 169.780716 64.156207 169.218136 \nC 64.718788 168.655555 65.034887 167.892425 65.034887 167.096815 \nC 65.034887 166.301206 64.718788 165.538076 64.156207 164.975495 \nC 63.593626 164.412914 62.830496 164.096815 62.034887 164.096815 \nC 61.239277 164.096815 60.476147 164.412914 59.913566 164.975495 \nC 59.350986 165.538076 59.034887 166.301206 59.034887 167.096815 \nC 59.034887 167.892425 59.350986 168.655555 59.913566 169.218136 \nC 60.476147 169.780716 61.239277 170.096815 62.034887 170.096815 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 168.395756 \nC 64.155276 168.395756 64.918406 168.079657 65.480987 167.517076 \nC 66.043568 166.954496 66.359667 166.191365 66.359667 165.395756 \nC 66.359667 164.600147 66.043568 163.837017 65.480987 163.274436 \nC 64.918406 162.711855 64.155276 162.395756 63.359667 162.395756 \nC 62.564058 162.395756 61.800927 162.711855 61.238347 163.274436 \nC 60.675766 163.837017 60.359667 164.600147 60.359667 165.395756 \nC 60.359667 166.191365 60.675766 166.954496 61.238347 167.517076 \nC 61.800927 168.079657 62.564058 168.395756 63.359667 168.395756 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 153.734504 \nC 66.804836 153.734504 67.567967 153.418405 68.130547 152.855824 \nC 68.693128 152.293244 69.009227 151.530113 69.009227 150.734504 \nC 69.009227 149.938895 68.693128 149.175764 68.130547 148.613184 \nC 67.567967 148.050603 66.804836 147.734504 66.009227 147.734504 \nC 65.213618 147.734504 64.450488 148.050603 63.887907 148.613184 \nC 63.325326 149.175764 63.009227 149.938895 63.009227 150.734504 \nC 63.009227 151.530113 63.325326 152.293244 63.887907 152.855824 \nC 64.450488 153.418405 65.213618 153.734504 66.009227 153.734504 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 130.437532 \nC 72.103957 130.437532 72.867087 130.121433 73.429668 129.558853 \nC 73.992249 128.996272 74.308348 128.233142 74.308348 127.437532 \nC 74.308348 126.641923 73.992249 125.878793 73.429668 125.316212 \nC 72.867087 124.753631 72.103957 124.437532 71.308348 124.437532 \nC 70.512738 124.437532 69.749608 124.753631 69.187027 125.316212 \nC 68.624447 125.878793 68.308348 126.641923 68.308348 127.437532 \nC 68.308348 128.233142 68.624447 128.996272 69.187027 129.558853 \nC 69.749608 130.121433 70.512738 130.437532 71.308348 130.437532 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 153.791508 \nC 61.491916 153.791508 62.255046 153.475409 62.817627 152.912828 \nC 63.380208 152.350247 63.696307 151.587117 63.696307 150.791508 \nC 63.696307 149.995898 63.380208 149.232768 62.817627 148.670187 \nC 62.255046 148.107607 61.491916 147.791508 60.696307 147.791508 \nC 59.900698 147.791508 59.137567 148.107607 58.574986 148.670187 \nC 58.012406 149.232768 57.696307 149.995898 57.696307 150.791508 \nC 57.696307 151.587117 58.012406 152.350247 58.574986 152.912828 \nC 59.137567 153.475409 59.900698 153.791508 60.696307 153.791508 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 151.410298 \nC 61.570115 151.410298 62.333245 151.094199 62.895826 150.531618 \nC 63.458407 149.969038 63.774506 149.205907 63.774506 148.410298 \nC 63.774506 147.614689 63.458407 146.851558 62.895826 146.288978 \nC 62.333245 145.726397 61.570115 145.410298 60.774506 145.410298 \nC 59.978896 145.410298 59.215766 145.726397 58.653185 146.288978 \nC 58.090605 146.851558 57.774506 147.614689 57.774506 148.410298 \nC 57.774506 149.205907 58.090605 149.969038 58.653185 150.531618 \nC 59.215766 151.094199 59.978896 151.410298 60.774506 151.410298 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 156.419931 \nC 61.836911 156.419931 62.600041 156.103832 63.162622 155.541251 \nC 63.725203 154.978671 64.041302 154.21554 64.041302 153.419931 \nC 64.041302 152.624322 63.725203 151.861191 63.162622 151.298611 \nC 62.600041 150.73603 61.836911 150.419931 61.041302 150.419931 \nC 60.245692 150.419931 59.482562 150.73603 58.919981 151.298611 \nC 58.357401 151.861191 58.041302 152.624322 58.041302 153.419931 \nC 58.041302 154.21554 58.357401 154.978671 58.919981 155.541251 \nC 59.482562 156.103832 60.245692 156.419931 61.041302 156.419931 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 105.432674 \nC 62.812096 105.432674 63.575227 105.116575 64.137807 104.553995 \nC 64.700388 103.991414 65.016487 103.228284 65.016487 102.432674 \nC 65.016487 101.637065 64.700388 100.873935 64.137807 100.311354 \nC 63.575227 99.748773 62.812096 99.432674 62.016487 99.432674 \nC 61.220878 99.432674 60.457747 99.748773 59.895167 100.311354 \nC 59.332586 100.873935 59.016487 101.637065 59.016487 102.432674 \nC 59.016487 103.228284 59.332586 103.991414 59.895167 104.553995 \nC 60.457747 105.116575 61.220878 105.432674 62.016487 105.432674 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 105.527927 \nC 66.528841 105.527927 67.291971 105.211828 67.854552 104.649248 \nC 68.417132 104.086667 68.733231 103.323537 68.733231 102.527927 \nC 68.733231 101.732318 68.417132 100.969188 67.854552 100.406607 \nC 67.291971 99.844026 66.528841 99.527927 65.733231 99.527927 \nC 64.937622 99.527927 64.174492 99.844026 63.611911 100.406607 \nC 63.04933 100.969188 62.733231 101.732318 62.733231 102.527927 \nC 62.733231 103.323537 63.04933 104.086667 63.611911 104.649248 \nC 64.174492 105.211828 64.937622 105.527927 65.733231 105.527927 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 159.064646 \nC 81.027823 159.064646 81.790954 158.748547 82.353534 158.185966 \nC 82.916115 157.623385 83.232214 156.860255 83.232214 156.064646 \nC 83.232214 155.269036 82.916115 154.505906 82.353534 153.943325 \nC 81.790954 153.380745 81.027823 153.064646 80.232214 153.064646 \nC 79.436605 153.064646 78.673474 153.380745 78.110894 153.943325 \nC 77.548313 154.505906 77.232214 155.269036 77.232214 156.064646 \nC 77.232214 156.860255 77.548313 157.623385 78.110894 158.185966 \nC 78.673474 158.748547 79.436605 159.064646 80.232214 159.064646 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 108.307047 \nC 138.287765 108.307047 139.050895 107.990948 139.613476 107.428367 \nC 140.176056 106.865786 140.492155 106.102656 140.492155 105.307047 \nC 140.492155 104.511437 140.176056 103.748307 139.613476 103.185726 \nC 139.050895 102.623146 138.287765 102.307047 137.492155 102.307047 \nC 136.696546 102.307047 135.933416 102.623146 135.370835 103.185726 \nC 134.808254 103.748307 134.492155 104.511437 134.492155 105.307047 \nC 134.492155 106.102656 134.808254 106.865786 135.370835 107.428367 \nC 135.933416 107.990948 136.696546 108.307047 137.492155 108.307047 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 141.429002 \nC 365.855552 141.429002 366.618683 141.112903 367.181264 140.550322 \nC 367.743844 139.987741 368.059943 139.224611 368.059943 138.429002 \nC 368.059943 137.633392 367.743844 136.870262 367.181264 136.307681 \nC 366.618683 135.745101 365.855552 135.429002 365.059943 135.429002 \nC 364.264334 135.429002 363.501204 135.745101 362.938623 136.307681 \nC 362.376042 136.870262 362.059943 137.633392 362.059943 138.429002 \nC 362.059943 139.224611 362.376042 139.987741 362.938623 140.550322 \nC 363.501204 141.112903 364.264334 141.429002 365.059943 141.429002 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 154.64631 \nC 61.671313 154.64631 62.434444 154.330211 62.997024 153.76763 \nC 63.559605 153.205049 63.875704 152.441919 63.875704 151.64631 \nC 63.875704 150.8507 63.559605 150.08757 62.997024 149.524989 \nC 62.434444 148.962409 61.671313 148.64631 60.875704 148.64631 \nC 60.080095 148.64631 59.316965 148.962409 58.754384 149.524989 \nC 58.191803 150.08757 57.875704 150.8507 57.875704 151.64631 \nC 57.875704 152.441919 58.191803 153.205049 58.754384 153.76763 \nC 59.316965 154.330211 60.080095 154.64631 60.875704 154.64631 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 157.087122 \nC 61.836911 157.087122 62.600041 156.771023 63.162622 156.208442 \nC 63.725203 155.645861 64.041302 154.882731 64.041302 154.087122 \nC 64.041302 153.291513 63.725203 152.528382 63.162622 151.965802 \nC 62.600041 151.403221 61.836911 151.087122 61.041302 151.087122 \nC 60.245692 151.087122 59.482562 151.403221 58.919981 151.965802 \nC 58.357401 152.528382 58.041302 153.291513 58.041302 154.087122 \nC 58.041302 154.882731 58.357401 155.645861 58.919981 156.208442 \nC 59.482562 156.771023 60.245692 157.087122 61.041302 157.087122 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 105.280059 \nC 62.168106 105.280059 62.931236 104.96396 63.493817 104.40138 \nC 64.056398 103.838799 64.372497 103.075669 64.372497 102.280059 \nC 64.372497 101.48445 64.056398 100.72132 63.493817 100.158739 \nC 62.931236 99.596158 62.168106 99.280059 61.372497 99.280059 \nC 60.576887 99.280059 59.813757 99.596158 59.251176 100.158739 \nC 58.688596 100.72132 58.372497 101.48445 58.372497 102.280059 \nC 58.372497 103.075669 58.688596 103.838799 59.251176 104.40138 \nC 59.813757 104.96396 60.576887 105.280059 61.372497 105.280059 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 162.711763 \nC 62.830496 162.711763 63.593626 162.395665 64.156207 161.833084 \nC 64.718788 161.270503 65.034887 160.507373 65.034887 159.711763 \nC 65.034887 158.916154 64.718788 158.153024 64.156207 157.590443 \nC 63.593626 157.027862 62.830496 156.711763 62.034887 156.711763 \nC 61.239277 156.711763 60.476147 157.027862 59.913566 157.590443 \nC 59.350986 158.153024 59.034887 158.916154 59.034887 159.711763 \nC 59.034887 160.507373 59.350986 161.270503 59.913566 161.833084 \nC 60.476147 162.395665 61.239277 162.711763 62.034887 162.711763 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 162.265125 \nC 64.155276 162.265125 64.918406 161.949026 65.480987 161.386446 \nC 66.043568 160.823865 66.359667 160.060735 66.359667 159.265125 \nC 66.359667 158.469516 66.043568 157.706386 65.480987 157.143805 \nC 64.918406 156.581224 64.155276 156.265125 63.359667 156.265125 \nC 62.564058 156.265125 61.800927 156.581224 61.238347 157.143805 \nC 60.675766 157.706386 60.359667 158.469516 60.359667 159.265125 \nC 60.359667 160.060735 60.675766 160.823865 61.238347 161.386446 \nC 61.800927 161.949026 62.564058 162.265125 63.359667 162.265125 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 104.560347 \nC 66.804836 104.560347 67.567967 104.244248 68.130547 103.681667 \nC 68.693128 103.119086 69.009227 102.355956 69.009227 101.560347 \nC 69.009227 100.764738 68.693128 100.001607 68.130547 99.439027 \nC 67.567967 98.876446 66.804836 98.560347 66.009227 98.560347 \nC 65.213618 98.560347 64.450488 98.876446 63.887907 99.439027 \nC 63.325326 100.001607 63.009227 100.764738 63.009227 101.560347 \nC 63.009227 102.355956 63.325326 103.119086 63.887907 103.681667 \nC 64.450488 104.244248 65.213618 104.560347 66.009227 104.560347 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 105.881919 \nC 72.103957 105.881919 72.867087 105.56582 73.429668 105.003239 \nC 73.992249 104.440659 74.308348 103.677528 74.308348 102.881919 \nC 74.308348 102.08631 73.992249 101.323179 73.429668 100.760599 \nC 72.867087 100.198018 72.103957 99.881919 71.308348 99.881919 \nC 70.512738 99.881919 69.749608 100.198018 69.187027 100.760599 \nC 68.624447 101.323179 68.308348 102.08631 68.308348 102.881919 \nC 68.308348 103.677528 68.624447 104.440659 69.187027 105.003239 \nC 69.749608 105.56582 70.512738 105.881919 71.308348 105.881919 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 113.297276 \nC 61.491916 113.297276 62.255046 112.981177 62.817627 112.418596 \nC 63.380208 111.856016 63.696307 111.092885 63.696307 110.297276 \nC 63.696307 109.501667 63.380208 108.738536 62.817627 108.175956 \nC 62.255046 107.613375 61.491916 107.297276 60.696307 107.297276 \nC 59.900698 107.297276 59.137567 107.613375 58.574986 108.175956 \nC 58.012406 108.738536 57.696307 109.501667 57.696307 110.297276 \nC 57.696307 111.092885 58.012406 111.856016 58.574986 112.418596 \nC 59.137567 112.981177 59.900698 113.297276 60.696307 113.297276 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 113.23318 \nC 61.570115 113.23318 62.333245 112.917081 62.895826 112.3545 \nC 63.458407 111.791919 63.774506 111.028789 63.774506 110.23318 \nC 63.774506 109.43757 63.458407 108.67444 62.895826 108.111859 \nC 62.333245 107.549279 61.570115 107.23318 60.774506 107.23318 \nC 59.978896 107.23318 59.215766 107.549279 58.653185 108.111859 \nC 58.090605 108.67444 57.774506 109.43757 57.774506 110.23318 \nC 57.774506 111.028789 58.090605 111.791919 58.653185 112.3545 \nC 59.215766 112.917081 59.978896 113.23318 60.774506 113.23318 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 198.140509 \nC 61.836911 198.140509 62.600041 197.82441 63.162622 197.261829 \nC 63.725203 196.699249 64.041302 195.936118 64.041302 195.140509 \nC 64.041302 194.3449 63.725203 193.58177 63.162622 193.019189 \nC 62.600041 192.456608 61.836911 192.140509 61.041302 192.140509 \nC 60.245692 192.140509 59.482562 192.456608 58.919981 193.019189 \nC 58.357401 193.58177 58.041302 194.3449 58.041302 195.140509 \nC 58.041302 195.936118 58.357401 196.699249 58.919981 197.261829 \nC 59.482562 197.82441 60.245692 198.140509 61.041302 198.140509 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 106.85209 \nC 62.812096 106.85209 63.575227 106.535991 64.137807 105.97341 \nC 64.700388 105.410829 65.016487 104.647699 65.016487 103.85209 \nC 65.016487 103.05648 64.700388 102.29335 64.137807 101.730769 \nC 63.575227 101.168189 62.812096 100.85209 62.016487 100.85209 \nC 61.220878 100.85209 60.457747 101.168189 59.895167 101.730769 \nC 59.332586 102.29335 59.016487 103.05648 59.016487 103.85209 \nC 59.016487 104.647699 59.332586 105.410829 59.895167 105.97341 \nC 60.457747 106.535991 61.220878 106.85209 62.016487 106.85209 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 157.362759 \nC 66.528841 157.362759 67.291971 157.046661 67.854552 156.48408 \nC 68.417132 155.921499 68.733231 155.158369 68.733231 154.362759 \nC 68.733231 153.56715 68.417132 152.80402 67.854552 152.241439 \nC 67.291971 151.678858 66.528841 151.362759 65.733231 151.362759 \nC 64.937622 151.362759 64.174492 151.678858 63.611911 152.241439 \nC 63.04933 152.80402 62.733231 153.56715 62.733231 154.362759 \nC 62.733231 155.158369 63.04933 155.921499 63.611911 156.48408 \nC 64.174492 157.046661 64.937622 157.362759 65.733231 157.362759 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 160.406238 \nC 81.027823 160.406238 81.790954 160.090139 82.353534 159.527559 \nC 82.916115 158.964978 83.232214 158.201848 83.232214 157.406238 \nC 83.232214 156.610629 82.916115 155.847499 82.353534 155.284918 \nC 81.790954 154.722337 81.027823 154.406238 80.232214 154.406238 \nC 79.436605 154.406238 78.673474 154.722337 78.110894 155.284918 \nC 77.548313 155.847499 77.232214 156.610629 77.232214 157.406238 \nC 77.232214 158.201848 77.548313 158.964978 78.110894 159.527559 \nC 78.673474 160.090139 79.436605 160.406238 80.232214 160.406238 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 116.901476 \nC 138.287765 116.901476 139.050895 116.585377 139.613476 116.022797 \nC 140.176056 115.460216 140.492155 114.697086 140.492155 113.901476 \nC 140.492155 113.105867 140.176056 112.342737 139.613476 111.780156 \nC 139.050895 111.217575 138.287765 110.901476 137.492155 110.901476 \nC 136.696546 110.901476 135.933416 111.217575 135.370835 111.780156 \nC 134.808254 112.342737 134.492155 113.105867 134.492155 113.901476 \nC 134.492155 114.697086 134.808254 115.460216 135.370835 116.022797 \nC 135.933416 116.585377 136.696546 116.901476 137.492155 116.901476 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 106.468078 \nC 365.855552 106.468078 366.618683 106.151979 367.181264 105.589399 \nC 367.743844 105.026818 368.059943 104.263688 368.059943 103.468078 \nC 368.059943 102.672469 367.743844 101.909339 367.181264 101.346758 \nC 366.618683 100.784177 365.855552 100.468078 365.059943 100.468078 \nC 364.264334 100.468078 363.501204 100.784177 362.938623 101.346758 \nC 362.376042 101.909339 362.059943 102.672469 362.059943 103.468078 \nC 362.059943 104.263688 362.376042 105.026818 362.938623 105.589399 \nC 363.501204 106.151979 364.264334 106.468078 365.059943 106.468078 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 104.949431 \nC 61.671313 104.949431 62.434444 104.633333 62.997024 104.070752 \nC 63.559605 103.508171 63.875704 102.745041 63.875704 101.949431 \nC 63.875704 101.153822 63.559605 100.390692 62.997024 99.828111 \nC 62.434444 99.26553 61.671313 98.949431 60.875704 98.949431 \nC 60.080095 98.949431 59.316965 99.26553 58.754384 99.828111 \nC 58.191803 100.390692 57.875704 101.153822 57.875704 101.949431 \nC 57.875704 102.745041 58.191803 103.508171 58.754384 104.070752 \nC 59.316965 104.633333 60.080095 104.949431 60.875704 104.949431 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 186.314851 \nC 61.836911 186.314851 62.600041 185.998752 63.162622 185.436172 \nC 63.725203 184.873591 64.041302 184.110461 64.041302 183.314851 \nC 64.041302 182.519242 63.725203 181.756112 63.162622 181.193531 \nC 62.600041 180.63095 61.836911 180.314851 61.041302 180.314851 \nC 60.245692 180.314851 59.482562 180.63095 58.919981 181.193531 \nC 58.357401 181.756112 58.041302 182.519242 58.041302 183.314851 \nC 58.041302 184.110461 58.357401 184.873591 58.919981 185.436172 \nC 59.482562 185.998752 60.245692 186.314851 61.041302 186.314851 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 105.233551 \nC 62.168106 105.233551 62.931236 104.917452 63.493817 104.354871 \nC 64.056398 103.79229 64.372497 103.02916 64.372497 102.233551 \nC 64.372497 101.437941 64.056398 100.674811 63.493817 100.11223 \nC 62.931236 99.54965 62.168106 99.233551 61.372497 99.233551 \nC 60.576887 99.233551 59.813757 99.54965 59.251176 100.11223 \nC 58.688596 100.674811 58.372497 101.437941 58.372497 102.233551 \nC 58.372497 103.02916 58.688596 103.79229 59.251176 104.354871 \nC 59.813757 104.917452 60.576887 105.233551 61.372497 105.233551 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 173.756195 \nC 62.830496 173.756195 63.593626 173.440096 64.156207 172.877516 \nC 64.718788 172.314935 65.034887 171.551805 65.034887 170.756195 \nC 65.034887 169.960586 64.718788 169.197456 64.156207 168.634875 \nC 63.593626 168.072294 62.830496 167.756195 62.034887 167.756195 \nC 61.239277 167.756195 60.476147 168.072294 59.913566 168.634875 \nC 59.350986 169.197456 59.034887 169.960586 59.034887 170.756195 \nC 59.034887 171.551805 59.350986 172.314935 59.913566 172.877516 \nC 60.476147 173.440096 61.239277 173.756195 62.034887 173.756195 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 199.677573 \nC 64.155276 199.677573 64.918406 199.361474 65.480987 198.798893 \nC 66.043568 198.236313 66.359667 197.473182 66.359667 196.677573 \nC 66.359667 195.881964 66.043568 195.118833 65.480987 194.556253 \nC 64.918406 193.993672 64.155276 193.677573 63.359667 193.677573 \nC 62.564058 193.677573 61.800927 193.993672 61.238347 194.556253 \nC 60.675766 195.118833 60.359667 195.881964 60.359667 196.677573 \nC 60.359667 197.473182 60.675766 198.236313 61.238347 198.798893 \nC 61.800927 199.361474 62.564058 199.677573 63.359667 199.677573 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 133.305938 \nC 66.804836 133.305938 67.567967 132.989839 68.130547 132.427258 \nC 68.693128 131.864678 69.009227 131.101547 69.009227 130.305938 \nC 69.009227 129.510329 68.693128 128.747199 68.130547 128.184618 \nC 67.567967 127.622037 66.804836 127.305938 66.009227 127.305938 \nC 65.213618 127.305938 64.450488 127.622037 63.887907 128.184618 \nC 63.325326 128.747199 63.009227 129.510329 63.009227 130.305938 \nC 63.009227 131.101547 63.325326 131.864678 63.887907 132.427258 \nC 64.450488 132.989839 65.213618 133.305938 66.009227 133.305938 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 132.172059 \nC 72.103957 132.172059 72.867087 131.85596 73.429668 131.293379 \nC 73.992249 130.730799 74.308348 129.967668 74.308348 129.172059 \nC 74.308348 128.37645 73.992249 127.613319 73.429668 127.050739 \nC 72.867087 126.488158 72.103957 126.172059 71.308348 126.172059 \nC 70.512738 126.172059 69.749608 126.488158 69.187027 127.050739 \nC 68.624447 127.613319 68.308348 128.37645 68.308348 129.172059 \nC 68.308348 129.967668 68.624447 130.730799 69.187027 131.293379 \nC 69.749608 131.85596 70.512738 132.172059 71.308348 132.172059 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 153.164293 \nC 61.491916 153.164293 62.255046 152.848194 62.817627 152.285613 \nC 63.380208 151.723032 63.696307 150.959902 63.696307 150.164293 \nC 63.696307 149.368683 63.380208 148.605553 62.817627 148.042972 \nC 62.255046 147.480392 61.491916 147.164293 60.696307 147.164293 \nC 59.900698 147.164293 59.137567 147.480392 58.574986 148.042972 \nC 58.012406 148.605553 57.696307 149.368683 57.696307 150.164293 \nC 57.696307 150.959902 58.012406 151.723032 58.574986 152.285613 \nC 59.137567 152.848194 59.900698 153.164293 60.696307 153.164293 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 154.96979 \nC 61.570115 154.96979 62.333245 154.653691 62.895826 154.09111 \nC 63.458407 153.52853 63.774506 152.765399 63.774506 151.96979 \nC 63.774506 151.174181 63.458407 150.41105 62.895826 149.84847 \nC 62.333245 149.285889 61.570115 148.96979 60.774506 148.96979 \nC 59.978896 148.96979 59.215766 149.285889 58.653185 149.84847 \nC 58.090605 150.41105 57.774506 151.174181 57.774506 151.96979 \nC 57.774506 152.765399 58.090605 153.52853 58.653185 154.09111 \nC 59.215766 154.653691 59.978896 154.96979 60.774506 154.96979 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 105.036343 \nC 61.836911 105.036343 62.600041 104.720244 63.162622 104.157663 \nC 63.725203 103.595082 64.041302 102.831952 64.041302 102.036343 \nC 64.041302 101.240733 63.725203 100.477603 63.162622 99.915022 \nC 62.600041 99.352442 61.836911 99.036343 61.041302 99.036343 \nC 60.245692 99.036343 59.482562 99.352442 58.919981 99.915022 \nC 58.357401 100.477603 58.041302 101.240733 58.041302 102.036343 \nC 58.041302 102.831952 58.357401 103.595082 58.919981 104.157663 \nC 59.482562 104.720244 60.245692 105.036343 61.041302 105.036343 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 158.141087 \nC 62.812096 158.141087 63.575227 157.824988 64.137807 157.262407 \nC 64.700388 156.699826 65.016487 155.936696 65.016487 155.141087 \nC 65.016487 154.345477 64.700388 153.582347 64.137807 153.019766 \nC 63.575227 152.457186 62.812096 152.141087 62.016487 152.141087 \nC 61.220878 152.141087 60.457747 152.457186 59.895167 153.019766 \nC 59.332586 153.582347 59.016487 154.345477 59.016487 155.141087 \nC 59.016487 155.936696 59.332586 156.699826 59.895167 157.262407 \nC 60.457747 157.824988 61.220878 158.141087 62.016487 158.141087 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 102.132285 \nC 66.528841 102.132285 67.291971 101.816186 67.854552 101.253605 \nC 68.417132 100.691024 68.733231 99.927894 68.733231 99.132285 \nC 68.733231 98.336675 68.417132 97.573545 67.854552 97.010964 \nC 67.291971 96.448384 66.528841 96.132285 65.733231 96.132285 \nC 64.937622 96.132285 64.174492 96.448384 63.611911 97.010964 \nC 63.04933 97.573545 62.733231 98.336675 62.733231 99.132285 \nC 62.733231 99.927894 63.04933 100.691024 63.611911 101.253605 \nC 64.174492 101.816186 64.937622 102.132285 65.733231 102.132285 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 168.619957 \nC 81.027823 168.619957 81.790954 168.303858 82.353534 167.741277 \nC 82.916115 167.178696 83.232214 166.415566 83.232214 165.619957 \nC 83.232214 164.824347 82.916115 164.061217 82.353534 163.498636 \nC 81.790954 162.936056 81.027823 162.619957 80.232214 162.619957 \nC 79.436605 162.619957 78.673474 162.936056 78.110894 163.498636 \nC 77.548313 164.061217 77.232214 164.824347 77.232214 165.619957 \nC 77.232214 166.415566 77.548313 167.178696 78.110894 167.741277 \nC 78.673474 168.303858 79.436605 168.619957 80.232214 168.619957 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 107.305259 \nC 138.287765 107.305259 139.050895 106.98916 139.613476 106.42658 \nC 140.176056 105.863999 140.492155 105.100869 140.492155 104.305259 \nC 140.492155 103.50965 140.176056 102.74652 139.613476 102.183939 \nC 139.050895 101.621358 138.287765 101.305259 137.492155 101.305259 \nC 136.696546 101.305259 135.933416 101.621358 135.370835 102.183939 \nC 134.808254 102.74652 134.492155 103.50965 134.492155 104.305259 \nC 134.492155 105.100869 134.808254 105.863999 135.370835 106.42658 \nC 135.933416 106.98916 136.696546 107.305259 137.492155 107.305259 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 107.195729 \nC 365.855552 107.195729 366.618683 106.87963 367.181264 106.317049 \nC 367.743844 105.754469 368.059943 104.991338 368.059943 104.195729 \nC 368.059943 103.40012 367.743844 102.636989 367.181264 102.074409 \nC 366.618683 101.511828 365.855552 101.195729 365.059943 101.195729 \nC 364.264334 101.195729 363.501204 101.511828 362.938623 102.074409 \nC 362.376042 102.636989 362.059943 103.40012 362.059943 104.195729 \nC 362.059943 104.991338 362.376042 105.754469 362.938623 106.317049 \nC 363.501204 106.87963 364.264334 107.195729 365.059943 107.195729 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 150.895012 \nC 61.671313 150.895012 62.434444 150.578913 62.997024 150.016332 \nC 63.559605 149.453751 63.875704 148.690621 63.875704 147.895012 \nC 63.875704 147.099402 63.559605 146.336272 62.997024 145.773691 \nC 62.434444 145.21111 61.671313 144.895012 60.875704 144.895012 \nC 60.080095 144.895012 59.316965 145.21111 58.754384 145.773691 \nC 58.191803 146.336272 57.875704 147.099402 57.875704 147.895012 \nC 57.875704 148.690621 58.191803 149.453751 58.754384 150.016332 \nC 59.316965 150.578913 60.080095 150.895012 60.875704 150.895012 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 191.849509 \nC 61.836911 191.849509 62.600041 191.53341 63.162622 190.970829 \nC 63.725203 190.408249 64.041302 189.645118 64.041302 188.849509 \nC 64.041302 188.0539 63.725203 187.290769 63.162622 186.728189 \nC 62.600041 186.165608 61.836911 185.849509 61.041302 185.849509 \nC 60.245692 185.849509 59.482562 186.165608 58.919981 186.728189 \nC 58.357401 187.290769 58.041302 188.0539 58.041302 188.849509 \nC 58.041302 189.645118 58.357401 190.408249 58.919981 190.970829 \nC 59.482562 191.53341 60.245692 191.849509 61.041302 191.849509 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 106.779343 \nC 62.168106 106.779343 62.931236 106.463244 63.493817 105.900663 \nC 64.056398 105.338082 64.372497 104.574952 64.372497 103.779343 \nC 64.372497 102.983734 64.056398 102.220603 63.493817 101.658022 \nC 62.931236 101.095442 62.168106 100.779343 61.372497 100.779343 \nC 60.576887 100.779343 59.813757 101.095442 59.251176 101.658022 \nC 58.688596 102.220603 58.372497 102.983734 58.372497 103.779343 \nC 58.372497 104.574952 58.688596 105.338082 59.251176 105.900663 \nC 59.813757 106.463244 60.576887 106.779343 61.372497 106.779343 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 183.863058 \nC 62.830496 183.863058 63.593626 183.546959 64.156207 182.984378 \nC 64.718788 182.421797 65.034887 181.658667 65.034887 180.863058 \nC 65.034887 180.067448 64.718788 179.304318 64.156207 178.741737 \nC 63.593626 178.179157 62.830496 177.863058 62.034887 177.863058 \nC 61.239277 177.863058 60.476147 178.179157 59.913566 178.741737 \nC 59.350986 179.304318 59.034887 180.067448 59.034887 180.863058 \nC 59.034887 181.658667 59.350986 182.421797 59.913566 182.984378 \nC 60.476147 183.546959 61.239277 183.863058 62.034887 183.863058 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 105.530499 \nC 64.155276 105.530499 64.918406 105.2144 65.480987 104.651819 \nC 66.043568 104.089238 66.359667 103.326108 66.359667 102.530499 \nC 66.359667 101.734889 66.043568 100.971759 65.480987 100.409178 \nC 64.918406 99.846598 64.155276 99.530499 63.359667 99.530499 \nC 62.564058 99.530499 61.800927 99.846598 61.238347 100.409178 \nC 60.675766 100.971759 60.359667 101.734889 60.359667 102.530499 \nC 60.359667 103.326108 60.675766 104.089238 61.238347 104.651819 \nC 61.800927 105.2144 62.564058 105.530499 63.359667 105.530499 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 103.56452 \nC 66.804836 103.56452 67.567967 103.248421 68.130547 102.68584 \nC 68.693128 102.12326 69.009227 101.360129 69.009227 100.56452 \nC 69.009227 99.768911 68.693128 99.00578 68.130547 98.4432 \nC 67.567967 97.880619 66.804836 97.56452 66.009227 97.56452 \nC 65.213618 97.56452 64.450488 97.880619 63.887907 98.4432 \nC 63.325326 99.00578 63.009227 99.768911 63.009227 100.56452 \nC 63.009227 101.360129 63.325326 102.12326 63.887907 102.68584 \nC 64.450488 103.248421 65.213618 103.56452 66.009227 103.56452 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 105.591862 \nC 72.103957 105.591862 72.867087 105.275764 73.429668 104.713183 \nC 73.992249 104.150602 74.308348 103.387472 74.308348 102.591862 \nC 74.308348 101.796253 73.992249 101.033123 73.429668 100.470542 \nC 72.867087 99.907961 72.103957 99.591862 71.308348 99.591862 \nC 70.512738 99.591862 69.749608 99.907961 69.187027 100.470542 \nC 68.624447 101.033123 68.308348 101.796253 68.308348 102.591862 \nC 68.308348 103.387472 68.624447 104.150602 69.187027 104.713183 \nC 69.749608 105.275764 70.512738 105.591862 71.308348 105.591862 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 117.961592 \nC 61.491916 117.961592 62.255046 117.645493 62.817627 117.082912 \nC 63.380208 116.520332 63.696307 115.757201 63.696307 114.961592 \nC 63.696307 114.165983 63.380208 113.402853 62.817627 112.840272 \nC 62.255046 112.277691 61.491916 111.961592 60.696307 111.961592 \nC 59.900698 111.961592 59.137567 112.277691 58.574986 112.840272 \nC 58.012406 113.402853 57.696307 114.165983 57.696307 114.961592 \nC 57.696307 115.757201 58.012406 116.520332 58.574986 117.082912 \nC 59.137567 117.645493 59.900698 117.961592 60.696307 117.961592 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 193.157518 \nC 61.836911 193.157518 62.600041 192.841419 63.162622 192.278838 \nC 63.725203 191.716257 64.041302 190.953127 64.041302 190.157518 \nC 64.041302 189.361908 63.725203 188.598778 63.162622 188.036197 \nC 62.600041 187.473617 61.836911 187.157518 61.041302 187.157518 \nC 60.245692 187.157518 59.482562 187.473617 58.919981 188.036197 \nC 58.357401 188.598778 58.041302 189.361908 58.041302 190.157518 \nC 58.041302 190.953127 58.357401 191.716257 58.919981 192.278838 \nC 59.482562 192.841419 60.245692 193.157518 61.041302 193.157518 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 176.628373 \nC 62.812096 176.628373 63.575227 176.312274 64.137807 175.749693 \nC 64.700388 175.187112 65.016487 174.423982 65.016487 173.628373 \nC 65.016487 172.832764 64.700388 172.069633 64.137807 171.507052 \nC 63.575227 170.944472 62.812096 170.628373 62.016487 170.628373 \nC 61.220878 170.628373 60.457747 170.944472 59.895167 171.507052 \nC 59.332586 172.069633 59.016487 172.832764 59.016487 173.628373 \nC 59.016487 174.423982 59.332586 175.187112 59.895167 175.749693 \nC 60.457747 176.312274 61.220878 176.628373 62.016487 176.628373 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 125.972654 \nC 66.528841 125.972654 67.291971 125.656555 67.854552 125.093974 \nC 68.417132 124.531393 68.733231 123.768263 68.733231 122.972654 \nC 68.733231 122.177044 68.417132 121.413914 67.854552 120.851333 \nC 67.291971 120.288753 66.528841 119.972654 65.733231 119.972654 \nC 64.937622 119.972654 64.174492 120.288753 63.611911 120.851333 \nC 63.04933 121.413914 62.733231 122.177044 62.733231 122.972654 \nC 62.733231 123.768263 63.04933 124.531393 63.611911 125.093974 \nC 64.174492 125.656555 64.937622 125.972654 65.733231 125.972654 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 166.502262 \nC 81.027823 166.502262 81.790954 166.186163 82.353534 165.623582 \nC 82.916115 165.061001 83.232214 164.297871 83.232214 163.502262 \nC 83.232214 162.706652 82.916115 161.943522 82.353534 161.380941 \nC 81.790954 160.818361 81.027823 160.502262 80.232214 160.502262 \nC 79.436605 160.502262 78.673474 160.818361 78.110894 161.380941 \nC 77.548313 161.943522 77.232214 162.706652 77.232214 163.502262 \nC 77.232214 164.297871 77.548313 165.061001 78.110894 165.623582 \nC 78.673474 166.186163 79.436605 166.502262 80.232214 166.502262 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 106.077442 \nC 365.855552 106.077442 366.618683 105.761343 367.181264 105.198762 \nC 367.743844 104.636181 368.059943 103.873051 368.059943 103.077442 \nC 368.059943 102.281833 367.743844 101.518702 367.181264 100.956121 \nC 366.618683 100.393541 365.855552 100.077442 365.059943 100.077442 \nC 364.264334 100.077442 363.501204 100.393541 362.938623 100.956121 \nC 362.376042 101.518702 362.059943 102.281833 362.059943 103.077442 \nC 362.059943 103.873051 362.376042 104.636181 362.938623 105.198762 \nC 363.501204 105.761343 364.264334 106.077442 365.059943 106.077442 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 217.756364 \nC 61.671313 217.756364 62.434444 217.440265 62.997024 216.877684 \nC 63.559605 216.315103 63.875704 215.551973 63.875704 214.756364 \nC 63.875704 213.960754 63.559605 213.197624 62.997024 212.635043 \nC 62.434444 212.072463 61.671313 211.756364 60.875704 211.756364 \nC 60.080095 211.756364 59.316965 212.072463 58.754384 212.635043 \nC 58.191803 213.197624 57.875704 213.960754 57.875704 214.756364 \nC 57.875704 215.551973 58.191803 216.315103 58.754384 216.877684 \nC 59.316965 217.440265 60.080095 217.756364 60.875704 217.756364 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 113.099035 \nC 61.836911 113.099035 62.600041 112.782936 63.162622 112.220355 \nC 63.725203 111.657775 64.041302 110.894644 64.041302 110.099035 \nC 64.041302 109.303426 63.725203 108.540295 63.162622 107.977715 \nC 62.600041 107.415134 61.836911 107.099035 61.041302 107.099035 \nC 60.245692 107.099035 59.482562 107.415134 58.919981 107.977715 \nC 58.357401 108.540295 58.041302 109.303426 58.041302 110.099035 \nC 58.041302 110.894644 58.357401 111.657775 58.919981 112.220355 \nC 59.482562 112.782936 60.245692 113.099035 61.041302 113.099035 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 202.48558 \nC 62.168106 202.48558 62.931236 202.169481 63.493817 201.606901 \nC 64.056398 201.04432 64.372497 200.28119 64.372497 199.48558 \nC 64.372497 198.689971 64.056398 197.926841 63.493817 197.36426 \nC 62.931236 196.801679 62.168106 196.48558 61.372497 196.48558 \nC 60.576887 196.48558 59.813757 196.801679 59.251176 197.36426 \nC 58.688596 197.926841 58.372497 198.689971 58.372497 199.48558 \nC 58.372497 200.28119 58.688596 201.04432 59.251176 201.606901 \nC 59.813757 202.169481 60.576887 202.48558 61.372497 202.48558 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 205.792415 \nC 62.830496 205.792415 63.593626 205.476316 64.156207 204.913735 \nC 64.718788 204.351155 65.034887 203.588024 65.034887 202.792415 \nC 65.034887 201.996806 64.718788 201.233675 64.156207 200.671095 \nC 63.593626 200.108514 62.830496 199.792415 62.034887 199.792415 \nC 61.239277 199.792415 60.476147 200.108514 59.913566 200.671095 \nC 59.350986 201.233675 59.034887 201.996806 59.034887 202.792415 \nC 59.034887 203.588024 59.350986 204.351155 59.913566 204.913735 \nC 60.476147 205.476316 61.239277 205.792415 62.034887 205.792415 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 171.271878 \nC 64.155276 171.271878 64.918406 170.955779 65.480987 170.393198 \nC 66.043568 169.830618 66.359667 169.067487 66.359667 168.271878 \nC 66.359667 167.476269 66.043568 166.713138 65.480987 166.150558 \nC 64.918406 165.587977 64.155276 165.271878 63.359667 165.271878 \nC 62.564058 165.271878 61.800927 165.587977 61.238347 166.150558 \nC 60.675766 166.713138 60.359667 167.476269 60.359667 168.271878 \nC 60.359667 169.067487 60.675766 169.830618 61.238347 170.393198 \nC 61.800927 170.955779 62.564058 171.271878 63.359667 171.271878 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 203.082083 \nC 66.804836 203.082083 67.567967 202.765984 68.130547 202.203404 \nC 68.693128 201.640823 69.009227 200.877693 69.009227 200.082083 \nC 69.009227 199.286474 68.693128 198.523344 68.130547 197.960763 \nC 67.567967 197.398182 66.804836 197.082083 66.009227 197.082083 \nC 65.213618 197.082083 64.450488 197.398182 63.887907 197.960763 \nC 63.325326 198.523344 63.009227 199.286474 63.009227 200.082083 \nC 63.009227 200.877693 63.325326 201.640823 63.887907 202.203404 \nC 64.450488 202.765984 65.213618 203.082083 66.009227 203.082083 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 138.131753 \nC 72.103957 138.131753 72.867087 137.815654 73.429668 137.253073 \nC 73.992249 136.690492 74.308348 135.927362 74.308348 135.131753 \nC 74.308348 134.336143 73.992249 133.573013 73.429668 133.010432 \nC 72.867087 132.447851 72.103957 132.131753 71.308348 132.131753 \nC 70.512738 132.131753 69.749608 132.447851 69.187027 133.010432 \nC 68.624447 133.573013 68.308348 134.336143 68.308348 135.131753 \nC 68.308348 135.927362 68.624447 136.690492 69.187027 137.253073 \nC 69.749608 137.815654 70.512738 138.131753 71.308348 138.131753 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 150.591479 \nC 61.491916 150.591479 62.255046 150.27538 62.817627 149.7128 \nC 63.380208 149.150219 63.696307 148.387089 63.696307 147.591479 \nC 63.696307 146.79587 63.380208 146.03274 62.817627 145.470159 \nC 62.255046 144.907578 61.491916 144.591479 60.696307 144.591479 \nC 59.900698 144.591479 59.137567 144.907578 58.574986 145.470159 \nC 58.012406 146.03274 57.696307 146.79587 57.696307 147.591479 \nC 57.696307 148.387089 58.012406 149.150219 58.574986 149.7128 \nC 59.137567 150.27538 59.900698 150.591479 60.696307 150.591479 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 155.838619 \nC 61.570115 155.838619 62.333245 155.52252 62.895826 154.959939 \nC 63.458407 154.397358 63.774506 153.634228 63.774506 152.838619 \nC 63.774506 152.043009 63.458407 151.279879 62.895826 150.717298 \nC 62.333245 150.154718 61.570115 149.838619 60.774506 149.838619 \nC 59.978896 149.838619 59.215766 150.154718 58.653185 150.717298 \nC 58.090605 151.279879 57.774506 152.043009 57.774506 152.838619 \nC 57.774506 153.634228 58.090605 154.397358 58.653185 154.959939 \nC 59.215766 155.52252 59.978896 155.838619 60.774506 155.838619 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 154.146504 \nC 61.836911 154.146504 62.600041 153.830405 63.162622 153.267824 \nC 63.725203 152.705243 64.041302 151.942113 64.041302 151.146504 \nC 64.041302 150.350894 63.725203 149.587764 63.162622 149.025183 \nC 62.600041 148.462603 61.836911 148.146504 61.041302 148.146504 \nC 60.245692 148.146504 59.482562 148.462603 58.919981 149.025183 \nC 58.357401 149.587764 58.041302 150.350894 58.041302 151.146504 \nC 58.041302 151.942113 58.357401 152.705243 58.919981 153.267824 \nC 59.482562 153.830405 60.245692 154.146504 61.041302 154.146504 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 156.682589 \nC 62.812096 156.682589 63.575227 156.36649 64.137807 155.803909 \nC 64.700388 155.241328 65.016487 154.478198 65.016487 153.682589 \nC 65.016487 152.88698 64.700388 152.123849 64.137807 151.561268 \nC 63.575227 150.998688 62.812096 150.682589 62.016487 150.682589 \nC 61.220878 150.682589 60.457747 150.998688 59.895167 151.561268 \nC 59.332586 152.123849 59.016487 152.88698 59.016487 153.682589 \nC 59.016487 154.478198 59.332586 155.241328 59.895167 155.803909 \nC 60.457747 156.36649 61.220878 156.682589 62.016487 156.682589 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 104.987292 \nC 66.528841 104.987292 67.291971 104.671193 67.854552 104.108613 \nC 68.417132 103.546032 68.733231 102.782901 68.733231 101.987292 \nC 68.733231 101.191683 68.417132 100.428553 67.854552 99.865972 \nC 67.291971 99.303391 66.528841 98.987292 65.733231 98.987292 \nC 64.937622 98.987292 64.174492 99.303391 63.611911 99.865972 \nC 63.04933 100.428553 62.733231 101.191683 62.733231 101.987292 \nC 62.733231 102.782901 63.04933 103.546032 63.611911 104.108613 \nC 64.174492 104.671193 64.937622 104.987292 65.733231 104.987292 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 106.031798 \nC 81.027823 106.031798 81.790954 105.715699 82.353534 105.153118 \nC 82.916115 104.590538 83.232214 103.827407 83.232214 103.031798 \nC 83.232214 102.236189 82.916115 101.473058 82.353534 100.910478 \nC 81.790954 100.347897 81.027823 100.031798 80.232214 100.031798 \nC 79.436605 100.031798 78.673474 100.347897 78.110894 100.910478 \nC 77.548313 101.473058 77.232214 102.236189 77.232214 103.031798 \nC 77.232214 103.827407 77.548313 104.590538 78.110894 105.153118 \nC 78.673474 105.715699 79.436605 106.031798 80.232214 106.031798 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 105.928122 \nC 138.287765 105.928122 139.050895 105.612023 139.613476 105.049443 \nC 140.176056 104.486862 140.492155 103.723732 140.492155 102.928122 \nC 140.492155 102.132513 140.176056 101.369383 139.613476 100.806802 \nC 139.050895 100.244221 138.287765 99.928122 137.492155 99.928122 \nC 136.696546 99.928122 135.933416 100.244221 135.370835 100.806802 \nC 134.808254 101.369383 134.492155 102.132513 134.492155 102.928122 \nC 134.492155 103.723732 134.808254 104.486862 135.370835 105.049443 \nC 135.933416 105.612023 136.696546 105.928122 137.492155 105.928122 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 155.161832 \nC 365.855552 155.161832 366.618683 154.845733 367.181264 154.283153 \nC 367.743844 153.720572 368.059943 152.957442 368.059943 152.161832 \nC 368.059943 151.366223 367.743844 150.603093 367.181264 150.040512 \nC 366.618683 149.477931 365.855552 149.161832 365.059943 149.161832 \nC 364.264334 149.161832 363.501204 149.477931 362.938623 150.040512 \nC 362.376042 150.603093 362.059943 151.366223 362.059943 152.161832 \nC 362.059943 152.957442 362.376042 153.720572 362.938623 154.283153 \nC 363.501204 154.845733 364.264334 155.161832 365.059943 155.161832 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 106.366929 \nC 61.671313 106.366929 62.434444 106.05083 62.997024 105.488249 \nC 63.559605 104.925668 63.875704 104.162538 63.875704 103.366929 \nC 63.875704 102.571319 63.559605 101.808189 62.997024 101.245608 \nC 62.434444 100.683027 61.671313 100.366929 60.875704 100.366929 \nC 60.080095 100.366929 59.316965 100.683027 58.754384 101.245608 \nC 58.191803 101.808189 57.875704 102.571319 57.875704 103.366929 \nC 57.875704 104.162538 58.191803 104.925668 58.754384 105.488249 \nC 59.316965 106.05083 60.080095 106.366929 60.875704 106.366929 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 191.653726 \nC 61.836911 191.653726 62.600041 191.337627 63.162622 190.775046 \nC 63.725203 190.212466 64.041302 189.449335 64.041302 188.653726 \nC 64.041302 187.858117 63.725203 187.094986 63.162622 186.532406 \nC 62.600041 185.969825 61.836911 185.653726 61.041302 185.653726 \nC 60.245692 185.653726 59.482562 185.969825 58.919981 186.532406 \nC 58.357401 187.094986 58.041302 187.858117 58.041302 188.653726 \nC 58.041302 189.449335 58.357401 190.212466 58.919981 190.775046 \nC 59.482562 191.337627 60.245692 191.653726 61.041302 191.653726 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 144.685239 \nC 62.168106 144.685239 62.931236 144.36914 63.493817 143.806559 \nC 64.056398 143.243979 64.372497 142.480848 64.372497 141.685239 \nC 64.372497 140.88963 64.056398 140.1265 63.493817 139.563919 \nC 62.931236 139.001338 62.168106 138.685239 61.372497 138.685239 \nC 60.576887 138.685239 59.813757 139.001338 59.251176 139.563919 \nC 58.688596 140.1265 58.372497 140.88963 58.372497 141.685239 \nC 58.372497 142.480848 58.688596 143.243979 59.251176 143.806559 \nC 59.813757 144.36914 60.576887 144.685239 61.372497 144.685239 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 164.423508 \nC 62.830496 164.423508 63.593626 164.107409 64.156207 163.544829 \nC 64.718788 162.982248 65.034887 162.219118 65.034887 161.423508 \nC 65.034887 160.627899 64.718788 159.864769 64.156207 159.302188 \nC 63.593626 158.739607 62.830496 158.423508 62.034887 158.423508 \nC 61.239277 158.423508 60.476147 158.739607 59.913566 159.302188 \nC 59.350986 159.864769 59.034887 160.627899 59.034887 161.423508 \nC 59.034887 162.219118 59.350986 162.982248 59.913566 163.544829 \nC 60.476147 164.107409 61.239277 164.423508 62.034887 164.423508 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 105.242723 \nC 64.155276 105.242723 64.918406 104.926624 65.480987 104.364044 \nC 66.043568 103.801463 66.359667 103.038333 66.359667 102.242723 \nC 66.359667 101.447114 66.043568 100.683984 65.480987 100.121403 \nC 64.918406 99.558822 64.155276 99.242723 63.359667 99.242723 \nC 62.564058 99.242723 61.800927 99.558822 61.238347 100.121403 \nC 60.675766 100.683984 60.359667 101.447114 60.359667 102.242723 \nC 60.359667 103.038333 60.675766 103.801463 61.238347 104.364044 \nC 61.800927 104.926624 62.564058 105.242723 63.359667 105.242723 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 149.799047 \nC 66.804836 149.799047 67.567967 149.482948 68.130547 148.920367 \nC 68.693128 148.357787 69.009227 147.594656 69.009227 146.799047 \nC 69.009227 146.003438 68.693128 145.240307 68.130547 144.677727 \nC 67.567967 144.115146 66.804836 143.799047 66.009227 143.799047 \nC 65.213618 143.799047 64.450488 144.115146 63.887907 144.677727 \nC 63.325326 145.240307 63.009227 146.003438 63.009227 146.799047 \nC 63.009227 147.594656 63.325326 148.357787 63.887907 148.920367 \nC 64.450488 149.482948 65.213618 149.799047 66.009227 149.799047 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 154.427843 \nC 72.103957 154.427843 72.867087 154.111744 73.429668 153.549163 \nC 73.992249 152.986582 74.308348 152.223452 74.308348 151.427843 \nC 74.308348 150.632234 73.992249 149.869103 73.429668 149.306522 \nC 72.867087 148.743942 72.103957 148.427843 71.308348 148.427843 \nC 70.512738 148.427843 69.749608 148.743942 69.187027 149.306522 \nC 68.624447 149.869103 68.308348 150.632234 68.308348 151.427843 \nC 68.308348 152.223452 68.624447 152.986582 69.187027 153.549163 \nC 69.749608 154.111744 70.512738 154.427843 71.308348 154.427843 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 107.698536 \nC 61.836911 107.698536 62.600041 107.382437 63.162622 106.819856 \nC 63.725203 106.257276 64.041302 105.494145 64.041302 104.698536 \nC 64.041302 103.902927 63.725203 103.139796 63.162622 102.577216 \nC 62.600041 102.014635 61.836911 101.698536 61.041302 101.698536 \nC 60.245692 101.698536 59.482562 102.014635 58.919981 102.577216 \nC 58.357401 103.139796 58.041302 103.902927 58.041302 104.698536 \nC 58.041302 105.494145 58.357401 106.257276 58.919981 106.819856 \nC 59.482562 107.382437 60.245692 107.698536 61.041302 107.698536 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 189.657811 \nC 62.812096 189.657811 63.575227 189.341712 64.137807 188.779132 \nC 64.700388 188.216551 65.016487 187.45342 65.016487 186.657811 \nC 65.016487 185.862202 64.700388 185.099072 64.137807 184.536491 \nC 63.575227 183.97391 62.812096 183.657811 62.016487 183.657811 \nC 61.220878 183.657811 60.457747 183.97391 59.895167 184.536491 \nC 59.332586 185.099072 59.016487 185.862202 59.016487 186.657811 \nC 59.016487 187.45342 59.332586 188.216551 59.895167 188.779132 \nC 60.457747 189.341712 61.220878 189.657811 62.016487 189.657811 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 115.293697 \nC 66.528841 115.293697 67.291971 114.977598 67.854552 114.415018 \nC 68.417132 113.852437 68.733231 113.089307 68.733231 112.293697 \nC 68.733231 111.498088 68.417132 110.734958 67.854552 110.172377 \nC 67.291971 109.609796 66.528841 109.293697 65.733231 109.293697 \nC 64.937622 109.293697 64.174492 109.609796 63.611911 110.172377 \nC 63.04933 110.734958 62.733231 111.498088 62.733231 112.293697 \nC 62.733231 113.089307 63.04933 113.852437 63.611911 114.415018 \nC 64.174492 114.977598 64.937622 115.293697 65.733231 115.293697 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 155.180699 \nC 81.027823 155.180699 81.790954 154.8646 82.353534 154.302019 \nC 82.916115 153.739439 83.232214 152.976308 83.232214 152.180699 \nC 83.232214 151.38509 82.916115 150.621959 82.353534 150.059379 \nC 81.790954 149.496798 81.027823 149.180699 80.232214 149.180699 \nC 79.436605 149.180699 78.673474 149.496798 78.110894 150.059379 \nC 77.548313 150.621959 77.232214 151.38509 77.232214 152.180699 \nC 77.232214 152.976308 77.548313 153.739439 78.110894 154.302019 \nC 78.673474 154.8646 79.436605 155.180699 80.232214 155.180699 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 107.127851 \nC 138.287765 107.127851 139.050895 106.811753 139.613476 106.249172 \nC 140.176056 105.686591 140.492155 104.923461 140.492155 104.127851 \nC 140.492155 103.332242 140.176056 102.569112 139.613476 102.006531 \nC 139.050895 101.44395 138.287765 101.127851 137.492155 101.127851 \nC 136.696546 101.127851 135.933416 101.44395 135.370835 102.006531 \nC 134.808254 102.569112 134.492155 103.332242 134.492155 104.127851 \nC 134.492155 104.923461 134.808254 105.686591 135.370835 106.249172 \nC 135.933416 106.811753 136.696546 107.127851 137.492155 107.127851 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 123.881611 \nC 365.855552 123.881611 366.618683 123.565513 367.181264 123.002932 \nC 367.743844 122.440351 368.059943 121.677221 368.059943 120.881611 \nC 368.059943 120.086002 367.743844 119.322872 367.181264 118.760291 \nC 366.618683 118.19771 365.855552 117.881611 365.059943 117.881611 \nC 364.264334 117.881611 363.501204 118.19771 362.938623 118.760291 \nC 362.376042 119.322872 362.059943 120.086002 362.059943 120.881611 \nC 362.059943 121.677221 362.376042 122.440351 362.938623 123.002932 \nC 363.501204 123.565513 364.264334 123.881611 365.059943 123.881611 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 185.977146 \nC 61.671313 185.977146 62.434444 185.661047 62.997024 185.098466 \nC 63.559605 184.535885 63.875704 183.772755 63.875704 182.977146 \nC 63.875704 182.181536 63.559605 181.418406 62.997024 180.855825 \nC 62.434444 180.293244 61.671313 179.977146 60.875704 179.977146 \nC 60.080095 179.977146 59.316965 180.293244 58.754384 180.855825 \nC 58.191803 181.418406 57.875704 182.181536 57.875704 182.977146 \nC 57.875704 183.772755 58.191803 184.535885 58.754384 185.098466 \nC 59.316965 185.661047 60.080095 185.977146 60.875704 185.977146 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 168.299422 \nC 61.836911 168.299422 62.600041 167.983323 63.162622 167.420743 \nC 63.725203 166.858162 64.041302 166.095032 64.041302 165.299422 \nC 64.041302 164.503813 63.725203 163.740683 63.162622 163.178102 \nC 62.600041 162.615521 61.836911 162.299422 61.041302 162.299422 \nC 60.245692 162.299422 59.482562 162.615521 58.919981 163.178102 \nC 58.357401 163.740683 58.041302 164.503813 58.041302 165.299422 \nC 58.041302 166.095032 58.357401 166.858162 58.919981 167.420743 \nC 59.482562 167.983323 60.245692 168.299422 61.041302 168.299422 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 177.996535 \nC 62.168106 177.996535 62.931236 177.680436 63.493817 177.117855 \nC 64.056398 176.555275 64.372497 175.792144 64.372497 174.996535 \nC 64.372497 174.200926 64.056398 173.437795 63.493817 172.875215 \nC 62.931236 172.312634 62.168106 171.996535 61.372497 171.996535 \nC 60.576887 171.996535 59.813757 172.312634 59.251176 172.875215 \nC 58.688596 173.437795 58.372497 174.200926 58.372497 174.996535 \nC 58.372497 175.792144 58.688596 176.555275 59.251176 177.117855 \nC 59.813757 177.680436 60.576887 177.996535 61.372497 177.996535 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 195.704501 \nC 62.830496 195.704501 63.593626 195.388402 64.156207 194.825822 \nC 64.718788 194.263241 65.034887 193.500111 65.034887 192.704501 \nC 65.034887 191.908892 64.718788 191.145762 64.156207 190.583181 \nC 63.593626 190.0206 62.830496 189.704501 62.034887 189.704501 \nC 61.239277 189.704501 60.476147 190.0206 59.913566 190.583181 \nC 59.350986 191.145762 59.034887 191.908892 59.034887 192.704501 \nC 59.034887 193.500111 59.350986 194.263241 59.913566 194.825822 \nC 60.476147 195.388402 61.239277 195.704501 62.034887 195.704501 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 105.373822 \nC 64.155276 105.373822 64.918406 105.057723 65.480987 104.495142 \nC 66.043568 103.932561 66.359667 103.169431 66.359667 102.373822 \nC 66.359667 101.578212 66.043568 100.815082 65.480987 100.252501 \nC 64.918406 99.689921 64.155276 99.373822 63.359667 99.373822 \nC 62.564058 99.373822 61.800927 99.689921 61.238347 100.252501 \nC 60.675766 100.815082 60.359667 101.578212 60.359667 102.373822 \nC 60.359667 103.169431 60.675766 103.932561 61.238347 104.495142 \nC 61.800927 105.057723 62.564058 105.373822 63.359667 105.373822 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 160.843377 \nC 66.804836 160.843377 67.567967 160.527278 68.130547 159.964697 \nC 68.693128 159.402117 69.009227 158.638986 69.009227 157.843377 \nC 69.009227 157.047768 68.693128 156.284637 68.130547 155.722057 \nC 67.567967 155.159476 66.804836 154.843377 66.009227 154.843377 \nC 65.213618 154.843377 64.450488 155.159476 63.887907 155.722057 \nC 63.325326 156.284637 63.009227 157.047768 63.009227 157.843377 \nC 63.009227 158.638986 63.325326 159.402117 63.887907 159.964697 \nC 64.450488 160.527278 65.213618 160.843377 66.009227 160.843377 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 153.643908 \nC 72.103957 153.643908 72.867087 153.327809 73.429668 152.765228 \nC 73.992249 152.202647 74.308348 151.439517 74.308348 150.643908 \nC 74.308348 149.848298 73.992249 149.085168 73.429668 148.522587 \nC 72.867087 147.960007 72.103957 147.643908 71.308348 147.643908 \nC 70.512738 147.643908 69.749608 147.960007 69.187027 148.522587 \nC 68.624447 149.085168 68.308348 149.848298 68.308348 150.643908 \nC 68.308348 151.439517 68.624447 152.202647 69.187027 152.765228 \nC 69.749608 153.327809 70.512738 153.643908 71.308348 153.643908 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 101.562066 \nC 61.491916 101.562066 62.255046 101.245967 62.817627 100.683386 \nC 63.380208 100.120805 63.696307 99.357675 63.696307 98.562066 \nC 63.696307 97.766456 63.380208 97.003326 62.817627 96.440745 \nC 62.255046 95.878165 61.491916 95.562066 60.696307 95.562066 \nC 59.900698 95.562066 59.137567 95.878165 58.574986 96.440745 \nC 58.012406 97.003326 57.696307 97.766456 57.696307 98.562066 \nC 57.696307 99.357675 58.012406 100.120805 58.574986 100.683386 \nC 59.137567 101.245967 59.900698 101.562066 60.696307 101.562066 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 106.354908 \nC 61.570115 106.354908 62.333245 106.03881 62.895826 105.476229 \nC 63.458407 104.913648 63.774506 104.150518 63.774506 103.354908 \nC 63.774506 102.559299 63.458407 101.796169 62.895826 101.233588 \nC 62.333245 100.671007 61.570115 100.354908 60.774506 100.354908 \nC 59.978896 100.354908 59.215766 100.671007 58.653185 101.233588 \nC 58.090605 101.796169 57.774506 102.559299 57.774506 103.354908 \nC 57.774506 104.150518 58.090605 104.913648 58.653185 105.476229 \nC 59.215766 106.03881 59.978896 106.354908 60.774506 106.354908 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 148.850188 \nC 61.836911 148.850188 62.600041 148.534089 63.162622 147.971508 \nC 63.725203 147.408928 64.041302 146.645797 64.041302 145.850188 \nC 64.041302 145.054579 63.725203 144.291448 63.162622 143.728868 \nC 62.600041 143.166287 61.836911 142.850188 61.041302 142.850188 \nC 60.245692 142.850188 59.482562 143.166287 58.919981 143.728868 \nC 58.357401 144.291448 58.041302 145.054579 58.041302 145.850188 \nC 58.041302 146.645797 58.357401 147.408928 58.919981 147.971508 \nC 59.482562 148.534089 60.245692 148.850188 61.041302 148.850188 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 105.692137 \nC 62.812096 105.692137 63.575227 105.376038 64.137807 104.813458 \nC 64.700388 104.250877 65.016487 103.487747 65.016487 102.692137 \nC 65.016487 101.896528 64.700388 101.133398 64.137807 100.570817 \nC 63.575227 100.008236 62.812096 99.692137 62.016487 99.692137 \nC 61.220878 99.692137 60.457747 100.008236 59.895167 100.570817 \nC 59.332586 101.133398 59.016487 101.896528 59.016487 102.692137 \nC 59.016487 103.487747 59.332586 104.250877 59.895167 104.813458 \nC 60.457747 105.376038 61.220878 105.692137 62.016487 105.692137 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 104.62609 \nC 66.528841 104.62609 67.291971 104.309991 67.854552 103.74741 \nC 68.417132 103.18483 68.733231 102.421699 68.733231 101.62609 \nC 68.733231 100.830481 68.417132 100.06735 67.854552 99.50477 \nC 67.291971 98.942189 66.528841 98.62609 65.733231 98.62609 \nC 64.937622 98.62609 64.174492 98.942189 63.611911 99.50477 \nC 63.04933 100.06735 62.733231 100.830481 62.733231 101.62609 \nC 62.733231 102.421699 63.04933 103.18483 63.611911 103.74741 \nC 64.174492 104.309991 64.937622 104.62609 65.733231 104.62609 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 102.884411 \nC 81.027823 102.884411 81.790954 102.568312 82.353534 102.005732 \nC 82.916115 101.443151 83.232214 100.680021 83.232214 99.884411 \nC 83.232214 99.088802 82.916115 98.325672 82.353534 97.763091 \nC 81.790954 97.20051 81.027823 96.884411 80.232214 96.884411 \nC 79.436605 96.884411 78.673474 97.20051 78.110894 97.763091 \nC 77.548313 98.325672 77.232214 99.088802 77.232214 99.884411 \nC 77.232214 100.680021 77.548313 101.443151 78.110894 102.005732 \nC 78.673474 102.568312 79.436605 102.884411 80.232214 102.884411 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 161.233493 \nC 138.287765 161.233493 139.050895 160.917394 139.613476 160.354813 \nC 140.176056 159.792232 140.492155 159.029102 140.492155 158.233493 \nC 140.492155 157.437883 140.176056 156.674753 139.613476 156.112172 \nC 139.050895 155.549592 138.287765 155.233493 137.492155 155.233493 \nC 136.696546 155.233493 135.933416 155.549592 135.370835 156.112172 \nC 134.808254 156.674753 134.492155 157.437883 134.492155 158.233493 \nC 134.492155 159.029102 134.808254 159.792232 135.370835 160.354813 \nC 135.933416 160.917394 136.696546 161.233493 137.492155 161.233493 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 103.835874 \nC 365.855552 103.835874 366.618683 103.519775 367.181264 102.957194 \nC 367.743844 102.394613 368.059943 101.631483 368.059943 100.835874 \nC 368.059943 100.040265 367.743844 99.277134 367.181264 98.714554 \nC 366.618683 98.151973 365.855552 97.835874 365.059943 97.835874 \nC 364.264334 97.835874 363.501204 98.151973 362.938623 98.714554 \nC 362.376042 99.277134 362.059943 100.040265 362.059943 100.835874 \nC 362.059943 101.631483 362.376042 102.394613 362.938623 102.957194 \nC 363.501204 103.519775 364.264334 103.835874 365.059943 103.835874 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 164.801958 \nC 61.671313 164.801958 62.434444 164.485859 62.997024 163.923278 \nC 63.559605 163.360698 63.875704 162.597567 63.875704 161.801958 \nC 63.875704 161.006349 63.559605 160.243218 62.997024 159.680638 \nC 62.434444 159.118057 61.671313 158.801958 60.875704 158.801958 \nC 60.080095 158.801958 59.316965 159.118057 58.754384 159.680638 \nC 58.191803 160.243218 57.875704 161.006349 57.875704 161.801958 \nC 57.875704 162.597567 58.191803 163.360698 58.754384 163.923278 \nC 59.316965 164.485859 60.080095 164.801958 60.875704 164.801958 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 146.017747 \nC 61.836911 146.017747 62.600041 145.701648 63.162622 145.139067 \nC 63.725203 144.576486 64.041302 143.813356 64.041302 143.017747 \nC 64.041302 142.222137 63.725203 141.459007 63.162622 140.896426 \nC 62.600041 140.333846 61.836911 140.017747 61.041302 140.017747 \nC 60.245692 140.017747 59.482562 140.333846 58.919981 140.896426 \nC 58.357401 141.459007 58.041302 142.222137 58.041302 143.017747 \nC 58.041302 143.813356 58.357401 144.576486 58.919981 145.139067 \nC 59.482562 145.701648 60.245692 146.017747 61.041302 146.017747 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 183.73225 \nC 62.168106 183.73225 62.931236 183.416151 63.493817 182.85357 \nC 64.056398 182.290989 64.372497 181.527859 64.372497 180.73225 \nC 64.372497 179.93664 64.056398 179.17351 63.493817 178.610929 \nC 62.931236 178.048349 62.168106 177.73225 61.372497 177.73225 \nC 60.576887 177.73225 59.813757 178.048349 59.251176 178.610929 \nC 58.688596 179.17351 58.372497 179.93664 58.372497 180.73225 \nC 58.372497 181.527859 58.688596 182.290989 59.251176 182.85357 \nC 59.813757 183.416151 60.576887 183.73225 61.372497 183.73225 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 107.160266 \nC 62.830496 107.160266 63.593626 106.844168 64.156207 106.281587 \nC 64.718788 105.719006 65.034887 104.955876 65.034887 104.160266 \nC 65.034887 103.364657 64.718788 102.601527 64.156207 102.038946 \nC 63.593626 101.476365 62.830496 101.160266 62.034887 101.160266 \nC 61.239277 101.160266 60.476147 101.476365 59.913566 102.038946 \nC 59.350986 102.601527 59.034887 103.364657 59.034887 104.160266 \nC 59.034887 104.955876 59.350986 105.719006 59.913566 106.281587 \nC 60.476147 106.844168 61.239277 107.160266 62.034887 107.160266 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 105.629075 \nC 64.155276 105.629075 64.918406 105.312976 65.480987 104.750395 \nC 66.043568 104.187814 66.359667 103.424684 66.359667 102.629075 \nC 66.359667 101.833465 66.043568 101.070335 65.480987 100.507754 \nC 64.918406 99.945174 64.155276 99.629075 63.359667 99.629075 \nC 62.564058 99.629075 61.800927 99.945174 61.238347 100.507754 \nC 60.675766 101.070335 60.359667 101.833465 60.359667 102.629075 \nC 60.359667 103.424684 60.675766 104.187814 61.238347 104.750395 \nC 61.800927 105.312976 62.564058 105.629075 63.359667 105.629075 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 128.896224 \nC 66.804836 128.896224 67.567967 128.580125 68.130547 128.017544 \nC 68.693128 127.454963 69.009227 126.691833 69.009227 125.896224 \nC 69.009227 125.100614 68.693128 124.337484 68.130547 123.774903 \nC 67.567967 123.212323 66.804836 122.896224 66.009227 122.896224 \nC 65.213618 122.896224 64.450488 123.212323 63.887907 123.774903 \nC 63.325326 124.337484 63.009227 125.100614 63.009227 125.896224 \nC 63.009227 126.691833 63.325326 127.454963 63.887907 128.017544 \nC 64.450488 128.580125 65.213618 128.896224 66.009227 128.896224 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 107.43912 \nC 72.103957 107.43912 72.867087 107.123021 73.429668 106.56044 \nC 73.992249 105.99786 74.308348 105.234729 74.308348 104.43912 \nC 74.308348 103.643511 73.992249 102.88038 73.429668 102.3178 \nC 72.867087 101.755219 72.103957 101.43912 71.308348 101.43912 \nC 70.512738 101.43912 69.749608 101.755219 69.187027 102.3178 \nC 68.624447 102.88038 68.308348 103.643511 68.308348 104.43912 \nC 68.308348 105.234729 68.624447 105.99786 69.187027 106.56044 \nC 69.749608 107.123021 70.512738 107.43912 71.308348 107.43912 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 159.979555 \nC 61.570115 159.979555 62.333245 159.663456 62.895826 159.100876 \nC 63.458407 158.538295 63.774506 157.775165 63.774506 156.979555 \nC 63.774506 156.183946 63.458407 155.420816 62.895826 154.858235 \nC 62.333245 154.295654 61.570115 153.979555 60.774506 153.979555 \nC 59.978896 153.979555 59.215766 154.295654 58.653185 154.858235 \nC 58.090605 155.420816 57.774506 156.183946 57.774506 156.979555 \nC 57.774506 157.775165 58.090605 158.538295 58.653185 159.100876 \nC 59.215766 159.663456 59.978896 159.979555 60.774506 159.979555 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 201.943562 \nC 61.836911 201.943562 62.600041 201.627463 63.162622 201.064882 \nC 63.725203 200.502302 64.041302 199.739171 64.041302 198.943562 \nC 64.041302 198.147953 63.725203 197.384823 63.162622 196.822242 \nC 62.600041 196.259661 61.836911 195.943562 61.041302 195.943562 \nC 60.245692 195.943562 59.482562 196.259661 58.919981 196.822242 \nC 58.357401 197.384823 58.041302 198.147953 58.041302 198.943562 \nC 58.041302 199.739171 58.357401 200.502302 58.919981 201.064882 \nC 59.482562 201.627463 60.245692 201.943562 61.041302 201.943562 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 207.112744 \nC 62.812096 207.112744 63.575227 206.796646 64.137807 206.234065 \nC 64.700388 205.671484 65.016487 204.908354 65.016487 204.112744 \nC 65.016487 203.317135 64.700388 202.554005 64.137807 201.991424 \nC 63.575227 201.428843 62.812096 201.112744 62.016487 201.112744 \nC 61.220878 201.112744 60.457747 201.428843 59.895167 201.991424 \nC 59.332586 202.554005 59.016487 203.317135 59.016487 204.112744 \nC 59.016487 204.908354 59.332586 205.671484 59.895167 206.234065 \nC 60.457747 206.796646 61.220878 207.112744 62.016487 207.112744 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 179.561333 \nC 66.528841 179.561333 67.291971 179.245234 67.854552 178.682654 \nC 68.417132 178.120073 68.733231 177.356943 68.733231 176.561333 \nC 68.733231 175.765724 68.417132 175.002594 67.854552 174.440013 \nC 67.291971 173.877432 66.528841 173.561333 65.733231 173.561333 \nC 64.937622 173.561333 64.174492 173.877432 63.611911 174.440013 \nC 63.04933 175.002594 62.733231 175.765724 62.733231 176.561333 \nC 62.733231 177.356943 63.04933 178.120073 63.611911 178.682654 \nC 64.174492 179.245234 64.937622 179.561333 65.733231 179.561333 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 156.486668 \nC 81.027823 156.486668 81.790954 156.170569 82.353534 155.607988 \nC 82.916115 155.045408 83.232214 154.282277 83.232214 153.486668 \nC 83.232214 152.691059 82.916115 151.927928 82.353534 151.365348 \nC 81.790954 150.802767 81.027823 150.486668 80.232214 150.486668 \nC 79.436605 150.486668 78.673474 150.802767 78.110894 151.365348 \nC 77.548313 151.927928 77.232214 152.691059 77.232214 153.486668 \nC 77.232214 154.282277 77.548313 155.045408 78.110894 155.607988 \nC 78.673474 156.170569 79.436605 156.486668 80.232214 156.486668 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 104.160801 \nC 138.287765 104.160801 139.050895 103.844702 139.613476 103.282121 \nC 140.176056 102.71954 140.492155 101.95641 140.492155 101.160801 \nC 140.492155 100.365191 140.176056 99.602061 139.613476 99.03948 \nC 139.050895 98.4769 138.287765 98.160801 137.492155 98.160801 \nC 136.696546 98.160801 135.933416 98.4769 135.370835 99.03948 \nC 134.808254 99.602061 134.492155 100.365191 134.492155 101.160801 \nC 134.492155 101.95641 134.808254 102.71954 135.370835 103.282121 \nC 135.933416 103.844702 136.696546 104.160801 137.492155 104.160801 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 105.061367 \nC 365.855552 105.061367 366.618683 104.745268 367.181264 104.182687 \nC 367.743844 103.620106 368.059943 102.856976 368.059943 102.061367 \nC 368.059943 101.265757 367.743844 100.502627 367.181264 99.940046 \nC 366.618683 99.377465 365.855552 99.061367 365.059943 99.061367 \nC 364.264334 99.061367 363.501204 99.377465 362.938623 99.940046 \nC 362.376042 100.502627 362.059943 101.265757 362.059943 102.061367 \nC 362.059943 102.856976 362.376042 103.620106 362.938623 104.182687 \nC 363.501204 104.745268 364.264334 105.061367 365.059943 105.061367 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 167.906323 \nC 61.671313 167.906323 62.434444 167.590224 62.997024 167.027643 \nC 63.559605 166.465062 63.875704 165.701932 63.875704 164.906323 \nC 63.875704 164.110713 63.559605 163.347583 62.997024 162.785002 \nC 62.434444 162.222422 61.671313 161.906323 60.875704 161.906323 \nC 60.080095 161.906323 59.316965 162.222422 58.754384 162.785002 \nC 58.191803 163.347583 57.875704 164.110713 57.875704 164.906323 \nC 57.875704 165.701932 58.191803 166.465062 58.754384 167.027643 \nC 59.316965 167.590224 60.080095 167.906323 60.875704 167.906323 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 195.972454 \nC 61.836911 195.972454 62.600041 195.656355 63.162622 195.093774 \nC 63.725203 194.531193 64.041302 193.768063 64.041302 192.972454 \nC 64.041302 192.176845 63.725203 191.413714 63.162622 190.851133 \nC 62.600041 190.288553 61.836911 189.972454 61.041302 189.972454 \nC 60.245692 189.972454 59.482562 190.288553 58.919981 190.851133 \nC 58.357401 191.413714 58.041302 192.176845 58.041302 192.972454 \nC 58.041302 193.768063 58.357401 194.531193 58.919981 195.093774 \nC 59.482562 195.656355 60.245692 195.972454 61.041302 195.972454 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 208.98736 \nC 62.168106 208.98736 62.931236 208.671262 63.493817 208.108681 \nC 64.056398 207.5461 64.372497 206.78297 64.372497 205.98736 \nC 64.372497 205.191751 64.056398 204.428621 63.493817 203.86604 \nC 62.931236 203.303459 62.168106 202.98736 61.372497 202.98736 \nC 60.576887 202.98736 59.813757 203.303459 59.251176 203.86604 \nC 58.688596 204.428621 58.372497 205.191751 58.372497 205.98736 \nC 58.372497 206.78297 58.688596 207.5461 59.251176 208.108681 \nC 59.813757 208.671262 60.576887 208.98736 61.372497 208.98736 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 156.553436 \nC 64.155276 156.553436 64.918406 156.237337 65.480987 155.674756 \nC 66.043568 155.112176 66.359667 154.349045 66.359667 153.553436 \nC 66.359667 152.757827 66.043568 151.994696 65.480987 151.432116 \nC 64.918406 150.869535 64.155276 150.553436 63.359667 150.553436 \nC 62.564058 150.553436 61.800927 150.869535 61.238347 151.432116 \nC 60.675766 151.994696 60.359667 152.757827 60.359667 153.553436 \nC 60.359667 154.349045 60.675766 155.112176 61.238347 155.674756 \nC 61.800927 156.237337 62.564058 156.553436 63.359667 156.553436 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 150.168039 \nC 66.804836 150.168039 67.567967 149.85194 68.130547 149.289359 \nC 68.693128 148.726778 69.009227 147.963648 69.009227 147.168039 \nC 69.009227 146.37243 68.693128 145.609299 68.130547 145.046718 \nC 67.567967 144.484138 66.804836 144.168039 66.009227 144.168039 \nC 65.213618 144.168039 64.450488 144.484138 63.887907 145.046718 \nC 63.325326 145.609299 63.009227 146.37243 63.009227 147.168039 \nC 63.009227 147.963648 63.325326 148.726778 63.887907 149.289359 \nC 64.450488 149.85194 65.213618 150.168039 66.009227 150.168039 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.696307 155.01325 \nC 61.491916 155.01325 62.255046 154.697151 62.817627 154.13457 \nC 63.380208 153.57199 63.696307 152.808859 63.696307 152.01325 \nC 63.696307 151.217641 63.380208 150.454511 62.817627 149.89193 \nC 62.255046 149.329349 61.491916 149.01325 60.696307 149.01325 \nC 59.900698 149.01325 59.137567 149.329349 58.574986 149.89193 \nC 58.012406 150.454511 57.696307 151.217641 57.696307 152.01325 \nC 57.696307 152.808859 58.012406 153.57199 58.574986 154.13457 \nC 59.137567 154.697151 59.900698 155.01325 60.696307 155.01325 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 155.910788 \nC 61.570115 155.910788 62.333245 155.594689 62.895826 155.032108 \nC 63.458407 154.469527 63.774506 153.706397 63.774506 152.910788 \nC 63.774506 152.115178 63.458407 151.352048 62.895826 150.789467 \nC 62.333245 150.226886 61.570115 149.910788 60.774506 149.910788 \nC 59.978896 149.910788 59.215766 150.226886 58.653185 150.789467 \nC 58.090605 151.352048 57.774506 152.115178 57.774506 152.910788 \nC 57.774506 153.706397 58.090605 154.469527 58.653185 155.032108 \nC 59.215766 155.594689 59.978896 155.910788 60.774506 155.910788 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 161.788877 \nC 61.836911 161.788877 62.600041 161.472778 63.162622 160.910197 \nC 63.725203 160.347616 64.041302 159.584486 64.041302 158.788877 \nC 64.041302 157.993267 63.725203 157.230137 63.162622 156.667556 \nC 62.600041 156.104975 61.836911 155.788877 61.041302 155.788877 \nC 60.245692 155.788877 59.482562 156.104975 58.919981 156.667556 \nC 58.357401 157.230137 58.041302 157.993267 58.041302 158.788877 \nC 58.041302 159.584486 58.357401 160.347616 58.919981 160.910197 \nC 59.482562 161.472778 60.245692 161.788877 61.041302 161.788877 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 105.26866 \nC 62.812096 105.26866 63.575227 104.952562 64.137807 104.389981 \nC 64.700388 103.8274 65.016487 103.06427 65.016487 102.26866 \nC 65.016487 101.473051 64.700388 100.709921 64.137807 100.14734 \nC 63.575227 99.584759 62.812096 99.26866 62.016487 99.26866 \nC 61.220878 99.26866 60.457747 99.584759 59.895167 100.14734 \nC 59.332586 100.709921 59.016487 101.473051 59.016487 102.26866 \nC 59.016487 103.06427 59.332586 103.8274 59.895167 104.389981 \nC 60.457747 104.952562 61.220878 105.26866 62.016487 105.26866 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 154.406496 \nC 66.528841 154.406496 67.291971 154.090397 67.854552 153.527816 \nC 68.417132 152.965236 68.733231 152.202105 68.733231 151.406496 \nC 68.733231 150.610887 68.417132 149.847757 67.854552 149.285176 \nC 67.291971 148.722595 66.528841 148.406496 65.733231 148.406496 \nC 64.937622 148.406496 64.174492 148.722595 63.611911 149.285176 \nC 63.04933 149.847757 62.733231 150.610887 62.733231 151.406496 \nC 62.733231 152.202105 63.04933 152.965236 63.611911 153.527816 \nC 64.174492 154.090397 64.937622 154.406496 65.733231 154.406496 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 145.564816 \nC 81.027823 145.564816 81.790954 145.248717 82.353534 144.686136 \nC 82.916115 144.123556 83.232214 143.360425 83.232214 142.564816 \nC 83.232214 141.769207 82.916115 141.006076 82.353534 140.443496 \nC 81.790954 139.880915 81.027823 139.564816 80.232214 139.564816 \nC 79.436605 139.564816 78.673474 139.880915 78.110894 140.443496 \nC 77.548313 141.006076 77.232214 141.769207 77.232214 142.564816 \nC 77.232214 143.360425 77.548313 144.123556 78.110894 144.686136 \nC 78.673474 145.248717 79.436605 145.564816 80.232214 145.564816 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 106.630454 \nC 138.287765 106.630454 139.050895 106.314355 139.613476 105.751775 \nC 140.176056 105.189194 140.492155 104.426064 140.492155 103.630454 \nC 140.492155 102.834845 140.176056 102.071715 139.613476 101.509134 \nC 139.050895 100.946553 138.287765 100.630454 137.492155 100.630454 \nC 136.696546 100.630454 135.933416 100.946553 135.370835 101.509134 \nC 134.808254 102.071715 134.492155 102.834845 134.492155 103.630454 \nC 134.492155 104.426064 134.808254 105.189194 135.370835 105.751775 \nC 135.933416 106.314355 136.696546 106.630454 137.492155 106.630454 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 104.674065 \nC 365.855552 104.674065 366.618683 104.357966 367.181264 103.795385 \nC 367.743844 103.232804 368.059943 102.469674 368.059943 101.674065 \nC 368.059943 100.878455 367.743844 100.115325 367.181264 99.552744 \nC 366.618683 98.990164 365.855552 98.674065 365.059943 98.674065 \nC 364.264334 98.674065 363.501204 98.990164 362.938623 99.552744 \nC 362.376042 100.115325 362.059943 100.878455 362.059943 101.674065 \nC 362.059943 102.469674 362.376042 103.232804 362.938623 103.795385 \nC 363.501204 104.357966 364.264334 104.674065 365.059943 104.674065 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 154.113453 \nC 61.671313 154.113453 62.434444 153.797354 62.997024 153.234774 \nC 63.559605 152.672193 63.875704 151.909063 63.875704 151.113453 \nC 63.875704 150.317844 63.559605 149.554714 62.997024 148.992133 \nC 62.434444 148.429552 61.671313 148.113453 60.875704 148.113453 \nC 60.080095 148.113453 59.316965 148.429552 58.754384 148.992133 \nC 58.191803 149.554714 57.875704 150.317844 57.875704 151.113453 \nC 57.875704 151.909063 58.191803 152.672193 58.754384 153.234774 \nC 59.316965 153.797354 60.080095 154.113453 60.875704 154.113453 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 105.356163 \nC 61.836911 105.356163 62.600041 105.040064 63.162622 104.477483 \nC 63.725203 103.914903 64.041302 103.151772 64.041302 102.356163 \nC 64.041302 101.560554 63.725203 100.797424 63.162622 100.234843 \nC 62.600041 99.672262 61.836911 99.356163 61.041302 99.356163 \nC 60.245692 99.356163 59.482562 99.672262 58.919981 100.234843 \nC 58.357401 100.797424 58.041302 101.560554 58.041302 102.356163 \nC 58.041302 103.151772 58.357401 103.914903 58.919981 104.477483 \nC 59.482562 105.040064 60.245692 105.356163 61.041302 105.356163 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 106.731049 \nC 62.168106 106.731049 62.931236 106.41495 63.493817 105.852369 \nC 64.056398 105.289788 64.372497 104.526658 64.372497 103.731049 \nC 64.372497 102.935439 64.056398 102.172309 63.493817 101.609728 \nC 62.931236 101.047148 62.168106 100.731049 61.372497 100.731049 \nC 60.576887 100.731049 59.813757 101.047148 59.251176 101.609728 \nC 58.688596 102.172309 58.372497 102.935439 58.372497 103.731049 \nC 58.372497 104.526658 58.688596 105.289788 59.251176 105.852369 \nC 59.813757 106.41495 60.576887 106.731049 61.372497 106.731049 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 169.169803 \nC 62.830496 169.169803 63.593626 168.853704 64.156207 168.291123 \nC 64.718788 167.728543 65.034887 166.965412 65.034887 166.169803 \nC 65.034887 165.374194 64.718788 164.611063 64.156207 164.048483 \nC 63.593626 163.485902 62.830496 163.169803 62.034887 163.169803 \nC 61.239277 163.169803 60.476147 163.485902 59.913566 164.048483 \nC 59.350986 164.611063 59.034887 165.374194 59.034887 166.169803 \nC 59.034887 166.965412 59.350986 167.728543 59.913566 168.291123 \nC 60.476147 168.853704 61.239277 169.169803 62.034887 169.169803 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 181.567805 \nC 64.155276 181.567805 64.918406 181.251706 65.480987 180.689126 \nC 66.043568 180.126545 66.359667 179.363415 66.359667 178.567805 \nC 66.359667 177.772196 66.043568 177.009066 65.480987 176.446485 \nC 64.918406 175.883904 64.155276 175.567805 63.359667 175.567805 \nC 62.564058 175.567805 61.800927 175.883904 61.238347 176.446485 \nC 60.675766 177.009066 60.359667 177.772196 60.359667 178.567805 \nC 60.359667 179.363415 60.675766 180.126545 61.238347 180.689126 \nC 61.800927 181.251706 62.564058 181.567805 63.359667 181.567805 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 165.600175 \nC 66.804836 165.600175 67.567967 165.284076 68.130547 164.721495 \nC 68.693128 164.158915 69.009227 163.395784 69.009227 162.600175 \nC 69.009227 161.804566 68.693128 161.041435 68.130547 160.478855 \nC 67.567967 159.916274 66.804836 159.600175 66.009227 159.600175 \nC 65.213618 159.600175 64.450488 159.916274 63.887907 160.478855 \nC 63.325326 161.041435 63.009227 161.804566 63.009227 162.600175 \nC 63.009227 163.395784 63.325326 164.158915 63.887907 164.721495 \nC 64.450488 165.284076 65.213618 165.600175 66.009227 165.600175 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 71.308348 104.356239 \nC 72.103957 104.356239 72.867087 104.04014 73.429668 103.477559 \nC 73.992249 102.914978 74.308348 102.151848 74.308348 101.356239 \nC 74.308348 100.56063 73.992249 99.797499 73.429668 99.234919 \nC 72.867087 98.672338 72.103957 98.356239 71.308348 98.356239 \nC 70.512738 98.356239 69.749608 98.672338 69.187027 99.234919 \nC 68.624447 99.797499 68.308348 100.56063 68.308348 101.356239 \nC 68.308348 102.151848 68.624447 102.914978 69.187027 103.477559 \nC 69.749608 104.04014 70.512738 104.356239 71.308348 104.356239 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.774506 161.808534 \nC 61.570115 161.808534 62.333245 161.492435 62.895826 160.929854 \nC 63.458407 160.367273 63.774506 159.604143 63.774506 158.808534 \nC 63.774506 158.012924 63.458407 157.249794 62.895826 156.687213 \nC 62.333245 156.124633 61.570115 155.808534 60.774506 155.808534 \nC 59.978896 155.808534 59.215766 156.124633 58.653185 156.687213 \nC 58.090605 157.249794 57.774506 158.012924 57.774506 158.808534 \nC 57.774506 159.604143 58.090605 160.367273 58.653185 160.929854 \nC 59.215766 161.492435 59.978896 161.808534 60.774506 161.808534 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.016487 204.133037 \nC 62.812096 204.133037 63.575227 203.816938 64.137807 203.254357 \nC 64.700388 202.691776 65.016487 201.928646 65.016487 201.133037 \nC 65.016487 200.337428 64.700388 199.574297 64.137807 199.011717 \nC 63.575227 198.449136 62.812096 198.133037 62.016487 198.133037 \nC 61.220878 198.133037 60.457747 198.449136 59.895167 199.011717 \nC 59.332586 199.574297 59.016487 200.337428 59.016487 201.133037 \nC 59.016487 201.928646 59.332586 202.691776 59.895167 203.254357 \nC 60.457747 203.816938 61.220878 204.133037 62.016487 204.133037 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 65.733231 112.697007 \nC 66.528841 112.697007 67.291971 112.380908 67.854552 111.818328 \nC 68.417132 111.255747 68.733231 110.492616 68.733231 109.697007 \nC 68.733231 108.901398 68.417132 108.138268 67.854552 107.575687 \nC 67.291971 107.013106 66.528841 106.697007 65.733231 106.697007 \nC 64.937622 106.697007 64.174492 107.013106 63.611911 107.575687 \nC 63.04933 108.138268 62.733231 108.901398 62.733231 109.697007 \nC 62.733231 110.492616 63.04933 111.255747 63.611911 111.818328 \nC 64.174492 112.380908 64.937622 112.697007 65.733231 112.697007 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 80.232214 168.800816 \nC 81.027823 168.800816 81.790954 168.484717 82.353534 167.922137 \nC 82.916115 167.359556 83.232214 166.596426 83.232214 165.800816 \nC 83.232214 165.005207 82.916115 164.242077 82.353534 163.679496 \nC 81.790954 163.116915 81.027823 162.800816 80.232214 162.800816 \nC 79.436605 162.800816 78.673474 163.116915 78.110894 163.679496 \nC 77.548313 164.242077 77.232214 165.005207 77.232214 165.800816 \nC 77.232214 166.596426 77.548313 167.359556 78.110894 167.922137 \nC 78.673474 168.484717 79.436605 168.800816 80.232214 168.800816 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 137.492155 157.22236 \nC 138.287765 157.22236 139.050895 156.906261 139.613476 156.34368 \nC 140.176056 155.781099 140.492155 155.017969 140.492155 154.22236 \nC 140.492155 153.42675 140.176056 152.66362 139.613476 152.101039 \nC 139.050895 151.538459 138.287765 151.22236 137.492155 151.22236 \nC 136.696546 151.22236 135.933416 151.538459 135.370835 152.101039 \nC 134.808254 152.66362 134.492155 153.42675 134.492155 154.22236 \nC 134.492155 155.017969 134.808254 155.781099 135.370835 156.34368 \nC 135.933416 156.906261 136.696546 157.22236 137.492155 157.22236 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 365.059943 105.177191 \nC 365.855552 105.177191 366.618683 104.861092 367.181264 104.298511 \nC 367.743844 103.73593 368.059943 102.9728 368.059943 102.177191 \nC 368.059943 101.381581 367.743844 100.618451 367.181264 100.05587 \nC 366.618683 99.49329 365.855552 99.177191 365.059943 99.177191 \nC 364.264334 99.177191 363.501204 99.49329 362.938623 100.05587 \nC 362.376042 100.618451 362.059943 101.381581 362.059943 102.177191 \nC 362.059943 102.9728 362.376042 103.73593 362.938623 104.298511 \nC 363.501204 104.861092 364.264334 105.177191 365.059943 105.177191 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 60.875704 205.716586 \nC 61.671313 205.716586 62.434444 205.400487 62.997024 204.837906 \nC 63.559605 204.275326 63.875704 203.512195 63.875704 202.716586 \nC 63.875704 201.920977 63.559605 201.157846 62.997024 200.595266 \nC 62.434444 200.032685 61.671313 199.716586 60.875704 199.716586 \nC 60.080095 199.716586 59.316965 200.032685 58.754384 200.595266 \nC 58.191803 201.157846 57.875704 201.920977 57.875704 202.716586 \nC 57.875704 203.512195 58.191803 204.275326 58.754384 204.837906 \nC 59.316965 205.400487 60.080095 205.716586 60.875704 205.716586 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.041302 138.487976 \nC 61.836911 138.487976 62.600041 138.171877 63.162622 137.609296 \nC 63.725203 137.046715 64.041302 136.283585 64.041302 135.487976 \nC 64.041302 134.692366 63.725203 133.929236 63.162622 133.366655 \nC 62.600041 132.804075 61.836911 132.487976 61.041302 132.487976 \nC 60.245692 132.487976 59.482562 132.804075 58.919981 133.366655 \nC 58.357401 133.929236 58.041302 134.692366 58.041302 135.487976 \nC 58.041302 136.283585 58.357401 137.046715 58.919981 137.609296 \nC 59.482562 138.171877 60.245692 138.487976 61.041302 138.487976 \nz\n\" style=\"fill:#eccdc8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 61.372497 157.699296 \nC 62.168106 157.699296 62.931236 157.383197 63.493817 156.820617 \nC 64.056398 156.258036 64.372497 155.494906 64.372497 154.699296 \nC 64.372497 153.903687 64.056398 153.140557 63.493817 152.577976 \nC 62.931236 152.015395 62.168106 151.699296 61.372497 151.699296 \nC 60.576887 151.699296 59.813757 152.015395 59.251176 152.577976 \nC 58.688596 153.140557 58.372497 153.903687 58.372497 154.699296 \nC 58.372497 155.494906 58.688596 156.258036 59.251176 156.820617 \nC 59.813757 157.383197 60.576887 157.699296 61.372497 157.699296 \nz\n\" style=\"fill:#eac7c3;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 62.034887 199.588545 \nC 62.830496 199.588545 63.593626 199.272446 64.156207 198.709865 \nC 64.718788 198.147284 65.034887 197.384154 65.034887 196.588545 \nC 65.034887 195.792935 64.718788 195.029805 64.156207 194.467224 \nC 63.593626 193.904644 62.830496 193.588545 62.034887 193.588545 \nC 61.239277 193.588545 60.476147 193.904644 59.913566 194.467224 \nC 59.350986 195.029805 59.034887 195.792935 59.034887 196.588545 \nC 59.034887 197.384154 59.350986 198.147284 59.913566 198.709865 \nC 60.476147 199.272446 61.239277 199.588545 62.034887 199.588545 \nz\n\" style=\"fill:#e4b8b8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 63.359667 104.721846 \nC 64.155276 104.721846 64.918406 104.405747 65.480987 103.843166 \nC 66.043568 103.280585 66.359667 102.517455 66.359667 101.721846 \nC 66.359667 100.926236 66.043568 100.163106 65.480987 99.600525 \nC 64.918406 99.037945 64.155276 98.721846 63.359667 98.721846 \nC 62.564058 98.721846 61.800927 99.037945 61.238347 99.600525 \nC 60.675766 100.163106 60.359667 100.926236 60.359667 101.721846 \nC 60.359667 102.517455 60.675766 103.280585 61.238347 103.843166 \nC 61.800927 104.405747 62.564058 104.721846 63.359667 104.721846 \nz\n\" style=\"fill:#d69ca8;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p7152d03894)\" d=\"M 66.009227 119.414389 \nC 66.804836 119.414389 67.567967 119.09829 68.130547 118.53571 \nC 68.693128 117.973129 69.009227 117.209999 69.009227 116.414389 \nC 69.009227 115.61878 68.693128 114.85565 68.130547 114.293069 \nC 67.567967 113.730488 66.804836 113.414389 66.009227 113.414389 \nC 65.213618 113.414389 64.450488 113.730488 63.887907 114.293069 \nC 63.325326 114.85565 63.009227 115.61878 63.009227 116.414389 \nC 63.009227 117.209999 63.325326 117.973129 63.887907 118.53571 \nC 64.450488 119.09829 65.213618 119.414389 66.009227 119.414389 \nz\n\" style=\"fill:#ab6990;stroke:#ffffff;stroke-width:0.48;\"/>\n   </g>\n   <g id=\"PathCollection_2\"/>\n   <g id=\"PathCollection_3\"/>\n   <g id=\"PathCollection_4\"/>\n   <g id=\"PathCollection_5\"/>\n   <g id=\"PathCollection_6\"/>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m70c276103b\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"60.652607\" xlink:href=\"#m70c276103b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(57.471357 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.651918\" xlink:href=\"#m70c276103b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20000 -->\n      <g transform=\"translate(90.745668 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"152.651228\" xlink:href=\"#m70c276103b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40000 -->\n      <g transform=\"translate(136.744978 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"198.650538\" xlink:href=\"#m70c276103b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60000 -->\n      <g transform=\"translate(182.744288 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"244.649849\" xlink:href=\"#m70c276103b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80000 -->\n      <g transform=\"translate(228.743599 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"290.649159\" xlink:href=\"#m70c276103b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100000 -->\n      <g transform=\"translate(271.561659 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"336.648469\" xlink:href=\"#m70c276103b\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120000 -->\n      <g transform=\"translate(317.560969 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- parameters -->\n     <g transform=\"translate(183.876563 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"165.869141\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"324.560547\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"386.083984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"425.292969\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"486.816406\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"527.929688\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m541a4da3f4\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m541a4da3f4\" y=\"173.085256\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(20.878125 176.884474)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m541a4da3f4\" y=\"95.315801\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(20.878125 99.11502)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.478125\" xlink:href=\"#m541a4da3f4\" y=\"17.546347\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{3}}$ -->\n      <g transform=\"translate(20.878125 21.345566)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"mc71c8501bf\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"213.74925\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"204.032833\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"196.496194\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"190.338312\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"185.131897\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"180.621895\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"176.643791\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"149.674317\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"135.979796\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"126.263379\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"118.72674\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"112.568858\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"107.362442\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"102.85244\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"98.874336\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"71.904863\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"58.210342\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"48.493924\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"40.957285\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"34.799403\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"29.592988\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"25.082986\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"45.478125\" xlink:href=\"#mc71c8501bf\" y=\"21.104882\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_12\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 125.577812)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 45.478125 224.64 \nL 45.478125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 380.278125 224.64 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 45.478125 224.64 \nL 380.278125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 45.478125 7.2 \nL 380.278125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 177.99375 219.64 \nL 247.7625 219.64 \nQ 249.7625 219.64 249.7625 217.64 \nL 249.7625 115.893125 \nQ 249.7625 113.893125 247.7625 113.893125 \nL 177.99375 113.893125 \nQ 175.99375 113.893125 175.99375 115.893125 \nL 175.99375 217.64 \nQ 175.99375 219.64 177.99375 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_13\">\n     <!-- layers -->\n     <g transform=\"translate(187.99375 125.491562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"209.765625\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"250.878906\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_7\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m8eb5cb8e03\" style=\"stroke:#d8a0aa;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#d8a0aa;stroke:#d8a0aa;\" x=\"189.99375\" xlink:href=\"#m8eb5cb8e03\" y=\"137.544688\"/>\n     </g>\n    </g>\n    <g id=\"text_14\">\n     <!-- 15 -->\n     <g transform=\"translate(207.99375 140.169688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_8\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m2f49cb91b4\" style=\"stroke:#b26f93;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#b26f93;stroke:#b26f93;\" x=\"189.99375\" xlink:href=\"#m2f49cb91b4\" y=\"152.222812\"/>\n     </g>\n    </g>\n    <g id=\"text_15\">\n     <!-- 30 -->\n     <g transform=\"translate(207.99375 154.847812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_9\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m630fc9f6aa\" style=\"stroke:#7b4779;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#7b4779;stroke:#7b4779;\" x=\"189.99375\" xlink:href=\"#m630fc9f6aa\" y=\"166.900937\"/>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- 45 -->\n     <g transform=\"translate(207.99375 169.525937)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_10\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m8ac3cda5e0\" style=\"stroke:#3c264d;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#3c264d;stroke:#3c264d;\" x=\"189.99375\" xlink:href=\"#m8ac3cda5e0\" y=\"181.579062\"/>\n     </g>\n    </g>\n    <g id=\"text_17\">\n     <!-- 60 -->\n     <g transform=\"translate(207.99375 184.204062)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-36\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- model -->\n     <g transform=\"translate(187.99375 198.882187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_11\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m9b598da789\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"189.99375\" xlink:href=\"#m9b598da789\" y=\"210.935312\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- ResNET -->\n     <g transform=\"translate(207.99375 213.560312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p7152d03894\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"45.478125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}