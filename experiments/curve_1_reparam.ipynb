{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<torch._C.Generator at 0x111f7c570>"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from deep_reparametrization.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    "    get_elastic_error_func,\n",
    ")\n",
    "from deep_reparametrization.ResNET import ResNET, init_zero\n",
    "import experiments.curves as c1\n",
    "\n",
    "# make reproducible\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_1/\"\n",
    "SET_NAME = \"eks_5\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "\n",
    "FOLDS = 1\n",
    "N = 128  # training points internal\n",
    "# lr_scheduler  = lambda optim:  torch.optim.lr_scheduler.ReduceLROnPlateau(optim , mode='min', factor=0.8, patience=10, verbose=True)\n",
    "loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=1e3, verbose=False)\n",
    "no_penalty_loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=0, verbose=False)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [2],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128, 256]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"model\": [FFNN, ResNET],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [10],\n",
    "    \"learning_rate\": [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = c1.q(x_train.detach())\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.93418568\n",
      "Training Loss:  0.93418568\n",
      "Training Loss:  0.93418568\n",
      "Training Loss:  0.93388301\n",
      "Training Loss:  0.93388301\n",
      "Training Loss:  0.93388301\n",
      "Training Loss:  0.93386149\n",
      "Training Loss:  0.93384498\n",
      "Training Loss:  0.93384498\n",
      "Training Loss:  0.93384498\n",
      "Final training Loss:  0.93384498\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00478804\n",
      "Training Loss:  0.00478804\n",
      "Training Loss:  0.00478804\n",
      "Training Loss:  0.00478804\n",
      "Training Loss:  0.00128755\n",
      "Training Loss:  0.00075672\n",
      "Training Loss:  0.00075653\n",
      "Training Loss:  0.00075645\n",
      "Training Loss:  0.00075645\n",
      "Training Loss:  0.00075645\n",
      "Final training Loss:  0.00075645\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Final training Loss:  4.663e-05\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00015439\n",
      "Training Loss:  0.00015439\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Final training Loss:  0.00014744\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01005564\n",
      "Training Loss:  0.01005564\n",
      "Training Loss:  0.01005564\n",
      "Training Loss:  0.00020929\n",
      "Training Loss:  0.00020923\n",
      "Training Loss:  0.00020918\n",
      "Training Loss:  0.00020918\n",
      "Training Loss:  0.00020918\n",
      "Training Loss:  0.00020918\n",
      "Training Loss:  0.00020918\n",
      "Final training Loss:  0.00020918\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00272022\n",
      "Training Loss:  0.00272022\n",
      "Training Loss:  0.00272022\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272016\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272015\n",
      "Final training Loss:  0.00272015\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00868353\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Final training Loss:  0.00477595\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Training Loss:  0.00604105\n",
      "Final training Loss:  0.00604105\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00151742\n",
      "Training Loss:  0.00151735\n",
      "Training Loss:  0.00151735\n",
      "Training Loss:  0.00059493\n",
      "Training Loss:  0.00059493\n",
      "Training Loss:  0.00059493\n",
      "Training Loss:  0.00059493\n",
      "Training Loss:  0.00059493\n",
      "Training Loss:  0.00059493\n",
      "Training Loss:  0.00059493\n",
      "Final training Loss:  0.00059493\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00398981\n",
      "Training Loss:  0.00398981\n",
      "Training Loss:  0.00398971\n",
      "Training Loss:  5.561e-05\n",
      "Training Loss:  5.561e-05\n",
      "Training Loss:  5.56e-05\n",
      "Training Loss:  5.56e-05\n",
      "Training Loss:  5.56e-05\n",
      "Training Loss:  5.56e-05\n",
      "Training Loss:  5.56e-05\n",
      "Final training Loss:  5.56e-05\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00124517\n",
      "Training Loss:  0.00124493\n",
      "Training Loss:  0.00124493\n",
      "Training Loss:  0.00124493\n",
      "Training Loss:  0.00124493\n",
      "Training Loss:  0.00124493\n",
      "Training Loss:  0.00124493\n",
      "Training Loss:  0.00124493\n",
      "Training Loss:  0.00124493\n",
      "Training Loss:  0.00124493\n",
      "Final training Loss:  0.00124493\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01904517\n",
      "Training Loss:  0.01658394\n",
      "Training Loss:  0.00092214\n",
      "Training Loss:  0.00070293\n",
      "Training Loss:  0.00070293\n",
      "Training Loss:  0.00070281\n",
      "Training Loss:  0.00070281\n",
      "Training Loss:  0.00070281\n",
      "Training Loss:  0.00070224\n",
      "Training Loss:  0.00070224\n",
      "Final training Loss:  0.00070224\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.40491343\n",
      "Training Loss:  6.40491343\n",
      "Training Loss:  2.25409579\n",
      "Training Loss:  2.25409579\n",
      "Training Loss:  2.25409579\n",
      "Training Loss:  2.25409579\n",
      "Training Loss:  0.31522343\n",
      "Training Loss:  0.31469202\n",
      "Training Loss:  0.31469181\n",
      "Training Loss:  0.31469181\n",
      "Final training Loss:  0.31469181\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10083008\n",
      "Final training Loss:  600.10083008\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.20863014\n",
      "Training Loss:  0.20863014\n",
      "Training Loss:  0.20862682\n",
      "Training Loss:  0.20862682\n",
      "Training Loss:  0.20862682\n",
      "Training Loss:  0.20862682\n",
      "Training Loss:  0.20862682\n",
      "Training Loss:  0.20862682\n",
      "Training Loss:  0.20862682\n",
      "Training Loss:  0.20862682\n",
      "Final training Loss:  0.20862682\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01649284\n",
      "Training Loss:  0.01580728\n",
      "Training Loss:  0.01580566\n",
      "Training Loss:  0.01580566\n",
      "Training Loss:  0.01580566\n",
      "Training Loss:  0.01580566\n",
      "Training Loss:  0.01580503\n",
      "Training Loss:  0.01580503\n",
      "Training Loss:  0.01580503\n",
      "Training Loss:  0.01580503\n",
      "Final training Loss:  0.01580503\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07146632\n",
      "Training Loss:  0.02620251\n",
      "Training Loss:  0.0261981\n",
      "Training Loss:  0.02619544\n",
      "Training Loss:  0.02619544\n",
      "Training Loss:  0.02619544\n",
      "Training Loss:  0.02619544\n",
      "Training Loss:  0.02619544\n",
      "Training Loss:  0.0261944\n",
      "Training Loss:  0.0261944\n",
      "Final training Loss:  0.0261944\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00386384\n",
      "Training Loss:  0.00386384\n",
      "Training Loss:  0.00386384\n",
      "Training Loss:  0.00386384\n",
      "Training Loss:  0.00386384\n",
      "Training Loss:  0.00386384\n",
      "Training Loss:  0.00386371\n",
      "Training Loss:  0.00386371\n",
      "Training Loss:  0.00386371\n",
      "Training Loss:  0.00386371\n",
      "Final training Loss:  0.00386371\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03354366\n",
      "Training Loss:  0.03354364\n",
      "Training Loss:  0.0335393\n",
      "Training Loss:  0.0335377\n",
      "Training Loss:  0.0335377\n",
      "Training Loss:  0.0335377\n",
      "Training Loss:  0.0335377\n",
      "Training Loss:  0.0335377\n",
      "Training Loss:  0.0335377\n",
      "Training Loss:  0.0335377\n",
      "Final training Loss:  0.0335377\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Training Loss:  0.00067407\n",
      "Final training Loss:  0.00067407\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Training Loss:  0.004168\n",
      "Final training Loss:  0.004168\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Training Loss:  0.01177175\n",
      "Final training Loss:  0.01177175\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.09043316\n",
      "Training Loss:  0.01672792\n",
      "Training Loss:  0.01672792\n",
      "Training Loss:  0.01672792\n",
      "Training Loss:  0.01672792\n",
      "Training Loss:  0.01672792\n",
      "Training Loss:  0.01672734\n",
      "Training Loss:  0.01672734\n",
      "Training Loss:  0.01672734\n",
      "Training Loss:  0.01672734\n",
      "Final training Loss:  0.01672734\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07826348\n",
      "Training Loss:  0.07826279\n",
      "Training Loss:  0.07826279\n",
      "Training Loss:  0.07825965\n",
      "Training Loss:  0.0782596\n",
      "Training Loss:  0.07825954\n",
      "Training Loss:  0.07825954\n",
      "Training Loss:  0.07825954\n",
      "Training Loss:  0.07825974\n",
      "Training Loss:  0.07825954\n",
      "Final training Loss:  0.07825954\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00143087\n",
      "Training Loss:  0.00143087\n",
      "Training Loss:  0.00143055\n",
      "Training Loss:  0.00143055\n",
      "Training Loss:  0.00143055\n",
      "Training Loss:  0.00143055\n",
      "Training Loss:  0.00143055\n",
      "Training Loss:  0.00143055\n",
      "Training Loss:  0.00143055\n",
      "Training Loss:  0.00143055\n",
      "Final training Loss:  0.00143055\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00059605\n",
      "Training Loss:  7.588e-05\n",
      "Training Loss:  7.588e-05\n",
      "Training Loss:  7.588e-05\n",
      "Training Loss:  7.588e-05\n",
      "Training Loss:  7.587e-05\n",
      "Training Loss:  7.587e-05\n",
      "Training Loss:  7.587e-05\n",
      "Training Loss:  7.587e-05\n",
      "Training Loss:  7.587e-05\n",
      "Final training Loss:  7.587e-05\n",
      "\n",
      "Running model (trial=0, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00092844\n",
      "Training Loss:  0.00092808\n",
      "Training Loss:  0.00092808\n",
      "Training Loss:  0.00092734\n",
      "Training Loss:  0.00092734\n",
      "Training Loss:  0.0009273\n",
      "Training Loss:  0.0009273\n",
      "Training Loss:  0.0009273\n",
      "Training Loss:  0.00092729\n",
      "Training Loss:  0.00092729\n",
      "Final training Loss:  0.00092729\n",
      "\n",
      "Running model (trial=0, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00204539\n",
      "Training Loss:  0.00204539\n",
      "Training Loss:  0.00204539\n",
      "Training Loss:  0.00204491\n",
      "Training Loss:  0.00204492\n",
      "Training Loss:  0.00204468\n",
      "Training Loss:  0.00204468\n",
      "Training Loss:  0.00204468\n",
      "Training Loss:  0.00204469\n",
      "Training Loss:  0.00204468\n",
      "Final training Loss:  0.00204468\n",
      "\n",
      "Running model (trial=1, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Training Loss:  0.29364246\n",
      "Final training Loss:  0.29364246\n",
      "\n",
      "Running model (trial=1, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Training Loss:  0.00068655\n",
      "Final training Loss:  0.00068655\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.762e-05\n",
      "Training Loss:  4.761e-05\n",
      "Training Loss:  4.761e-05\n",
      "Training Loss:  4.761e-05\n",
      "Training Loss:  4.761e-05\n",
      "Training Loss:  4.761e-05\n",
      "Training Loss:  4.761e-05\n",
      "Training Loss:  4.761e-05\n",
      "Training Loss:  4.761e-05\n",
      "Training Loss:  4.761e-05\n",
      "Final training Loss:  4.761e-05\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Training Loss:  0.00029145\n",
      "Final training Loss:  0.00029145\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00827506\n",
      "Training Loss:  0.00827462\n",
      "Training Loss:  0.00827458\n",
      "Training Loss:  0.00827458\n",
      "Training Loss:  0.00827458\n",
      "Training Loss:  0.00827458\n",
      "Training Loss:  0.00827458\n",
      "Training Loss:  0.00827458\n",
      "Training Loss:  0.00827444\n",
      "Training Loss:  0.00827444\n",
      "Final training Loss:  0.00827444\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00098502\n",
      "Training Loss:  0.00098502\n",
      "Training Loss:  0.00098489\n",
      "Training Loss:  0.00098489\n",
      "Training Loss:  0.00098489\n",
      "Training Loss:  0.00098471\n",
      "Training Loss:  0.00098471\n",
      "Training Loss:  0.00098461\n",
      "Training Loss:  0.00098461\n",
      "Training Loss:  0.00098443\n",
      "Final training Loss:  0.00098443\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00163076\n",
      "Training Loss:  0.00163076\n",
      "Training Loss:  0.00163076\n",
      "Training Loss:  0.00163075\n",
      "Training Loss:  0.00163075\n",
      "Training Loss:  0.00163074\n",
      "Training Loss:  0.00163074\n",
      "Training Loss:  0.00163074\n",
      "Training Loss:  0.00163074\n",
      "Training Loss:  0.00163074\n",
      "Final training Loss:  0.00163074\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0191765\n",
      "Training Loss:  0.0191765\n",
      "Training Loss:  0.01917637\n",
      "Training Loss:  0.01917437\n",
      "Training Loss:  0.01917437\n",
      "Training Loss:  0.01917437\n",
      "Training Loss:  0.01917437\n",
      "Training Loss:  0.01917437\n",
      "Training Loss:  0.01917437\n",
      "Training Loss:  0.01917437\n",
      "Final training Loss:  0.01917437\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.167872\n",
      "Training Loss:  0.167872\n",
      "Training Loss:  0.02810496\n",
      "Training Loss:  0.02810361\n",
      "Training Loss:  0.02810361\n",
      "Training Loss:  0.02810361\n",
      "Training Loss:  0.02810361\n",
      "Training Loss:  0.02810361\n",
      "Training Loss:  0.02810361\n",
      "Training Loss:  0.02810361\n",
      "Final training Loss:  0.02810361\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00403881\n",
      "Training Loss:  0.00209665\n",
      "Training Loss:  0.00209665\n",
      "Training Loss:  0.00209665\n",
      "Training Loss:  0.00209665\n",
      "Training Loss:  0.00209643\n",
      "Training Loss:  0.000138\n",
      "Training Loss:  0.000138\n",
      "Training Loss:  0.00013798\n",
      "Training Loss:  0.00013798\n",
      "Final training Loss:  0.00013798\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Training Loss:  0.0043954\n",
      "Final training Loss:  0.0043954\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00734095\n",
      "Training Loss:  0.00734095\n",
      "Training Loss:  0.00734095\n",
      "Training Loss:  0.00734051\n",
      "Training Loss:  0.00734051\n",
      "Training Loss:  0.00734051\n",
      "Training Loss:  0.00734051\n",
      "Training Loss:  0.00734051\n",
      "Training Loss:  0.00734051\n",
      "Training Loss:  0.00734051\n",
      "Final training Loss:  0.00734051\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00279544\n",
      "Training Loss:  0.00279522\n",
      "Training Loss:  0.00279455\n",
      "Training Loss:  0.00279453\n",
      "Training Loss:  0.00279453\n",
      "Training Loss:  0.00279453\n",
      "Training Loss:  0.00279453\n",
      "Training Loss:  0.00279453\n",
      "Training Loss:  0.00279453\n",
      "Training Loss:  0.00279453\n",
      "Final training Loss:  0.00279453\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Final training Loss:  600.10125732\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  6.62663698\n",
      "Training Loss:  6.12335777\n",
      "Training Loss:  6.12335777\n",
      "Training Loss:  6.12335777\n",
      "Training Loss:  6.12335777\n",
      "Training Loss:  6.12335777\n",
      "Training Loss:  6.12335777\n",
      "Training Loss:  6.12335777\n",
      "Training Loss:  6.12335777\n",
      "Training Loss:  6.12335777\n",
      "Final training Loss:  6.12335777\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.52337855\n",
      "Training Loss:  0.52337855\n",
      "Training Loss:  0.52336735\n",
      "Training Loss:  0.52336735\n",
      "Training Loss:  0.52336735\n",
      "Training Loss:  0.52336735\n",
      "Training Loss:  0.52336735\n",
      "Training Loss:  0.52336735\n",
      "Training Loss:  0.52336735\n",
      "Training Loss:  0.52336735\n",
      "Final training Loss:  0.52336735\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.16777045\n",
      "Training Loss:  0.0480121\n",
      "Training Loss:  0.04799489\n",
      "Training Loss:  0.04351323\n",
      "Training Loss:  0.03635435\n",
      "Training Loss:  0.03635435\n",
      "Training Loss:  0.03635435\n",
      "Training Loss:  0.03635391\n",
      "Training Loss:  0.03382589\n",
      "Training Loss:  0.03382589\n",
      "Final training Loss:  0.03382589\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0030065\n",
      "Training Loss:  0.00300649\n",
      "Training Loss:  0.00300649\n",
      "Training Loss:  0.00300649\n",
      "Training Loss:  0.00300649\n",
      "Training Loss:  0.00300603\n",
      "Training Loss:  0.00300603\n",
      "Training Loss:  0.00300603\n",
      "Training Loss:  0.00300553\n",
      "Training Loss:  0.00300553\n",
      "Final training Loss:  0.00300553\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0031971\n",
      "Training Loss:  0.0031971\n",
      "Training Loss:  0.0031971\n",
      "Training Loss:  0.00319689\n",
      "Training Loss:  0.00319689\n",
      "Training Loss:  0.00319673\n",
      "Training Loss:  0.00319673\n",
      "Training Loss:  0.00319673\n",
      "Training Loss:  0.00319673\n",
      "Training Loss:  0.0031967\n",
      "Final training Loss:  0.0031967\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02979087\n",
      "Training Loss:  0.02979087\n",
      "Training Loss:  0.02979087\n",
      "Training Loss:  0.02979087\n",
      "Training Loss:  0.02262722\n",
      "Training Loss:  0.02262722\n",
      "Training Loss:  0.02262722\n",
      "Training Loss:  0.02262722\n",
      "Training Loss:  0.02262722\n",
      "Training Loss:  0.02262722\n",
      "Final training Loss:  0.02262722\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01822827\n",
      "Training Loss:  0.01822827\n",
      "Training Loss:  0.01822827\n",
      "Training Loss:  0.01822827\n",
      "Training Loss:  0.01822799\n",
      "Training Loss:  0.01822587\n",
      "Training Loss:  0.01822587\n",
      "Training Loss:  0.01822587\n",
      "Training Loss:  0.01622999\n",
      "Training Loss:  0.01622918\n",
      "Final training Loss:  0.01622918\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00514358\n",
      "Training Loss:  0.00514358\n",
      "Training Loss:  0.00514358\n",
      "Training Loss:  0.00514328\n",
      "Training Loss:  0.00514328\n",
      "Training Loss:  0.00391748\n",
      "Training Loss:  0.00391748\n",
      "Training Loss:  0.00391735\n",
      "Training Loss:  0.00391735\n",
      "Training Loss:  0.00391735\n",
      "Final training Loss:  0.00391735\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00211434\n",
      "Training Loss:  0.00211401\n",
      "Training Loss:  0.00211401\n",
      "Training Loss:  0.00211395\n",
      "Training Loss:  0.00211395\n",
      "Training Loss:  0.00211395\n",
      "Training Loss:  0.00191668\n",
      "Training Loss:  0.00191668\n",
      "Training Loss:  0.00191668\n",
      "Training Loss:  0.00191668\n",
      "Final training Loss:  0.00191668\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.702e-05\n",
      "Training Loss:  8.701e-05\n",
      "Training Loss:  8.697e-05\n",
      "Training Loss:  8.697e-05\n",
      "Training Loss:  8.697e-05\n",
      "Training Loss:  8.697e-05\n",
      "Training Loss:  8.696e-05\n",
      "Training Loss:  8.696e-05\n",
      "Training Loss:  8.696e-05\n",
      "Training Loss:  8.696e-05\n",
      "Final training Loss:  8.696e-05\n",
      "\n",
      "Running model (trial=1, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00542615\n",
      "Training Loss:  0.00542615\n",
      "Training Loss:  0.00542615\n",
      "Training Loss:  0.00542615\n",
      "Training Loss:  0.00542615\n",
      "Training Loss:  0.00542612\n",
      "Training Loss:  0.00542612\n",
      "Training Loss:  0.00542612\n",
      "Training Loss:  0.00536916\n",
      "Training Loss:  0.00536916\n",
      "Final training Loss:  0.00536916\n",
      "\n",
      "Running model (trial=1, mod=53, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00028415\n",
      "Training Loss:  0.00028414\n",
      "Training Loss:  0.00028414\n",
      "Training Loss:  0.00028414\n",
      "Training Loss:  0.00028406\n",
      "Training Loss:  0.00028406\n",
      "Training Loss:  0.00028407\n",
      "Training Loss:  0.00028406\n",
      "Training Loss:  0.00028406\n",
      "Training Loss:  0.00028407\n",
      "Final training Loss:  0.00028407\n",
      "\n",
      "Running model (trial=1, mod=54, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026378\n",
      "Training Loss:  0.00026366\n",
      "Final training Loss:  0.00026366\n",
      "\n",
      "Running model (trial=1, mod=55, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01971013\n",
      "Training Loss:  0.01971013\n",
      "Training Loss:  0.01971013\n",
      "Training Loss:  0.01971013\n",
      "Training Loss:  0.01971009\n",
      "Training Loss:  0.01971009\n",
      "Training Loss:  0.01971009\n",
      "Training Loss:  0.01971009\n",
      "Training Loss:  0.01971009\n",
      "Training Loss:  0.01971009\n",
      "Final training Loss:  0.01971009\n",
      "\n",
      "Running model (trial=2, mod=56, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.38861418\n",
      "Training Loss:  2.38861418\n",
      "Training Loss:  2.38861418\n",
      "Training Loss:  1.42302155\n",
      "Training Loss:  1.42002738\n",
      "Training Loss:  1.42002738\n",
      "Training Loss:  1.42002738\n",
      "Training Loss:  1.42002738\n",
      "Training Loss:  1.41999781\n",
      "Training Loss:  1.41999781\n",
      "Final training Loss:  1.41999781\n",
      "\n",
      "Running model (trial=2, mod=57, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00837408\n",
      "Training Loss:  0.00837401\n",
      "Training Loss:  0.00837401\n",
      "Training Loss:  0.00837401\n",
      "Training Loss:  0.00837401\n",
      "Training Loss:  0.0083737\n",
      "Training Loss:  0.0083737\n",
      "Training Loss:  0.0083737\n",
      "Training Loss:  0.0083737\n",
      "Training Loss:  0.0083737\n",
      "Final training Loss:  0.0083737\n",
      "\n",
      "Running model (trial=2, mod=58, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00053188\n",
      "Training Loss:  0.00053188\n",
      "Training Loss:  0.00053181\n",
      "Training Loss:  0.00053181\n",
      "Training Loss:  0.00053181\n",
      "Training Loss:  0.00053181\n",
      "Training Loss:  0.00053181\n",
      "Training Loss:  0.00053181\n",
      "Training Loss:  0.00053181\n",
      "Training Loss:  0.00053181\n",
      "Final training Loss:  0.00053181\n",
      "\n",
      "Running model (trial=2, mod=59, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00115995\n",
      "Training Loss:  0.00115995\n",
      "Training Loss:  0.00115995\n",
      "Training Loss:  0.0011599\n",
      "Training Loss:  0.0011599\n",
      "Training Loss:  0.00115989\n",
      "Training Loss:  0.00115989\n",
      "Training Loss:  0.00115989\n",
      "Training Loss:  0.00115989\n",
      "Training Loss:  0.00115989\n",
      "Final training Loss:  0.00115989\n",
      "\n",
      "Running model (trial=2, mod=60, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01011351\n",
      "Training Loss:  0.01011321\n",
      "Training Loss:  0.01011321\n",
      "Training Loss:  0.01011316\n",
      "Training Loss:  0.01011316\n",
      "Training Loss:  0.01011316\n",
      "Training Loss:  0.01011316\n",
      "Training Loss:  0.01011293\n",
      "Training Loss:  0.01011293\n",
      "Training Loss:  0.01011293\n",
      "Final training Loss:  0.01011293\n",
      "\n",
      "Running model (trial=2, mod=61, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00788447\n",
      "Training Loss:  0.0012939\n",
      "Training Loss:  0.0012939\n",
      "Training Loss:  0.0012939\n",
      "Training Loss:  0.0012939\n",
      "Training Loss:  0.0012939\n",
      "Training Loss:  0.0012939\n",
      "Training Loss:  0.0012939\n",
      "Training Loss:  0.0012939\n",
      "Training Loss:  0.0012939\n",
      "Final training Loss:  0.0012939\n",
      "\n",
      "Running model (trial=2, mod=62, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00201532\n",
      "Training Loss:  0.00201532\n",
      "Training Loss:  0.00201532\n",
      "Training Loss:  0.00201528\n",
      "Training Loss:  0.00201528\n",
      "Training Loss:  0.00201528\n",
      "Training Loss:  0.00201528\n",
      "Training Loss:  0.00201528\n",
      "Training Loss:  0.00201528\n",
      "Training Loss:  0.00201528\n",
      "Final training Loss:  0.00201528\n",
      "\n",
      "Running model (trial=2, mod=63, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Training Loss:  0.01493257\n",
      "Final training Loss:  0.01493257\n",
      "\n",
      "Running model (trial=2, mod=64, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Training Loss:  0.00070166\n",
      "Final training Loss:  0.00070166\n",
      "\n",
      "Running model (trial=2, mod=65, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00168171\n",
      "Training Loss:  0.00168167\n",
      "Training Loss:  0.00168167\n",
      "Training Loss:  0.00168166\n",
      "Training Loss:  0.00168166\n",
      "Training Loss:  0.00168166\n",
      "Training Loss:  0.00168166\n",
      "Training Loss:  0.00168166\n",
      "Training Loss:  0.00168166\n",
      "Training Loss:  0.00168166\n",
      "Final training Loss:  0.00168166\n",
      "\n",
      "Running model (trial=2, mod=66, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00011138\n",
      "Training Loss:  0.00011138\n",
      "Training Loss:  0.00011138\n",
      "Training Loss:  0.00011134\n",
      "Training Loss:  0.00011134\n",
      "Training Loss:  0.00011134\n",
      "Training Loss:  0.00011134\n",
      "Training Loss:  0.00011134\n",
      "Training Loss:  0.00011134\n",
      "Training Loss:  0.00011134\n",
      "Final training Loss:  0.00011134\n",
      "\n",
      "Running model (trial=2, mod=67, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00351538\n",
      "Training Loss:  0.00351503\n",
      "Training Loss:  0.00351503\n",
      "Training Loss:  0.00351494\n",
      "Training Loss:  0.00351494\n",
      "Training Loss:  0.00351494\n",
      "Training Loss:  0.00351494\n",
      "Training Loss:  0.00351494\n",
      "Training Loss:  0.00351494\n",
      "Training Loss:  0.00351494\n",
      "Final training Loss:  0.00351494\n",
      "\n",
      "Running model (trial=2, mod=68, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Training Loss:  600.10241699\n",
      "Final training Loss:  600.10241699\n",
      "\n",
      "Running model (trial=2, mod=69, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Final training Loss:  600.10125732\n",
      "\n",
      "Running model (trial=2, mod=70, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.29865786\n",
      "Training Loss:  0.29865366\n",
      "Training Loss:  0.29865366\n",
      "Training Loss:  0.29865366\n",
      "Training Loss:  0.29865366\n",
      "Training Loss:  0.29865366\n",
      "Training Loss:  0.29865366\n",
      "Training Loss:  0.29865366\n",
      "Training Loss:  0.29865366\n",
      "Training Loss:  0.29865366\n",
      "Final training Loss:  0.29865366\n",
      "\n",
      "Running model (trial=2, mod=71, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00290189\n",
      "Training Loss:  0.00290189\n",
      "Training Loss:  0.00290189\n",
      "Training Loss:  0.00290189\n",
      "Training Loss:  0.00290181\n",
      "Training Loss:  0.00290181\n",
      "Training Loss:  0.00290181\n",
      "Training Loss:  0.00290181\n",
      "Training Loss:  0.00290181\n",
      "Training Loss:  0.00290181\n",
      "Final training Loss:  0.00290181\n",
      "\n",
      "Running model (trial=2, mod=72, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0076128\n",
      "Training Loss:  0.00761261\n",
      "Training Loss:  0.00761201\n",
      "Training Loss:  0.00761201\n",
      "Training Loss:  0.00761201\n",
      "Training Loss:  0.00761201\n",
      "Training Loss:  0.00761199\n",
      "Training Loss:  0.00761199\n",
      "Training Loss:  0.00761199\n",
      "Training Loss:  0.00761199\n",
      "Final training Loss:  0.00761199\n",
      "\n",
      "Running model (trial=2, mod=73, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00010739\n",
      "Training Loss:  0.00010683\n",
      "Training Loss:  0.00010683\n",
      "Training Loss:  0.00010683\n",
      "Training Loss:  0.00010683\n",
      "Training Loss:  9.74e-05\n",
      "Training Loss:  9.74e-05\n",
      "Training Loss:  9.74e-05\n",
      "Training Loss:  9.74e-05\n",
      "Training Loss:  9.74e-05\n",
      "Final training Loss:  9.74e-05\n",
      "\n",
      "Running model (trial=2, mod=74, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00395312\n",
      "Training Loss:  0.00395185\n",
      "Training Loss:  0.00395185\n",
      "Training Loss:  0.00395185\n",
      "Training Loss:  0.00395185\n",
      "Training Loss:  0.00395157\n",
      "Training Loss:  0.00395157\n",
      "Training Loss:  0.00395157\n",
      "Training Loss:  0.00395157\n",
      "Training Loss:  0.00395157\n",
      "Final training Loss:  0.00395157\n",
      "\n",
      "Running model (trial=2, mod=75, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05172055\n",
      "Training Loss:  0.05172055\n",
      "Training Loss:  0.05171704\n",
      "Training Loss:  0.05171636\n",
      "Training Loss:  0.05171636\n",
      "Training Loss:  0.05171636\n",
      "Training Loss:  0.05171636\n",
      "Training Loss:  0.05171636\n",
      "Training Loss:  0.05171636\n",
      "Training Loss:  0.05171636\n",
      "Final training Loss:  0.05171636\n",
      "\n",
      "Running model (trial=2, mod=76, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01803435\n",
      "Training Loss:  0.01803435\n",
      "Training Loss:  0.01803435\n",
      "Training Loss:  0.01803427\n",
      "Training Loss:  0.0180341\n",
      "Training Loss:  0.0180341\n",
      "Training Loss:  0.0180341\n",
      "Training Loss:  0.0180341\n",
      "Training Loss:  0.0180341\n",
      "Training Loss:  0.0180341\n",
      "Final training Loss:  0.0180341\n",
      "\n",
      "Running model (trial=2, mod=77, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00332247\n",
      "Training Loss:  0.00332247\n",
      "Training Loss:  0.00332247\n",
      "Training Loss:  0.00332247\n",
      "Training Loss:  0.00332247\n",
      "Training Loss:  0.00332247\n",
      "Training Loss:  0.00332244\n",
      "Training Loss:  0.00332244\n",
      "Training Loss:  0.00332244\n",
      "Training Loss:  0.00332244\n",
      "Final training Loss:  0.00332244\n",
      "\n",
      "Running model (trial=2, mod=78, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Training Loss:  0.00659017\n",
      "Final training Loss:  0.00659017\n",
      "\n",
      "Running model (trial=2, mod=79, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02808991\n",
      "Training Loss:  0.02808991\n",
      "Training Loss:  0.02808991\n",
      "Training Loss:  0.02808991\n",
      "Training Loss:  0.02808991\n",
      "Training Loss:  0.02808971\n",
      "Training Loss:  0.02808945\n",
      "Training Loss:  0.02808846\n",
      "Training Loss:  0.02808846\n",
      "Training Loss:  0.02808846\n",
      "Final training Loss:  0.02808846\n",
      "\n",
      "Running model (trial=2, mod=80, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00360679\n",
      "Training Loss:  0.00360679\n",
      "Training Loss:  0.00360633\n",
      "Training Loss:  0.00360633\n",
      "Training Loss:  0.00360633\n",
      "Training Loss:  0.00360633\n",
      "Training Loss:  0.00360633\n",
      "Training Loss:  0.00360633\n",
      "Training Loss:  0.00360633\n",
      "Training Loss:  0.00360633\n",
      "Final training Loss:  0.00360633\n",
      "\n",
      "Running model (trial=2, mod=81, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00296454\n",
      "Training Loss:  0.00296454\n",
      "Training Loss:  0.00296454\n",
      "Training Loss:  0.00296453\n",
      "Training Loss:  0.00296453\n",
      "Training Loss:  0.00296453\n",
      "Training Loss:  0.00296453\n",
      "Training Loss:  0.00296453\n",
      "Training Loss:  0.00296453\n",
      "Training Loss:  0.00296453\n",
      "Final training Loss:  0.00296453\n",
      "\n",
      "Running model (trial=2, mod=82, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0004448\n",
      "Training Loss:  0.00044452\n",
      "Training Loss:  0.00044437\n",
      "Training Loss:  0.00044437\n",
      "Training Loss:  0.00044437\n",
      "Training Loss:  0.00044437\n",
      "Training Loss:  0.00044437\n",
      "Training Loss:  0.00044437\n",
      "Training Loss:  0.00044437\n",
      "Training Loss:  0.00044437\n",
      "Final training Loss:  0.00044437\n",
      "\n",
      "Running model (trial=2, mod=83, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00501986\n",
      "Training Loss:  0.00501986\n",
      "Training Loss:  0.00501986\n",
      "Training Loss:  0.00317541\n",
      "Training Loss:  0.0031751\n",
      "Training Loss:  0.00317278\n",
      "Training Loss:  0.00315243\n",
      "Training Loss:  0.00315165\n",
      "Training Loss:  0.00315165\n",
      "Training Loss:  0.00315165\n",
      "Final training Loss:  0.00315165\n",
      "\n",
      "Running model (trial=3, mod=84, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02077332\n",
      "Training Loss:  0.02077332\n",
      "Training Loss:  0.02077332\n",
      "Training Loss:  0.02077332\n",
      "Training Loss:  0.02077328\n",
      "Training Loss:  0.02077328\n",
      "Training Loss:  0.02077328\n",
      "Training Loss:  0.02077328\n",
      "Training Loss:  0.02077328\n",
      "Training Loss:  0.02077328\n",
      "Final training Loss:  0.02077328\n",
      "\n",
      "Running model (trial=3, mod=85, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00034375\n",
      "Training Loss:  0.00034368\n",
      "Training Loss:  0.00034368\n",
      "Training Loss:  0.00034368\n",
      "Training Loss:  0.00034367\n",
      "Training Loss:  0.00034367\n",
      "Training Loss:  0.00034367\n",
      "Training Loss:  0.00034366\n",
      "Training Loss:  0.00034366\n",
      "Training Loss:  0.00034366\n",
      "Final training Loss:  0.00034366\n",
      "\n",
      "Running model (trial=3, mod=86, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Training Loss:  0.00286667\n",
      "Final training Loss:  0.00286667\n",
      "\n",
      "Running model (trial=3, mod=87, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01319045\n",
      "Training Loss:  0.01319045\n",
      "Training Loss:  0.01319003\n",
      "Training Loss:  0.01319003\n",
      "Training Loss:  0.01319003\n",
      "Training Loss:  0.01318986\n",
      "Training Loss:  0.01318986\n",
      "Training Loss:  0.00074944\n",
      "Training Loss:  0.00074944\n",
      "Training Loss:  0.00074944\n",
      "Final training Loss:  0.00074944\n",
      "\n",
      "Running model (trial=3, mod=88, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00815958\n",
      "Training Loss:  0.00190076\n",
      "Training Loss:  0.00190076\n",
      "Training Loss:  0.00190076\n",
      "Training Loss:  0.00190076\n",
      "Training Loss:  0.00190076\n",
      "Training Loss:  0.00190076\n",
      "Training Loss:  0.00190076\n",
      "Training Loss:  0.00190076\n",
      "Training Loss:  0.00190076\n",
      "Final training Loss:  0.00190076\n",
      "\n",
      "Running model (trial=3, mod=89, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00037604\n",
      "Training Loss:  0.00037604\n",
      "Training Loss:  0.00037604\n",
      "Training Loss:  0.000376\n",
      "Training Loss:  0.00013861\n",
      "Training Loss:  0.00013854\n",
      "Training Loss:  0.00013851\n",
      "Training Loss:  0.00013851\n",
      "Training Loss:  0.00013851\n",
      "Training Loss:  0.00013851\n",
      "Final training Loss:  0.00013851\n",
      "\n",
      "Running model (trial=3, mod=90, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01843575\n",
      "Training Loss:  0.01843575\n",
      "Training Loss:  0.01843575\n",
      "Training Loss:  0.01843524\n",
      "Training Loss:  0.01843524\n",
      "Training Loss:  0.01843354\n",
      "Training Loss:  0.01843354\n",
      "Training Loss:  0.01843354\n",
      "Training Loss:  0.01843354\n",
      "Training Loss:  0.01843354\n",
      "Final training Loss:  0.01843354\n",
      "\n",
      "Running model (trial=3, mod=91, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03490748\n",
      "Training Loss:  0.01082593\n",
      "Training Loss:  0.01082581\n",
      "Training Loss:  0.01082574\n",
      "Training Loss:  0.01082574\n",
      "Training Loss:  0.01082574\n",
      "Training Loss:  0.01082574\n",
      "Training Loss:  0.01082574\n",
      "Training Loss:  0.01082575\n",
      "Training Loss:  0.01082574\n",
      "Final training Loss:  0.01082574\n",
      "\n",
      "Running model (trial=3, mod=92, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00166629\n",
      "Training Loss:  0.00166629\n",
      "Training Loss:  0.00166618\n",
      "Training Loss:  0.00166618\n",
      "Training Loss:  0.00166618\n",
      "Training Loss:  0.00166618\n",
      "Training Loss:  0.00166597\n",
      "Training Loss:  0.00166597\n",
      "Training Loss:  0.00166597\n",
      "Training Loss:  0.00166597\n",
      "Final training Loss:  0.00166597\n",
      "\n",
      "Running model (trial=3, mod=93, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01131134\n",
      "Training Loss:  0.01131134\n",
      "Training Loss:  0.01131134\n",
      "Training Loss:  0.01131096\n",
      "Training Loss:  0.01131096\n",
      "Training Loss:  0.01131096\n",
      "Training Loss:  0.01131096\n",
      "Training Loss:  0.01131061\n",
      "Training Loss:  0.01131027\n",
      "Training Loss:  0.01131027\n",
      "Final training Loss:  0.01131027\n",
      "\n",
      "Running model (trial=3, mod=94, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0006722\n",
      "Training Loss:  0.0006722\n",
      "Training Loss:  0.0006722\n",
      "Training Loss:  0.00026079\n",
      "Training Loss:  0.00026079\n",
      "Training Loss:  0.00026076\n",
      "Training Loss:  0.00026076\n",
      "Training Loss:  0.00026076\n",
      "Training Loss:  0.00026076\n",
      "Training Loss:  0.00026076\n",
      "Final training Loss:  0.00026076\n",
      "\n",
      "Running model (trial=3, mod=95, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00903304\n",
      "Training Loss:  0.00091957\n",
      "Training Loss:  0.00091945\n",
      "Training Loss:  0.00091945\n",
      "Training Loss:  0.00091945\n",
      "Training Loss:  0.00091945\n",
      "Training Loss:  0.00091943\n",
      "Training Loss:  0.00091943\n",
      "Training Loss:  0.00091943\n",
      "Training Loss:  0.00091943\n",
      "Final training Loss:  0.00091943\n",
      "\n",
      "Running model (trial=3, mod=96, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.40425634\n",
      "Training Loss:  0.40425056\n",
      "Training Loss:  0.40420824\n",
      "Training Loss:  0.40420824\n",
      "Training Loss:  0.40420824\n",
      "Training Loss:  0.40420744\n",
      "Training Loss:  0.40420744\n",
      "Training Loss:  0.40418798\n",
      "Training Loss:  0.40418798\n",
      "Training Loss:  0.40418798\n",
      "Final training Loss:  0.40418798\n",
      "\n",
      "Running model (trial=3, mod=97, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Final training Loss:  600.1015625\n",
      "\n",
      "Running model (trial=3, mod=98, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.54408938\n",
      "Training Loss:  0.54408413\n",
      "Training Loss:  0.54408413\n",
      "Training Loss:  0.54408413\n",
      "Training Loss:  0.54408413\n",
      "Training Loss:  0.54408413\n",
      "Training Loss:  0.54408413\n",
      "Training Loss:  0.54408413\n",
      "Training Loss:  0.54408413\n",
      "Training Loss:  0.54408413\n",
      "Final training Loss:  0.54408413\n",
      "\n",
      "Running model (trial=3, mod=99, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00855495\n",
      "Training Loss:  0.00855469\n",
      "Training Loss:  0.00844761\n",
      "Training Loss:  0.00844761\n",
      "Training Loss:  0.00844741\n",
      "Training Loss:  0.00844741\n",
      "Training Loss:  0.00844741\n",
      "Training Loss:  0.00844741\n",
      "Training Loss:  0.00844741\n",
      "Training Loss:  0.00844741\n",
      "Final training Loss:  0.00844741\n",
      "\n",
      "Running model (trial=3, mod=100, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02390213\n",
      "Training Loss:  0.00181407\n",
      "Training Loss:  0.00181407\n",
      "Training Loss:  0.00181407\n",
      "Training Loss:  0.00181407\n",
      "Training Loss:  0.00181407\n",
      "Training Loss:  0.00181407\n",
      "Training Loss:  0.00181407\n",
      "Training Loss:  0.00181407\n",
      "Training Loss:  0.00181407\n",
      "Final training Loss:  0.00181407\n",
      "\n",
      "Running model (trial=3, mod=101, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01310749\n",
      "Training Loss:  0.01310749\n",
      "Training Loss:  0.01310729\n",
      "Training Loss:  0.01310729\n",
      "Training Loss:  0.01310729\n",
      "Training Loss:  0.01310696\n",
      "Training Loss:  0.00024738\n",
      "Training Loss:  0.00024674\n",
      "Training Loss:  0.00024674\n",
      "Training Loss:  0.00024662\n",
      "Final training Loss:  0.00024662\n",
      "\n",
      "Running model (trial=3, mod=102, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02041426\n",
      "Training Loss:  0.02041426\n",
      "Training Loss:  0.02041426\n",
      "Training Loss:  0.02041202\n",
      "Training Loss:  0.02041202\n",
      "Training Loss:  0.02041193\n",
      "Training Loss:  0.02041193\n",
      "Training Loss:  0.02041193\n",
      "Training Loss:  0.02041193\n",
      "Training Loss:  0.02041193\n",
      "Final training Loss:  0.02041193\n",
      "\n",
      "Running model (trial=3, mod=103, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02146994\n",
      "Training Loss:  0.02146577\n",
      "Training Loss:  0.02146473\n",
      "Training Loss:  0.02146473\n",
      "Training Loss:  0.02146315\n",
      "Training Loss:  0.02146315\n",
      "Training Loss:  0.02146315\n",
      "Training Loss:  0.02146315\n",
      "Training Loss:  0.02146315\n",
      "Training Loss:  0.02146315\n",
      "Final training Loss:  0.02146315\n",
      "\n",
      "Running model (trial=3, mod=104, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00317084\n",
      "Training Loss:  0.00317084\n",
      "Training Loss:  0.00317072\n",
      "Training Loss:  0.00317072\n",
      "Training Loss:  0.00317072\n",
      "Training Loss:  0.00317072\n",
      "Training Loss:  0.00317068\n",
      "Training Loss:  0.00317068\n",
      "Training Loss:  0.00317068\n",
      "Training Loss:  0.00317068\n",
      "Final training Loss:  0.00317068\n",
      "\n",
      "Running model (trial=3, mod=105, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00335398\n",
      "Training Loss:  0.0033536\n",
      "Training Loss:  0.00335359\n",
      "Training Loss:  0.00335359\n",
      "Training Loss:  0.00335359\n",
      "Training Loss:  0.00335362\n",
      "Training Loss:  0.00335362\n",
      "Training Loss:  0.00335359\n",
      "Training Loss:  0.00335359\n",
      "Training Loss:  0.00335362\n",
      "Final training Loss:  0.00335362\n",
      "\n",
      "Running model (trial=3, mod=106, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00658116\n",
      "Training Loss:  0.00658116\n",
      "Training Loss:  0.00658116\n",
      "Training Loss:  0.00658115\n",
      "Training Loss:  0.00658115\n",
      "Training Loss:  0.00658115\n",
      "Training Loss:  0.00658115\n",
      "Training Loss:  0.00658115\n",
      "Training Loss:  0.00658115\n",
      "Training Loss:  0.00658115\n",
      "Final training Loss:  0.00658115\n",
      "\n",
      "Running model (trial=3, mod=107, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.32116386\n",
      "Training Loss:  0.00319408\n",
      "Training Loss:  0.00319408\n",
      "Training Loss:  0.00319408\n",
      "Training Loss:  0.00319408\n",
      "Training Loss:  0.00319408\n",
      "Training Loss:  0.00319408\n",
      "Training Loss:  0.00319408\n",
      "Training Loss:  0.00319408\n",
      "Training Loss:  0.00319408\n",
      "Final training Loss:  0.00319408\n",
      "\n",
      "Running model (trial=3, mod=108, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00697816\n",
      "Training Loss:  0.00697813\n",
      "Training Loss:  0.00697813\n",
      "Training Loss:  0.00697813\n",
      "Training Loss:  0.00697813\n",
      "Training Loss:  0.00697813\n",
      "Training Loss:  0.00697813\n",
      "Training Loss:  0.00697813\n",
      "Training Loss:  0.00697813\n",
      "Training Loss:  0.00697813\n",
      "Final training Loss:  0.00697813\n",
      "\n",
      "Running model (trial=3, mod=109, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00209325\n",
      "Training Loss:  0.00209325\n",
      "Training Loss:  0.00209325\n",
      "Training Loss:  0.00209325\n",
      "Training Loss:  0.00209325\n",
      "Training Loss:  0.00209316\n",
      "Training Loss:  0.0020932\n",
      "Training Loss:  0.00209316\n",
      "Training Loss:  0.00209316\n",
      "Training Loss:  0.0020932\n",
      "Final training Loss:  0.0020932\n",
      "\n",
      "Running model (trial=3, mod=110, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Training Loss:  0.00138211\n",
      "Final training Loss:  0.00138211\n",
      "\n",
      "Running model (trial=3, mod=111, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Training Loss:  0.00797691\n",
      "Final training Loss:  0.00797691\n",
      "\n",
      "Running model (trial=4, mod=112, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06431495\n",
      "Training Loss:  0.06431495\n",
      "Training Loss:  0.06431495\n",
      "Training Loss:  0.06431495\n",
      "Training Loss:  0.06431495\n",
      "Training Loss:  0.00214167\n",
      "Training Loss:  0.00214167\n",
      "Training Loss:  0.00214083\n",
      "Training Loss:  0.00213985\n",
      "Training Loss:  0.00213862\n",
      "Final training Loss:  0.00213862\n",
      "\n",
      "Running model (trial=4, mod=113, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=114, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00229409\n",
      "Training Loss:  0.002294\n",
      "Training Loss:  0.00168288\n",
      "Training Loss:  0.00168288\n",
      "Training Loss:  0.00168286\n",
      "Training Loss:  0.00168286\n",
      "Training Loss:  0.00168283\n",
      "Training Loss:  0.00168283\n",
      "Training Loss:  0.00168283\n",
      "Training Loss:  0.00168283\n",
      "Final training Loss:  0.00168283\n",
      "\n",
      "Running model (trial=4, mod=115, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00374815\n",
      "Training Loss:  0.00374815\n",
      "Training Loss:  0.00374815\n",
      "Training Loss:  0.00374816\n",
      "Training Loss:  0.00374811\n",
      "Training Loss:  0.00374811\n",
      "Training Loss:  0.00374811\n",
      "Training Loss:  0.00374811\n",
      "Training Loss:  0.00374811\n",
      "Training Loss:  0.00374811\n",
      "Final training Loss:  0.00374811\n",
      "\n",
      "Running model (trial=4, mod=116, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00167806\n",
      "Training Loss:  0.00051757\n",
      "Training Loss:  0.00051757\n",
      "Training Loss:  0.00051752\n",
      "Training Loss:  0.00051741\n",
      "Training Loss:  0.00051741\n",
      "Training Loss:  0.00051741\n",
      "Training Loss:  0.00051739\n",
      "Training Loss:  0.00051739\n",
      "Training Loss:  0.00051737\n",
      "Final training Loss:  0.00051737\n",
      "\n",
      "Running model (trial=4, mod=117, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00606879\n",
      "Training Loss:  0.00606879\n",
      "Training Loss:  0.00606875\n",
      "Training Loss:  0.00606871\n",
      "Training Loss:  0.00606871\n",
      "Training Loss:  0.00606871\n",
      "Training Loss:  0.00606871\n",
      "Training Loss:  0.00606867\n",
      "Training Loss:  0.00606867\n",
      "Training Loss:  0.00606845\n",
      "Final training Loss:  0.00606845\n",
      "\n",
      "Running model (trial=4, mod=118, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Training Loss:  0.01830461\n",
      "Final training Loss:  0.01830461\n",
      "\n",
      "Running model (trial=4, mod=119, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00048827\n",
      "Training Loss:  0.00048827\n",
      "Training Loss:  0.00048827\n",
      "Training Loss:  0.00048808\n",
      "Training Loss:  0.00048808\n",
      "Training Loss:  0.00048808\n",
      "Training Loss:  0.00048808\n",
      "Training Loss:  0.00048803\n",
      "Training Loss:  0.00048803\n",
      "Training Loss:  0.00048804\n",
      "Final training Loss:  0.00048804\n",
      "\n",
      "Running model (trial=4, mod=120, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00060792\n",
      "Training Loss:  0.00060792\n",
      "Training Loss:  0.00060792\n",
      "Training Loss:  0.00060792\n",
      "Training Loss:  0.00060792\n",
      "Training Loss:  0.00060792\n",
      "Training Loss:  0.00060792\n",
      "Training Loss:  0.00060792\n",
      "Training Loss:  0.00060244\n",
      "Training Loss:  0.00060244\n",
      "Final training Loss:  0.00060244\n",
      "\n",
      "Running model (trial=4, mod=121, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00153709\n",
      "Training Loss:  0.0013327\n",
      "Training Loss:  0.0013327\n",
      "Training Loss:  0.0013327\n",
      "Training Loss:  0.0013327\n",
      "Training Loss:  0.00133269\n",
      "Training Loss:  0.00133269\n",
      "Training Loss:  0.00133269\n",
      "Training Loss:  0.00133267\n",
      "Training Loss:  0.00133267\n",
      "Final training Loss:  0.00133267\n",
      "\n",
      "Running model (trial=4, mod=122, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00030375\n",
      "Training Loss:  0.00030371\n",
      "Training Loss:  0.00030371\n",
      "Training Loss:  0.00030371\n",
      "Training Loss:  0.00030371\n",
      "Training Loss:  0.00030371\n",
      "Training Loss:  0.00030371\n",
      "Training Loss:  0.00030371\n",
      "Training Loss:  0.00030371\n",
      "Training Loss:  0.00030371\n",
      "Final training Loss:  0.00030371\n",
      "\n",
      "Running model (trial=4, mod=123, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00109574\n",
      "Training Loss:  0.00109565\n",
      "Training Loss:  0.00109565\n",
      "Training Loss:  0.00109565\n",
      "Training Loss:  0.00109565\n",
      "Training Loss:  0.00109565\n",
      "Training Loss:  0.00109565\n",
      "Training Loss:  0.00109565\n",
      "Training Loss:  0.00109565\n",
      "Training Loss:  0.00109565\n",
      "Final training Loss:  0.00109565\n",
      "\n",
      "Running model (trial=4, mod=124, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Training Loss:  599.98077393\n",
      "Final training Loss:  599.98077393\n",
      "\n",
      "Running model (trial=4, mod=125, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10131836\n",
      "Final training Loss:  600.10131836\n",
      "\n",
      "Running model (trial=4, mod=126, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.97967958\n",
      "Training Loss:  4.91293478\n",
      "Training Loss:  4.91293478\n",
      "Training Loss:  4.91293478\n",
      "Training Loss:  4.91293478\n",
      "Training Loss:  4.91293478\n",
      "Training Loss:  4.91293478\n",
      "Training Loss:  4.91293478\n",
      "Training Loss:  4.91293478\n",
      "Training Loss:  4.91293478\n",
      "Final training Loss:  4.91293478\n",
      "\n",
      "Running model (trial=4, mod=127, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Training Loss:  59.869133\n",
      "Final training Loss:  59.869133\n",
      "\n",
      "Running model (trial=4, mod=128, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00865737\n",
      "Training Loss:  0.00865737\n",
      "Training Loss:  0.00865584\n",
      "Training Loss:  0.00865584\n",
      "Training Loss:  0.00865527\n",
      "Training Loss:  0.00865527\n",
      "Training Loss:  0.00865527\n",
      "Training Loss:  0.00865385\n",
      "Training Loss:  0.00865385\n",
      "Training Loss:  0.00865382\n",
      "Final training Loss:  0.00865382\n",
      "\n",
      "Running model (trial=4, mod=129, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02777163\n",
      "Training Loss:  0.02777163\n",
      "Training Loss:  0.02777163\n",
      "Training Loss:  0.02777163\n",
      "Training Loss:  0.02777103\n",
      "Training Loss:  0.02777103\n",
      "Training Loss:  0.02777103\n",
      "Training Loss:  0.02777103\n",
      "Training Loss:  0.02777103\n",
      "Training Loss:  0.0277709\n",
      "Final training Loss:  0.0277709\n",
      "\n",
      "Running model (trial=4, mod=130, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00415829\n",
      "Training Loss:  0.00415829\n",
      "Training Loss:  0.00415829\n",
      "Training Loss:  0.00415829\n",
      "Training Loss:  0.00415829\n",
      "Training Loss:  0.00213481\n",
      "Training Loss:  0.00213481\n",
      "Training Loss:  0.00213481\n",
      "Training Loss:  0.00213472\n",
      "Training Loss:  0.00213463\n",
      "Final training Loss:  0.00213463\n",
      "\n",
      "Running model (trial=4, mod=131, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00244963\n",
      "Training Loss:  0.00244963\n",
      "Training Loss:  0.00244296\n",
      "Training Loss:  0.00244296\n",
      "Training Loss:  0.00244261\n",
      "Training Loss:  0.00244138\n",
      "Training Loss:  0.00244138\n",
      "Training Loss:  0.00244134\n",
      "Training Loss:  0.00244134\n",
      "Training Loss:  0.00244095\n",
      "Final training Loss:  0.00244095\n",
      "\n",
      "Running model (trial=4, mod=132, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Training Loss:  0.00486383\n",
      "Final training Loss:  0.00486383\n",
      "\n",
      "Running model (trial=4, mod=133, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00199159\n",
      "Training Loss:  0.00199115\n",
      "Training Loss:  0.00199103\n",
      "Training Loss:  0.00199103\n",
      "Training Loss:  0.00199103\n",
      "Training Loss:  0.00199103\n",
      "Training Loss:  0.00199103\n",
      "Training Loss:  0.00199103\n",
      "Training Loss:  0.00199103\n",
      "Training Loss:  0.00199103\n",
      "Final training Loss:  0.00199103\n",
      "\n",
      "Running model (trial=4, mod=134, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00563547\n",
      "Training Loss:  0.00563547\n",
      "Training Loss:  0.00563473\n",
      "Training Loss:  0.00563447\n",
      "Training Loss:  0.0056343\n",
      "Training Loss:  0.0056343\n",
      "Training Loss:  0.0056343\n",
      "Training Loss:  0.0056343\n",
      "Training Loss:  0.0056343\n",
      "Training Loss:  0.0056343\n",
      "Final training Loss:  0.0056343\n",
      "\n",
      "Running model (trial=4, mod=135, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Training Loss:  0.0330032\n",
      "Final training Loss:  0.0330032\n",
      "\n",
      "Running model (trial=4, mod=136, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0008357\n",
      "Training Loss:  0.0008357\n",
      "Training Loss:  0.0008357\n",
      "Training Loss:  0.0008357\n",
      "Training Loss:  0.0008357\n",
      "Training Loss:  0.00083566\n",
      "Training Loss:  0.00083559\n",
      "Training Loss:  0.00083553\n",
      "Training Loss:  0.00083553\n",
      "Training Loss:  0.00083553\n",
      "Final training Loss:  0.00083553\n",
      "\n",
      "Running model (trial=4, mod=137, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00404784\n",
      "Training Loss:  0.00404784\n",
      "Training Loss:  0.00404784\n",
      "Training Loss:  0.00028856\n",
      "Training Loss:  0.00028856\n",
      "Training Loss:  0.00028852\n",
      "Training Loss:  0.00028852\n",
      "Training Loss:  0.00028852\n",
      "Training Loss:  0.00028852\n",
      "Training Loss:  0.00028852\n",
      "Final training Loss:  0.00028852\n",
      "\n",
      "Running model (trial=4, mod=138, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00416465\n",
      "Training Loss:  0.00416465\n",
      "Training Loss:  0.00416465\n",
      "Training Loss:  0.00416465\n",
      "Training Loss:  0.00052423\n",
      "Training Loss:  0.00052415\n",
      "Training Loss:  0.00052415\n",
      "Training Loss:  0.00052415\n",
      "Training Loss:  0.00052415\n",
      "Training Loss:  0.00052415\n",
      "Final training Loss:  0.00052415\n",
      "\n",
      "Running model (trial=4, mod=139, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00033189\n",
      "Training Loss:  0.0003318\n",
      "Training Loss:  0.00030083\n",
      "Training Loss:  0.00030063\n",
      "Training Loss:  0.00030063\n",
      "Training Loss:  0.00030063\n",
      "Training Loss:  0.00030063\n",
      "Training Loss:  0.00030063\n",
      "Training Loss:  0.00030063\n",
      "Training Loss:  0.00030063\n",
      "Final training Loss:  0.00030063\n",
      "\n",
      "Running model (trial=5, mod=140, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.29964784\n",
      "Training Loss:  0.29313833\n",
      "Training Loss:  0.29313833\n",
      "Training Loss:  0.29313797\n",
      "Training Loss:  0.29313797\n",
      "Training Loss:  0.29313797\n",
      "Training Loss:  0.29313797\n",
      "Training Loss:  0.28948694\n",
      "Training Loss:  0.28948694\n",
      "Training Loss:  0.28948694\n",
      "Final training Loss:  0.28948694\n",
      "\n",
      "Running model (trial=5, mod=141, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00040445\n",
      "Training Loss:  0.00040445\n",
      "Training Loss:  0.00040443\n",
      "Training Loss:  0.00040443\n",
      "Training Loss:  0.00040443\n",
      "Training Loss:  0.00040443\n",
      "Training Loss:  0.00040443\n",
      "Training Loss:  0.00040443\n",
      "Training Loss:  0.00040443\n",
      "Training Loss:  0.00040443\n",
      "Final training Loss:  0.00040443\n",
      "\n",
      "Running model (trial=5, mod=142, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00136559\n",
      "Training Loss:  0.00136559\n",
      "Training Loss:  0.00136559\n",
      "Training Loss:  0.00136559\n",
      "Training Loss:  0.00136559\n",
      "Training Loss:  0.00136559\n",
      "Training Loss:  0.00136557\n",
      "Training Loss:  0.00136557\n",
      "Training Loss:  0.00136557\n",
      "Training Loss:  0.00136551\n",
      "Final training Loss:  0.00136551\n",
      "\n",
      "Running model (trial=5, mod=143, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00018981\n",
      "Training Loss:  0.00018981\n",
      "Training Loss:  0.00018981\n",
      "Training Loss:  0.00018979\n",
      "Training Loss:  0.00018979\n",
      "Training Loss:  0.00018979\n",
      "Training Loss:  0.00018979\n",
      "Training Loss:  0.00018979\n",
      "Training Loss:  0.00018979\n",
      "Training Loss:  0.00018977\n",
      "Final training Loss:  0.00018977\n",
      "\n",
      "Running model (trial=5, mod=144, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00103106\n",
      "Training Loss:  0.00103104\n",
      "Training Loss:  0.00103104\n",
      "Training Loss:  0.00103104\n",
      "Training Loss:  0.00103104\n",
      "Training Loss:  0.00103103\n",
      "Training Loss:  0.00103103\n",
      "Training Loss:  0.00103097\n",
      "Training Loss:  0.00103097\n",
      "Training Loss:  0.00103093\n",
      "Final training Loss:  0.00103093\n",
      "\n",
      "Running model (trial=5, mod=145, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Training Loss:  0.00024023\n",
      "Final training Loss:  0.00024023\n",
      "\n",
      "Running model (trial=5, mod=146, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00395301\n",
      "Training Loss:  0.00395301\n",
      "Training Loss:  0.00395301\n",
      "Training Loss:  0.00395301\n",
      "Training Loss:  0.00395301\n",
      "Training Loss:  0.00202085\n",
      "Training Loss:  0.00202085\n",
      "Training Loss:  0.00202085\n",
      "Training Loss:  0.00202085\n",
      "Training Loss:  0.00202083\n",
      "Final training Loss:  0.00202083\n",
      "\n",
      "Running model (trial=5, mod=147, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00026582\n",
      "Training Loss:  0.00026575\n",
      "Training Loss:  0.00026575\n",
      "Training Loss:  0.00026575\n",
      "Training Loss:  0.00026575\n",
      "Training Loss:  0.00026575\n",
      "Training Loss:  0.00026575\n",
      "Training Loss:  0.00026575\n",
      "Training Loss:  0.00026575\n",
      "Training Loss:  0.00026575\n",
      "Final training Loss:  0.00026575\n",
      "\n",
      "Running model (trial=5, mod=148, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00224445\n",
      "Training Loss:  0.00224444\n",
      "Training Loss:  0.00224444\n",
      "Training Loss:  0.00224444\n",
      "Training Loss:  0.00224444\n",
      "Training Loss:  0.00224443\n",
      "Training Loss:  0.00224443\n",
      "Training Loss:  0.00224443\n",
      "Training Loss:  0.00224443\n",
      "Training Loss:  0.00224443\n",
      "Final training Loss:  0.00224443\n",
      "\n",
      "Running model (trial=5, mod=149, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00676426\n",
      "Training Loss:  0.00676396\n",
      "Training Loss:  0.00676396\n",
      "Training Loss:  0.00676396\n",
      "Training Loss:  0.00676396\n",
      "Training Loss:  0.00676396\n",
      "Training Loss:  0.00676396\n",
      "Training Loss:  0.00676362\n",
      "Training Loss:  0.00676362\n",
      "Training Loss:  0.00676362\n",
      "Final training Loss:  0.00676362\n",
      "\n",
      "Running model (trial=5, mod=150, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00039819\n",
      "Training Loss:  0.00011441\n",
      "Training Loss:  0.0001035\n",
      "Training Loss:  0.00010349\n",
      "Training Loss:  0.00010348\n",
      "Training Loss:  0.00010348\n",
      "Training Loss:  0.00010348\n",
      "Training Loss:  0.00010348\n",
      "Training Loss:  0.00010348\n",
      "Training Loss:  0.00010348\n",
      "Final training Loss:  0.00010348\n",
      "\n",
      "Running model (trial=5, mod=151, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00519188\n",
      "Training Loss:  0.00240351\n",
      "Training Loss:  0.00175509\n",
      "Training Loss:  0.00175509\n",
      "Training Loss:  0.00175488\n",
      "Training Loss:  0.00175462\n",
      "Training Loss:  0.00175462\n",
      "Training Loss:  0.00175462\n",
      "Training Loss:  0.00175462\n",
      "Training Loss:  0.00175462\n",
      "Final training Loss:  0.00175462\n",
      "\n",
      "Running model (trial=5, mod=152, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.53245974\n",
      "Training Loss:  4.53245974\n",
      "Training Loss:  0.6789121\n",
      "Training Loss:  0.6789121\n",
      "Training Loss:  0.6789121\n",
      "Training Loss:  0.6789121\n",
      "Training Loss:  0.6789121\n",
      "Training Loss:  0.6789121\n",
      "Training Loss:  0.67888862\n",
      "Training Loss:  0.67888808\n",
      "Final training Loss:  0.67888808\n",
      "\n",
      "Running model (trial=5, mod=153, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10150146\n",
      "Training Loss:  600.10150146\n",
      "Training Loss:  600.10150146\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Final training Loss:  600.10144043\n",
      "\n",
      "Running model (trial=5, mod=154, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.50480926\n",
      "Training Loss:  0.30172938\n",
      "Training Loss:  0.30172938\n",
      "Training Loss:  0.30172938\n",
      "Training Loss:  0.30172938\n",
      "Training Loss:  0.30172938\n",
      "Training Loss:  0.30172938\n",
      "Training Loss:  0.30172938\n",
      "Training Loss:  0.30172938\n",
      "Training Loss:  0.30172938\n",
      "Final training Loss:  0.30172938\n",
      "\n",
      "Running model (trial=5, mod=155, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00389829\n",
      "Training Loss:  0.00389754\n",
      "Training Loss:  0.00389509\n",
      "Training Loss:  0.00389399\n",
      "Training Loss:  0.00389399\n",
      "Training Loss:  0.00389382\n",
      "Training Loss:  0.00389382\n",
      "Training Loss:  0.00389382\n",
      "Training Loss:  0.00389382\n",
      "Training Loss:  0.00389382\n",
      "Final training Loss:  0.00389382\n",
      "\n",
      "Running model (trial=5, mod=156, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02283864\n",
      "Training Loss:  0.02283864\n",
      "Training Loss:  0.02283571\n",
      "Training Loss:  0.02283571\n",
      "Training Loss:  0.02283571\n",
      "Training Loss:  0.02283571\n",
      "Training Loss:  0.02283571\n",
      "Training Loss:  0.02283571\n",
      "Training Loss:  0.02283571\n",
      "Training Loss:  0.02283571\n",
      "Final training Loss:  0.02283571\n",
      "\n",
      "Running model (trial=5, mod=157, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00174005\n",
      "Training Loss:  0.00174005\n",
      "Training Loss:  0.00173915\n",
      "Training Loss:  0.00173915\n",
      "Training Loss:  0.00173915\n",
      "Training Loss:  0.00173915\n",
      "Training Loss:  0.00173915\n",
      "Training Loss:  0.00173915\n",
      "Training Loss:  0.00173915\n",
      "Training Loss:  0.00173915\n",
      "Final training Loss:  0.00173915\n",
      "\n",
      "Running model (trial=5, mod=158, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00431259\n",
      "Training Loss:  0.00431259\n",
      "Training Loss:  0.00431259\n",
      "Training Loss:  0.00431259\n",
      "Training Loss:  0.00431259\n",
      "Training Loss:  0.00431259\n",
      "Training Loss:  0.00431215\n",
      "Training Loss:  0.00431215\n",
      "Training Loss:  0.00431215\n",
      "Training Loss:  0.00431215\n",
      "Final training Loss:  0.00431215\n",
      "\n",
      "Running model (trial=5, mod=159, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00613476\n",
      "Training Loss:  0.00613324\n",
      "Training Loss:  0.00613324\n",
      "Training Loss:  0.00613295\n",
      "Training Loss:  0.00613281\n",
      "Training Loss:  0.00613281\n",
      "Training Loss:  0.00613281\n",
      "Training Loss:  0.00613281\n",
      "Training Loss:  0.00613281\n",
      "Training Loss:  0.00613281\n",
      "Final training Loss:  0.00613281\n",
      "\n",
      "Running model (trial=5, mod=160, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00350245\n",
      "Training Loss:  0.00350245\n",
      "Training Loss:  0.00350245\n",
      "Training Loss:  0.00350245\n",
      "Training Loss:  0.00350245\n",
      "Training Loss:  0.00350245\n",
      "Training Loss:  0.00350245\n",
      "Training Loss:  0.00350245\n",
      "Training Loss:  0.00348917\n",
      "Training Loss:  0.00347305\n",
      "Final training Loss:  0.00347305\n",
      "\n",
      "Running model (trial=5, mod=161, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00399252\n",
      "Training Loss:  0.00329504\n",
      "Training Loss:  0.00329504\n",
      "Training Loss:  0.00329504\n",
      "Training Loss:  0.00329504\n",
      "Training Loss:  0.00329504\n",
      "Training Loss:  0.00329504\n",
      "Training Loss:  0.00329504\n",
      "Training Loss:  0.00329504\n",
      "Training Loss:  0.00329504\n",
      "Final training Loss:  0.00329504\n",
      "\n",
      "Running model (trial=5, mod=162, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01018393\n",
      "Training Loss:  0.01018393\n",
      "Training Loss:  0.01018393\n",
      "Training Loss:  0.00306406\n",
      "Training Loss:  0.00282811\n",
      "Training Loss:  0.00282811\n",
      "Training Loss:  0.00282811\n",
      "Training Loss:  0.00282811\n",
      "Training Loss:  0.00282811\n",
      "Training Loss:  0.00282811\n",
      "Final training Loss:  0.00282811\n",
      "\n",
      "Running model (trial=5, mod=163, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00087218\n",
      "Training Loss:  0.00087218\n",
      "Training Loss:  0.00087218\n",
      "Training Loss:  0.0008721\n",
      "Training Loss:  0.00087209\n",
      "Training Loss:  0.00087209\n",
      "Training Loss:  0.00087209\n",
      "Training Loss:  0.00087209\n",
      "Training Loss:  0.00087209\n",
      "Training Loss:  0.00087209\n",
      "Final training Loss:  0.00087209\n",
      "\n",
      "Running model (trial=5, mod=164, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Training Loss:  0.0014498\n",
      "Final training Loss:  0.0014498\n",
      "\n",
      "Running model (trial=5, mod=165, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00201251\n",
      "Training Loss:  0.00201228\n",
      "Training Loss:  0.00201228\n",
      "Training Loss:  0.00201228\n",
      "Training Loss:  0.00201228\n",
      "Training Loss:  0.00201228\n",
      "Training Loss:  0.00201228\n",
      "Training Loss:  0.00201228\n",
      "Training Loss:  0.00201228\n",
      "Training Loss:  0.00201228\n",
      "Final training Loss:  0.00201228\n",
      "\n",
      "Running model (trial=5, mod=166, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00080825\n",
      "Training Loss:  0.00080825\n",
      "Training Loss:  0.00080826\n",
      "Training Loss:  0.00080825\n",
      "Training Loss:  0.00080825\n",
      "Training Loss:  0.00080825\n",
      "Training Loss:  0.00080825\n",
      "Training Loss:  0.00080825\n",
      "Training Loss:  0.00080825\n",
      "Training Loss:  0.00080813\n",
      "Final training Loss:  0.00080813\n",
      "\n",
      "Running model (trial=5, mod=167, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00136383\n",
      "Training Loss:  0.00117141\n",
      "Training Loss:  0.00117139\n",
      "Training Loss:  0.00117139\n",
      "Training Loss:  0.00117139\n",
      "Training Loss:  0.00117139\n",
      "Training Loss:  0.00117134\n",
      "Training Loss:  0.00117134\n",
      "Training Loss:  0.00117134\n",
      "Training Loss:  0.00117134\n",
      "Final training Loss:  0.00117134\n",
      "\n",
      "Running model (trial=6, mod=168, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.92080873\n",
      "Training Loss:  0.92080873\n",
      "Training Loss:  0.92079329\n",
      "Training Loss:  0.92078996\n",
      "Training Loss:  0.92078996\n",
      "Training Loss:  0.91253477\n",
      "Training Loss:  0.91253209\n",
      "Training Loss:  0.91253209\n",
      "Training Loss:  0.91253209\n",
      "Training Loss:  0.91253209\n",
      "Final training Loss:  0.91253209\n",
      "\n",
      "Running model (trial=6, mod=169, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Training Loss:  0.00017239\n",
      "Final training Loss:  0.00017239\n",
      "\n",
      "Running model (trial=6, mod=170, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Training Loss:  0.00239186\n",
      "Final training Loss:  0.00239186\n",
      "\n",
      "Running model (trial=6, mod=171, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0001029\n",
      "Training Loss:  0.00010284\n",
      "Training Loss:  0.00010284\n",
      "Training Loss:  0.00010284\n",
      "Training Loss:  0.00010284\n",
      "Training Loss:  0.00010284\n",
      "Training Loss:  0.00010284\n",
      "Training Loss:  0.00010284\n",
      "Training Loss:  0.00010284\n",
      "Training Loss:  0.00010284\n",
      "Final training Loss:  0.00010284\n",
      "\n",
      "Running model (trial=6, mod=172, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00270151\n",
      "Training Loss:  0.00270135\n",
      "Training Loss:  0.00270135\n",
      "Training Loss:  0.00270128\n",
      "Training Loss:  0.00270115\n",
      "Training Loss:  0.00270115\n",
      "Training Loss:  0.00270115\n",
      "Training Loss:  0.00270107\n",
      "Training Loss:  0.00270107\n",
      "Training Loss:  0.00270107\n",
      "Final training Loss:  0.00270107\n",
      "\n",
      "Running model (trial=6, mod=173, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00159174\n",
      "Training Loss:  0.00159174\n",
      "Training Loss:  0.00159174\n",
      "Training Loss:  0.00159174\n",
      "Training Loss:  0.00159174\n",
      "Training Loss:  0.00159174\n",
      "Training Loss:  0.00159168\n",
      "Training Loss:  0.00159168\n",
      "Training Loss:  0.00159162\n",
      "Training Loss:  0.00159162\n",
      "Final training Loss:  0.00159162\n",
      "\n",
      "Running model (trial=6, mod=174, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00162163\n",
      "Training Loss:  0.00162163\n",
      "Training Loss:  0.00162119\n",
      "Training Loss:  0.00162119\n",
      "Training Loss:  0.00162112\n",
      "Training Loss:  0.00162112\n",
      "Training Loss:  0.00162112\n",
      "Training Loss:  0.00162112\n",
      "Training Loss:  0.00162112\n",
      "Training Loss:  0.00162112\n",
      "Final training Loss:  0.00162112\n",
      "\n",
      "Running model (trial=6, mod=175, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0047149\n",
      "Training Loss:  0.00471416\n",
      "Training Loss:  0.00471416\n",
      "Training Loss:  0.00471246\n",
      "Training Loss:  0.00471246\n",
      "Training Loss:  0.00471246\n",
      "Training Loss:  0.00471098\n",
      "Training Loss:  0.00471098\n",
      "Training Loss:  0.00471098\n",
      "Training Loss:  0.00471098\n",
      "Final training Loss:  0.00471098\n",
      "\n",
      "Running model (trial=6, mod=176, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00145877\n",
      "Training Loss:  0.00014495\n",
      "Training Loss:  0.00014488\n",
      "Training Loss:  0.00014488\n",
      "Training Loss:  0.00014488\n",
      "Training Loss:  0.00014488\n",
      "Training Loss:  0.00014488\n",
      "Training Loss:  0.00014486\n",
      "Training Loss:  0.00014486\n",
      "Training Loss:  0.00014486\n",
      "Final training Loss:  0.00014486\n",
      "\n",
      "Running model (trial=6, mod=177, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0014072\n",
      "Training Loss:  0.00092311\n",
      "Training Loss:  0.00071132\n",
      "Training Loss:  0.00071132\n",
      "Training Loss:  0.00066578\n",
      "Training Loss:  0.00066578\n",
      "Training Loss:  0.00066578\n",
      "Training Loss:  0.00066578\n",
      "Training Loss:  0.00066563\n",
      "Training Loss:  0.00066561\n",
      "Final training Loss:  0.00066561\n",
      "\n",
      "Running model (trial=6, mod=178, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00179044\n",
      "Training Loss:  0.00179033\n",
      "Training Loss:  0.00145045\n",
      "Training Loss:  0.00145042\n",
      "Training Loss:  0.0014503\n",
      "Training Loss:  0.00022415\n",
      "Training Loss:  0.00022415\n",
      "Training Loss:  0.00022415\n",
      "Training Loss:  0.00022415\n",
      "Training Loss:  0.00022415\n",
      "Final training Loss:  0.00022415\n",
      "\n",
      "Running model (trial=6, mod=179, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00759075\n",
      "Training Loss:  0.0075907\n",
      "Training Loss:  0.0075907\n",
      "Training Loss:  0.0075907\n",
      "Training Loss:  0.0075907\n",
      "Training Loss:  0.00758976\n",
      "Training Loss:  0.00758976\n",
      "Training Loss:  0.00758976\n",
      "Training Loss:  0.00758976\n",
      "Training Loss:  0.00758976\n",
      "Final training Loss:  0.00758976\n",
      "\n",
      "Running model (trial=6, mod=180, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.37277472\n",
      "Training Loss:  1.37268233\n",
      "Training Loss:  1.37268233\n",
      "Training Loss:  1.37262738\n",
      "Training Loss:  1.37262738\n",
      "Training Loss:  1.37262738\n",
      "Training Loss:  1.37262738\n",
      "Training Loss:  1.37262738\n",
      "Training Loss:  1.37262738\n",
      "Training Loss:  1.37262738\n",
      "Final training Loss:  1.37262738\n",
      "\n",
      "Running model (trial=6, mod=181, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10113525\n",
      "Final training Loss:  600.10113525\n",
      "\n",
      "Running model (trial=6, mod=182, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00220718\n",
      "Training Loss:  0.00220718\n",
      "Training Loss:  0.00220718\n",
      "Training Loss:  0.00220708\n",
      "Training Loss:  0.00220708\n",
      "Training Loss:  0.00220708\n",
      "Training Loss:  0.00220708\n",
      "Training Loss:  0.00220705\n",
      "Training Loss:  0.00220705\n",
      "Training Loss:  0.00220704\n",
      "Final training Loss:  0.00220704\n",
      "\n",
      "Running model (trial=6, mod=183, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05833406\n",
      "Training Loss:  0.00608082\n",
      "Training Loss:  0.00607816\n",
      "Training Loss:  0.00406837\n",
      "Training Loss:  0.00406837\n",
      "Training Loss:  0.00335094\n",
      "Training Loss:  0.00335094\n",
      "Training Loss:  0.00335094\n",
      "Training Loss:  0.00335094\n",
      "Training Loss:  0.00335094\n",
      "Final training Loss:  0.00335094\n",
      "\n",
      "Running model (trial=6, mod=184, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00233211\n",
      "Training Loss:  0.0023321\n",
      "Training Loss:  0.0023321\n",
      "Training Loss:  0.00233205\n",
      "Training Loss:  0.00233205\n",
      "Training Loss:  0.00233205\n",
      "Training Loss:  0.00233205\n",
      "Training Loss:  0.00233205\n",
      "Training Loss:  0.00233205\n",
      "Training Loss:  0.00233205\n",
      "Final training Loss:  0.00233205\n",
      "\n",
      "Running model (trial=6, mod=185, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00012661\n",
      "Training Loss:  0.00012661\n",
      "Training Loss:  0.00012661\n",
      "Training Loss:  0.00012661\n",
      "Training Loss:  0.00012657\n",
      "Training Loss:  0.00012653\n",
      "Training Loss:  0.00012653\n",
      "Training Loss:  0.00012653\n",
      "Training Loss:  0.00012653\n",
      "Training Loss:  0.00012653\n",
      "Final training Loss:  0.00012653\n",
      "\n",
      "Running model (trial=6, mod=186, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0063414\n",
      "Training Loss:  0.0063414\n",
      "Training Loss:  0.00634136\n",
      "Training Loss:  0.00634136\n",
      "Training Loss:  0.00634136\n",
      "Training Loss:  0.00634134\n",
      "Training Loss:  0.00634134\n",
      "Training Loss:  0.00634134\n",
      "Training Loss:  0.0063413\n",
      "Training Loss:  0.0063413\n",
      "Final training Loss:  0.0063413\n",
      "\n",
      "Running model (trial=6, mod=187, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00092427\n",
      "Training Loss:  0.00092427\n",
      "Training Loss:  0.00092418\n",
      "Training Loss:  0.00092418\n",
      "Training Loss:  0.00092418\n",
      "Training Loss:  0.00092418\n",
      "Training Loss:  0.00092418\n",
      "Training Loss:  0.00092418\n",
      "Training Loss:  0.00092418\n",
      "Training Loss:  0.00092418\n",
      "Final training Loss:  0.00092418\n",
      "\n",
      "Running model (trial=6, mod=188, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00487551\n",
      "Training Loss:  0.00487546\n",
      "Training Loss:  0.0041518\n",
      "Training Loss:  0.00415175\n",
      "Training Loss:  0.00415175\n",
      "Training Loss:  0.00415175\n",
      "Training Loss:  0.00415175\n",
      "Training Loss:  0.00276617\n",
      "Training Loss:  0.00276617\n",
      "Training Loss:  0.00276617\n",
      "Final training Loss:  0.00276617\n",
      "\n",
      "Running model (trial=6, mod=189, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04832347\n",
      "Training Loss:  0.0483202\n",
      "Training Loss:  0.0483202\n",
      "Training Loss:  0.0483184\n",
      "Training Loss:  0.0483184\n",
      "Training Loss:  0.0483184\n",
      "Training Loss:  0.0483184\n",
      "Training Loss:  0.04831708\n",
      "Training Loss:  0.04831708\n",
      "Training Loss:  0.04831708\n",
      "Final training Loss:  0.04831708\n",
      "\n",
      "Running model (trial=6, mod=190, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.252e-05\n",
      "Training Loss:  7.252e-05\n",
      "Training Loss:  7.25e-05\n",
      "Training Loss:  7.25e-05\n",
      "Training Loss:  7.25e-05\n",
      "Training Loss:  7.25e-05\n",
      "Training Loss:  7.25e-05\n",
      "Training Loss:  7.25e-05\n",
      "Training Loss:  7.25e-05\n",
      "Training Loss:  7.25e-05\n",
      "Final training Loss:  7.25e-05\n",
      "\n",
      "Running model (trial=6, mod=191, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.34891114\n",
      "Training Loss:  0.12658036\n",
      "Training Loss:  0.12656738\n",
      "Training Loss:  0.12656738\n",
      "Training Loss:  0.12655093\n",
      "Training Loss:  0.12520903\n",
      "Training Loss:  0.09916911\n",
      "Training Loss:  0.09916911\n",
      "Training Loss:  0.09916911\n",
      "Training Loss:  0.09916911\n",
      "Final training Loss:  0.09916911\n",
      "\n",
      "Running model (trial=6, mod=192, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0033788\n",
      "Training Loss:  0.00337867\n",
      "Training Loss:  0.00337853\n",
      "Training Loss:  0.00337853\n",
      "Training Loss:  0.00337853\n",
      "Training Loss:  0.00337845\n",
      "Training Loss:  0.00337833\n",
      "Training Loss:  0.00337833\n",
      "Training Loss:  0.00337816\n",
      "Training Loss:  0.00337816\n",
      "Final training Loss:  0.00337816\n",
      "\n",
      "Running model (trial=6, mod=193, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01223148\n",
      "Training Loss:  0.01223148\n",
      "Training Loss:  0.01223148\n",
      "Training Loss:  0.00162861\n",
      "Training Loss:  0.00162861\n",
      "Training Loss:  0.00162861\n",
      "Training Loss:  0.00162861\n",
      "Training Loss:  0.00162861\n",
      "Training Loss:  0.00162861\n",
      "Training Loss:  0.00162861\n",
      "Final training Loss:  0.00162861\n",
      "\n",
      "Running model (trial=6, mod=194, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00062711\n",
      "Training Loss:  0.00062703\n",
      "Training Loss:  0.00062686\n",
      "Training Loss:  0.00014064\n",
      "Training Loss:  0.00014064\n",
      "Training Loss:  0.00014064\n",
      "Training Loss:  0.00014064\n",
      "Training Loss:  0.00014028\n",
      "Training Loss:  0.00014027\n",
      "Training Loss:  0.00014027\n",
      "Final training Loss:  0.00014027\n",
      "\n",
      "Running model (trial=6, mod=195, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Training Loss:  78.25773621\n",
      "Final training Loss:  78.25773621\n",
      "\n",
      "Running model (trial=7, mod=196, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61160314\n",
      "Training Loss:  0.61112148\n",
      "Training Loss:  0.61111689\n",
      "Training Loss:  0.61108726\n",
      "Training Loss:  0.61108726\n",
      "Training Loss:  0.61107868\n",
      "Training Loss:  0.61107868\n",
      "Training Loss:  0.61107868\n",
      "Training Loss:  0.61107868\n",
      "Training Loss:  0.61107868\n",
      "Final training Loss:  0.61107868\n",
      "\n",
      "Running model (trial=7, mod=197, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.503e-05\n",
      "Training Loss:  7.503e-05\n",
      "Training Loss:  7.503e-05\n",
      "Training Loss:  7.502e-05\n",
      "Training Loss:  7.5e-05\n",
      "Training Loss:  7.5e-05\n",
      "Training Loss:  7.5e-05\n",
      "Training Loss:  7.5e-05\n",
      "Training Loss:  7.498e-05\n",
      "Training Loss:  7.498e-05\n",
      "Final training Loss:  7.498e-05\n",
      "\n",
      "Running model (trial=7, mod=198, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00017276\n",
      "Training Loss:  0.00017276\n",
      "Training Loss:  0.00017276\n",
      "Training Loss:  0.00017276\n",
      "Training Loss:  0.00017276\n",
      "Training Loss:  0.00017273\n",
      "Training Loss:  0.00017273\n",
      "Training Loss:  0.00017272\n",
      "Training Loss:  0.00017272\n",
      "Training Loss:  0.00017272\n",
      "Final training Loss:  0.00017272\n",
      "\n",
      "Running model (trial=7, mod=199, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00592008\n",
      "Training Loss:  0.00128892\n",
      "Training Loss:  0.00128892\n",
      "Training Loss:  0.00128873\n",
      "Training Loss:  0.00128873\n",
      "Training Loss:  0.00128873\n",
      "Training Loss:  0.00128873\n",
      "Training Loss:  0.00128873\n",
      "Training Loss:  0.00128873\n",
      "Training Loss:  0.00128873\n",
      "Final training Loss:  0.00128873\n",
      "\n",
      "Running model (trial=7, mod=200, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0014316\n",
      "Training Loss:  0.00130305\n",
      "Training Loss:  0.00130305\n",
      "Training Loss:  0.00130287\n",
      "Training Loss:  0.00130287\n",
      "Training Loss:  0.00130285\n",
      "Training Loss:  0.00130285\n",
      "Training Loss:  0.00130285\n",
      "Training Loss:  0.00130285\n",
      "Training Loss:  0.00130285\n",
      "Final training Loss:  0.00130285\n",
      "\n",
      "Running model (trial=7, mod=201, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00230204\n",
      "Training Loss:  0.00230204\n",
      "Training Loss:  0.00230193\n",
      "Training Loss:  0.00230193\n",
      "Training Loss:  0.00230193\n",
      "Training Loss:  0.00230193\n",
      "Training Loss:  0.00230193\n",
      "Training Loss:  0.00230193\n",
      "Training Loss:  0.00230193\n",
      "Training Loss:  0.00230193\n",
      "Final training Loss:  0.00230193\n",
      "\n",
      "Running model (trial=7, mod=202, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0056627\n",
      "Training Loss:  0.00391217\n",
      "Training Loss:  0.00391205\n",
      "Training Loss:  0.00391205\n",
      "Training Loss:  0.00391205\n",
      "Training Loss:  0.00391205\n",
      "Training Loss:  0.00391206\n",
      "Training Loss:  0.00391205\n",
      "Training Loss:  0.00391205\n",
      "Training Loss:  0.00391206\n",
      "Final training Loss:  0.00391206\n",
      "\n",
      "Running model (trial=7, mod=203, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765996\n",
      "Training Loss:  0.00765984\n",
      "Final training Loss:  0.00765984\n",
      "\n",
      "Running model (trial=7, mod=204, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00582519\n",
      "Training Loss:  0.00582519\n",
      "Training Loss:  0.00582481\n",
      "Training Loss:  0.0058246\n",
      "Training Loss:  0.0058246\n",
      "Training Loss:  0.0058246\n",
      "Training Loss:  0.0058246\n",
      "Training Loss:  0.0058246\n",
      "Training Loss:  0.0058246\n",
      "Training Loss:  0.0058246\n",
      "Final training Loss:  0.0058246\n",
      "\n",
      "Running model (trial=7, mod=205, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00129054\n",
      "Training Loss:  0.00129054\n",
      "Training Loss:  0.00129054\n",
      "Training Loss:  0.0012905\n",
      "Training Loss:  0.0012905\n",
      "Training Loss:  0.00111614\n",
      "Training Loss:  0.00111599\n",
      "Training Loss:  0.00111594\n",
      "Training Loss:  0.00111594\n",
      "Training Loss:  0.00111571\n",
      "Final training Loss:  0.00111571\n",
      "\n",
      "Running model (trial=7, mod=206, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00017145\n",
      "Training Loss:  0.00017145\n",
      "Training Loss:  0.00017144\n",
      "Training Loss:  0.00017144\n",
      "Training Loss:  0.00017144\n",
      "Training Loss:  0.00017144\n",
      "Training Loss:  0.00017144\n",
      "Training Loss:  0.00017139\n",
      "Training Loss:  0.00017139\n",
      "Training Loss:  0.00017139\n",
      "Final training Loss:  0.00017139\n",
      "\n",
      "Running model (trial=7, mod=207, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00295869\n",
      "Training Loss:  0.00199162\n",
      "Training Loss:  0.00199162\n",
      "Training Loss:  0.00199162\n",
      "Training Loss:  0.00194396\n",
      "Training Loss:  0.00194396\n",
      "Training Loss:  0.00194396\n",
      "Training Loss:  0.00194388\n",
      "Training Loss:  0.00194388\n",
      "Training Loss:  0.00194388\n",
      "Final training Loss:  0.00194388\n",
      "\n",
      "Running model (trial=7, mod=208, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.0993042\n",
      "Training Loss:  600.0993042\n",
      "Training Loss:  600.097229\n",
      "Training Loss:  600.097229\n",
      "Training Loss:  600.097229\n",
      "Training Loss:  600.097229\n",
      "Training Loss:  600.097229\n",
      "Training Loss:  600.097229\n",
      "Training Loss:  600.097229\n",
      "Training Loss:  600.097229\n",
      "Final training Loss:  600.097229\n",
      "\n",
      "Running model (trial=7, mod=209, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10119629\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10119629\n",
      "Training Loss:  600.10113525\n",
      "Training Loss:  600.10107422\n",
      "Training Loss:  600.10107422\n",
      "Training Loss:  600.10107422\n",
      "Final training Loss:  600.10107422\n",
      "\n",
      "Running model (trial=7, mod=210, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0108852\n",
      "Training Loss:  0.0108852\n",
      "Training Loss:  0.00385091\n",
      "Training Loss:  0.00385091\n",
      "Training Loss:  0.00385091\n",
      "Training Loss:  0.00378712\n",
      "Training Loss:  0.00378712\n",
      "Training Loss:  0.00378706\n",
      "Training Loss:  0.00378706\n",
      "Training Loss:  0.00378692\n",
      "Final training Loss:  0.00378692\n",
      "\n",
      "Running model (trial=7, mod=211, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01671116\n",
      "Training Loss:  0.01671008\n",
      "Training Loss:  0.01671008\n",
      "Training Loss:  0.01671008\n",
      "Training Loss:  0.01671008\n",
      "Training Loss:  0.01671002\n",
      "Training Loss:  0.01671002\n",
      "Training Loss:  0.01671002\n",
      "Training Loss:  0.01671002\n",
      "Training Loss:  0.01671002\n",
      "Final training Loss:  0.01671002\n",
      "\n",
      "Running model (trial=7, mod=212, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00762681\n",
      "Training Loss:  0.00762681\n",
      "Training Loss:  0.00762681\n",
      "Training Loss:  0.00762681\n",
      "Training Loss:  0.00762646\n",
      "Training Loss:  0.00762646\n",
      "Training Loss:  0.00762646\n",
      "Training Loss:  0.00762646\n",
      "Training Loss:  0.00762646\n",
      "Training Loss:  0.00762613\n",
      "Final training Loss:  0.00762613\n",
      "\n",
      "Running model (trial=7, mod=213, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00071566\n",
      "Training Loss:  0.00071047\n",
      "Training Loss:  0.00070735\n",
      "Training Loss:  0.00070733\n",
      "Training Loss:  0.0007068\n",
      "Training Loss:  0.00070667\n",
      "Training Loss:  0.00070667\n",
      "Training Loss:  0.00070144\n",
      "Training Loss:  0.00070144\n",
      "Training Loss:  0.00070144\n",
      "Final training Loss:  0.00070144\n",
      "\n",
      "Running model (trial=7, mod=214, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.002431\n",
      "Training Loss:  0.002431\n",
      "Training Loss:  0.002431\n",
      "Training Loss:  0.002431\n",
      "Training Loss:  0.00243099\n",
      "Training Loss:  0.00243099\n",
      "Training Loss:  0.00243099\n",
      "Training Loss:  0.00243097\n",
      "Training Loss:  0.00243097\n",
      "Training Loss:  0.00243097\n",
      "Final training Loss:  0.00243097\n",
      "\n",
      "Running model (trial=7, mod=215, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Training Loss:  0.01408352\n",
      "Final training Loss:  0.01408352\n",
      "\n",
      "Running model (trial=7, mod=216, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00081469\n",
      "Training Loss:  0.00073567\n",
      "Training Loss:  0.00073567\n",
      "Training Loss:  0.00073567\n",
      "Training Loss:  0.00073561\n",
      "Training Loss:  0.00073561\n",
      "Training Loss:  0.00073558\n",
      "Training Loss:  0.00073547\n",
      "Training Loss:  0.00073547\n",
      "Training Loss:  0.00073547\n",
      "Final training Loss:  0.00073547\n",
      "\n",
      "Running model (trial=7, mod=217, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00491195\n",
      "Training Loss:  0.00491195\n",
      "Training Loss:  0.0049114\n",
      "Training Loss:  0.0049114\n",
      "Training Loss:  0.0049114\n",
      "Training Loss:  0.0049114\n",
      "Training Loss:  0.0049114\n",
      "Training Loss:  0.0049114\n",
      "Training Loss:  0.0049114\n",
      "Training Loss:  0.0049114\n",
      "Final training Loss:  0.0049114\n",
      "\n",
      "Running model (trial=7, mod=218, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00427151\n",
      "Training Loss:  0.00427151\n",
      "Training Loss:  0.00427151\n",
      "Training Loss:  0.00427151\n",
      "Training Loss:  0.00427151\n",
      "Training Loss:  0.00427151\n",
      "Training Loss:  0.00427065\n",
      "Training Loss:  0.00427065\n",
      "Training Loss:  0.00427065\n",
      "Training Loss:  0.00427065\n",
      "Final training Loss:  0.00427065\n",
      "\n",
      "Running model (trial=7, mod=219, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01182567\n",
      "Training Loss:  0.0118255\n",
      "Training Loss:  0.0118255\n",
      "Training Loss:  0.0118255\n",
      "Training Loss:  0.0118255\n",
      "Training Loss:  0.0118255\n",
      "Training Loss:  0.0118255\n",
      "Training Loss:  0.0118255\n",
      "Training Loss:  0.0118255\n",
      "Training Loss:  0.0118255\n",
      "Final training Loss:  0.0118255\n",
      "\n",
      "Running model (trial=7, mod=220, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00191857\n",
      "Training Loss:  0.0019184\n",
      "Training Loss:  0.00191539\n",
      "Training Loss:  0.00191539\n",
      "Training Loss:  0.00191539\n",
      "Training Loss:  0.00191539\n",
      "Training Loss:  0.00167779\n",
      "Training Loss:  0.00167779\n",
      "Training Loss:  0.00167691\n",
      "Training Loss:  0.00054592\n",
      "Final training Loss:  0.00054592\n",
      "\n",
      "Running model (trial=7, mod=221, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00150038\n",
      "Training Loss:  0.00150019\n",
      "Training Loss:  0.00150019\n",
      "Training Loss:  0.00150017\n",
      "Training Loss:  0.00150017\n",
      "Training Loss:  0.00150017\n",
      "Training Loss:  0.00150012\n",
      "Training Loss:  0.00150006\n",
      "Training Loss:  0.00150006\n",
      "Training Loss:  0.00150006\n",
      "Final training Loss:  0.00150006\n",
      "\n",
      "Running model (trial=7, mod=222, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02646597\n",
      "Training Loss:  0.00997663\n",
      "Training Loss:  0.00679937\n",
      "Training Loss:  0.00679937\n",
      "Training Loss:  0.00679937\n",
      "Training Loss:  0.00679934\n",
      "Training Loss:  0.00679934\n",
      "Training Loss:  0.00679934\n",
      "Training Loss:  0.00679934\n",
      "Training Loss:  0.00679934\n",
      "Final training Loss:  0.00679934\n",
      "\n",
      "Running model (trial=7, mod=223, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00530597\n",
      "Training Loss:  0.00530597\n",
      "Training Loss:  0.00198963\n",
      "Training Loss:  0.00198862\n",
      "Training Loss:  0.00198847\n",
      "Training Loss:  0.00198847\n",
      "Training Loss:  0.00198847\n",
      "Training Loss:  0.00198848\n",
      "Training Loss:  0.00198847\n",
      "Training Loss:  0.00198847\n",
      "Final training Loss:  0.00198847\n",
      "\n",
      "Running model (trial=8, mod=224, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.35982457\n",
      "Training Loss:  0.35982457\n",
      "Training Loss:  0.35982457\n",
      "Training Loss:  0.35982457\n",
      "Training Loss:  0.35982457\n",
      "Training Loss:  0.35982457\n",
      "Training Loss:  0.35979125\n",
      "Training Loss:  0.35979125\n",
      "Training Loss:  0.35979015\n",
      "Training Loss:  0.35979015\n",
      "Final training Loss:  0.35979015\n",
      "\n",
      "Running model (trial=8, mod=225, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00327943\n",
      "Training Loss:  0.00327943\n",
      "Training Loss:  0.00327943\n",
      "Training Loss:  0.00327943\n",
      "Training Loss:  0.00327943\n",
      "Training Loss:  0.00327943\n",
      "Training Loss:  0.00327943\n",
      "Training Loss:  0.00327928\n",
      "Training Loss:  0.00327928\n",
      "Training Loss:  0.00327928\n",
      "Final training Loss:  0.00327928\n",
      "\n",
      "Running model (trial=8, mod=226, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03095751\n",
      "Training Loss:  0.00153493\n",
      "Training Loss:  0.00153493\n",
      "Training Loss:  0.00153491\n",
      "Training Loss:  0.00153491\n",
      "Training Loss:  0.00153491\n",
      "Training Loss:  0.00153491\n",
      "Training Loss:  0.00153485\n",
      "Training Loss:  0.00153485\n",
      "Training Loss:  0.00153485\n",
      "Final training Loss:  0.00153485\n",
      "\n",
      "Running model (trial=8, mod=227, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00442668\n",
      "Training Loss:  0.00442663\n",
      "Training Loss:  0.00442652\n",
      "Training Loss:  0.00442652\n",
      "Training Loss:  0.00442652\n",
      "Training Loss:  0.00442652\n",
      "Training Loss:  0.00442652\n",
      "Training Loss:  0.00442652\n",
      "Training Loss:  0.00442652\n",
      "Training Loss:  0.00442652\n",
      "Final training Loss:  0.00442652\n",
      "\n",
      "Running model (trial=8, mod=228, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00294919\n",
      "Training Loss:  0.00294919\n",
      "Training Loss:  0.00294888\n",
      "Training Loss:  0.00294888\n",
      "Training Loss:  0.00294868\n",
      "Training Loss:  0.00294868\n",
      "Training Loss:  0.00294868\n",
      "Training Loss:  0.00294868\n",
      "Training Loss:  0.00294868\n",
      "Training Loss:  0.00294868\n",
      "Final training Loss:  0.00294868\n",
      "\n",
      "Running model (trial=8, mod=229, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00268896\n",
      "Training Loss:  0.00268896\n",
      "Training Loss:  0.00268893\n",
      "Training Loss:  0.00268891\n",
      "Training Loss:  0.00092066\n",
      "Training Loss:  0.00092066\n",
      "Training Loss:  0.00092066\n",
      "Training Loss:  0.00092066\n",
      "Training Loss:  0.00092066\n",
      "Training Loss:  0.00092066\n",
      "Final training Loss:  0.00092066\n",
      "\n",
      "Running model (trial=8, mod=230, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Training Loss:  0.00062007\n",
      "Final training Loss:  0.00062007\n",
      "\n",
      "Running model (trial=8, mod=231, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00075118\n",
      "Training Loss:  0.00075118\n",
      "Training Loss:  0.00075118\n",
      "Training Loss:  0.00075118\n",
      "Training Loss:  0.00075111\n",
      "Training Loss:  0.00030245\n",
      "Training Loss:  0.00030233\n",
      "Training Loss:  0.00030233\n",
      "Training Loss:  0.00030233\n",
      "Training Loss:  0.00030233\n",
      "Final training Loss:  0.00030233\n",
      "\n",
      "Running model (trial=8, mod=232, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0011665\n",
      "Training Loss:  0.0011665\n",
      "Training Loss:  7.925e-05\n",
      "Training Loss:  7.925e-05\n",
      "Training Loss:  7.925e-05\n",
      "Training Loss:  7.925e-05\n",
      "Training Loss:  7.925e-05\n",
      "Training Loss:  7.925e-05\n",
      "Training Loss:  7.925e-05\n",
      "Training Loss:  7.925e-05\n",
      "Final training Loss:  7.925e-05\n",
      "\n",
      "Running model (trial=8, mod=233, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00127835\n",
      "Training Loss:  0.00127803\n",
      "Training Loss:  0.00025518\n",
      "Training Loss:  0.00025518\n",
      "Training Loss:  0.00025518\n",
      "Training Loss:  0.00025518\n",
      "Training Loss:  0.00025518\n",
      "Training Loss:  0.00025518\n",
      "Training Loss:  0.00025518\n",
      "Training Loss:  0.00025518\n",
      "Final training Loss:  0.00025518\n",
      "\n",
      "Running model (trial=8, mod=234, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00038233\n",
      "Training Loss:  0.00038191\n",
      "Training Loss:  0.00038191\n",
      "Training Loss:  0.00038191\n",
      "Training Loss:  0.00038121\n",
      "Training Loss:  0.00036634\n",
      "Training Loss:  0.00036395\n",
      "Training Loss:  0.00036395\n",
      "Training Loss:  0.00036395\n",
      "Training Loss:  0.00036395\n",
      "Final training Loss:  0.00036395\n",
      "\n",
      "Running model (trial=8, mod=235, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.09085965\n",
      "Training Loss:  0.09085965\n",
      "Training Loss:  0.01835161\n",
      "Training Loss:  0.01835161\n",
      "Training Loss:  0.01699584\n",
      "Training Loss:  0.01699315\n",
      "Training Loss:  0.01699315\n",
      "Training Loss:  0.01699229\n",
      "Training Loss:  0.01699229\n",
      "Training Loss:  0.01699229\n",
      "Final training Loss:  0.01699229\n",
      "\n",
      "Running model (trial=8, mod=236, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.08355713\n",
      "Training Loss:  600.08349609\n",
      "Training Loss:  600.08337402\n",
      "Training Loss:  600.08337402\n",
      "Training Loss:  600.08337402\n",
      "Training Loss:  600.08337402\n",
      "Training Loss:  600.08337402\n",
      "Training Loss:  600.08337402\n",
      "Training Loss:  600.08337402\n",
      "Training Loss:  600.08337402\n",
      "Final training Loss:  600.08337402\n",
      "\n",
      "Running model (trial=8, mod=237, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10119629\n",
      "Training Loss:  600.10119629\n",
      "Training Loss:  600.10113525\n",
      "Training Loss:  600.10113525\n",
      "Training Loss:  600.10113525\n",
      "Training Loss:  600.10113525\n",
      "Training Loss:  600.10113525\n",
      "Training Loss:  600.10113525\n",
      "Training Loss:  600.10113525\n",
      "Training Loss:  600.10113525\n",
      "Final training Loss:  600.10113525\n",
      "\n",
      "Running model (trial=8, mod=238, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01925025\n",
      "Training Loss:  0.01925025\n",
      "Training Loss:  0.01925025\n",
      "Training Loss:  0.01925025\n",
      "Training Loss:  0.01925025\n",
      "Training Loss:  0.01925025\n",
      "Training Loss:  0.01924911\n",
      "Training Loss:  0.01924911\n",
      "Training Loss:  0.01924911\n",
      "Training Loss:  0.01924911\n",
      "Final training Loss:  0.01924911\n",
      "\n",
      "Running model (trial=8, mod=239, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00056048\n",
      "Training Loss:  0.00056038\n",
      "Training Loss:  0.00056038\n",
      "Training Loss:  0.00056038\n",
      "Training Loss:  0.00056038\n",
      "Training Loss:  0.00056037\n",
      "Training Loss:  0.00054791\n",
      "Training Loss:  0.00054786\n",
      "Training Loss:  0.00054786\n",
      "Training Loss:  0.00054786\n",
      "Final training Loss:  0.00054786\n",
      "\n",
      "Running model (trial=8, mod=240, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00179497\n",
      "Training Loss:  0.00179497\n",
      "Training Loss:  0.00179497\n",
      "Training Loss:  0.00179496\n",
      "Training Loss:  0.00179493\n",
      "Training Loss:  0.00179493\n",
      "Training Loss:  0.00179493\n",
      "Training Loss:  0.00179493\n",
      "Training Loss:  0.00179493\n",
      "Training Loss:  0.00179493\n",
      "Final training Loss:  0.00179493\n",
      "\n",
      "Running model (trial=8, mod=241, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00286231\n",
      "Training Loss:  0.00286217\n",
      "Training Loss:  0.00286216\n",
      "Training Loss:  0.00286216\n",
      "Training Loss:  0.00286216\n",
      "Training Loss:  0.00285751\n",
      "Training Loss:  0.00285751\n",
      "Training Loss:  0.00285751\n",
      "Training Loss:  0.00285751\n",
      "Training Loss:  0.00285751\n",
      "Final training Loss:  0.00285751\n",
      "\n",
      "Running model (trial=8, mod=242, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00685056\n",
      "Training Loss:  0.00684937\n",
      "Training Loss:  0.00595769\n",
      "Training Loss:  0.00595769\n",
      "Training Loss:  0.00595769\n",
      "Training Loss:  0.00595769\n",
      "Training Loss:  0.00595769\n",
      "Training Loss:  0.00595769\n",
      "Training Loss:  0.00595769\n",
      "Training Loss:  0.00595769\n",
      "Final training Loss:  0.00595769\n",
      "\n",
      "Running model (trial=8, mod=243, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Training Loss:  0.04452095\n",
      "Final training Loss:  0.04452095\n",
      "\n",
      "Running model (trial=8, mod=244, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02782956\n",
      "Training Loss:  0.02782956\n",
      "Training Loss:  0.0278277\n",
      "Training Loss:  0.0278277\n",
      "Training Loss:  0.0278277\n",
      "Training Loss:  0.0278277\n",
      "Training Loss:  0.0278277\n",
      "Training Loss:  0.0278277\n",
      "Training Loss:  0.0278277\n",
      "Training Loss:  0.0278277\n",
      "Final training Loss:  0.0278277\n",
      "\n",
      "Running model (trial=8, mod=245, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01660025\n",
      "Training Loss:  0.01660025\n",
      "Training Loss:  0.01660025\n",
      "Training Loss:  0.01660025\n",
      "Training Loss:  0.01660025\n",
      "Training Loss:  0.01660025\n",
      "Training Loss:  0.01660025\n",
      "Training Loss:  0.01660016\n",
      "Training Loss:  0.01660016\n",
      "Training Loss:  0.01660016\n",
      "Final training Loss:  0.01660016\n",
      "\n",
      "Running model (trial=8, mod=246, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00388795\n",
      "Training Loss:  0.0038873\n",
      "Training Loss:  0.0038873\n",
      "Training Loss:  0.00388719\n",
      "Training Loss:  0.00388719\n",
      "Training Loss:  0.00388719\n",
      "Training Loss:  0.00388704\n",
      "Training Loss:  0.00388704\n",
      "Training Loss:  0.00388704\n",
      "Training Loss:  0.00388704\n",
      "Final training Loss:  0.00388704\n",
      "\n",
      "Running model (trial=8, mod=247, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Training Loss:  0.0254052\n",
      "Final training Loss:  0.0254052\n",
      "\n",
      "Running model (trial=8, mod=248, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00323507\n",
      "Training Loss:  0.00323501\n",
      "Training Loss:  0.00323501\n",
      "Training Loss:  0.00323501\n",
      "Training Loss:  0.00323501\n",
      "Training Loss:  0.00323501\n",
      "Training Loss:  0.0032341\n",
      "Training Loss:  0.0032341\n",
      "Training Loss:  0.0032341\n",
      "Training Loss:  0.0032341\n",
      "Final training Loss:  0.0032341\n",
      "\n",
      "Running model (trial=8, mod=249, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Training Loss:  0.00013654\n",
      "Final training Loss:  0.00013654\n",
      "\n",
      "Running model (trial=8, mod=250, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0012065\n",
      "Training Loss:  0.0012065\n",
      "Training Loss:  0.0012065\n",
      "Training Loss:  0.00120645\n",
      "Training Loss:  0.00120645\n",
      "Training Loss:  0.00120645\n",
      "Training Loss:  0.00120644\n",
      "Training Loss:  0.00120644\n",
      "Training Loss:  0.00120644\n",
      "Training Loss:  0.00120644\n",
      "Final training Loss:  0.00120644\n",
      "\n",
      "Running model (trial=8, mod=251, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00853833\n",
      "Training Loss:  0.00144814\n",
      "Training Loss:  0.00144814\n",
      "Training Loss:  0.00042435\n",
      "Training Loss:  0.00042435\n",
      "Training Loss:  0.00042435\n",
      "Training Loss:  0.00042435\n",
      "Training Loss:  0.00042435\n",
      "Training Loss:  0.00042435\n",
      "Training Loss:  0.00042435\n",
      "Final training Loss:  0.00042435\n",
      "\n",
      "Running model (trial=9, mod=252, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03828312\n",
      "Training Loss:  0.03828248\n",
      "Training Loss:  0.03828248\n",
      "Training Loss:  0.03828248\n",
      "Training Loss:  0.03828236\n",
      "Training Loss:  0.03828236\n",
      "Training Loss:  0.03828236\n",
      "Training Loss:  0.03828236\n",
      "Training Loss:  0.03828236\n",
      "Training Loss:  0.03828236\n",
      "Final training Loss:  0.03828236\n",
      "\n",
      "Running model (trial=9, mod=253, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02130179\n",
      "Training Loss:  0.02130175\n",
      "Training Loss:  0.02130162\n",
      "Training Loss:  0.0213015\n",
      "Training Loss:  0.02030495\n",
      "Training Loss:  0.02030495\n",
      "Training Loss:  0.02030495\n",
      "Training Loss:  0.02030495\n",
      "Training Loss:  0.02030495\n",
      "Training Loss:  0.02030495\n",
      "Final training Loss:  0.02030495\n",
      "\n",
      "Running model (trial=9, mod=254, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00013002\n",
      "Training Loss:  0.00013002\n",
      "Training Loss:  0.00013\n",
      "Training Loss:  0.00013\n",
      "Training Loss:  0.00013\n",
      "Training Loss:  0.00012061\n",
      "Training Loss:  0.00012061\n",
      "Training Loss:  0.00012061\n",
      "Training Loss:  0.00012061\n",
      "Training Loss:  0.00012061\n",
      "Final training Loss:  0.00012061\n",
      "\n",
      "Running model (trial=9, mod=255, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00162093\n",
      "Training Loss:  0.00010417\n",
      "Training Loss:  0.00010412\n",
      "Training Loss:  0.00010405\n",
      "Training Loss:  9.638e-05\n",
      "Training Loss:  9.638e-05\n",
      "Training Loss:  9.638e-05\n",
      "Training Loss:  9.588e-05\n",
      "Training Loss:  9.58e-05\n",
      "Training Loss:  9.573e-05\n",
      "Final training Loss:  9.573e-05\n",
      "\n",
      "Running model (trial=9, mod=256, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00670543\n",
      "Training Loss:  0.00670543\n",
      "Training Loss:  0.00670519\n",
      "Training Loss:  0.00670519\n",
      "Training Loss:  0.00670519\n",
      "Training Loss:  0.00670519\n",
      "Training Loss:  0.00670519\n",
      "Training Loss:  0.00670519\n",
      "Training Loss:  0.00670514\n",
      "Training Loss:  0.00670514\n",
      "Final training Loss:  0.00670514\n",
      "\n",
      "Running model (trial=9, mod=257, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00293368\n",
      "Training Loss:  0.00122561\n",
      "Training Loss:  0.00122523\n",
      "Training Loss:  0.00122523\n",
      "Training Loss:  0.00122523\n",
      "Training Loss:  0.00122523\n",
      "Training Loss:  0.00122523\n",
      "Training Loss:  0.00122523\n",
      "Training Loss:  0.00122523\n",
      "Training Loss:  0.00122523\n",
      "Final training Loss:  0.00122523\n",
      "\n",
      "Running model (trial=9, mod=258, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02070153\n",
      "Training Loss:  0.02070153\n",
      "Training Loss:  0.02070153\n",
      "Training Loss:  0.00025851\n",
      "Training Loss:  0.00025842\n",
      "Training Loss:  0.00025831\n",
      "Training Loss:  0.00025831\n",
      "Training Loss:  0.00025831\n",
      "Training Loss:  0.00025831\n",
      "Training Loss:  0.00025831\n",
      "Final training Loss:  0.00025831\n",
      "\n",
      "Running model (trial=9, mod=259, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00367437\n",
      "Training Loss:  0.00268037\n",
      "Training Loss:  0.00268038\n",
      "Training Loss:  0.00268037\n",
      "Training Loss:  0.00268037\n",
      "Training Loss:  0.00268037\n",
      "Training Loss:  0.00268037\n",
      "Training Loss:  0.00268037\n",
      "Training Loss:  0.00268035\n",
      "Training Loss:  0.00268035\n",
      "Final training Loss:  0.00268035\n",
      "\n",
      "Running model (trial=9, mod=260, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00041249\n",
      "Training Loss:  0.000249\n",
      "Training Loss:  0.000249\n",
      "Training Loss:  0.000249\n",
      "Training Loss:  0.000249\n",
      "Training Loss:  0.00024899\n",
      "Training Loss:  0.00024899\n",
      "Training Loss:  0.00024899\n",
      "Training Loss:  0.00024899\n",
      "Training Loss:  0.00024899\n",
      "Final training Loss:  0.00024899\n",
      "\n",
      "Running model (trial=9, mod=261, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00051356\n",
      "Training Loss:  0.00051356\n",
      "Training Loss:  0.00051356\n",
      "Training Loss:  0.00011477\n",
      "Training Loss:  0.00011477\n",
      "Training Loss:  0.00011477\n",
      "Training Loss:  0.00011477\n",
      "Training Loss:  0.00011477\n",
      "Training Loss:  0.00011477\n",
      "Training Loss:  0.00011477\n",
      "Final training Loss:  0.00011477\n",
      "\n",
      "Running model (trial=9, mod=262, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Training Loss:  0.00715414\n",
      "Final training Loss:  0.00715414\n",
      "\n",
      "Running model (trial=9, mod=263, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00262656\n",
      "Training Loss:  0.00262632\n",
      "Training Loss:  0.00262632\n",
      "Training Loss:  0.00262632\n",
      "Training Loss:  0.00262632\n",
      "Training Loss:  0.00262632\n",
      "Training Loss:  0.00262632\n",
      "Training Loss:  0.00262632\n",
      "Training Loss:  0.00262632\n",
      "Training Loss:  0.00262632\n",
      "Final training Loss:  0.00262632\n",
      "\n",
      "Running model (trial=9, mod=264, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  18.33161163\n",
      "Training Loss:  18.33161163\n",
      "Training Loss:  15.32807541\n",
      "Training Loss:  0.02812365\n",
      "Training Loss:  0.0120153\n",
      "Training Loss:  0.01055282\n",
      "Training Loss:  0.01054419\n",
      "Training Loss:  0.01054419\n",
      "Training Loss:  0.01054141\n",
      "Training Loss:  0.01054034\n",
      "Final training Loss:  0.01054034\n",
      "\n",
      "Running model (trial=9, mod=265, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Training Loss:  600.10144043\n",
      "Final training Loss:  600.10144043\n",
      "\n",
      "Running model (trial=9, mod=266, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.58541435\n",
      "Training Loss:  0.58541435\n",
      "Training Loss:  0.58541435\n",
      "Training Loss:  0.58541435\n",
      "Training Loss:  0.58541435\n",
      "Training Loss:  0.58541435\n",
      "Training Loss:  0.58541435\n",
      "Training Loss:  0.58540976\n",
      "Training Loss:  0.55692935\n",
      "Training Loss:  0.55691868\n",
      "Final training Loss:  0.55691868\n",
      "\n",
      "Running model (trial=9, mod=267, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Training Loss:  0.00428486\n",
      "Final training Loss:  0.00428486\n",
      "\n",
      "Running model (trial=9, mod=268, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Training Loss:  0.00121343\n",
      "Final training Loss:  0.00121343\n",
      "\n",
      "Running model (trial=9, mod=269, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00642433\n",
      "Training Loss:  0.00642433\n",
      "Training Loss:  0.00642391\n",
      "Training Loss:  0.00642344\n",
      "Training Loss:  0.00507539\n",
      "Training Loss:  0.00507539\n",
      "Training Loss:  0.00507539\n",
      "Training Loss:  0.00507519\n",
      "Training Loss:  0.00507519\n",
      "Training Loss:  0.00507519\n",
      "Final training Loss:  0.00507519\n",
      "\n",
      "Running model (trial=9, mod=270, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00210344\n",
      "Training Loss:  0.00018175\n",
      "Training Loss:  0.00018175\n",
      "Training Loss:  0.00018155\n",
      "Training Loss:  0.00018155\n",
      "Training Loss:  0.00018155\n",
      "Training Loss:  0.00018153\n",
      "Training Loss:  0.00018153\n",
      "Training Loss:  0.00018153\n",
      "Training Loss:  0.00018153\n",
      "Final training Loss:  0.00018153\n",
      "\n",
      "Running model (trial=9, mod=271, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00417646\n",
      "Training Loss:  0.00417646\n",
      "Training Loss:  0.00417646\n",
      "Training Loss:  0.00417646\n",
      "Training Loss:  0.00417646\n",
      "Training Loss:  0.00417636\n",
      "Training Loss:  0.00417636\n",
      "Training Loss:  0.00417636\n",
      "Training Loss:  0.00417632\n",
      "Training Loss:  0.00417627\n",
      "Final training Loss:  0.00417627\n",
      "\n",
      "Running model (trial=9, mod=272, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01681974\n",
      "Training Loss:  0.01681609\n",
      "Training Loss:  0.01681609\n",
      "Training Loss:  0.01681609\n",
      "Training Loss:  0.01681609\n",
      "Training Loss:  0.01681609\n",
      "Training Loss:  0.01681581\n",
      "Training Loss:  0.01681581\n",
      "Training Loss:  0.01681581\n",
      "Training Loss:  0.01681581\n",
      "Final training Loss:  0.01681581\n",
      "\n",
      "Running model (trial=9, mod=273, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 256, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.74121004\n",
      "Training Loss:  0.00739061\n",
      "Training Loss:  0.00739061\n",
      "Training Loss:  0.00739061\n",
      "Training Loss:  0.00739061\n",
      "Training Loss:  0.00739051\n",
      "Training Loss:  0.00739045\n",
      "Training Loss:  0.00739044\n",
      "Training Loss:  0.00739044\n",
      "Training Loss:  0.00739039\n",
      "Final training Loss:  0.00739039\n",
      "\n",
      "Running model (trial=9, mod=274, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02815838\n",
      "Training Loss:  0.02815759\n",
      "Training Loss:  0.02815563\n",
      "Training Loss:  0.02815563\n",
      "Training Loss:  0.02815563\n",
      "Training Loss:  0.02815563\n",
      "Training Loss:  0.02815547\n",
      "Training Loss:  0.02815547\n",
      "Training Loss:  0.02815547\n",
      "Training Loss:  0.02815547\n",
      "Final training Loss:  0.02815547\n",
      "\n",
      "Running model (trial=9, mod=275, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03637239\n",
      "Training Loss:  0.03637202\n",
      "Training Loss:  0.03637202\n",
      "Training Loss:  0.03637202\n",
      "Training Loss:  0.03637202\n",
      "Training Loss:  0.03637202\n",
      "Training Loss:  0.03637202\n",
      "Training Loss:  0.03637202\n",
      "Training Loss:  0.03637183\n",
      "Training Loss:  0.03637183\n",
      "Final training Loss:  0.03637183\n",
      "\n",
      "Running model (trial=9, mod=276, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00121212\n",
      "Training Loss:  0.00121212\n",
      "Training Loss:  0.00121212\n",
      "Training Loss:  0.00121208\n",
      "Training Loss:  0.00121207\n",
      "Training Loss:  0.00074544\n",
      "Training Loss:  0.00074544\n",
      "Training Loss:  0.00074544\n",
      "Training Loss:  0.00074544\n",
      "Training Loss:  0.00074536\n",
      "Final training Loss:  0.00074536\n",
      "\n",
      "Running model (trial=9, mod=277, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00418813\n",
      "Training Loss:  0.00418813\n",
      "Training Loss:  0.00418813\n",
      "Training Loss:  0.00418813\n",
      "Training Loss:  0.00418813\n",
      "Training Loss:  0.00418775\n",
      "Training Loss:  0.00418775\n",
      "Training Loss:  0.00418775\n",
      "Training Loss:  0.00418775\n",
      "Training Loss:  0.00418775\n",
      "Final training Loss:  0.00418775\n",
      "\n",
      "Running model (trial=9, mod=278, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00251137\n",
      "Training Loss:  0.00251137\n",
      "Training Loss:  0.00251137\n",
      "Training Loss:  0.00228529\n",
      "Training Loss:  0.00228529\n",
      "Training Loss:  0.00193144\n",
      "Training Loss:  0.00193142\n",
      "Training Loss:  0.00193093\n",
      "Training Loss:  0.00193093\n",
      "Training Loss:  0.00193093\n",
      "Final training Loss:  0.00193093\n",
      "\n",
      "Running model (trial=9, mod=279, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNET.ResNET'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x124cf00d0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00120378\n",
      "Training Loss:  0.00120217\n",
      "Training Loss:  0.00120217\n",
      "Training Loss:  0.00120217\n",
      "Training Loss:  0.00120217\n",
      "Training Loss:  0.00120217\n",
      "Training Loss:  0.00120217\n",
      "Training Loss:  0.00120217\n",
      "Training Loss:  0.00120145\n",
      "Training Loss:  0.00120145\n",
      "Final training Loss:  0.00120145\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "x_train_ = x_train.detach()\n",
    "x_sorted, indices = torch.sort(x_train_, dim=0)\n",
    "plot_kwargs = {\n",
    "    \"x_test\": x_sorted,\n",
    "    \"x_train\": x_sorted,\n",
    "    \"y_train\": c1.ksi(x_sorted),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"Analytical solution\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: invalid value encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: no_penalty_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "\n",
    "# make data frame\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type})\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 393.378125 262.19625\" width=\"393.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-31T14:40:49.238766</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 393.378125 262.19625 \nL 393.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \nL 386.178125 7.2 \nL 51.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"mffc2e8d6c7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.554947\" xlink:href=\"#mffc2e8d6c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(158.754947 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.994106\" xlink:href=\"#mffc2e8d6c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(303.194106 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m154b86cf2f\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"66.596307\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"92.03078\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"110.076826\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"124.074427\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"135.5113\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"145.181038\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"153.557346\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"160.945773\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"211.035466\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"236.469939\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"254.515986\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"268.513586\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"279.950459\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"289.620197\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"297.996505\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"305.384932\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_19\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"355.474625\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_20\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"380.909099\" xlink:href=\"#m154b86cf2f\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Neurons -->\n     <g transform=\"translate(198.016406 252.916563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"199.707031\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"238.570312\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"299.751953\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"363.130859\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_21\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9570a73f68\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9570a73f68\" y=\"204.592313\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{-3}}$ -->\n      <g transform=\"translate(20.878125 208.391532)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9570a73f68\" y=\"157.768604\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(20.878125 161.567823)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9570a73f68\" y=\"110.944895\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(20.878125 114.744114)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9570a73f68\" y=\"64.121186\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(26.778125 67.920405)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9570a73f68\" y=\"17.297478\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(26.778125 21.096696)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_26\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m36c514932a\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"223.22534\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"218.687654\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"214.980094\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"211.845397\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"209.129999\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"206.734848\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"190.496972\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"182.251726\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"176.401631\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"171.863945\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"168.156385\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"165.021688\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"162.30629\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"159.911139\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"143.673263\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"135.428017\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"129.577922\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"125.040236\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"121.332676\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"118.197979\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"115.482581\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"113.087431\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_28\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"96.849554\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_29\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"88.604309\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_30\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"82.754213\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_31\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"78.216527\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_32\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"74.508968\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_33\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"71.374271\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_34\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"68.658873\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_35\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"66.263722\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_36\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"50.025846\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_37\">\n     <g id=\"line2d_57\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"41.7806\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_38\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"35.930505\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_39\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"31.392818\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_40\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"27.685259\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_41\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"24.550562\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_42\">\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"21.835164\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_43\">\n     <g id=\"line2d_63\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m36c514932a\" y=\"19.440013\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- $E$ -->\n     <g transform=\"translate(14.798438 119.12)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 66.596307 89.074408 \nL 66.596307 72.960167 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 110.076826 200.308417 \nL 110.076826 166.48213 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 153.557346 204.463005 \nL 153.557346 186.145195 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 197.037865 214.756364 \nL 197.037865 192.842764 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 240.518385 188.89205 \nL 240.518385 172.132585 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 283.998904 202.903947 \nL 283.998904 186.481489 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 327.479424 184.201085 \nL 327.479424 161.005481 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 370.959943 175.390187 \nL 370.959943 159.641345 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 66.596307 87.163787 \nL 66.596307 48.03615 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 110.076826 159.552935 \nL 110.076826 17.083636 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 153.557346 150.582292 \nL 153.557346 136.615715 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 197.037865 193.837932 \nL 197.037865 163.138706 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 240.518385 175.481394 \nL 240.518385 152.790748 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 283.998904 158.807937 \nL 283.998904 140.185371 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 327.479424 167.342605 \nL 327.479424 151.98596 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 370.959943 170.385803 \nL 370.959943 147.766501 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_64\">\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 66.596307 79.294607 \nL 110.076826 177.341199 \nL 153.557346 193.717693 \nL 197.037865 200.582791 \nL 240.518385 178.728773 \nL 283.998904 193.270391 \nL 327.479424 170.469569 \nL 370.959943 165.908572 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_65\">\n    <path clip-path=\"url(#p51efd07f4d)\" d=\"M 66.596307 59.300072 \nL 110.076826 31.160486 \nL 153.557346 142.372937 \nL 197.037865 173.817779 \nL 240.518385 161.711738 \nL 283.998904 147.182535 \nL 327.479424 158.191763 \nL 370.959943 156.850192 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_66\"/>\n   <g id=\"line2d_67\"/>\n   <g id=\"line2d_68\"/>\n   <g id=\"line2d_69\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 224.64 \nL 51.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.178125 224.64 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 7.2 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 309.409375 59.234375 \nL 379.178125 59.234375 \nQ 381.178125 59.234375 381.178125 57.234375 \nL 381.178125 14.2 \nQ 381.178125 12.2 379.178125 12.2 \nL 309.409375 12.2 \nQ 307.409375 12.2 307.409375 14.2 \nL 307.409375 57.234375 \nQ 307.409375 59.234375 309.409375 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_10\">\n     <!-- model -->\n     <g transform=\"translate(328.724219 23.798438)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_70\">\n     <path d=\"M 311.409375 34.976563 \nL 331.409375 34.976563 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_71\"/>\n    <g id=\"text_11\">\n     <!-- FFNN -->\n     <g transform=\"translate(339.409375 38.476563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_72\">\n     <path d=\"M 311.409375 49.654688 \nL 331.409375 49.654688 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_73\"/>\n    <g id=\"text_12\">\n     <!-- ResNET -->\n     <g transform=\"translate(339.409375 53.154688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p51efd07f4d\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"51.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "JVBERi0xLjQKJazcIKu6CjEgMCBvYmoKPDwgL1BhZ2VzIDIgMCBSIC9UeXBlIC9DYXRhbG9nID4+CmVuZG9iago4IDAgb2JqCjw8IC9FeHRHU3RhdGUgNCAwIFIgL0ZvbnQgMyAwIFIgL1BhdHRlcm4gNSAwIFIKL1Byb2NTZXQgWyAvUERGIC9UZXh0IC9JbWFnZUIgL0ltYWdlQyAvSW1hZ2VJIF0gL1NoYWRpbmcgNiAwIFIKL1hPYmplY3QgNyAwIFIgPj4KZW5kb2JqCjExIDAgb2JqCjw8IC9Bbm5vdHMgMTAgMCBSIC9Db250ZW50cyA5IDAgUgovR3JvdXAgPDwgL0NTIC9EZXZpY2VSR0IgL1MgL1RyYW5zcGFyZW5jeSAvVHlwZSAvR3JvdXAgPj4KL01lZGlhQm94IFsgMCAwIDM5My44NzE4NzUgMjYzLjUxMTg3NSBdIC9QYXJlbnQgMiAwIFIgL1Jlc291cmNlcyA4IDAgUgovVHlwZSAvUGFnZSA+PgplbmRvYmoKOSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDEyIDAgUiA+PgpzdHJlYW0KeJy9WcuOFTcQ3fdXeDksMGWXXbaXQTwkFkjAKFlEWaDJQEAzg2BE+P0cu+9t29WXSQAFpEHd59axXVV2PdzOvF8e/OLM21tD5j3+vhhnnpoHjy7/fndx+fLpQ3NxuxDw64UL25xcThGvV+OrF7bRtccrCE+vfy3LzYLxwXmKod8uS3RHHufjE0bPYkXDVyPso1g+DtsHmWDM9gb6+FWft5gQOtkMrer0QBYn2VLIUtI0/QhHy8fpl4dY+5flI/4nc58wnotlFZSYiY1nW6K5uF4enlczWZJA4qM5/3N58ATSZM7fLGfunjl/v2Cd4pmCrCaEyBkdfggYjFJ2YMKqvj0chkjDCI/PlxdL02dh523IKUeZzTjAd+rBxKtgzN4X4/2sR0kS/w89/KiHYEmSLBWiOLtjQKVvBmhRvI0+RJ6kOzgLO0c2Jowy22iEFcEHGyW7pDbHACsC3rFMmvfygCrxELGbQ8p5lh9gRYjBUnRUlAYDrAjibOASPavt3eGZ4J2D8Yp38wwjrAh4L8KSy0wYYEWIEdYoidUMA6wIgiNGiTVhgBUhkw0heFJLGmBFKGSd4xJmP4+wJmQbCmHNitDhmcAU8Z4l+/l4DrAiwBxFMuuwOMCKkOFS8jkUFUc7rLaGCmRYe2yxqli/Htl2/E0/77Se7t/N2fPLz5/uGV+DbjRnH25u75k/zPmzMSD1iByzLa4gfGA5IR+DUEdPrcbTxg41iogPEjn/aERCNGTPPvg1sZwMSXwMSR9r/KZkqtrrE+jZE1dKsPkw/sV1NdH9R5fvX//6+dXrm9v71+9uPt+aRx/MixPWcHB5CuyTm8wxwHfbw5GzPntmDkg6P8Mg/n82SES4Fil5tseG/os5QBESCt7nGH408/4Xc7j/ZA5wyzrKt5qjFMuFauCc7NHhkwaRzg+2+JJQcZHjn5DCSZciQy0WxHqHRJMmXQb4bl3gCOsjh1iKw8H4yWWVTLoEwrFDDKpBNZQ92jQZxHEesHzvZvEN1eIIvuywtFl8Q5U4nkQEFcIk3lEtHizyQEaBM4lvqBYXm1Iin2bxDVXiiS05pD2exDuqxJGQvNSfJvGOavGEHEw+hFl8Q5V4gQ1CKvNaNlALoySOUa1kA7Vwy+14nKU3VIm3OE0op+cNM8A7AiOSUS2AZsIGa4KDEBoi52dChzXBo6nBT3HeNgOsCYx2hlEnzuYc4B0B5khOuCjCBmsCTrhLKAQUocM7AtyD8YqyUod3BGiHyrUERdhgTYgoE7Gz1I4bYE0Q9KQIOeqsD7AmJIRoSkhvM6HDmoDKNaE/EeXpDu8I0Xr4JyhPd3hHqMU0Ct6kCBusCWiyHCck3pnQ4R0BsS8yRlOEDd4RkvUhO735OqwI3tUWVrDciTDAmoACNiFbp/mIDvCOkGzM0E/JH1EtzjCGK7VtneQ7vCOgHfAiThRhg3cEVNHisPMVYYM1ISAUJklu3qsDvCOgtcSj2noDPKTwmr3v1zzugkW4bccMcuVQBx2zd1pLpJZ5/TF5P+6p96M5dSeEghflhXcJTZz5dGl+MzfGm2cGjWm9z7HOOzTlArchdwc5/EttvphRMaN/NS/1XVa/WajHL3AKkqYLBzjKxugStTPzarwtEEYoR3WS50sEbFBsthTigdCb8/WuqGWQAU0JFYvIcXzQkfTFtfSfIuauvhrghNJEilt99WqpviMojxoEGgj6ON960Q4j/XEq6/6HfIaexQuqZcFIhF9y6443GAsiJqSQlYCmBMUj6hD8giLUgVEby44iEdfWkddYCkJCzxlZkHBzrn6RXGcY4JrUMqzFK+GF+R6XY6NhxZkYhXZMcDrFmoqLwFV3+hkVYsjU1tRhjIEcFZOPOz/X1ZaYWwYZ4FrGhtqKlZ2jHToBGDxmnl3tam1bDgd58rUUHEBYkGdfOyL4Ds9l52yYFl4KrWQc4LrA5LGAtHO3o1DPXmHlb+cZbY9ECjuHF7GYu1WCo8PbbVAsLu4cXhgOx6byyuGutrcRx+tHPP69h3z0PtTG8F7Uoc1QNJG0YnhwGNySSnC5hrnRW829lKs2o0tg34wDUqXH84QmIaDgqNKjbVEhB3QqdcrRggmHkpl+3tkYTwFV87jcivlxryNncawF1mwf6G0rynk2EOoGKYFaOTZuTlS/aMHWXDnuQIcz6QsrC9VLEfbkWik7hY9aWiGsDDaqutN2vT8rePoTw1e+GdT9fOrjw/VXPz6A8U0fMSb5YaQ7Z3jwC69fMZ7Vzyb4+9JUPXxEYSq2JdzmwCItBSDob6Nt8FW7EoSzPbZChys4yAJDyz2jchC9WEY4lL7MEUe5LKFNMs4HvQ/S4+I6ejFo0lGMTPWaJzgc9XEMSkfZPtuG9ZXVUY/opsbVCHaNh4m6cfbWvajflh7uvi0dviqNH1Z8sd6vVy9IFRYq3323ev3hz8ur8T71dPwz3xD/YIbD+tHqxXDYHbwD20ajdpAOO+yEPny0hY+IX3KnLk+ePH9+WpU5QJm7A1Rfv6t1Q+A0K9DRb9PA1eTaeF9X4eU9Exh1mDm7vH3++HzU5sXyD/py1tEKZW5kc3RyZWFtCmVuZG9iagoxMiAwIG9iagoxOTI1CmVuZG9iagoxMCAwIG9iagpbIF0KZW5kb2JqCjE3IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggOTAgPj4Kc3RyZWFtCnicNc27DcAwCATQniluBPMxhH2iKIWzfxuciAaeAHGuggGJKq4DIYmTiT0/PtTDRc4GNy/J5NZIWF0smjNaxlCLEqe2THDoXrL+sNyR9eFvHbjopusFM5AcmAplbmRzdHJlYW0KZW5kb2JqCjE1IDAgb2JqCjw8IC9CYXNlRm9udCAvRGVqYVZ1U2Fucy1PYmxpcXVlIC9DaGFyUHJvY3MgMTYgMCBSCi9FbmNvZGluZyA8PCAvRGlmZmVyZW5jZXMgWyA2OSAvRSBdIC9UeXBlIC9FbmNvZGluZyA+PiAvRmlyc3RDaGFyIDAKL0ZvbnRCQm94IFsgLTEwMTYgLTM1MSAxNjYwIDEwNjggXSAvRm9udERlc2NyaXB0b3IgMTQgMCBSCi9Gb250TWF0cml4IFsgMC4wMDEgMCAwIDAuMDAxIDAgMCBdIC9MYXN0Q2hhciAyNTUgL05hbWUgL0RlamFWdVNhbnMtT2JsaXF1ZQovU3VidHlwZSAvVHlwZTMgL1R5cGUgL0ZvbnQgL1dpZHRocyAxMyAwIFIgPj4KZW5kb2JqCjE0IDAgb2JqCjw8IC9Bc2NlbnQgOTI5IC9DYXBIZWlnaHQgMCAvRGVzY2VudCAtMjM2IC9GbGFncyA5NgovRm9udEJCb3ggWyAtMTAxNiAtMzUxIDE2NjAgMTA2OCBdIC9Gb250TmFtZSAvRGVqYVZ1U2Fucy1PYmxpcXVlCi9JdGFsaWNBbmdsZSAwIC9NYXhXaWR0aCAxMzUwIC9TdGVtViAwIC9UeXBlIC9Gb250RGVzY3JpcHRvciAvWEhlaWdodCAwID4+CmVuZG9iagoxMyAwIG9iagpbIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwCjYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgMzE4IDQwMSA0NjAgODM4IDYzNgo5NTAgNzgwIDI3NSAzOTAgMzkwIDUwMCA4MzggMzE4IDM2MSAzMTggMzM3IDYzNiA2MzYgNjM2IDYzNiA2MzYgNjM2IDYzNiA2MzYKNjM2IDYzNiAzMzcgMzM3IDgzOCA4MzggODM4IDUzMSAxMDAwIDY4NCA2ODYgNjk4IDc3MCA2MzIgNTc1IDc3NSA3NTIgMjk1CjI5NSA2NTYgNTU3IDg2MyA3NDggNzg3IDYwMyA3ODcgNjk1IDYzNSA2MTEgNzMyIDY4NCA5ODkgNjg1IDYxMSA2ODUgMzkwIDMzNwozOTAgODM4IDUwMCA1MDAgNjEzIDYzNSA1NTAgNjM1IDYxNSAzNTIgNjM1IDYzNCAyNzggMjc4IDU3OSAyNzggOTc0IDYzNCA2MTIKNjM1IDYzNSA0MTEgNTIxIDM5MiA2MzQgNTkyIDgxOCA1OTIgNTkyIDUyNSA2MzYgMzM3IDYzNiA4MzggNjAwIDYzNiA2MDAgMzE4CjM1MiA1MTggMTAwMCA1MDAgNTAwIDUwMCAxMzUwIDYzNSA0MDAgMTA3MCA2MDAgNjg1IDYwMCA2MDAgMzE4IDMxOCA1MTggNTE4CjU5MCA1MDAgMTAwMCA1MDAgMTAwMCA1MjEgNDAwIDEwMjggNjAwIDUyNSA2MTEgMzE4IDQwMSA2MzYgNjM2IDYzNiA2MzYgMzM3CjUwMCA1MDAgMTAwMCA0NzEgNjE3IDgzOCAzNjEgMTAwMCA1MDAgNTAwIDgzOCA0MDEgNDAxIDUwMCA2MzYgNjM2IDMxOCA1MDAKNDAxIDQ3MSA2MTcgOTY5IDk2OSA5NjkgNTMxIDY4NCA2ODQgNjg0IDY4NCA2ODQgNjg0IDk3NCA2OTggNjMyIDYzMiA2MzIgNjMyCjI5NSAyOTUgMjk1IDI5NSA3NzUgNzQ4IDc4NyA3ODcgNzg3IDc4NyA3ODcgODM4IDc4NyA3MzIgNzMyIDczMiA3MzIgNjExIDYwOAo2MzAgNjEzIDYxMyA2MTMgNjEzIDYxMyA2MTMgOTk1IDU1MCA2MTUgNjE1IDYxNSA2MTUgMjc4IDI3OCAyNzggMjc4IDYxMiA2MzQKNjEyIDYxMiA2MTIgNjEyIDYxMiA4MzggNjEyIDYzNCA2MzQgNjM0IDYzNCA1OTIgNjM1IDU5MiBdCmVuZG9iagoxNiAwIG9iago8PCAvRSAxNyAwIFIgPj4KZW5kb2JqCjIyIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggODEgPj4Kc3RyZWFtCnicTc27DcAgDATQnik8AuD/PlGUItm/jQ0RobGfdCedYIcKbnFYDLQ7HK341FOYfegeEpJQc91EWDMl2oSkX/rLMMOYWMi2rzdXrnK+FtwciwplbmRzdHJlYW0KZW5kb2JqCjIzIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggNzYgPj4Kc3RyZWFtCnicMzU3VTBQsLQAEqaG5grmRpYKKYZcQD6IlcsFE8sBs8xMzIAsQ0tklomxIZBlYmGGxDI2sYDKIlgGQBpsTQ7M9ByuDK40ADUXGQUKZW5kc3RyZWFtCmVuZG9iagoyNCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDc3ID4+CnN0cmVhbQp4nDWNwQ3AMAgD/0zBCDiFUPapqj7S/b8tRHzsMwjserJwpEwT9hF8gf6c9NI4ULTITBlo2rO+2CS5g5cjlCea0qti9edFD90fyZ4YDAplbmRzdHJlYW0KZW5kb2JqCjI1IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjMyID4+CnN0cmVhbQp4nD2QS3IEIQxD95xCRwB/4TydSs2i5/7byO6ZbJCqwPITcRwTZ/OICKQc/KxhZlATvIeFQ9VgO6DrwGdATuAaLnQpcKPahHN8ncObCpq4h8dstUisneVMIeowJkls6EnINs5ocuOc3KpU3kxrvcbim3J3u8pr2pbCvYfK+jjjVDmrKmuRNhGZRWsbwUYe7LDPo6toy1kq3DeMTV0TlcObxe5Z3cniiu+vXOPVLMHM98O3vxwfV93oKsfYyoTZUpPm0jn1r5bR+nC0i4V64Ud7JkhwdasgVaXWztpTev1T3CT6/QP0wVcdCmVuZHN0cmVhbQplbmRvYmoKMjYgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCA2NiA+PgpzdHJlYW0KeJwzMzRUMFDQNQISZoYmCuZGlgophlxAPoiVywUTywGzzEzMgCxjU1MklgGQNjI1g9MQGaABcAZEfwZXGgBSaxTACmVuZHN0cmVhbQplbmRvYmoKMjcgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAyMzEgPj4Kc3RyZWFtCnicNU85kgQhDMt5hT4wVRjbQL+np7Y22Pl/upKZTpDwIcnTEx2ZeJkjI7Bmx9taZCBm4FNMxb/2tA8TqvfgHiKUiwthhpFw1qzjbp6OF/92lc9YB+82+IpZXhDYwkzWVxZnLtsFY2mcxDnJboxdE7GNda2nU1hHMKEMhHS2w5Qgc1Sk9MmOMuboOJEnnovv9tssdjl+DusLNo0hFef4KnqCNoOi7HnvAhpyQf9d3fgeRbvoJSAbCRbWUWLunOWEX712dB61KBJzQppBLhMhzekqphCaUKyzo6BSUXCpPqforJ9/5V9cLQplbmRzdHJlYW0KZW5kb2JqCjI4IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjQ5ID4+CnN0cmVhbQp4nD1QO45EIQzrOYUv8CTyI3AeRqstZu/frgOaKVBMfrYzJNARgUcMMZSv4yWtoK6Bv4tC8W7i64PCIKtDUiDOeg+IdOymNpETOh2cMz9hN2OOwEUxBpzpdKY9ByY5+8IKhHMbZexWSCeJqiKO6jOOKZ4qe594FiztyDZbJ5I95CDhUlKJyaWflMo/bcqUCjpm0QQsErngZBNNOMu7SVKMGZQy6h6mdiJ9rDzIozroZE3OrCOZ2dNP25n4HHC3X9pkTpXHdB7M+Jy0zoM5Fbr344k2B02N2ujs9xNpKi9Sux1anX51EpXdGOcYEpdnfxnfZP/5B/6HWiIKZW5kc3RyZWFtCmVuZG9iagoyOSAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDQ3ID4+CnN0cmVhbQp4nDMyt1AwULA0ARKGFiYK5mYGCimGXJYQVi4XTCwHzALRlnAKIp7BlQYAuWcNJwplbmRzdHJlYW0KZW5kb2JqCjMwIDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjU4ID4+CnN0cmVhbQp4nEWRS3IEIAhE956CI4D85DyTSmUxuf82Dc5kNnaXqP2ESiOmEiznFHkwfcnyzWS26Xc5VjsbBRRFKJjJVeixAqs7U8SZa4lq62Nl5LjTOwbFG85dOalkcaOMdVR1KnBMz5X1Ud35dlmUfUcOZQrYrHMcbODKbcMYJ0abre4O94kgTydTR8XtINnwByeNfZWrK3CdbPbRSzAOBP1CE5jki0DrDIHGzVP05BLs4+N254Fgb3kRSNkQyJEhGB2Cdp1c/+LW+b3/cYY7z7UZrhzv4neY1nbHX2KSFXMBi9wpqOdrLlrXGTrekzPH5Kb7hs65YJe7g0zv+T/Wz/r+Ax4pZvoKZW5kc3RyZWFtCmVuZG9iagozMSAwIG9iago8PCAvQkJveCBbIC0xMDIxIC00NjMgMTc5NCAxMjMzIF0gL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAzOQovU3VidHlwZSAvRm9ybSAvVHlwZSAvWE9iamVjdCA+PgpzdHJlYW0KeJzjMjQwUzA2NVXI5TI3NgKzcsAsI3MjIAski2BBZDO40gAV8wp8CmVuZHN0cmVhbQplbmRvYmoKMzIgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAxNjMgPj4Kc3RyZWFtCnicRZA7EgMhDEN7TqEj+CMDPs9mMik2929j2GxSwNNYIIO7E4LU2oKJ6IKHtiXdBe+tBGdj/Ok2bjUS5AR1gFak42iUUn25xWmVdPFoNnMrC60THWYOepSjGaAQOhXe7aLkcqbuzvlDcPVf9b9i3TmbiYHJyh0IzepT3Pk2O6K6usn+pMfcrNd+K+xVYWlZS8sJt527ZkAJ3FM52qs9Px8KOvYKZW5kc3RyZWFtCmVuZG9iagozMyAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDIxOCA+PgpzdHJlYW0KeJw9ULmNBDEMy12FGljAeu2pZxaLS6b/9Ej59iLRFkVSKjWZkikvdZQlWVPeOnyWxA55huVuZDYlKkUvk7Al99AK8X2J5hT33dWWs0M0l2g5fgszKqobHdNLNppwKhO6oNzDM/oNbXQDVocesVsg0KRg17YgcscPGAzBmROLIgxKTQb/rnKPn16LGz7D8UMUkZIO5jX/WP3ycw2vU48nkW5vvuJenKkOAxEckpq8I11YsS4SEWk1QU3PwFotgLu3Xv4btCO6DED2icRxmlKOob9rcKXPL+UnU9gKZW5kc3RyZWFtCmVuZG9iagozNCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDgzID4+CnN0cmVhbQp4nEWMuw3AMAhEe6ZgBH4m9j5RlMLevw0QJW64J909XB0JmSluM8NDBp4MLIZdcYH0ljALXEdQjp3so2HVvuoEjfWmUvPvD5Se7KzihusBAkIaZgplbmRzdHJlYW0KZW5kb2JqCjM1IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTYwID4+CnN0cmVhbQp4nEWQORIDMQgEc72CJ0hcgvesy7XB+v+pB9ZHoukCNBy6Fk3KehRoPumxRqG60GvoLEqSRMEWkh1Qp2OIOyhITEhjkki2HoMjmlizXZiZVCqzUuG0acXCv9la1chEjXCN/InpBlT8T+pclPBNg6+SMfoYVLw7g4xJ+F5F3Fox7f5EMLEZ9glvRSYFhImxqdm+z2CGzPcK1zjH8w1MgjfrCmVuZHN0cmVhbQplbmRvYmoKMzYgMCBvYmoKPDwgL0ZpbHRlciAvRmxhdGVEZWNvZGUgL0xlbmd0aCAzMzQgPj4Kc3RyZWFtCnicLVJLcsUgDNtzCl2gM/gH5DzpdLp4vf+2kpNFRg5g9DHlholKfFkgt6PWxLeNzECF4a+rzIXPSNvIOojLkIu4ki2Fe0Qs5DHEPMSC76vxHh75rMzJswfGL9l3Dyv21IRlIePFGdphFcdhFeRYsHUhqnt4U6TDqSTY44v/PsVzLQQtfEbQgF/kn6+O4PmSFmn3mG3TrnqwTDuqpLAcbE9zXiZfWme5Oh7PB8n2rtgRUrsCFIW5M85z4SjTVka0FnY2SGpcbG+O/VhK0IVuXEaKI5CfqSI8oKTJzCYK4o+cHnIqA2Hqmq50chtVcaeezDWbi7czSWbrvkixmcJ5XTiz/gxTZrV5J89yotSpCO+xZ0vQ0Dmunr2WWWh0mxO8pITPxk5PTr5XM+shORUJqWJaV8FpFJliCdsSX1NRU5p6Gf778u7xO37+ASxzfHMKZW5kc3RyZWFtCmVuZG9iagozNyAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDM0MCA+PgpzdHJlYW0KeJw1UjluBDEM6/0KfSCAbtvv2SBIkfy/DanZFANxdFKUO1pUdsuHhVS17HT5tJXaEjfkd2WFxAnJqxLtUoZIqLxWIdXvmTKvtzVnBMhSpcLkpORxyYI/w6WnC8f5trGv5cgdjx5YFSOhRMAyxcToGpbO7rBmW36WacCPeIScK9Ytx1gFUhvdOO2K96F5LbIGiL2ZlooKHVaJFn5B8aBHjX32GFRYINHtHElwjIlQkYB2gdpIDDl7LHZRH/QzKDET6NobRdxBgSWSmDnFunT03/jQsaD+2Iw3vzoq6VtaWWPSPhvtlMYsMul6WPR089bHgws076L859UMEjRljZLGB63aOYaimVFWeLdDkw3NMcch8w6ewxkJSvo8FL+PJRMdlMjfDg2hf18eo4ycNt4C5qI/bRUHDuKzw165gRVKF2uS9wGpTOiB6f+v8bW+19cfHe2AxgplbmRzdHJlYW0KZW5kb2JqCjM4IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMjUxID4+CnN0cmVhbQp4nC1RSXIDQQi7zyv0hGan32OXK4fk/9cIygcGDYtAdFrioIyfICxXvOWRq2jD3zMxgt8Fh34r121Y5EBUIEljUDWhdvF69B7YcZgJzJPWsAxmrA/8jCnc6MXhMRlnt9dl1BDsXa89mUHJrFzEJRMXTNVhI2cOP5kyLrRzPTcg50ZYl2GQblYaMxKONIVIIYWqm6TOBEESjK5GjTZyFPulL490hlWNqDHscy1tX89NOGvQ7Fis8uSUHl1xLicXL6wc9PU2AxdRaazyQEjA/W4P9XOyk994S+fOFtPje83J8sJUYMWb125ANtXi37yI4/uMr+fn+fwDX2BbiAplbmRzdHJlYW0KZW5kb2JqCjM5IDAgb2JqCjw8IC9GaWx0ZXIgL0ZsYXRlRGVjb2RlIC9MZW5ndGggMTc0ID4+CnN0cmVhbQp4nE2QSQ5DIQxD95zCF6iEM8DnPL+qumjvv61DB3WB/OQgcDw80HEkLnRk6IyOK5sc48CzIGPi0Tj/ybg+xDFB3aItWJd2x9nMEnPCMjECtkbJ2TyiwA/HXAgSZJcfvsAgIl2P+VbzWZP0z7c73Y+6tGZfPaLAiewIxbABV4D9useBS8L5XtPklyolYxOH8oHqIlI2O6EQtVTscqqKs92bK3AV9PzRQ+7tBbUjPN8KZW5kc3RyZWFtCmVuZG9iago0MCAwIG9iago8PCAvRmlsdGVyIC9GbGF0ZURlY29kZSAvTGVuZ3RoIDIxNSA+PgpzdHJlYW0KeJw1UTkOAyEM7PcV/kAkjC94T6Iozf6/zYzRVh7BXIa0lCGZ8lKTqCHlUz56mS6cutzXzGo055a0LXOAuLa8L62SwIlmiIPBaZi4AZo8AUPX0ahRQxce0NSlUyiw3AQ+irduD91jtYGXtiHniSBiKBksQc2pRRMWbc8npDW/Xosb3pft3chTpcaWGIEGAVY4HNfo1/CVPU8m0XQVMtSrNcsYCRNFIjz5jqbVE+taNNIyEtTGEaxqA7w7/TBOAAATccsCZJ9KlLPkxG+x9LMGV/r+AZ9HVJYKZW5kc3RyZWFtCmVuZG9iagoyMCAwIG9iago8PCAvQmFzZUZvbnQgL0RlamFWdVNhbnMgL0NoYXJQcm9jcyAyMSAwIFIKL0VuY29kaW5nIDw8Ci9EaWZmZXJlbmNlcyBbIDQ4IC96ZXJvIC9vbmUgL3R3byAvdGhyZWUgNjkgL0UgL0YgNzggL04gODIgL1IgODQgL1QgMTAwIC9kIC9lIDEwOCAvbCAvbQovbiAvbyAxMTQgL3IgL3MgMTE3IC91IF0KL1R5cGUgL0VuY29kaW5nID4+Ci9GaXJzdENoYXIgMCAvRm9udEJCb3ggWyAtMTAyMSAtNDYzIDE3OTQgMTIzMyBdIC9Gb250RGVzY3JpcHRvciAxOSAwIFIKL0ZvbnRNYXRyaXggWyAwLjAwMSAwIDAgMC4wMDEgMCAwIF0gL0xhc3RDaGFyIDI1NSAvTmFtZSAvRGVqYVZ1U2FucwovU3VidHlwZSAvVHlwZTMgL1R5cGUgL0ZvbnQgL1dpZHRocyAxOCAwIFIgPj4KZW5kb2JqCjE5IDAgb2JqCjw8IC9Bc2NlbnQgOTI5IC9DYXBIZWlnaHQgMCAvRGVzY2VudCAtMjM2IC9GbGFncyAzMgovRm9udEJCb3ggWyAtMTAyMSAtNDYzIDE3OTQgMTIzMyBdIC9Gb250TmFtZSAvRGVqYVZ1U2FucyAvSXRhbGljQW5nbGUgMAovTWF4V2lkdGggMTM0MiAvU3RlbVYgMCAvVHlwZSAvRm9udERlc2NyaXB0b3IgL1hIZWlnaHQgMCA+PgplbmRvYmoKMTggMCBvYmoKWyA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMAo2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDYwMCA2MDAgNjAwIDMxOCA0MDEgNDYwIDgzOCA2MzYKOTUwIDc4MCAyNzUgMzkwIDM5MCA1MDAgODM4IDMxOCAzNjEgMzE4IDMzNyA2MzYgNjM2IDYzNiA2MzYgNjM2IDYzNiA2MzYgNjM2CjYzNiA2MzYgMzM3IDMzNyA4MzggODM4IDgzOCA1MzEgMTAwMCA2ODQgNjg2IDY5OCA3NzAgNjMyIDU3NSA3NzUgNzUyIDI5NQoyOTUgNjU2IDU1NyA4NjMgNzQ4IDc4NyA2MDMgNzg3IDY5NSA2MzUgNjExIDczMiA2ODQgOTg5IDY4NSA2MTEgNjg1IDM5MCAzMzcKMzkwIDgzOCA1MDAgNTAwIDYxMyA2MzUgNTUwIDYzNSA2MTUgMzUyIDYzNSA2MzQgMjc4IDI3OCA1NzkgMjc4IDk3NCA2MzQgNjEyCjYzNSA2MzUgNDExIDUyMSAzOTIgNjM0IDU5MiA4MTggNTkyIDU5MiA1MjUgNjM2IDMzNyA2MzYgODM4IDYwMCA2MzYgNjAwIDMxOAozNTIgNTE4IDEwMDAgNTAwIDUwMCA1MDAgMTM0MiA2MzUgNDAwIDEwNzAgNjAwIDY4NSA2MDAgNjAwIDMxOCAzMTggNTE4IDUxOAo1OTAgNTAwIDEwMDAgNTAwIDEwMDAgNTIxIDQwMCAxMDIzIDYwMCA1MjUgNjExIDMxOCA0MDEgNjM2IDYzNiA2MzYgNjM2IDMzNwo1MDAgNTAwIDEwMDAgNDcxIDYxMiA4MzggMzYxIDEwMDAgNTAwIDUwMCA4MzggNDAxIDQwMSA1MDAgNjM2IDYzNiAzMTggNTAwCjQwMSA0NzEgNjEyIDk2OSA5NjkgOTY5IDUzMSA2ODQgNjg0IDY4NCA2ODQgNjg0IDY4NCA5NzQgNjk4IDYzMiA2MzIgNjMyIDYzMgoyOTUgMjk1IDI5NSAyOTUgNzc1IDc0OCA3ODcgNzg3IDc4NyA3ODcgNzg3IDgzOCA3ODcgNzMyIDczMiA3MzIgNzMyIDYxMSA2MDUKNjMwIDYxMyA2MTMgNjEzIDYxMyA2MTMgNjEzIDk4MiA1NTAgNjE1IDYxNSA2MTUgNjE1IDI3OCAyNzggMjc4IDI3OCA2MTIgNjM0CjYxMiA2MTIgNjEyIDYxMiA2MTIgODM4IDYxMiA2MzQgNjM0IDYzNCA2MzQgNTkyIDYzNSA1OTIgXQplbmRvYmoKMjEgMCBvYmoKPDwgL0UgMjIgMCBSIC9GIDIzIDAgUiAvTiAyNCAwIFIgL1IgMjUgMCBSIC9UIDI2IDAgUiAvZCAyNyAwIFIgL2UgMjggMCBSCi9sIDI5IDAgUiAvbSAzMCAwIFIgL24gMzIgMCBSIC9vIDMzIDAgUiAvb25lIDM0IDAgUiAvciAzNSAwIFIgL3MgMzYgMCBSCi90aHJlZSAzNyAwIFIgL3R3byAzOCAwIFIgL3UgMzkgMCBSIC96ZXJvIDQwIDAgUiA+PgplbmRvYmoKMyAwIG9iago8PCAvRjEgMjAgMCBSIC9GMiAxNSAwIFIgPj4KZW5kb2JqCjQgMCBvYmoKPDwgL0ExIDw8IC9DQSAwIC9UeXBlIC9FeHRHU3RhdGUgL2NhIDEgPj4KL0EyIDw8IC9DQSAxIC9UeXBlIC9FeHRHU3RhdGUgL2NhIDEgPj4KL0EzIDw8IC9DQSAwLjggL1R5cGUgL0V4dEdTdGF0ZSAvY2EgMC44ID4+ID4+CmVuZG9iago1IDAgb2JqCjw8ID4+CmVuZG9iago2IDAgb2JqCjw8ID4+CmVuZG9iago3IDAgb2JqCjw8IC9GMS1EZWphVnVTYW5zLW1pbnVzIDMxIDAgUiA+PgplbmRvYmoKMiAwIG9iago8PCAvQ291bnQgMSAvS2lkcyBbIDExIDAgUiBdIC9UeXBlIC9QYWdlcyA+PgplbmRvYmoKNDEgMCBvYmoKPDwgL0NyZWF0aW9uRGF0ZSAoRDoyMDIxMTIzMTE0NDA0OSswMicwMCcpCi9DcmVhdG9yIChNYXRwbG90bGliIHYzLjQuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZykKL1Byb2R1Y2VyIChNYXRwbG90bGliIHBkZiBiYWNrZW5kIHYzLjQuMCkgPj4KZW5kb2JqCnhyZWYKMCA0MgowMDAwMDAwMDAwIDY1NTM1IGYgCjAwMDAwMDAwMTYgMDAwMDAgbiAKMDAwMDAxMTA3OSAwMDAwMCBuIAowMDAwMDEwODAzIDAwMDAwIG4gCjAwMDAwMTA4NDYgMDAwMDAgbiAKMDAwMDAxMDk4OCAwMDAwMCBuIAowMDAwMDExMDA5IDAwMDAwIG4gCjAwMDAwMTEwMzAgMDAwMDAgbiAKMDAwMDAwMDA2NSAwMDAwMCBuIAowMDAwMDAwNDAyIDAwMDAwIG4gCjAwMDAwMDI0MjMgMDAwMDAgbiAKMDAwMDAwMDIwOCAwMDAwMCBuIAowMDAwMDAyNDAyIDAwMDAwIG4gCjAwMDAwMDMxMjggMDAwMDAgbiAKMDAwMDAwMjkyMCAwMDAwMCBuIAowMDAwMDAyNjA1IDAwMDAwIG4gCjAwMDAwMDQxODEgMDAwMDAgbiAKMDAwMDAwMjQ0MyAwMDAwMCBuIAowMDAwMDA5NTM3IDAwMDAwIG4gCjAwMDAwMDkzMzcgMDAwMDAgbiAKMDAwMDAwODk0OCAwMDAwMCBuIAowMDAwMDEwNTkwIDAwMDAwIG4gCjAwMDAwMDQyMTMgMDAwMDAgbiAKMDAwMDAwNDM2NiAwMDAwMCBuIAowMDAwMDA0NTE0IDAwMDAwIG4gCjAwMDAwMDQ2NjMgMDAwMDAgbiAKMDAwMDAwNDk2OCAwMDAwMCBuIAowMDAwMDA1MTA2IDAwMDAwIG4gCjAwMDAwMDU0MTAgMDAwMDAgbiAKMDAwMDAwNTczMiAwMDAwMCBuIAowMDAwMDA1ODUxIDAwMDAwIG4gCjAwMDAwMDYxODIgMDAwMDAgbiAKMDAwMDAwNjM1NCAwMDAwMCBuIAowMDAwMDA2NTkwIDAwMDAwIG4gCjAwMDAwMDY4ODEgMDAwMDAgbiAKMDAwMDAwNzAzNiAwMDAwMCBuIAowMDAwMDA3MjY5IDAwMDAwIG4gCjAwMDAwMDc2NzYgMDAwMDAgbiAKMDAwMDAwODA4OSAwMDAwMCBuIAowMDAwMDA4NDEzIDAwMDAwIG4gCjAwMDAwMDg2NjAgMDAwMDAgbiAKMDAwMDAxMTEzOSAwMDAwMCBuIAp0cmFpbGVyCjw8IC9JbmZvIDQxIDAgUiAvUm9vdCAxIDAgUiAvU2l6ZSA0MiA+PgpzdGFydHhyZWYKMTEyOTYKJSVFT0YK\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "4 180\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 1]), len(d_results_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 393.378125 262.19625\" width=\"393.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-31T14:40:51.275638</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 393.378125 262.19625 \nL 393.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \nL 386.178125 7.2 \nL 51.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m89cda7feda\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.596307\" xlink:href=\"#m89cda7feda\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(57.796307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.81113\" xlink:href=\"#m89cda7feda\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(260.01113 239.238437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"ma1c77143b1\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"57.343464\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"127.469034\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"163.077297\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"188.341761\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"207.938402\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"223.950024\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"237.487657\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"249.214489\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"259.558287\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"329.683857\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"365.29212\" xlink:href=\"#ma1c77143b1\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Layers -->\n     <g transform=\"translate(202.232031 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 531 \nL 3531 531 \nL 3531 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4c\"/>\n      <use x=\"55.712891\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"116.992188\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"176.171875\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"237.695312\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"278.808594\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m1f0caa24b5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m1f0caa24b5\" y=\"205.119571\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{-3}}$ -->\n      <g transform=\"translate(20.878125 208.91879)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m1f0caa24b5\" y=\"167.515692\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(20.878125 171.314911)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m1f0caa24b5\" y=\"129.911814\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(20.878125 133.711032)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m1f0caa24b5\" y=\"92.307935\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(26.778125 96.107153)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m1f0caa24b5\" y=\"54.704056\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(26.778125 58.503275)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m1f0caa24b5\" y=\"17.100177\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(26.778125 20.899396)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_20\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"maac21226a6\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"220.083659\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"216.439467\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"213.461945\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"210.944486\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"208.763764\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"206.84023\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"193.799676\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"187.177961\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"182.47978\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"178.835588\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"175.858066\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"173.340607\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"171.159885\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"169.236351\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"156.195797\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"149.574082\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"144.875901\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"141.231709\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"138.254187\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"135.736728\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"133.556006\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_28\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"131.632473\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_29\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"118.591918\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_30\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"111.970204\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_31\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"107.272023\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_32\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"103.62783\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_33\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"100.650308\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_34\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"98.132849\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_35\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"95.952127\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_36\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"94.028594\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_37\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"80.988039\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_38\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"74.366325\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_39\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"69.668144\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_40\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"66.023951\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_41\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"63.046429\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_42\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"60.52897\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_43\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"58.348248\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_44\">\n     <g id=\"line2d_57\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"56.424715\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_45\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"43.38416\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_46\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"36.762446\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_47\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"32.064265\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_48\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"28.420072\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_49\">\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"25.44255\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_50\">\n     <g id=\"line2d_63\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"22.925091\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_51\">\n     <g id=\"line2d_64\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"20.744369\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_52\">\n     <g id=\"line2d_65\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#maac21226a6\" y=\"18.820836\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- $E$ -->\n     <g transform=\"translate(14.798438 119.12)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 66.596307 205.290623 \nL 66.596307 173.228037 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 127.469034 205.592321 \nL 127.469034 189.925584 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 188.341761 214.756364 \nL 188.341761 190.790869 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 249.214489 189.293403 \nL 249.214489 174.500452 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 310.087216 43.103172 \nL 310.087216 25.37779 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 370.959943 17.083664 \nL 370.959943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 66.596307 181.152637 \nL 66.596307 166.298487 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 127.469034 161.336278 \nL 127.469034 150.270424 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 188.341761 194.653417 \nL 188.341761 183.964181 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 249.214489 205.931148 \nL 249.214489 193.372637 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 310.087216 210.379439 \nL 310.087216 191.581211 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 370.959943 187.010402 \nL 370.959943 47.828816 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_66\">\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 66.596307 182.407421 \nL 127.469034 196.38622 \nL 188.341761 199.25522 \nL 249.214489 180.759473 \nL 310.087216 31.946759 \nL 370.959943 17.08365 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_67\">\n    <path clip-path=\"url(#p76605f67a9)\" d=\"M 66.596307 171.895259 \nL 127.469034 155.151512 \nL 188.341761 188.573371 \nL 249.214489 198.343184 \nL 310.087216 199.164332 \nL 370.959943 59.145982 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_68\"/>\n   <g id=\"line2d_69\"/>\n   <g id=\"line2d_70\"/>\n   <g id=\"line2d_71\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 224.64 \nL 51.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.178125 224.64 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 7.2 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 58.378125 59.234375 \nL 128.146875 59.234375 \nQ 130.146875 59.234375 130.146875 57.234375 \nL 130.146875 14.2 \nQ 130.146875 12.2 128.146875 12.2 \nL 58.378125 12.2 \nQ 56.378125 12.2 56.378125 14.2 \nL 56.378125 57.234375 \nQ 56.378125 59.234375 58.378125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- model -->\n     <g transform=\"translate(77.692969 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_72\">\n     <path d=\"M 60.378125 34.976562 \nL 80.378125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_73\"/>\n    <g id=\"text_12\">\n     <!-- FFNN -->\n     <g transform=\"translate(88.378125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_74\">\n     <path d=\"M 60.378125 49.654687 \nL 80.378125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_75\"/>\n    <g id=\"text_13\">\n     <!-- ResNET -->\n     <g transform=\"translate(88.378125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p76605f67a9\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"51.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Layers\", ylabel=\"Error\")\n",
    "fig_layers.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "16 140\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 1]), len(d_results_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 393.378125 262.19625\" width=\"393.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2021-12-31T14:40:52.629924</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 393.378125 262.19625 \nL 393.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \nL 386.178125 7.2 \nL 51.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 84.307214 \nC 67.391916 84.307214 68.155046 83.991115 68.717627 83.428534 \nC 69.280208 82.865954 69.596307 82.102823 69.596307 81.307214 \nC 69.596307 80.511605 69.280208 79.748475 68.717627 79.185894 \nC 68.155046 78.623313 67.391916 78.307214 66.596307 78.307214 \nC 65.800698 78.307214 65.037567 78.623313 64.474986 79.185894 \nC 63.912406 79.748475 63.596307 80.511605 63.596307 81.307214 \nC 63.596307 82.102823 63.912406 82.865954 64.474986 83.428534 \nC 65.037567 83.991115 65.800698 84.307214 66.596307 84.307214 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 179.974371 \nC 67.470115 179.974371 68.233245 179.658272 68.795826 179.095691 \nC 69.358407 178.533111 69.674506 177.76998 69.674506 176.974371 \nC 69.674506 176.178762 69.358407 175.415631 68.795826 174.853051 \nC 68.233245 174.29047 67.470115 173.974371 66.674506 173.974371 \nC 65.878896 173.974371 65.115766 174.29047 64.553185 174.853051 \nC 63.990605 175.415631 63.674506 176.178762 63.674506 176.974371 \nC 63.674506 177.76998 63.990605 178.533111 64.553185 179.095691 \nC 65.115766 179.658272 65.878896 179.974371 66.674506 179.974371 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 217.756364 \nC 67.736911 217.756364 68.500041 217.440265 69.062622 216.877684 \nC 69.625203 216.315103 69.941302 215.551973 69.941302 214.756364 \nC 69.941302 213.960754 69.625203 213.197624 69.062622 212.635043 \nC 68.500041 212.072463 67.736911 211.756364 66.941302 211.756364 \nC 66.145692 211.756364 65.382562 212.072463 64.819981 212.635043 \nC 64.257401 213.197624 63.941302 213.960754 63.941302 214.756364 \nC 63.941302 215.551973 64.257401 216.315103 64.819981 216.877684 \nC 65.382562 217.440265 66.145692 217.756364 66.941302 217.756364 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 202.13839 \nC 68.712096 202.13839 69.475227 201.822291 70.037807 201.25971 \nC 70.600388 200.69713 70.916487 199.933999 70.916487 199.13839 \nC 70.916487 198.342781 70.600388 197.579651 70.037807 197.01707 \nC 69.475227 196.454489 68.712096 196.13839 67.916487 196.13839 \nC 67.120878 196.13839 66.357747 196.454489 65.795167 197.01707 \nC 65.232586 197.579651 64.916487 198.342781 64.916487 199.13839 \nC 64.916487 199.933999 65.232586 200.69713 65.795167 201.25971 \nC 66.357747 201.822291 67.120878 202.13839 67.916487 202.13839 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 197.427601 \nC 72.428841 197.427601 73.191971 197.111503 73.754552 196.548922 \nC 74.317132 195.986341 74.633231 195.223211 74.633231 194.427601 \nC 74.633231 193.631992 74.317132 192.868862 73.754552 192.306281 \nC 73.191971 191.7437 72.428841 191.427601 71.633231 191.427601 \nC 70.837622 191.427601 70.074492 191.7437 69.511911 192.306281 \nC 68.94933 192.868862 68.633231 193.631992 68.633231 194.427601 \nC 68.633231 195.223211 68.94933 195.986341 69.511911 196.548922 \nC 70.074492 197.111503 70.837622 197.427601 71.633231 197.427601 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 162.649088 \nC 86.927823 162.649088 87.690954 162.332989 88.253534 161.770409 \nC 88.816115 161.207828 89.132214 160.444698 89.132214 159.649088 \nC 89.132214 158.853479 88.816115 158.090349 88.253534 157.527768 \nC 87.690954 156.965187 86.927823 156.649088 86.132214 156.649088 \nC 85.336605 156.649088 84.573474 156.965187 84.010894 157.527768 \nC 83.448313 158.090349 83.132214 158.853479 83.132214 159.649088 \nC 83.132214 160.444698 83.448313 161.207828 84.010894 161.770409 \nC 84.573474 162.332989 85.336605 162.649088 86.132214 162.649088 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 155.001609 \nC 144.187765 155.001609 144.950895 154.68551 145.513476 154.122929 \nC 146.076056 153.560348 146.392155 152.797218 146.392155 152.001609 \nC 146.392155 151.205999 146.076056 150.442869 145.513476 149.880288 \nC 144.950895 149.317707 144.187765 149.001609 143.392155 149.001609 \nC 142.596546 149.001609 141.833416 149.317707 141.270835 149.880288 \nC 140.708254 150.442869 140.392155 151.205999 140.392155 152.001609 \nC 140.392155 152.797218 140.708254 153.560348 141.270835 154.122929 \nC 141.833416 154.68551 142.596546 155.001609 143.392155 155.001609 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 151.827585 \nC 371.755552 151.827585 372.518683 151.511486 373.081264 150.948905 \nC 373.643844 150.386324 373.959943 149.623194 373.959943 148.827585 \nC 373.959943 148.031975 373.643844 147.268845 373.081264 146.706264 \nC 372.518683 146.143684 371.755552 145.827585 370.959943 145.827585 \nC 370.164334 145.827585 369.401204 146.143684 368.838623 146.706264 \nC 368.276042 147.268845 367.959943 148.031975 367.959943 148.827585 \nC 367.959943 149.623194 368.276042 150.386324 368.838623 150.948905 \nC 369.401204 151.511486 370.164334 151.827585 370.959943 151.827585 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 183.245506 \nC 67.571313 183.245506 68.334444 182.929407 68.897024 182.366827 \nC 69.459605 181.804246 69.775704 181.041116 69.775704 180.245506 \nC 69.775704 179.449897 69.459605 178.686767 68.897024 178.124186 \nC 68.334444 177.561605 67.571313 177.245506 66.775704 177.245506 \nC 65.980095 177.245506 65.216965 177.561605 64.654384 178.124186 \nC 64.091803 178.686767 63.775704 179.449897 63.775704 180.245506 \nC 63.775704 181.041116 64.091803 181.804246 64.654384 182.366827 \nC 65.216965 182.929407 65.980095 183.245506 66.775704 183.245506 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 215.462576 \nC 67.736911 215.462576 68.500041 215.146477 69.062622 214.583897 \nC 69.625203 214.021316 69.941302 213.258185 69.941302 212.462576 \nC 69.941302 211.666967 69.625203 210.903837 69.062622 210.341256 \nC 68.500041 209.778675 67.736911 209.462576 66.941302 209.462576 \nC 66.145692 209.462576 65.382562 209.778675 64.819981 210.341256 \nC 64.257401 210.903837 63.941302 211.666967 63.941302 212.462576 \nC 63.941302 213.258185 64.257401 214.021316 64.819981 214.583897 \nC 65.382562 215.146477 66.145692 215.462576 66.941302 215.462576 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 173.221609 \nC 68.068106 173.221609 68.831236 172.90551 69.393817 172.34293 \nC 69.956398 171.780349 70.272497 171.017219 70.272497 170.221609 \nC 70.272497 169.426 69.956398 168.66287 69.393817 168.100289 \nC 68.831236 167.537708 68.068106 167.221609 67.272497 167.221609 \nC 66.476887 167.221609 65.713757 167.537708 65.151176 168.100289 \nC 64.588596 168.66287 64.272497 169.426 64.272497 170.221609 \nC 64.272497 171.017219 64.588596 171.780349 65.151176 172.34293 \nC 65.713757 172.90551 66.476887 173.221609 67.272497 173.221609 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 181.032143 \nC 68.730496 181.032143 69.493626 180.716044 70.056207 180.153464 \nC 70.618788 179.590883 70.934887 178.827752 70.934887 178.032143 \nC 70.934887 177.236534 70.618788 176.473404 70.056207 175.910823 \nC 69.493626 175.348242 68.730496 175.032143 67.934887 175.032143 \nC 67.139277 175.032143 66.376147 175.348242 65.813566 175.910823 \nC 65.250986 176.473404 64.934887 177.236534 64.934887 178.032143 \nC 64.934887 178.827752 65.250986 179.590883 65.813566 180.153464 \nC 66.376147 180.716044 67.139277 181.032143 67.934887 181.032143 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 98.229718 \nC 70.055276 98.229718 70.818406 97.913619 71.380987 97.351038 \nC 71.943568 96.788457 72.259667 96.025327 72.259667 95.229718 \nC 72.259667 94.434108 71.943568 93.670978 71.380987 93.108397 \nC 70.818406 92.545817 70.055276 92.229718 69.259667 92.229718 \nC 68.464058 92.229718 67.700927 92.545817 67.138347 93.108397 \nC 66.575766 93.670978 66.259667 94.434108 66.259667 95.229718 \nC 66.259667 96.025327 66.575766 96.788457 67.138347 97.351038 \nC 67.700927 97.913619 68.464058 98.229718 69.259667 98.229718 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083731 \nC 72.704836 20.083731 73.467967 19.767633 74.030547 19.205052 \nC 74.593128 18.642471 74.909227 17.879341 74.909227 17.083731 \nC 74.909227 16.288122 74.593128 15.524992 74.030547 14.962411 \nC 73.467967 14.39983 72.704836 14.083731 71.909227 14.083731 \nC 71.113618 14.083731 70.350488 14.39983 69.787907 14.962411 \nC 69.225326 15.524992 68.909227 16.288122 68.909227 17.083731 \nC 68.909227 17.879341 69.225326 18.642471 69.787907 19.205052 \nC 70.350488 19.767633 71.113618 20.083731 71.909227 20.083731 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 103.855636 \nL 66.596307 102.355636 \nL 68.096307 103.855636 \nL 69.596307 102.355636 \nL 68.096307 100.855636 \nL 69.596307 99.355636 \nL 68.096307 97.855636 \nL 66.596307 99.355636 \nL 65.096307 97.855636 \nL 63.596307 99.355636 \nL 65.096307 100.855636 \nL 63.596307 102.355636 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 138.769535 \nL 66.674506 137.269535 \nL 68.174506 138.769535 \nL 69.674506 137.269535 \nL 68.174506 135.769535 \nL 69.674506 134.269535 \nL 68.174506 132.769535 \nL 66.674506 134.269535 \nL 65.174506 132.769535 \nL 63.674506 134.269535 \nL 65.174506 135.769535 \nL 63.674506 137.269535 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 131.912996 \nL 66.941302 130.412996 \nL 68.441302 131.912996 \nL 69.941302 130.412996 \nL 68.441302 128.912996 \nL 69.941302 127.412996 \nL 68.441302 125.912996 \nL 66.941302 127.412996 \nL 65.441302 125.912996 \nL 63.941302 127.412996 \nL 65.441302 128.912996 \nL 63.941302 130.412996 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 157.881284 \nL 67.916487 156.381284 \nL 69.416487 157.881284 \nL 70.916487 156.381284 \nL 69.416487 154.881284 \nL 70.916487 153.381284 \nL 69.416487 151.881284 \nL 67.916487 153.381284 \nL 66.416487 151.881284 \nL 64.916487 153.381284 \nL 66.416487 154.881284 \nL 64.916487 156.381284 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 128.570654 \nL 71.633231 127.070654 \nL 73.133231 128.570654 \nL 74.633231 127.070654 \nL 73.133231 125.570654 \nL 74.633231 124.070654 \nL 73.133231 122.570654 \nL 71.633231 124.070654 \nL 70.133231 122.570654 \nL 68.633231 124.070654 \nL 70.133231 125.570654 \nL 68.633231 127.070654 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 181.582923 \nL 86.132214 180.082923 \nL 87.632214 181.582923 \nL 89.132214 180.082923 \nL 87.632214 178.582923 \nL 89.132214 177.082923 \nL 87.632214 175.582923 \nL 86.132214 177.082923 \nL 84.632214 175.582923 \nL 83.132214 177.082923 \nL 84.632214 178.582923 \nL 83.132214 180.082923 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 156.84842 \nL 143.392155 155.34842 \nL 144.892155 156.84842 \nL 146.392155 155.34842 \nL 144.892155 153.84842 \nL 146.392155 152.34842 \nL 144.892155 150.84842 \nL 143.392155 152.34842 \nL 141.892155 150.84842 \nL 140.392155 152.34842 \nL 141.892155 153.84842 \nL 140.392155 155.34842 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 142.756253 \nL 370.959943 141.256253 \nL 372.459943 142.756253 \nL 373.959943 141.256253 \nL 372.459943 139.756253 \nL 373.959943 138.256253 \nL 372.459943 136.756253 \nL 370.959943 138.256253 \nL 369.459943 136.756253 \nL 367.959943 138.256253 \nL 369.459943 139.756253 \nL 367.959943 141.256253 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 137.99376 \nL 66.775704 136.49376 \nL 68.275704 137.99376 \nL 69.775704 136.49376 \nL 68.275704 134.99376 \nL 69.775704 133.49376 \nL 68.275704 131.99376 \nL 66.775704 133.49376 \nL 65.275704 131.99376 \nL 63.775704 133.49376 \nL 65.275704 134.99376 \nL 63.775704 136.49376 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 117.264237 \nL 66.941302 115.764237 \nL 68.441302 117.264237 \nL 69.941302 115.764237 \nL 68.441302 114.264237 \nL 69.941302 112.764237 \nL 68.441302 111.264237 \nL 66.941302 112.764237 \nL 65.441302 111.264237 \nL 63.941302 112.764237 \nL 65.441302 114.264237 \nL 63.941302 115.764237 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 171.339295 \nL 67.272497 169.839295 \nL 68.772497 171.339295 \nL 70.272497 169.839295 \nL 68.772497 168.339295 \nL 70.272497 166.839295 \nL 68.772497 165.339295 \nL 67.272497 166.839295 \nL 65.772497 165.339295 \nL 64.272497 166.839295 \nL 65.772497 168.339295 \nL 64.272497 169.839295 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 211.147784 \nL 67.934887 209.647784 \nL 69.434887 211.147784 \nL 70.934887 209.647784 \nL 69.434887 208.147784 \nL 70.934887 206.647784 \nL 69.434887 205.147784 \nL 67.934887 206.647784 \nL 66.434887 205.147784 \nL 64.934887 206.647784 \nL 66.434887 208.147784 \nL 64.934887 209.647784 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 177.233276 \nL 69.259667 175.733276 \nL 70.759667 177.233276 \nL 72.259667 175.733276 \nL 70.759667 174.233276 \nL 72.259667 172.733276 \nL 70.759667 171.233276 \nL 69.259667 172.733276 \nL 67.759667 171.233276 \nL 66.259667 172.733276 \nL 67.759667 174.233276 \nL 66.259667 175.733276 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 166.508327 \nL 71.909227 165.008327 \nL 73.409227 166.508327 \nL 74.909227 165.008327 \nL 73.409227 163.508327 \nL 74.909227 162.008327 \nL 73.409227 160.508327 \nL 71.909227 162.008327 \nL 70.409227 160.508327 \nL 68.909227 162.008327 \nL 70.409227 163.508327 \nL 68.909227 165.008327 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 99.720117 \nC 67.391916 99.720117 68.155046 99.404018 68.717627 98.841438 \nC 69.280208 98.278857 69.596307 97.515727 69.596307 96.720117 \nC 69.596307 95.924508 69.280208 95.161378 68.717627 94.598797 \nC 68.155046 94.036216 67.391916 93.720117 66.596307 93.720117 \nC 65.800698 93.720117 65.037567 94.036216 64.474986 94.598797 \nC 63.912406 95.161378 63.596307 95.924508 63.596307 96.720117 \nC 63.596307 97.515727 63.912406 98.278857 64.474986 98.841438 \nC 65.037567 99.404018 65.800698 99.720117 66.596307 99.720117 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 181.314289 \nC 67.470115 181.314289 68.233245 180.99819 68.795826 180.435609 \nC 69.358407 179.873029 69.674506 179.109898 69.674506 178.314289 \nC 69.674506 177.51868 69.358407 176.755549 68.795826 176.192969 \nC 68.233245 175.630388 67.470115 175.314289 66.674506 175.314289 \nC 65.878896 175.314289 65.115766 175.630388 64.553185 176.192969 \nC 63.990605 176.755549 63.674506 177.51868 63.674506 178.314289 \nC 63.674506 179.109898 63.990605 179.873029 64.553185 180.435609 \nC 65.115766 180.99819 65.878896 181.314289 66.674506 181.314289 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 217.478622 \nC 67.736911 217.478622 68.500041 217.162523 69.062622 216.599942 \nC 69.625203 216.037361 69.941302 215.274231 69.941302 214.478622 \nC 69.941302 213.683012 69.625203 212.919882 69.062622 212.357301 \nC 68.500041 211.794721 67.736911 211.478622 66.941302 211.478622 \nC 66.145692 211.478622 65.382562 211.794721 64.819981 212.357301 \nC 64.257401 212.919882 63.941302 213.683012 63.941302 214.478622 \nC 63.941302 215.274231 64.257401 216.037361 64.819981 216.599942 \nC 65.382562 217.162523 66.145692 217.478622 66.941302 217.478622 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 192.91515 \nC 68.712096 192.91515 69.475227 192.599051 70.037807 192.03647 \nC 70.600388 191.473889 70.916487 190.710759 70.916487 189.91515 \nC 70.916487 189.11954 70.600388 188.35641 70.037807 187.793829 \nC 69.475227 187.231249 68.712096 186.91515 67.916487 186.91515 \nC 67.120878 186.91515 66.357747 187.231249 65.795167 187.793829 \nC 65.232586 188.35641 64.916487 189.11954 64.916487 189.91515 \nC 64.916487 190.710759 65.232586 191.473889 65.795167 192.03647 \nC 66.357747 192.599051 67.120878 192.91515 67.916487 192.91515 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 147.538001 \nC 72.428841 147.538001 73.191971 147.221902 73.754552 146.659322 \nC 74.317132 146.096741 74.633231 145.333611 74.633231 144.538001 \nC 74.633231 143.742392 74.317132 142.979262 73.754552 142.416681 \nC 73.191971 141.8541 72.428841 141.538001 71.633231 141.538001 \nC 70.837622 141.538001 70.074492 141.8541 69.511911 142.416681 \nC 68.94933 142.979262 68.633231 143.742392 68.633231 144.538001 \nC 68.633231 145.333611 68.94933 146.096741 69.511911 146.659322 \nC 70.074492 147.221902 70.837622 147.538001 71.633231 147.538001 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 176.403639 \nC 86.927823 176.403639 87.690954 176.08754 88.253534 175.52496 \nC 88.816115 174.962379 89.132214 174.199249 89.132214 173.403639 \nC 89.132214 172.60803 88.816115 171.8449 88.253534 171.282319 \nC 87.690954 170.719738 86.927823 170.403639 86.132214 170.403639 \nC 85.336605 170.403639 84.573474 170.719738 84.010894 171.282319 \nC 83.448313 171.8449 83.132214 172.60803 83.132214 173.403639 \nC 83.132214 174.199249 83.448313 174.962379 84.010894 175.52496 \nC 84.573474 176.08754 85.336605 176.403639 86.132214 176.403639 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 169.580231 \nC 144.187765 169.580231 144.950895 169.264132 145.513476 168.701551 \nC 146.076056 168.13897 146.392155 167.37584 146.392155 166.580231 \nC 146.392155 165.784621 146.076056 165.021491 145.513476 164.45891 \nC 144.950895 163.89633 144.187765 163.580231 143.392155 163.580231 \nC 142.596546 163.580231 141.833416 163.89633 141.270835 164.45891 \nC 140.708254 165.021491 140.392155 165.784621 140.392155 166.580231 \nC 140.392155 167.37584 140.708254 168.13897 141.270835 168.701551 \nC 141.833416 169.264132 142.596546 169.580231 143.392155 169.580231 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 136.161425 \nC 371.755552 136.161425 372.518683 135.845326 373.081264 135.282745 \nC 373.643844 134.720164 373.959943 133.957034 373.959943 133.161425 \nC 373.959943 132.365815 373.643844 131.602685 373.081264 131.040104 \nC 372.518683 130.477524 371.755552 130.161425 370.959943 130.161425 \nC 370.164334 130.161425 369.401204 130.477524 368.838623 131.040104 \nC 368.276042 131.602685 367.959943 132.365815 367.959943 133.161425 \nC 367.959943 133.957034 368.276042 134.720164 368.838623 135.282745 \nC 369.401204 135.845326 370.164334 136.161425 370.959943 136.161425 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 130.960902 \nC 67.571313 130.960902 68.334444 130.644803 68.897024 130.082222 \nC 69.459605 129.519641 69.775704 128.756511 69.775704 127.960902 \nC 69.775704 127.165292 69.459605 126.402162 68.897024 125.839581 \nC 68.334444 125.277001 67.571313 124.960902 66.775704 124.960902 \nC 65.980095 124.960902 65.216965 125.277001 64.654384 125.839581 \nC 64.091803 126.402162 63.775704 127.165292 63.775704 127.960902 \nC 63.775704 128.756511 64.091803 129.519641 64.654384 130.082222 \nC 65.216965 130.644803 65.980095 130.960902 66.775704 130.960902 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 203.088661 \nC 67.736911 203.088661 68.500041 202.772562 69.062622 202.209981 \nC 69.625203 201.647401 69.941302 200.88427 69.941302 200.088661 \nC 69.941302 199.293052 69.625203 198.529921 69.062622 197.967341 \nC 68.500041 197.40476 67.736911 197.088661 66.941302 197.088661 \nC 66.145692 197.088661 65.382562 197.40476 64.819981 197.967341 \nC 64.257401 198.529921 63.941302 199.293052 63.941302 200.088661 \nC 63.941302 200.88427 64.257401 201.647401 64.819981 202.209981 \nC 65.382562 202.772562 66.145692 203.088661 66.941302 203.088661 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 156.12435 \nC 68.068106 156.12435 68.831236 155.808251 69.393817 155.24567 \nC 69.956398 154.683089 70.272497 153.919959 70.272497 153.12435 \nC 70.272497 152.32874 69.956398 151.56561 69.393817 151.003029 \nC 68.831236 150.440449 68.068106 150.12435 67.272497 150.12435 \nC 66.476887 150.12435 65.713757 150.440449 65.151176 151.003029 \nC 64.588596 151.56561 64.272497 152.32874 64.272497 153.12435 \nC 64.272497 153.919959 64.588596 154.683089 65.151176 155.24567 \nC 65.713757 155.808251 66.476887 156.12435 67.272497 156.12435 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 149.164706 \nC 68.730496 149.164706 69.493626 148.848607 70.056207 148.286026 \nC 70.618788 147.723445 70.934887 146.960315 70.934887 146.164706 \nC 70.934887 145.369096 70.618788 144.605966 70.056207 144.043385 \nC 69.493626 143.480804 68.730496 143.164706 67.934887 143.164706 \nC 67.139277 143.164706 66.376147 143.480804 65.813566 144.043385 \nC 65.250986 144.605966 64.934887 145.369096 64.934887 146.164706 \nC 64.934887 146.960315 65.250986 147.723445 65.813566 148.286026 \nC 66.376147 148.848607 67.139277 149.164706 67.934887 149.164706 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 162.293632 \nC 70.055276 162.293632 70.818406 161.977533 71.380987 161.414953 \nC 71.943568 160.852372 72.259667 160.089242 72.259667 159.293632 \nC 72.259667 158.498023 71.943568 157.734893 71.380987 157.172312 \nC 70.818406 156.609731 70.055276 156.293632 69.259667 156.293632 \nC 68.464058 156.293632 67.700927 156.609731 67.138347 157.172312 \nC 66.575766 157.734893 66.259667 158.498023 66.259667 159.293632 \nC 66.259667 160.089242 66.575766 160.852372 67.138347 161.414953 \nC 67.700927 161.977533 68.464058 162.293632 69.259667 162.293632 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083671 \nC 72.704836 20.083671 73.467967 19.767573 74.030547 19.204992 \nC 74.593128 18.642411 74.909227 17.879281 74.909227 17.083671 \nC 74.909227 16.288062 74.593128 15.524932 74.030547 14.962351 \nC 73.467967 14.39977 72.704836 14.083671 71.909227 14.083671 \nC 71.113618 14.083671 70.350488 14.39977 69.787907 14.962351 \nC 69.225326 15.524932 68.909227 16.288062 68.909227 17.083671 \nC 68.909227 17.879281 69.225326 18.642411 69.787907 19.204992 \nC 70.350488 19.767573 71.113618 20.083671 71.909227 20.083671 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 58.227195 \nL 66.596307 56.727195 \nL 68.096307 58.227195 \nL 69.596307 56.727195 \nL 68.096307 55.227195 \nL 69.596307 53.727195 \nL 68.096307 52.227195 \nL 66.596307 53.727195 \nL 65.096307 52.227195 \nL 63.596307 53.727195 \nL 65.096307 55.227195 \nL 63.596307 56.727195 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 91.888929 \nL 66.674506 90.388929 \nL 68.174506 91.888929 \nL 69.674506 90.388929 \nL 68.174506 88.888929 \nL 69.674506 87.388929 \nL 68.174506 85.888929 \nL 66.674506 87.388929 \nL 65.174506 85.888929 \nL 63.674506 87.388929 \nL 65.174506 88.888929 \nL 63.674506 90.388929 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 128.484189 \nL 66.941302 126.984189 \nL 68.441302 128.484189 \nL 69.941302 126.984189 \nL 68.441302 125.484189 \nL 69.941302 123.984189 \nL 68.441302 122.484189 \nL 66.941302 123.984189 \nL 65.441302 122.484189 \nL 63.941302 123.984189 \nL 65.441302 125.484189 \nL 63.941302 126.984189 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 161.290359 \nL 67.916487 159.790359 \nL 69.416487 161.290359 \nL 70.916487 159.790359 \nL 69.416487 158.290359 \nL 70.916487 156.790359 \nL 69.416487 155.290359 \nL 67.916487 156.790359 \nL 66.416487 155.290359 \nL 64.916487 156.790359 \nL 66.416487 158.290359 \nL 64.916487 159.790359 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 160.446318 \nL 71.633231 158.946318 \nL 73.133231 160.446318 \nL 74.633231 158.946318 \nL 73.133231 157.446318 \nL 74.633231 155.946318 \nL 73.133231 154.446318 \nL 71.633231 155.946318 \nL 70.133231 154.446318 \nL 68.633231 155.946318 \nL 70.133231 157.446318 \nL 68.633231 158.946318 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 133.970109 \nL 86.132214 132.470109 \nL 87.632214 133.970109 \nL 89.132214 132.470109 \nL 87.632214 130.970109 \nL 89.132214 129.470109 \nL 87.632214 127.970109 \nL 86.132214 129.470109 \nL 84.632214 127.970109 \nL 83.132214 129.470109 \nL 84.632214 130.970109 \nL 83.132214 132.470109 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 138.423154 \nL 143.392155 136.923154 \nL 144.892155 138.423154 \nL 146.392155 136.923154 \nL 144.892155 135.423154 \nL 146.392155 133.923154 \nL 144.892155 132.423154 \nL 143.392155 133.923154 \nL 141.892155 132.423154 \nL 140.392155 133.923154 \nL 141.892155 135.423154 \nL 140.392155 136.923154 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 157.694157 \nL 370.959943 156.194157 \nL 372.459943 157.694157 \nL 373.959943 156.194157 \nL 372.459943 154.694157 \nL 373.959943 153.194157 \nL 372.459943 151.694157 \nL 370.959943 153.194157 \nL 369.459943 151.694157 \nL 367.959943 153.194157 \nL 369.459943 154.694157 \nL 367.959943 156.194157 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 167.396736 \nL 66.775704 165.896736 \nL 68.275704 167.396736 \nL 69.775704 165.896736 \nL 68.275704 164.396736 \nL 69.775704 162.896736 \nL 68.275704 161.396736 \nL 66.775704 162.896736 \nL 65.275704 161.396736 \nL 63.775704 162.896736 \nL 65.275704 164.396736 \nL 63.775704 165.896736 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 209.327195 \nL 66.941302 207.827195 \nL 68.441302 209.327195 \nL 69.941302 207.827195 \nL 68.441302 206.327195 \nL 69.941302 204.827195 \nL 68.441302 203.327195 \nL 66.941302 204.827195 \nL 65.441302 203.327195 \nL 63.941302 204.827195 \nL 65.441302 206.327195 \nL 63.941302 207.827195 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 153.407904 \nL 67.272497 151.907904 \nL 68.772497 153.407904 \nL 70.272497 151.907904 \nL 68.772497 150.407904 \nL 70.272497 148.907904 \nL 68.772497 147.407904 \nL 67.272497 148.907904 \nL 65.772497 147.407904 \nL 64.272497 148.907904 \nL 65.772497 150.407904 \nL 64.272497 151.907904 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 193.256743 \nL 67.934887 191.756743 \nL 69.434887 193.256743 \nL 70.934887 191.756743 \nL 69.434887 190.256743 \nL 70.934887 188.756743 \nL 69.434887 187.256743 \nL 67.934887 188.756743 \nL 66.434887 187.256743 \nL 64.934887 188.756743 \nL 66.434887 190.256743 \nL 64.934887 191.756743 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 194.29539 \nL 69.259667 192.79539 \nL 70.759667 194.29539 \nL 72.259667 192.79539 \nL 70.759667 191.29539 \nL 72.259667 189.79539 \nL 70.759667 188.29539 \nL 69.259667 189.79539 \nL 67.759667 188.29539 \nL 66.259667 189.79539 \nL 67.759667 191.29539 \nL 66.259667 192.79539 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 135.769755 \nL 71.909227 134.269755 \nL 73.409227 135.769755 \nL 74.909227 134.269755 \nL 73.409227 132.769755 \nL 74.909227 131.269755 \nL 73.409227 129.769755 \nL 71.909227 131.269755 \nL 70.409227 129.769755 \nL 68.909227 131.269755 \nL 70.409227 132.769755 \nL 68.909227 134.269755 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 77.88498 \nC 67.391916 77.88498 68.155046 77.568881 68.717627 77.0063 \nC 69.280208 76.443719 69.596307 75.680589 69.596307 74.88498 \nC 69.596307 74.089371 69.280208 73.32624 68.717627 72.763659 \nC 68.155046 72.201079 67.391916 71.88498 66.596307 71.88498 \nC 65.800698 71.88498 65.037567 72.201079 64.474986 72.763659 \nC 63.912406 73.32624 63.596307 74.089371 63.596307 74.88498 \nC 63.596307 75.680589 63.912406 76.443719 64.474986 77.0063 \nC 65.037567 77.568881 65.800698 77.88498 66.596307 77.88498 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 147.390745 \nC 67.470115 147.390745 68.233245 147.074646 68.795826 146.512065 \nC 69.358407 145.949484 69.674506 145.186354 69.674506 144.390745 \nC 69.674506 143.595135 69.358407 142.832005 68.795826 142.269424 \nC 68.233245 141.706844 67.470115 141.390745 66.674506 141.390745 \nC 65.878896 141.390745 65.115766 141.706844 64.553185 142.269424 \nC 63.990605 142.832005 63.674506 143.595135 63.674506 144.390745 \nC 63.674506 145.186354 63.990605 145.949484 64.553185 146.512065 \nC 65.115766 147.074646 65.878896 147.390745 66.674506 147.390745 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 184.767308 \nC 67.736911 184.767308 68.500041 184.451209 69.062622 183.888628 \nC 69.625203 183.326048 69.941302 182.562917 69.941302 181.767308 \nC 69.941302 180.971699 69.625203 180.208568 69.062622 179.645988 \nC 68.500041 179.083407 67.736911 178.767308 66.941302 178.767308 \nC 66.145692 178.767308 65.382562 179.083407 64.819981 179.645988 \nC 64.257401 180.208568 63.941302 180.971699 63.941302 181.767308 \nC 63.941302 182.562917 64.257401 183.326048 64.819981 183.888628 \nC 65.382562 184.451209 66.145692 184.767308 66.941302 184.767308 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 174.192743 \nC 68.712096 174.192743 69.475227 173.876644 70.037807 173.314063 \nC 70.600388 172.751482 70.916487 171.988352 70.916487 171.192743 \nC 70.916487 170.397133 70.600388 169.634003 70.037807 169.071422 \nC 69.475227 168.508841 68.712096 168.192743 67.916487 168.192743 \nC 67.120878 168.192743 66.357747 168.508841 65.795167 169.071422 \nC 65.232586 169.634003 64.916487 170.397133 64.916487 171.192743 \nC 64.916487 171.988352 65.232586 172.751482 65.795167 173.314063 \nC 66.357747 173.876644 67.120878 174.192743 67.916487 174.192743 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 144.817814 \nC 72.428841 144.817814 73.191971 144.501715 73.754552 143.939134 \nC 74.317132 143.376553 74.633231 142.613423 74.633231 141.817814 \nC 74.633231 141.022205 74.317132 140.259074 73.754552 139.696494 \nC 73.191971 139.133913 72.428841 138.817814 71.633231 138.817814 \nC 70.837622 138.817814 70.074492 139.133913 69.511911 139.696494 \nC 68.94933 140.259074 68.633231 141.022205 68.633231 141.817814 \nC 68.633231 142.613423 68.94933 143.376553 69.511911 143.939134 \nC 70.074492 144.501715 70.837622 144.817814 71.633231 144.817814 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 172.735088 \nC 86.927823 172.735088 87.690954 172.418989 88.253534 171.856408 \nC 88.816115 171.293827 89.132214 170.530697 89.132214 169.735088 \nC 89.132214 168.939478 88.816115 168.176348 88.253534 167.613767 \nC 87.690954 167.051187 86.927823 166.735088 86.132214 166.735088 \nC 85.336605 166.735088 84.573474 167.051187 84.010894 167.613767 \nC 83.448313 168.176348 83.132214 168.939478 83.132214 169.735088 \nC 83.132214 170.530697 83.448313 171.293827 84.010894 171.856408 \nC 84.573474 172.418989 85.336605 172.735088 86.132214 172.735088 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 166.710674 \nC 144.187765 166.710674 144.950895 166.394575 145.513476 165.831994 \nC 146.076056 165.269414 146.392155 164.506283 146.392155 163.710674 \nC 146.392155 162.915065 146.076056 162.151934 145.513476 161.589354 \nC 144.950895 161.026773 144.187765 160.710674 143.392155 160.710674 \nC 142.596546 160.710674 141.833416 161.026773 141.270835 161.589354 \nC 140.708254 162.151934 140.392155 162.915065 140.392155 163.710674 \nC 140.392155 164.506283 140.708254 165.269414 141.270835 165.831994 \nC 141.833416 166.394575 142.596546 166.710674 143.392155 166.710674 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 139.531325 \nC 371.755552 139.531325 372.518683 139.215226 373.081264 138.652645 \nC 373.643844 138.090064 373.959943 137.326934 373.959943 136.531325 \nC 373.959943 135.735715 373.643844 134.972585 373.081264 134.410004 \nC 372.518683 133.847424 371.755552 133.531325 370.959943 133.531325 \nC 370.164334 133.531325 369.401204 133.847424 368.838623 134.410004 \nC 368.276042 134.972585 367.959943 135.735715 367.959943 136.531325 \nC 367.959943 137.326934 368.276042 138.090064 368.838623 138.652645 \nC 369.401204 139.215226 370.164334 139.531325 370.959943 139.531325 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 180.990139 \nC 67.571313 180.990139 68.334444 180.67404 68.897024 180.111459 \nC 69.459605 179.548879 69.775704 178.785748 69.775704 177.990139 \nC 69.775704 177.19453 69.459605 176.431399 68.897024 175.868819 \nC 68.334444 175.306238 67.571313 174.990139 66.775704 174.990139 \nC 65.980095 174.990139 65.216965 175.306238 64.654384 175.868819 \nC 64.091803 176.431399 63.775704 177.19453 63.775704 177.990139 \nC 63.775704 178.785748 64.091803 179.548879 64.654384 180.111459 \nC 65.216965 180.67404 65.980095 180.990139 66.775704 180.990139 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 169.165305 \nC 67.736911 169.165305 68.500041 168.849206 69.062622 168.286626 \nC 69.625203 167.724045 69.941302 166.960915 69.941302 166.165305 \nC 69.941302 165.369696 69.625203 164.606566 69.062622 164.043985 \nC 68.500041 163.481404 67.736911 163.165305 66.941302 163.165305 \nC 66.145692 163.165305 65.382562 163.481404 64.819981 164.043985 \nC 64.257401 164.606566 63.941302 165.369696 63.941302 166.165305 \nC 63.941302 166.960915 64.257401 167.724045 64.819981 168.286626 \nC 65.382562 168.849206 66.145692 169.165305 66.941302 169.165305 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 205.949521 \nC 68.068106 205.949521 68.831236 205.633422 69.393817 205.070842 \nC 69.956398 204.508261 70.272497 203.745131 70.272497 202.949521 \nC 70.272497 202.153912 69.956398 201.390782 69.393817 200.828201 \nC 68.831236 200.26562 68.068106 199.949521 67.272497 199.949521 \nC 66.476887 199.949521 65.713757 200.26562 65.151176 200.828201 \nC 64.588596 201.390782 64.272497 202.153912 64.272497 202.949521 \nC 64.272497 203.745131 64.588596 204.508261 65.151176 205.070842 \nC 65.713757 205.633422 66.476887 205.949521 67.272497 205.949521 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 159.167391 \nC 68.730496 159.167391 69.493626 158.851292 70.056207 158.288711 \nC 70.618788 157.72613 70.934887 156.963 70.934887 156.167391 \nC 70.934887 155.371781 70.618788 154.608651 70.056207 154.04607 \nC 69.493626 153.48349 68.730496 153.167391 67.934887 153.167391 \nC 67.139277 153.167391 66.376147 153.48349 65.813566 154.04607 \nC 65.250986 154.608651 64.934887 155.371781 64.934887 156.167391 \nC 64.934887 156.963 65.250986 157.72613 65.813566 158.288711 \nC 66.376147 158.851292 67.139277 159.167391 67.934887 159.167391 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 20.086091 \nC 70.055276 20.086091 70.818406 19.769992 71.380987 19.207411 \nC 71.943568 18.64483 72.259667 17.8817 72.259667 17.086091 \nC 72.259667 16.290481 71.943568 15.527351 71.380987 14.96477 \nC 70.818406 14.40219 70.055276 14.086091 69.259667 14.086091 \nC 68.464058 14.086091 67.700927 14.40219 67.138347 14.96477 \nC 66.575766 15.527351 66.259667 16.290481 66.259667 17.086091 \nC 66.259667 17.8817 66.575766 18.64483 67.138347 19.207411 \nC 67.700927 19.769992 68.464058 20.086091 69.259667 20.086091 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083677 \nC 72.704836 20.083677 73.467967 19.767578 74.030547 19.204997 \nC 74.593128 18.642416 74.909227 17.879286 74.909227 17.083677 \nC 74.909227 16.288067 74.593128 15.524937 74.030547 14.962356 \nC 73.467967 14.399776 72.704836 14.083677 71.909227 14.083677 \nC 71.113618 14.083677 70.350488 14.399776 69.787907 14.962356 \nC 69.225326 15.524937 68.909227 16.288067 68.909227 17.083677 \nC 68.909227 17.879286 69.225326 18.642416 69.787907 19.204997 \nC 70.350488 19.767578 71.113618 20.083677 71.909227 20.083677 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 99.268367 \nL 66.596307 97.768367 \nL 68.096307 99.268367 \nL 69.596307 97.768367 \nL 68.096307 96.268367 \nL 69.596307 94.768367 \nL 68.096307 93.268367 \nL 66.596307 94.768367 \nL 65.096307 93.268367 \nL 63.596307 94.768367 \nL 65.096307 96.268367 \nL 63.596307 97.768367 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 161.766863 \nL 66.674506 160.266863 \nL 68.174506 161.766863 \nL 69.674506 160.266863 \nL 68.174506 158.766863 \nL 69.674506 157.266863 \nL 68.174506 155.766863 \nL 66.674506 157.266863 \nL 65.174506 155.766863 \nL 63.674506 157.266863 \nL 65.174506 158.766863 \nL 63.674506 160.266863 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 148.679754 \nL 66.941302 147.179754 \nL 68.441302 148.679754 \nL 69.941302 147.179754 \nL 68.441302 145.679754 \nL 69.941302 144.179754 \nL 68.441302 142.679754 \nL 66.941302 144.179754 \nL 65.441302 142.679754 \nL 63.941302 144.179754 \nL 65.441302 145.679754 \nL 63.941302 147.179754 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 207.806556 \nL 67.916487 206.306556 \nL 69.416487 207.806556 \nL 70.916487 206.306556 \nL 69.416487 204.806556 \nL 70.916487 203.306556 \nL 69.416487 201.806556 \nL 67.916487 203.306556 \nL 66.416487 201.806556 \nL 64.916487 203.306556 \nL 66.416487 204.806556 \nL 64.916487 206.306556 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 157.560835 \nL 71.633231 156.060835 \nL 73.133231 157.560835 \nL 74.633231 156.060835 \nL 73.133231 154.560835 \nL 74.633231 153.060835 \nL 73.133231 151.560835 \nL 71.633231 153.060835 \nL 70.133231 151.560835 \nL 68.633231 153.060835 \nL 70.133231 154.560835 \nL 68.633231 156.060835 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 122.762588 \nL 86.132214 121.262588 \nL 87.632214 122.762588 \nL 89.132214 121.262588 \nL 87.632214 119.762588 \nL 89.132214 118.262588 \nL 87.632214 116.762588 \nL 86.132214 118.262588 \nL 84.632214 116.762588 \nL 83.132214 118.262588 \nL 84.632214 119.762588 \nL 83.132214 121.262588 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 136.974103 \nL 143.392155 135.474103 \nL 144.892155 136.974103 \nL 146.392155 135.474103 \nL 144.892155 133.974103 \nL 146.392155 132.474103 \nL 144.892155 130.974103 \nL 143.392155 132.474103 \nL 141.892155 130.974103 \nL 140.392155 132.474103 \nL 141.892155 133.974103 \nL 140.392155 135.474103 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 159.920103 \nL 370.959943 158.420103 \nL 372.459943 159.920103 \nL 373.959943 158.420103 \nL 372.459943 156.920103 \nL 373.959943 155.420103 \nL 372.459943 153.920103 \nL 370.959943 155.420103 \nL 369.459943 153.920103 \nL 367.959943 155.420103 \nL 369.459943 156.920103 \nL 367.959943 158.420103 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 150.631798 \nL 66.775704 149.131798 \nL 68.275704 150.631798 \nL 69.775704 149.131798 \nL 68.275704 147.631798 \nL 69.775704 146.131798 \nL 68.275704 144.631798 \nL 66.775704 146.131798 \nL 65.275704 144.631798 \nL 63.775704 146.131798 \nL 65.275704 147.631798 \nL 63.775704 149.131798 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 130.972961 \nL 66.941302 129.472961 \nL 68.441302 130.972961 \nL 69.941302 129.472961 \nL 68.441302 127.972961 \nL 69.941302 126.472961 \nL 68.441302 124.972961 \nL 66.941302 126.472961 \nL 65.441302 124.972961 \nL 63.941302 126.472961 \nL 65.441302 127.972961 \nL 63.941302 129.472961 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 158.815031 \nL 67.272497 157.315031 \nL 68.772497 158.815031 \nL 70.272497 157.315031 \nL 68.772497 155.815031 \nL 70.272497 154.315031 \nL 68.772497 152.815031 \nL 67.272497 154.315031 \nL 65.772497 152.815031 \nL 64.272497 154.315031 \nL 65.772497 155.815031 \nL 64.272497 157.315031 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 161.47511 \nL 67.934887 159.97511 \nL 69.434887 161.47511 \nL 70.934887 159.97511 \nL 69.434887 158.47511 \nL 70.934887 156.97511 \nL 69.434887 155.47511 \nL 67.934887 156.97511 \nL 66.434887 155.47511 \nL 64.934887 156.97511 \nL 66.434887 158.47511 \nL 64.934887 159.97511 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 187.230767 \nL 69.259667 185.730767 \nL 70.759667 187.230767 \nL 72.259667 185.730767 \nL 70.759667 184.230767 \nL 72.259667 182.730767 \nL 70.759667 181.230767 \nL 69.259667 182.730767 \nL 67.759667 181.230767 \nL 66.259667 182.730767 \nL 67.759667 184.230767 \nL 66.259667 185.730767 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 160.630572 \nL 71.909227 159.130572 \nL 73.409227 160.630572 \nL 74.909227 159.130572 \nL 73.409227 157.630572 \nL 74.909227 156.130572 \nL 73.409227 154.630572 \nL 71.909227 156.130572 \nL 70.409227 154.630572 \nL 68.909227 156.130572 \nL 70.409227 157.630572 \nL 68.909227 159.130572 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 135.079502 \nC 67.391916 135.079502 68.155046 134.763403 68.717627 134.200822 \nC 69.280208 133.638241 69.596307 132.875111 69.596307 132.079502 \nC 69.596307 131.283892 69.280208 130.520762 68.717627 129.958181 \nC 68.155046 129.395601 67.391916 129.079502 66.596307 129.079502 \nC 65.800698 129.079502 65.037567 129.395601 64.474986 129.958181 \nC 63.912406 130.520762 63.596307 131.283892 63.596307 132.079502 \nC 63.596307 132.875111 63.912406 133.638241 64.474986 134.200822 \nC 65.037567 134.763403 65.800698 135.079502 66.596307 135.079502 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 190.674141 \nC 67.470115 190.674141 68.233245 190.358042 68.795826 189.795461 \nC 69.358407 189.232881 69.674506 188.46975 69.674506 187.674141 \nC 69.674506 186.878532 69.358407 186.115401 68.795826 185.552821 \nC 68.233245 184.99024 67.470115 184.674141 66.674506 184.674141 \nC 65.878896 184.674141 65.115766 184.99024 64.553185 185.552821 \nC 63.990605 186.115401 63.674506 186.878532 63.674506 187.674141 \nC 63.674506 188.46975 63.990605 189.232881 64.553185 189.795461 \nC 65.115766 190.358042 65.878896 190.674141 66.674506 190.674141 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 161.933923 \nC 67.736911 161.933923 68.500041 161.617824 69.062622 161.055243 \nC 69.625203 160.492662 69.941302 159.729532 69.941302 158.933923 \nC 69.941302 158.138313 69.625203 157.375183 69.062622 156.812602 \nC 68.500041 156.250021 67.736911 155.933923 66.941302 155.933923 \nC 66.145692 155.933923 65.382562 156.250021 64.819981 156.812602 \nC 64.257401 157.375183 63.941302 158.138313 63.941302 158.933923 \nC 63.941302 159.729532 64.257401 160.492662 64.819981 161.055243 \nC 65.382562 161.617824 66.145692 161.933923 66.941302 161.933923 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 180.174283 \nC 68.712096 180.174283 69.475227 179.858184 70.037807 179.295603 \nC 70.600388 178.733023 70.916487 177.969892 70.916487 177.174283 \nC 70.916487 176.378674 70.600388 175.615543 70.037807 175.052963 \nC 69.475227 174.490382 68.712096 174.174283 67.916487 174.174283 \nC 67.120878 174.174283 66.357747 174.490382 65.795167 175.052963 \nC 65.232586 175.615543 64.916487 176.378674 64.916487 177.174283 \nC 64.916487 177.969892 65.232586 178.733023 65.795167 179.295603 \nC 66.357747 179.858184 67.120878 180.174283 67.916487 180.174283 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 167.512193 \nC 72.428841 167.512193 73.191971 167.196094 73.754552 166.633514 \nC 74.317132 166.070933 74.633231 165.307803 74.633231 164.512193 \nC 74.633231 163.716584 74.317132 162.953454 73.754552 162.390873 \nC 73.191971 161.828292 72.428841 161.512193 71.633231 161.512193 \nC 70.837622 161.512193 70.074492 161.828292 69.511911 162.390873 \nC 68.94933 162.953454 68.633231 163.716584 68.633231 164.512193 \nC 68.633231 165.307803 68.94933 166.070933 69.511911 166.633514 \nC 70.074492 167.196094 70.837622 167.512193 71.633231 167.512193 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 203.071011 \nC 86.927823 203.071011 87.690954 202.754912 88.253534 202.192332 \nC 88.816115 201.629751 89.132214 200.866621 89.132214 200.071011 \nC 89.132214 199.275402 88.816115 198.512272 88.253534 197.949691 \nC 87.690954 197.38711 86.927823 197.071011 86.132214 197.071011 \nC 85.336605 197.071011 84.573474 197.38711 84.010894 197.949691 \nC 83.448313 198.512272 83.132214 199.275402 83.132214 200.071011 \nC 83.132214 200.866621 83.448313 201.629751 84.010894 202.192332 \nC 84.573474 202.754912 85.336605 203.071011 86.132214 203.071011 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 136.676393 \nC 144.187765 136.676393 144.950895 136.360294 145.513476 135.797713 \nC 146.076056 135.235133 146.392155 134.472002 146.392155 133.676393 \nC 146.392155 132.880784 146.076056 132.117653 145.513476 131.555073 \nC 144.950895 130.992492 144.187765 130.676393 143.392155 130.676393 \nC 142.596546 130.676393 141.833416 130.992492 141.270835 131.555073 \nC 140.708254 132.117653 140.392155 132.880784 140.392155 133.676393 \nC 140.392155 134.472002 140.708254 135.235133 141.270835 135.797713 \nC 141.833416 136.360294 142.596546 136.676393 143.392155 136.676393 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 143.894462 \nC 371.755552 143.894462 372.518683 143.578363 373.081264 143.015782 \nC 373.643844 142.453201 373.959943 141.690071 373.959943 140.894462 \nC 373.959943 140.098853 373.643844 139.335722 373.081264 138.773142 \nC 372.518683 138.210561 371.755552 137.894462 370.959943 137.894462 \nC 370.164334 137.894462 369.401204 138.210561 368.838623 138.773142 \nC 368.276042 139.335722 367.959943 140.098853 367.959943 140.894462 \nC 367.959943 141.690071 368.276042 142.453201 368.838623 143.015782 \nC 369.401204 143.578363 370.164334 143.894462 370.959943 143.894462 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 169.320152 \nC 67.571313 169.320152 68.334444 169.004053 68.897024 168.441472 \nC 69.459605 167.878891 69.775704 167.115761 69.775704 166.320152 \nC 69.775704 165.524542 69.459605 164.761412 68.897024 164.198831 \nC 68.334444 163.636251 67.571313 163.320152 66.775704 163.320152 \nC 65.980095 163.320152 65.216965 163.636251 64.654384 164.198831 \nC 64.091803 164.761412 63.775704 165.524542 63.775704 166.320152 \nC 63.775704 167.115761 64.091803 167.878891 64.654384 168.441472 \nC 65.216965 169.004053 65.980095 169.320152 66.775704 169.320152 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 143.306115 \nC 67.736911 143.306115 68.500041 142.990016 69.062622 142.427435 \nC 69.625203 141.864854 69.941302 141.101724 69.941302 140.306115 \nC 69.941302 139.510506 69.625203 138.747375 69.062622 138.184795 \nC 68.500041 137.622214 67.736911 137.306115 66.941302 137.306115 \nC 66.145692 137.306115 65.382562 137.622214 64.819981 138.184795 \nC 64.257401 138.747375 63.941302 139.510506 63.941302 140.306115 \nC 63.941302 141.101724 64.257401 141.864854 64.819981 142.427435 \nC 65.382562 142.990016 66.145692 143.306115 66.941302 143.306115 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 194.44984 \nC 68.068106 194.44984 68.831236 194.133741 69.393817 193.57116 \nC 69.956398 193.00858 70.272497 192.245449 70.272497 191.44984 \nC 70.272497 190.654231 69.956398 189.8911 69.393817 189.32852 \nC 68.831236 188.765939 68.068106 188.44984 67.272497 188.44984 \nC 66.476887 188.44984 65.713757 188.765939 65.151176 189.32852 \nC 64.588596 189.8911 64.272497 190.654231 64.272497 191.44984 \nC 64.272497 192.245449 64.588596 193.00858 65.151176 193.57116 \nC 65.713757 194.133741 66.476887 194.44984 67.272497 194.44984 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 177.327889 \nC 68.730496 177.327889 69.493626 177.01179 70.056207 176.449209 \nC 70.618788 175.886629 70.934887 175.123498 70.934887 174.327889 \nC 70.934887 173.53228 70.618788 172.769149 70.056207 172.206569 \nC 69.493626 171.643988 68.730496 171.327889 67.934887 171.327889 \nC 67.139277 171.327889 66.376147 171.643988 65.813566 172.206569 \nC 65.250986 172.769149 64.934887 173.53228 64.934887 174.327889 \nC 64.934887 175.123498 65.250986 175.886629 65.813566 176.449209 \nC 66.376147 177.01179 67.139277 177.327889 67.934887 177.327889 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 95.638781 \nC 70.055276 95.638781 70.818406 95.322682 71.380987 94.760102 \nC 71.943568 94.197521 72.259667 93.434391 72.259667 92.638781 \nC 72.259667 91.843172 71.943568 91.080042 71.380987 90.517461 \nC 70.818406 89.95488 70.055276 89.638781 69.259667 89.638781 \nC 68.464058 89.638781 67.700927 89.95488 67.138347 90.517461 \nC 66.575766 91.080042 66.259667 91.843172 66.259667 92.638781 \nC 66.259667 93.434391 66.575766 94.197521 67.138347 94.760102 \nC 67.700927 95.322682 68.464058 95.638781 69.259667 95.638781 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083636 \nC 72.704836 20.083636 73.467967 19.767537 74.030547 19.204957 \nC 74.593128 18.642376 74.909227 17.879246 74.909227 17.083636 \nC 74.909227 16.288027 74.593128 15.524897 74.030547 14.962316 \nC 73.467967 14.399735 72.704836 14.083636 71.909227 14.083636 \nC 71.113618 14.083636 70.350488 14.399735 69.787907 14.962316 \nC 69.225326 15.524897 68.909227 16.288027 68.909227 17.083636 \nC 68.909227 17.879246 69.225326 18.642376 69.787907 19.204957 \nC 70.350488 19.767537 71.113618 20.083636 71.909227 20.083636 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 91.170852 \nL 66.596307 89.670852 \nL 68.096307 91.170852 \nL 69.596307 89.670852 \nL 68.096307 88.170852 \nL 69.596307 86.670852 \nL 68.096307 85.170852 \nL 66.596307 86.670852 \nL 65.096307 85.170852 \nL 63.596307 86.670852 \nL 65.096307 88.170852 \nL 63.596307 89.670852 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 147.264623 \nL 66.674506 145.764623 \nL 68.174506 147.264623 \nL 69.674506 145.764623 \nL 68.174506 144.264623 \nL 69.674506 142.764623 \nL 68.174506 141.264623 \nL 66.674506 142.764623 \nL 65.174506 141.264623 \nL 63.674506 142.764623 \nL 65.174506 144.264623 \nL 63.674506 145.764623 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 168.140894 \nL 66.941302 166.640894 \nL 68.441302 168.140894 \nL 69.941302 166.640894 \nL 68.441302 165.140894 \nL 69.941302 163.640894 \nL 68.441302 162.140894 \nL 66.941302 163.640894 \nL 65.441302 162.140894 \nL 63.941302 163.640894 \nL 65.441302 165.140894 \nL 63.941302 166.640894 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 195.217217 \nL 67.916487 193.717217 \nL 69.416487 195.217217 \nL 70.916487 193.717217 \nL 69.416487 192.217217 \nL 70.916487 190.717217 \nL 69.416487 189.217217 \nL 67.916487 190.717217 \nL 66.416487 189.217217 \nL 64.916487 190.717217 \nL 66.416487 192.217217 \nL 64.916487 193.717217 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 135.304399 \nL 71.633231 133.804399 \nL 73.133231 135.304399 \nL 74.633231 133.804399 \nL 73.133231 132.304399 \nL 74.633231 130.804399 \nL 73.133231 129.304399 \nL 71.633231 130.804399 \nL 70.133231 129.304399 \nL 68.633231 130.804399 \nL 70.133231 132.304399 \nL 68.633231 133.804399 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 134.627072 \nL 86.132214 133.127072 \nL 87.632214 134.627072 \nL 89.132214 133.127072 \nL 87.632214 131.627072 \nL 89.132214 130.127072 \nL 87.632214 128.627072 \nL 86.132214 130.127072 \nL 84.632214 128.627072 \nL 83.132214 130.127072 \nL 84.632214 131.627072 \nL 83.132214 133.127072 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 160.576295 \nL 143.392155 159.076295 \nL 144.892155 160.576295 \nL 146.392155 159.076295 \nL 144.892155 157.576295 \nL 146.392155 156.076295 \nL 144.892155 154.576295 \nL 143.392155 156.076295 \nL 141.892155 154.576295 \nL 140.392155 156.076295 \nL 141.892155 157.576295 \nL 140.392155 159.076295 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 159.800298 \nL 370.959943 158.300298 \nL 372.459943 159.800298 \nL 373.959943 158.300298 \nL 372.459943 156.800298 \nL 373.959943 155.300298 \nL 372.459943 153.800298 \nL 370.959943 155.300298 \nL 369.459943 153.800298 \nL 367.959943 155.300298 \nL 369.459943 156.800298 \nL 367.959943 158.300298 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 150.657922 \nL 66.775704 149.157922 \nL 68.275704 150.657922 \nL 69.775704 149.157922 \nL 68.275704 147.657922 \nL 69.775704 146.157922 \nL 68.275704 144.657922 \nL 66.775704 146.157922 \nL 65.275704 144.657922 \nL 63.775704 146.157922 \nL 65.275704 147.657922 \nL 63.775704 149.157922 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 160.474281 \nL 66.941302 158.974281 \nL 68.441302 160.474281 \nL 69.941302 158.974281 \nL 68.441302 157.474281 \nL 69.941302 155.974281 \nL 68.441302 154.474281 \nL 66.941302 155.974281 \nL 65.441302 154.474281 \nL 63.941302 155.974281 \nL 65.441302 157.474281 \nL 63.941302 158.974281 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 149.855133 \nL 67.272497 148.355133 \nL 68.772497 149.855133 \nL 70.272497 148.355133 \nL 68.772497 146.855133 \nL 70.272497 145.355133 \nL 68.772497 143.855133 \nL 67.272497 145.355133 \nL 65.772497 143.855133 \nL 64.272497 145.355133 \nL 65.772497 146.855133 \nL 64.272497 148.355133 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 166.187652 \nL 67.934887 164.687652 \nL 69.434887 166.187652 \nL 70.934887 164.687652 \nL 69.434887 163.187652 \nL 70.934887 161.687652 \nL 69.434887 160.187652 \nL 67.934887 161.687652 \nL 66.434887 160.187652 \nL 64.934887 161.687652 \nL 66.434887 163.187652 \nL 64.934887 164.687652 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 171.840876 \nL 69.259667 170.340876 \nL 70.759667 171.840876 \nL 72.259667 170.340876 \nL 70.759667 168.840876 \nL 72.259667 167.340876 \nL 70.759667 165.840876 \nL 69.259667 167.340876 \nL 67.759667 165.840876 \nL 66.259667 167.340876 \nL 67.759667 168.840876 \nL 66.259667 170.340876 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 148.034044 \nL 71.909227 146.534044 \nL 73.409227 148.034044 \nL 74.909227 146.534044 \nL 73.409227 145.034044 \nL 74.909227 143.534044 \nL 73.409227 142.034044 \nL 71.909227 143.534044 \nL 70.409227 142.034044 \nL 68.909227 143.534044 \nL 70.409227 145.034044 \nL 68.909227 146.534044 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 165.930571 \nC 67.391916 165.930571 68.155046 165.614472 68.717627 165.051891 \nC 69.280208 164.48931 69.596307 163.72618 69.596307 162.930571 \nC 69.596307 162.134962 69.280208 161.371831 68.717627 160.80925 \nC 68.155046 160.24667 67.391916 159.930571 66.596307 159.930571 \nC 65.800698 159.930571 65.037567 160.24667 64.474986 160.80925 \nC 63.912406 161.371831 63.596307 162.134962 63.596307 162.930571 \nC 63.596307 163.72618 63.912406 164.48931 64.474986 165.051891 \nC 65.037567 165.614472 65.800698 165.930571 66.596307 165.930571 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 169.162947 \nC 67.736911 169.162947 68.500041 168.846848 69.062622 168.284267 \nC 69.625203 167.721686 69.941302 166.958556 69.941302 166.162947 \nC 69.941302 165.367337 69.625203 164.604207 69.062622 164.041626 \nC 68.500041 163.479046 67.736911 163.162947 66.941302 163.162947 \nC 66.145692 163.162947 65.382562 163.479046 64.819981 164.041626 \nC 64.257401 164.604207 63.941302 165.367337 63.941302 166.162947 \nC 63.941302 166.958556 64.257401 167.721686 64.819981 168.284267 \nC 65.382562 168.846848 66.145692 169.162947 66.941302 169.162947 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 158.293252 \nC 68.712096 158.293252 69.475227 157.977153 70.037807 157.414573 \nC 70.600388 156.851992 70.916487 156.088862 70.916487 155.293252 \nC 70.916487 154.497643 70.600388 153.734513 70.037807 153.171932 \nC 69.475227 152.609351 68.712096 152.293252 67.916487 152.293252 \nC 67.120878 152.293252 66.357747 152.609351 65.795167 153.171932 \nC 65.232586 153.734513 64.916487 154.497643 64.916487 155.293252 \nC 64.916487 156.088862 65.232586 156.851992 65.795167 157.414573 \nC 66.357747 157.977153 67.120878 158.293252 67.916487 158.293252 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 185.173631 \nC 72.428841 185.173631 73.191971 184.857532 73.754552 184.294951 \nC 74.317132 183.73237 74.633231 182.96924 74.633231 182.173631 \nC 74.633231 181.378021 74.317132 180.614891 73.754552 180.05231 \nC 73.191971 179.489729 72.428841 179.173631 71.633231 179.173631 \nC 70.837622 179.173631 70.074492 179.489729 69.511911 180.05231 \nC 68.94933 180.614891 68.633231 181.378021 68.633231 182.173631 \nC 68.633231 182.96924 68.94933 183.73237 69.511911 184.294951 \nC 70.074492 184.857532 70.837622 185.173631 71.633231 185.173631 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 151.74576 \nC 86.927823 151.74576 87.690954 151.429661 88.253534 150.867081 \nC 88.816115 150.3045 89.132214 149.54137 89.132214 148.74576 \nC 89.132214 147.950151 88.816115 147.187021 88.253534 146.62444 \nC 87.690954 146.061859 86.927823 145.74576 86.132214 145.74576 \nC 85.336605 145.74576 84.573474 146.061859 84.010894 146.62444 \nC 83.448313 147.187021 83.132214 147.950151 83.132214 148.74576 \nC 83.132214 149.54137 83.448313 150.3045 84.010894 150.867081 \nC 84.573474 151.429661 85.336605 151.74576 86.132214 151.74576 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 136.780887 \nC 144.187765 136.780887 144.950895 136.464788 145.513476 135.902207 \nC 146.076056 135.339627 146.392155 134.576496 146.392155 133.780887 \nC 146.392155 132.985278 146.076056 132.222147 145.513476 131.659567 \nC 144.950895 131.096986 144.187765 130.780887 143.392155 130.780887 \nC 142.596546 130.780887 141.833416 131.096986 141.270835 131.659567 \nC 140.708254 132.222147 140.392155 132.985278 140.392155 133.780887 \nC 140.392155 134.576496 140.708254 135.339627 141.270835 135.902207 \nC 141.833416 136.464788 142.596546 136.780887 143.392155 136.780887 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 185.958902 \nC 371.755552 185.958902 372.518683 185.642803 373.081264 185.080222 \nC 373.643844 184.517641 373.959943 183.754511 373.959943 182.958902 \nC 373.959943 182.163292 373.643844 181.400162 373.081264 180.837581 \nC 372.518683 180.275001 371.755552 179.958902 370.959943 179.958902 \nC 370.164334 179.958902 369.401204 180.275001 368.838623 180.837581 \nC 368.276042 181.400162 367.959943 182.163292 367.959943 182.958902 \nC 367.959943 183.754511 368.276042 184.517641 368.838623 185.080222 \nC 369.401204 185.642803 370.164334 185.958902 370.959943 185.958902 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 183.119966 \nC 67.571313 183.119966 68.334444 182.803867 68.897024 182.241286 \nC 69.459605 181.678706 69.775704 180.915575 69.775704 180.119966 \nC 69.775704 179.324357 69.459605 178.561226 68.897024 177.998646 \nC 68.334444 177.436065 67.571313 177.119966 66.775704 177.119966 \nC 65.980095 177.119966 65.216965 177.436065 64.654384 177.998646 \nC 64.091803 178.561226 63.775704 179.324357 63.775704 180.119966 \nC 63.775704 180.915575 64.091803 181.678706 64.654384 182.241286 \nC 65.216965 182.803867 65.980095 183.119966 66.775704 183.119966 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 172.325602 \nC 67.736911 172.325602 68.500041 172.009503 69.062622 171.446922 \nC 69.625203 170.884341 69.941302 170.121211 69.941302 169.325602 \nC 69.941302 168.529992 69.625203 167.766862 69.062622 167.204281 \nC 68.500041 166.641701 67.736911 166.325602 66.941302 166.325602 \nC 66.145692 166.325602 65.382562 166.641701 64.819981 167.204281 \nC 64.257401 167.766862 63.941302 168.529992 63.941302 169.325602 \nC 63.941302 170.121211 64.257401 170.884341 64.819981 171.446922 \nC 65.382562 172.009503 66.145692 172.325602 66.941302 172.325602 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 192.394233 \nC 68.068106 192.394233 68.831236 192.078134 69.393817 191.515553 \nC 69.956398 190.952972 70.272497 190.189842 70.272497 189.394233 \nC 70.272497 188.598623 69.956398 187.835493 69.393817 187.272912 \nC 68.831236 186.710332 68.068106 186.394233 67.272497 186.394233 \nC 66.476887 186.394233 65.713757 186.710332 65.151176 187.272912 \nC 64.588596 187.835493 64.272497 188.598623 64.272497 189.394233 \nC 64.272497 190.189842 64.588596 190.952972 65.151176 191.515553 \nC 65.713757 192.078134 66.476887 192.394233 67.272497 192.394233 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 174.988715 \nC 68.730496 174.988715 69.493626 174.672617 70.056207 174.110036 \nC 70.618788 173.547455 70.934887 172.784325 70.934887 171.988715 \nC 70.934887 171.193106 70.618788 170.429976 70.056207 169.867395 \nC 69.493626 169.304814 68.730496 168.988715 67.934887 168.988715 \nC 67.139277 168.988715 66.376147 169.304814 65.813566 169.867395 \nC 65.250986 170.429976 64.934887 171.193106 64.934887 171.988715 \nC 64.934887 172.784325 65.250986 173.547455 65.813566 174.110036 \nC 66.376147 174.672617 67.139277 174.988715 67.934887 174.988715 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 20.101532 \nC 70.055276 20.101532 70.818406 19.785433 71.380987 19.222852 \nC 71.943568 18.660271 72.259667 17.897141 72.259667 17.101532 \nC 72.259667 16.305922 71.943568 15.542792 71.380987 14.980211 \nC 70.818406 14.417631 70.055276 14.101532 69.259667 14.101532 \nC 68.464058 14.101532 67.700927 14.417631 67.138347 14.980211 \nC 66.575766 15.542792 66.259667 16.305922 66.259667 17.101532 \nC 66.259667 17.897141 66.575766 18.660271 67.138347 19.222852 \nC 67.700927 19.785433 68.464058 20.101532 69.259667 20.101532 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083661 \nC 72.704836 20.083661 73.467967 19.767562 74.030547 19.204982 \nC 74.593128 18.642401 74.909227 17.87927 74.909227 17.083661 \nC 74.909227 16.288052 74.593128 15.524922 74.030547 14.962341 \nC 73.467967 14.39976 72.704836 14.083661 71.909227 14.083661 \nC 71.113618 14.083661 70.350488 14.39976 69.787907 14.962341 \nC 69.225326 15.524922 68.909227 16.288052 68.909227 17.083661 \nC 68.909227 17.87927 69.225326 18.642401 69.787907 19.204982 \nC 70.350488 19.767562 71.113618 20.083661 71.909227 20.083661 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 61.31756 \nL 66.596307 59.81756 \nL 68.096307 61.31756 \nL 69.596307 59.81756 \nL 68.096307 58.31756 \nL 69.596307 56.81756 \nL 68.096307 55.31756 \nL 66.596307 56.81756 \nL 65.096307 55.31756 \nL 63.596307 56.81756 \nL 65.096307 58.31756 \nL 63.596307 59.81756 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 29.490119 \nL 66.674506 27.990119 \nL 68.174506 29.490119 \nL 69.674506 27.990119 \nL 68.174506 26.490119 \nL 69.674506 24.990119 \nL 68.174506 23.490119 \nL 66.674506 24.990119 \nL 65.174506 23.490119 \nL 63.674506 24.990119 \nL 65.174506 26.490119 \nL 63.674506 27.990119 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 146.961436 \nL 66.941302 145.461436 \nL 68.441302 146.961436 \nL 69.941302 145.461436 \nL 68.441302 143.961436 \nL 69.941302 142.461436 \nL 68.441302 140.961436 \nL 66.941302 142.461436 \nL 65.441302 140.961436 \nL 63.941302 142.461436 \nL 65.441302 143.961436 \nL 63.941302 145.461436 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 131.135674 \nL 67.916487 129.635674 \nL 69.416487 131.135674 \nL 70.916487 129.635674 \nL 69.416487 128.135674 \nL 70.916487 126.635674 \nL 69.416487 125.135674 \nL 67.916487 126.635674 \nL 66.416487 125.135674 \nL 64.916487 126.635674 \nL 66.416487 128.135674 \nL 64.916487 129.635674 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 165.927562 \nL 71.633231 164.427562 \nL 73.133231 165.927562 \nL 74.633231 164.427562 \nL 73.133231 162.927562 \nL 74.633231 161.427562 \nL 73.133231 159.927562 \nL 71.633231 161.427562 \nL 70.133231 159.927562 \nL 68.633231 161.427562 \nL 70.133231 162.927562 \nL 68.633231 164.427562 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 164.116789 \nL 86.132214 162.616789 \nL 87.632214 164.116789 \nL 89.132214 162.616789 \nL 87.632214 161.116789 \nL 89.132214 159.616789 \nL 87.632214 158.116789 \nL 86.132214 159.616789 \nL 84.632214 158.116789 \nL 83.132214 159.616789 \nL 84.632214 161.116789 \nL 83.132214 162.616789 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 154.752834 \nL 143.392155 153.252834 \nL 144.892155 154.752834 \nL 146.392155 153.252834 \nL 144.892155 151.752834 \nL 146.392155 150.252834 \nL 144.892155 148.752834 \nL 143.392155 150.252834 \nL 141.892155 148.752834 \nL 140.392155 150.252834 \nL 141.892155 151.752834 \nL 140.392155 153.252834 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 166.896519 \nL 370.959943 165.396519 \nL 372.459943 166.896519 \nL 373.959943 165.396519 \nL 372.459943 163.896519 \nL 373.959943 162.396519 \nL 372.459943 160.896519 \nL 370.959943 162.396519 \nL 369.459943 160.896519 \nL 367.959943 162.396519 \nL 369.459943 163.896519 \nL 367.959943 165.396519 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 152.755472 \nL 66.775704 151.255472 \nL 68.275704 152.755472 \nL 69.775704 151.255472 \nL 68.275704 149.755472 \nL 69.775704 148.255472 \nL 68.275704 146.755472 \nL 66.775704 148.255472 \nL 65.275704 146.755472 \nL 63.775704 148.255472 \nL 65.275704 149.755472 \nL 63.775704 151.255472 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 128.791547 \nL 66.941302 127.291547 \nL 68.441302 128.791547 \nL 69.941302 127.291547 \nL 68.441302 125.791547 \nL 69.941302 124.291547 \nL 68.441302 122.791547 \nL 66.941302 124.291547 \nL 65.441302 122.791547 \nL 63.941302 124.291547 \nL 65.441302 125.791547 \nL 63.941302 127.291547 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 178.657043 \nL 67.272497 177.157043 \nL 68.772497 178.657043 \nL 70.272497 177.157043 \nL 68.772497 175.657043 \nL 70.272497 174.157043 \nL 68.772497 172.657043 \nL 67.272497 174.157043 \nL 65.772497 172.657043 \nL 64.272497 174.157043 \nL 65.772497 175.657043 \nL 64.272497 177.157043 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 193.073715 \nL 67.934887 191.573715 \nL 69.434887 193.073715 \nL 70.934887 191.573715 \nL 69.434887 190.073715 \nL 70.934887 188.573715 \nL 69.434887 187.073715 \nL 67.934887 188.573715 \nL 66.434887 187.073715 \nL 64.934887 188.573715 \nL 66.434887 190.073715 \nL 64.934887 191.573715 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 185.011994 \nL 69.259667 183.511994 \nL 70.759667 185.011994 \nL 72.259667 183.511994 \nL 70.759667 182.011994 \nL 72.259667 180.511994 \nL 70.759667 179.011994 \nL 69.259667 180.511994 \nL 67.759667 179.011994 \nL 66.259667 180.511994 \nL 67.759667 182.011994 \nL 66.259667 183.511994 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 192.549181 \nL 71.909227 191.049181 \nL 73.409227 192.549181 \nL 74.909227 191.049181 \nL 73.409227 189.549181 \nL 74.909227 188.049181 \nL 73.409227 186.549181 \nL 71.909227 188.049181 \nL 70.409227 186.549181 \nL 68.909227 188.049181 \nL 70.409227 189.549181 \nL 68.909227 191.049181 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 99.543963 \nC 67.391916 99.543963 68.155046 99.227864 68.717627 98.665283 \nC 69.280208 98.102702 69.596307 97.339572 69.596307 96.543963 \nC 69.596307 95.748353 69.280208 94.985223 68.717627 94.422642 \nC 68.155046 93.860061 67.391916 93.543963 66.596307 93.543963 \nC 65.800698 93.543963 65.037567 93.860061 64.474986 94.422642 \nC 63.912406 94.985223 63.596307 95.748353 63.596307 96.543963 \nC 63.596307 97.339572 63.912406 98.102702 64.474986 98.665283 \nC 65.037567 99.227864 65.800698 99.543963 66.596307 99.543963 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 188.483613 \nC 67.470115 188.483613 68.233245 188.167514 68.795826 187.604933 \nC 69.358407 187.042353 69.674506 186.279222 69.674506 185.483613 \nC 69.674506 184.688004 69.358407 183.924873 68.795826 183.362293 \nC 68.233245 182.799712 67.470115 182.483613 66.674506 182.483613 \nC 65.878896 182.483613 65.115766 182.799712 64.553185 183.362293 \nC 63.990605 183.924873 63.674506 184.688004 63.674506 185.483613 \nC 63.674506 186.279222 63.990605 187.042353 64.553185 187.604933 \nC 65.115766 188.167514 65.878896 188.483613 66.674506 188.483613 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 172.004573 \nC 67.736911 172.004573 68.500041 171.688474 69.062622 171.125893 \nC 69.625203 170.563312 69.941302 169.800182 69.941302 169.004573 \nC 69.941302 168.208963 69.625203 167.445833 69.062622 166.883252 \nC 68.500041 166.320672 67.736911 166.004573 66.941302 166.004573 \nC 66.145692 166.004573 65.382562 166.320672 64.819981 166.883252 \nC 64.257401 167.445833 63.941302 168.208963 63.941302 169.004573 \nC 63.941302 169.800182 64.257401 170.563312 64.819981 171.125893 \nC 65.382562 171.688474 66.145692 172.004573 66.941302 172.004573 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 198.787464 \nC 68.712096 198.787464 69.475227 198.471365 70.037807 197.908784 \nC 70.600388 197.346204 70.916487 196.583073 70.916487 195.787464 \nC 70.916487 194.991855 70.600388 194.228725 70.037807 193.666144 \nC 69.475227 193.103563 68.712096 192.787464 67.916487 192.787464 \nC 67.120878 192.787464 66.357747 193.103563 65.795167 193.666144 \nC 65.232586 194.228725 64.916487 194.991855 64.916487 195.787464 \nC 64.916487 196.583073 65.232586 197.346204 65.795167 197.908784 \nC 66.357747 198.471365 67.120878 198.787464 67.916487 198.787464 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 175.800896 \nC 72.428841 175.800896 73.191971 175.484797 73.754552 174.922217 \nC 74.317132 174.359636 74.633231 173.596506 74.633231 172.800896 \nC 74.633231 172.005287 74.317132 171.242157 73.754552 170.679576 \nC 73.191971 170.116995 72.428841 169.800896 71.633231 169.800896 \nC 70.837622 169.800896 70.074492 170.116995 69.511911 170.679576 \nC 68.94933 171.242157 68.633231 172.005287 68.633231 172.800896 \nC 68.633231 173.596506 68.94933 174.359636 69.511911 174.922217 \nC 70.074492 175.484797 70.837622 175.800896 71.633231 175.800896 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 195.561928 \nC 86.927823 195.561928 87.690954 195.245829 88.253534 194.683249 \nC 88.816115 194.120668 89.132214 193.357537 89.132214 192.561928 \nC 89.132214 191.766319 88.816115 191.003189 88.253534 190.440608 \nC 87.690954 189.878027 86.927823 189.561928 86.132214 189.561928 \nC 85.336605 189.561928 84.573474 189.878027 84.010894 190.440608 \nC 83.448313 191.003189 83.132214 191.766319 83.132214 192.561928 \nC 83.132214 193.357537 83.448313 194.120668 84.010894 194.683249 \nC 84.573474 195.245829 85.336605 195.561928 86.132214 195.561928 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 166.676263 \nC 144.187765 166.676263 144.950895 166.360165 145.513476 165.797584 \nC 146.076056 165.235003 146.392155 164.471873 146.392155 163.676263 \nC 146.392155 162.880654 146.076056 162.117524 145.513476 161.554943 \nC 144.950895 160.992362 144.187765 160.676263 143.392155 160.676263 \nC 142.596546 160.676263 141.833416 160.992362 141.270835 161.554943 \nC 140.708254 162.117524 140.392155 162.880654 140.392155 163.676263 \nC 140.392155 164.471873 140.708254 165.235003 141.270835 165.797584 \nC 141.833416 166.360165 142.596546 166.676263 143.392155 166.676263 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 194.206258 \nC 371.755552 194.206258 372.518683 193.890159 373.081264 193.327578 \nC 373.643844 192.764997 373.959943 192.001867 373.959943 191.206258 \nC 373.959943 190.410648 373.643844 189.647518 373.081264 189.084937 \nC 372.518683 188.522357 371.755552 188.206258 370.959943 188.206258 \nC 370.164334 188.206258 369.401204 188.522357 368.838623 189.084937 \nC 368.276042 189.647518 367.959943 190.410648 367.959943 191.206258 \nC 367.959943 192.001867 368.276042 192.764997 368.838623 193.327578 \nC 369.401204 193.890159 370.164334 194.206258 370.959943 194.206258 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 165.259817 \nC 67.571313 165.259817 68.334444 164.943718 68.897024 164.381138 \nC 69.459605 163.818557 69.775704 163.055426 69.775704 162.259817 \nC 69.775704 161.464208 69.459605 160.701078 68.897024 160.138497 \nC 68.334444 159.575916 67.571313 159.259817 66.775704 159.259817 \nC 65.980095 159.259817 65.216965 159.575916 64.654384 160.138497 \nC 64.091803 160.701078 63.775704 161.464208 63.775704 162.259817 \nC 63.775704 163.055426 64.091803 163.818557 64.654384 164.381138 \nC 65.216965 164.943718 65.980095 165.259817 66.775704 165.259817 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 150.281958 \nC 67.736911 150.281958 68.500041 149.965859 69.062622 149.403279 \nC 69.625203 148.840698 69.941302 148.077568 69.941302 147.281958 \nC 69.941302 146.486349 69.625203 145.723219 69.062622 145.160638 \nC 68.500041 144.598057 67.736911 144.281958 66.941302 144.281958 \nC 66.145692 144.281958 65.382562 144.598057 64.819981 145.160638 \nC 64.257401 145.723219 63.941302 146.486349 63.941302 147.281958 \nC 63.941302 148.077568 64.257401 148.840698 64.819981 149.403279 \nC 65.382562 149.965859 66.145692 150.281958 66.941302 150.281958 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 206.983206 \nC 68.068106 206.983206 68.831236 206.667108 69.393817 206.104527 \nC 69.956398 205.541946 70.272497 204.778816 70.272497 203.983206 \nC 70.272497 203.187597 69.956398 202.424467 69.393817 201.861886 \nC 68.831236 201.299305 68.068106 200.983206 67.272497 200.983206 \nC 66.476887 200.983206 65.713757 201.299305 65.151176 201.861886 \nC 64.588596 202.424467 64.272497 203.187597 64.272497 203.983206 \nC 64.272497 204.778816 64.588596 205.541946 65.151176 206.104527 \nC 65.713757 206.667108 66.476887 206.983206 67.272497 206.983206 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 168.587747 \nC 68.730496 168.587747 69.493626 168.271648 70.056207 167.709068 \nC 70.618788 167.146487 70.934887 166.383357 70.934887 165.587747 \nC 70.934887 164.792138 70.618788 164.029008 70.056207 163.466427 \nC 69.493626 162.903846 68.730496 162.587747 67.934887 162.587747 \nC 67.139277 162.587747 66.376147 162.903846 65.813566 163.466427 \nC 65.250986 164.029008 64.934887 164.792138 64.934887 165.587747 \nC 64.934887 166.383357 65.250986 167.146487 65.813566 167.709068 \nC 66.376147 168.271648 67.139277 168.587747 67.934887 168.587747 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 87.828849 \nC 70.055276 87.828849 70.818406 87.51275 71.380987 86.950169 \nC 71.943568 86.387588 72.259667 85.624458 72.259667 84.828849 \nC 72.259667 84.03324 71.943568 83.270109 71.380987 82.707528 \nC 70.818406 82.144948 70.055276 81.828849 69.259667 81.828849 \nC 68.464058 81.828849 67.700927 82.144948 67.138347 82.707528 \nC 66.575766 83.270109 66.259667 84.03324 66.259667 84.828849 \nC 66.259667 85.624458 66.575766 86.387588 67.138347 86.950169 \nC 67.700927 87.51275 68.464058 87.828849 69.259667 87.828849 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083656 \nC 72.704836 20.083656 73.467967 19.767557 74.030547 19.204976 \nC 74.593128 18.642396 74.909227 17.879265 74.909227 17.083656 \nC 74.909227 16.288047 74.593128 15.524916 74.030547 14.962336 \nC 73.467967 14.399755 72.704836 14.083656 71.909227 14.083656 \nC 71.113618 14.083656 70.350488 14.399755 69.787907 14.962336 \nC 69.225326 15.524916 68.909227 16.288047 68.909227 17.083656 \nC 68.909227 17.879265 69.225326 18.642396 69.787907 19.204976 \nC 70.350488 19.767557 71.113618 20.083656 71.909227 20.083656 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 99.126023 \nL 66.596307 97.626023 \nL 68.096307 99.126023 \nL 69.596307 97.626023 \nL 68.096307 96.126023 \nL 69.596307 94.626023 \nL 68.096307 93.126023 \nL 66.596307 94.626023 \nL 65.096307 93.126023 \nL 63.596307 94.626023 \nL 65.096307 96.126023 \nL 63.596307 97.626023 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 157.780182 \nL 66.674506 156.280182 \nL 68.174506 157.780182 \nL 69.674506 156.280182 \nL 68.174506 154.780182 \nL 69.674506 153.280182 \nL 68.174506 151.780182 \nL 66.674506 153.280182 \nL 65.174506 151.780182 \nL 63.674506 153.280182 \nL 65.174506 154.780182 \nL 63.674506 156.280182 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 133.775544 \nL 66.941302 132.275544 \nL 68.441302 133.775544 \nL 69.941302 132.275544 \nL 68.441302 130.775544 \nL 69.941302 129.275544 \nL 68.441302 127.775544 \nL 66.941302 129.275544 \nL 65.441302 127.775544 \nL 63.941302 129.275544 \nL 65.441302 130.775544 \nL 63.941302 132.275544 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 168.6971 \nL 67.916487 167.1971 \nL 69.416487 168.6971 \nL 70.916487 167.1971 \nL 69.416487 165.6971 \nL 70.916487 164.1971 \nL 69.416487 162.6971 \nL 67.916487 164.1971 \nL 66.416487 162.6971 \nL 64.916487 164.1971 \nL 66.416487 165.6971 \nL 64.916487 167.1971 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 156.38679 \nL 71.633231 154.88679 \nL 73.133231 156.38679 \nL 74.633231 154.88679 \nL 73.133231 153.38679 \nL 74.633231 151.88679 \nL 73.133231 150.38679 \nL 71.633231 151.88679 \nL 70.133231 150.38679 \nL 68.633231 151.88679 \nL 70.133231 153.38679 \nL 68.633231 154.88679 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 151.60517 \nL 86.132214 150.10517 \nL 87.632214 151.60517 \nL 89.132214 150.10517 \nL 87.632214 148.60517 \nL 89.132214 147.10517 \nL 87.632214 145.60517 \nL 86.132214 147.10517 \nL 84.632214 145.60517 \nL 83.132214 147.10517 \nL 84.632214 148.60517 \nL 83.132214 150.10517 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 159.357102 \nL 143.392155 157.857102 \nL 144.892155 159.357102 \nL 146.392155 157.857102 \nL 144.892155 156.357102 \nL 146.392155 154.857102 \nL 144.892155 153.357102 \nL 143.392155 154.857102 \nL 141.892155 153.357102 \nL 140.392155 154.857102 \nL 141.892155 156.357102 \nL 140.392155 157.857102 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 160.041458 \nL 370.959943 158.541458 \nL 372.459943 160.041458 \nL 373.959943 158.541458 \nL 372.459943 157.041458 \nL 373.959943 155.541458 \nL 372.459943 154.041458 \nL 370.959943 155.541458 \nL 369.459943 154.041458 \nL 367.959943 155.541458 \nL 369.459943 157.041458 \nL 367.959943 158.541458 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 162.115479 \nL 66.775704 160.615479 \nL 68.275704 162.115479 \nL 69.775704 160.615479 \nL 68.275704 159.115479 \nL 69.775704 157.615479 \nL 68.275704 156.115479 \nL 66.775704 157.615479 \nL 65.275704 156.115479 \nL 63.775704 157.615479 \nL 65.275704 159.115479 \nL 63.775704 160.615479 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 178.045898 \nL 66.941302 176.545898 \nL 68.441302 178.045898 \nL 69.941302 176.545898 \nL 68.441302 175.045898 \nL 69.941302 173.545898 \nL 68.441302 172.045898 \nL 66.941302 173.545898 \nL 65.441302 172.045898 \nL 63.941302 173.545898 \nL 65.441302 175.045898 \nL 63.941302 176.545898 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 171.154329 \nL 67.272497 169.654329 \nL 68.772497 171.154329 \nL 70.272497 169.654329 \nL 68.772497 168.154329 \nL 70.272497 166.654329 \nL 68.772497 165.154329 \nL 67.272497 166.654329 \nL 65.772497 165.154329 \nL 64.272497 166.654329 \nL 65.772497 168.154329 \nL 64.272497 169.654329 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 166.72119 \nL 67.934887 165.22119 \nL 69.434887 166.72119 \nL 70.934887 165.22119 \nL 69.434887 163.72119 \nL 70.934887 162.22119 \nL 69.434887 160.72119 \nL 67.934887 162.22119 \nL 66.434887 160.72119 \nL 64.934887 162.22119 \nL 66.434887 163.72119 \nL 64.934887 165.22119 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 179.090273 \nL 69.259667 177.590273 \nL 70.759667 179.090273 \nL 72.259667 177.590273 \nL 70.759667 176.090273 \nL 72.259667 174.590273 \nL 70.759667 173.090273 \nL 69.259667 174.590273 \nL 67.759667 173.090273 \nL 66.259667 174.590273 \nL 67.759667 176.090273 \nL 66.259667 177.590273 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 174.07596 \nL 71.909227 172.57596 \nL 73.409227 174.07596 \nL 74.909227 172.57596 \nL 73.409227 171.07596 \nL 74.909227 169.57596 \nL 73.409227 168.07596 \nL 71.909227 169.57596 \nL 70.409227 168.07596 \nL 68.909227 169.57596 \nL 70.409227 171.07596 \nL 68.909227 172.57596 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 84.000235 \nC 67.391916 84.000235 68.155046 83.684137 68.717627 83.121556 \nC 69.280208 82.558975 69.596307 81.795845 69.596307 81.000235 \nC 69.596307 80.204626 69.280208 79.441496 68.717627 78.878915 \nC 68.155046 78.316334 67.391916 78.000235 66.596307 78.000235 \nC 65.800698 78.000235 65.037567 78.316334 64.474986 78.878915 \nC 63.912406 79.441496 63.596307 80.204626 63.596307 81.000235 \nC 63.596307 81.795845 63.912406 82.558975 64.474986 83.121556 \nC 65.037567 83.684137 65.800698 84.000235 66.596307 84.000235 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 200.023223 \nC 67.470115 200.023223 68.233245 199.707124 68.795826 199.144544 \nC 69.358407 198.581963 69.674506 197.818832 69.674506 197.023223 \nC 69.674506 196.227614 69.358407 195.464484 68.795826 194.901903 \nC 68.233245 194.339322 67.470115 194.023223 66.674506 194.023223 \nC 65.878896 194.023223 65.115766 194.339322 64.553185 194.901903 \nC 63.990605 195.464484 63.674506 196.227614 63.674506 197.023223 \nC 63.674506 197.818832 63.990605 198.581963 64.553185 199.144544 \nC 65.115766 199.707124 65.878896 200.023223 66.674506 200.023223 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 164.393902 \nC 67.736911 164.393902 68.500041 164.077803 69.062622 163.515222 \nC 69.625203 162.952642 69.941302 162.189511 69.941302 161.393902 \nC 69.941302 160.598293 69.625203 159.835162 69.062622 159.272582 \nC 68.500041 158.710001 67.736911 158.393902 66.941302 158.393902 \nC 66.145692 158.393902 65.382562 158.710001 64.819981 159.272582 \nC 64.257401 159.835162 63.941302 160.598293 63.941302 161.393902 \nC 63.941302 162.189511 64.257401 162.952642 64.819981 163.515222 \nC 65.382562 164.077803 66.145692 164.393902 66.941302 164.393902 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 207.057622 \nC 68.712096 207.057622 69.475227 206.741523 70.037807 206.178942 \nC 70.600388 205.616361 70.916487 204.853231 70.916487 204.057622 \nC 70.916487 203.262013 70.600388 202.498882 70.037807 201.936302 \nC 69.475227 201.373721 68.712096 201.057622 67.916487 201.057622 \nC 67.120878 201.057622 66.357747 201.373721 65.795167 201.936302 \nC 65.232586 202.498882 64.916487 203.262013 64.916487 204.057622 \nC 64.916487 204.853231 65.232586 205.616361 65.795167 206.178942 \nC 66.357747 206.741523 67.120878 207.057622 67.916487 207.057622 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 162.741593 \nC 72.428841 162.741593 73.191971 162.425494 73.754552 161.862914 \nC 74.317132 161.300333 74.633231 160.537203 74.633231 159.741593 \nC 74.633231 158.945984 74.317132 158.182854 73.754552 157.620273 \nC 73.191971 157.057692 72.428841 156.741593 71.633231 156.741593 \nC 70.837622 156.741593 70.074492 157.057692 69.511911 157.620273 \nC 68.94933 158.182854 68.633231 158.945984 68.633231 159.741593 \nC 68.633231 160.537203 68.94933 161.300333 69.511911 161.862914 \nC 70.074492 162.425494 70.837622 162.741593 71.633231 162.741593 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 169.918319 \nC 86.927823 169.918319 87.690954 169.60222 88.253534 169.039639 \nC 88.816115 168.477058 89.132214 167.713928 89.132214 166.918319 \nC 89.132214 166.122709 88.816115 165.359579 88.253534 164.796998 \nC 87.690954 164.234418 86.927823 163.918319 86.132214 163.918319 \nC 85.336605 163.918319 84.573474 164.234418 84.010894 164.796998 \nC 83.448313 165.359579 83.132214 166.122709 83.132214 166.918319 \nC 83.132214 167.713928 83.448313 168.477058 84.010894 169.039639 \nC 84.573474 169.60222 85.336605 169.918319 86.132214 169.918319 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 169.668315 \nC 144.187765 169.668315 144.950895 169.352216 145.513476 168.789636 \nC 146.076056 168.227055 146.392155 167.463925 146.392155 166.668315 \nC 146.392155 165.872706 146.076056 165.109576 145.513476 164.546995 \nC 144.950895 163.984414 144.187765 163.668315 143.392155 163.668315 \nC 142.596546 163.668315 141.833416 163.984414 141.270835 164.546995 \nC 140.708254 165.109576 140.392155 165.872706 140.392155 166.668315 \nC 140.392155 167.463925 140.708254 168.227055 141.270835 168.789636 \nC 141.833416 169.352216 142.596546 169.668315 143.392155 169.668315 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 155.195766 \nC 371.755552 155.195766 372.518683 154.879667 373.081264 154.317087 \nC 373.643844 153.754506 373.959943 152.991376 373.959943 152.195766 \nC 373.959943 151.400157 373.643844 150.637027 373.081264 150.074446 \nC 372.518683 149.511865 371.755552 149.195766 370.959943 149.195766 \nC 370.164334 149.195766 369.401204 149.511865 368.838623 150.074446 \nC 368.276042 150.637027 367.959943 151.400157 367.959943 152.195766 \nC 367.959943 152.991376 368.276042 153.754506 368.838623 154.317087 \nC 369.401204 154.879667 370.164334 155.195766 370.959943 155.195766 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 202.448158 \nC 67.571313 202.448158 68.334444 202.132059 68.897024 201.569479 \nC 69.459605 201.006898 69.775704 200.243768 69.775704 199.448158 \nC 69.775704 198.652549 69.459605 197.889419 68.897024 197.326838 \nC 68.334444 196.764257 67.571313 196.448158 66.775704 196.448158 \nC 65.980095 196.448158 65.216965 196.764257 64.654384 197.326838 \nC 64.091803 197.889419 63.775704 198.652549 63.775704 199.448158 \nC 63.775704 200.243768 64.091803 201.006898 64.654384 201.569479 \nC 65.216965 202.132059 65.980095 202.448158 66.775704 202.448158 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 181.711042 \nC 67.736911 181.711042 68.500041 181.394943 69.062622 180.832362 \nC 69.625203 180.269781 69.941302 179.506651 69.941302 178.711042 \nC 69.941302 177.915433 69.625203 177.152302 69.062622 176.589722 \nC 68.500041 176.027141 67.736911 175.711042 66.941302 175.711042 \nC 66.145692 175.711042 65.382562 176.027141 64.819981 176.589722 \nC 64.257401 177.152302 63.941302 177.915433 63.941302 178.711042 \nC 63.941302 179.506651 64.257401 180.269781 64.819981 180.832362 \nC 65.382562 181.394943 66.145692 181.711042 66.941302 181.711042 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 196.466838 \nC 68.068106 196.466838 68.831236 196.150739 69.393817 195.588158 \nC 69.956398 195.025577 70.272497 194.262447 70.272497 193.466838 \nC 70.272497 192.671228 69.956398 191.908098 69.393817 191.345517 \nC 68.831236 190.782937 68.068106 190.466838 67.272497 190.466838 \nC 66.476887 190.466838 65.713757 190.782937 65.151176 191.345517 \nC 64.588596 191.908098 64.272497 192.671228 64.272497 193.466838 \nC 64.272497 194.262447 64.588596 195.025577 65.151176 195.588158 \nC 65.713757 196.150739 66.476887 196.466838 67.272497 196.466838 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 148.71644 \nC 68.730496 148.71644 69.493626 148.400341 70.056207 147.83776 \nC 70.618788 147.27518 70.934887 146.512049 70.934887 145.71644 \nC 70.934887 144.920831 70.618788 144.157701 70.056207 143.59512 \nC 69.493626 143.032539 68.730496 142.71644 67.934887 142.71644 \nC 67.139277 142.71644 66.376147 143.032539 65.813566 143.59512 \nC 65.250986 144.157701 64.934887 144.920831 64.934887 145.71644 \nC 64.934887 146.512049 65.250986 147.27518 65.813566 147.83776 \nC 66.376147 148.400341 67.139277 148.71644 67.934887 148.71644 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 79.143711 \nC 70.055276 79.143711 70.818406 78.827612 71.380987 78.265031 \nC 71.943568 77.70245 72.259667 76.93932 72.259667 76.143711 \nC 72.259667 75.348102 71.943568 74.584971 71.380987 74.02239 \nC 70.818406 73.45981 70.055276 73.143711 69.259667 73.143711 \nC 68.464058 73.143711 67.700927 73.45981 67.138347 74.02239 \nC 66.575766 74.584971 66.259667 75.348102 66.259667 76.143711 \nC 66.259667 76.93932 66.575766 77.70245 67.138347 78.265031 \nC 67.700927 78.827612 68.464058 79.143711 69.259667 79.143711 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083705 \nC 72.704836 20.083705 73.467967 19.767606 74.030547 19.205025 \nC 74.593128 18.642444 74.909227 17.879314 74.909227 17.083705 \nC 74.909227 16.288095 74.593128 15.524965 74.030547 14.962384 \nC 73.467967 14.399803 72.704836 14.083705 71.909227 14.083705 \nC 71.113618 14.083705 70.350488 14.399803 69.787907 14.962384 \nC 69.225326 15.524965 68.909227 16.288095 68.909227 17.083705 \nC 68.909227 17.879314 69.225326 18.642444 69.787907 19.205025 \nC 70.350488 19.767606 71.113618 20.083705 71.909227 20.083705 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 165.484322 \nL 66.596307 163.984322 \nL 68.096307 165.484322 \nL 69.596307 163.984322 \nL 68.096307 162.484322 \nL 69.596307 160.984322 \nL 68.096307 159.484322 \nL 66.596307 160.984322 \nL 65.096307 159.484322 \nL 63.596307 160.984322 \nL 65.096307 162.484322 \nL 63.596307 163.984322 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 159.829479 \nL 66.674506 158.329479 \nL 68.174506 159.829479 \nL 69.674506 158.329479 \nL 68.174506 156.829479 \nL 69.674506 155.329479 \nL 68.174506 153.829479 \nL 66.674506 155.329479 \nL 65.174506 153.829479 \nL 63.674506 155.329479 \nL 65.174506 156.829479 \nL 63.674506 158.329479 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 164.737968 \nL 66.941302 163.237968 \nL 68.441302 164.737968 \nL 69.941302 163.237968 \nL 68.441302 161.737968 \nL 69.941302 160.237968 \nL 68.441302 158.737968 \nL 66.941302 160.237968 \nL 65.441302 158.737968 \nL 63.941302 160.237968 \nL 65.441302 161.737968 \nL 63.941302 163.237968 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 204.274984 \nL 67.916487 202.774984 \nL 69.416487 204.274984 \nL 70.916487 202.774984 \nL 69.416487 201.274984 \nL 70.916487 199.774984 \nL 69.416487 198.274984 \nL 67.916487 199.774984 \nL 66.416487 198.274984 \nL 64.916487 199.774984 \nL 66.416487 201.274984 \nL 64.916487 202.774984 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 151.166451 \nL 71.633231 149.666451 \nL 73.133231 151.166451 \nL 74.633231 149.666451 \nL 73.133231 148.166451 \nL 74.633231 146.666451 \nL 73.133231 145.166451 \nL 71.633231 146.666451 \nL 70.133231 145.166451 \nL 68.633231 146.666451 \nL 70.133231 148.166451 \nL 68.633231 149.666451 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 177.302052 \nL 86.132214 175.802052 \nL 87.632214 177.302052 \nL 89.132214 175.802052 \nL 87.632214 174.302052 \nL 89.132214 172.802052 \nL 87.632214 171.302052 \nL 86.132214 172.802052 \nL 84.632214 171.302052 \nL 83.132214 172.802052 \nL 84.632214 174.302052 \nL 83.132214 175.802052 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 162.407404 \nL 143.392155 160.907404 \nL 144.892155 162.407404 \nL 146.392155 160.907404 \nL 144.892155 159.407404 \nL 146.392155 157.907404 \nL 144.892155 156.407404 \nL 143.392155 157.907404 \nL 141.892155 156.407404 \nL 140.392155 157.907404 \nL 141.892155 159.407404 \nL 140.392155 160.907404 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 123.663197 \nL 370.959943 122.163197 \nL 372.459943 123.663197 \nL 373.959943 122.163197 \nL 372.459943 120.663197 \nL 373.959943 119.163197 \nL 372.459943 117.663197 \nL 370.959943 119.163197 \nL 369.459943 117.663197 \nL 367.959943 119.163197 \nL 369.459943 120.663197 \nL 367.959943 122.163197 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 211.80708 \nL 66.775704 210.30708 \nL 68.275704 211.80708 \nL 69.775704 210.30708 \nL 68.275704 208.80708 \nL 69.775704 207.30708 \nL 68.275704 205.80708 \nL 66.775704 207.30708 \nL 65.275704 205.80708 \nL 63.775704 207.30708 \nL 65.275704 208.80708 \nL 63.775704 210.30708 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 114.194496 \nL 66.941302 112.694496 \nL 68.441302 114.194496 \nL 69.941302 112.694496 \nL 68.441302 111.194496 \nL 69.941302 109.694496 \nL 68.441302 108.194496 \nL 66.941302 109.694496 \nL 65.441302 108.194496 \nL 63.941302 109.694496 \nL 65.441302 111.194496 \nL 63.941302 112.694496 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 159.708014 \nL 67.272497 158.208014 \nL 68.772497 159.708014 \nL 70.272497 158.208014 \nL 68.772497 156.708014 \nL 70.272497 155.208014 \nL 68.772497 153.708014 \nL 67.272497 155.208014 \nL 65.772497 153.708014 \nL 64.272497 155.208014 \nL 65.772497 156.708014 \nL 64.272497 158.208014 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 169.612823 \nL 67.934887 168.112823 \nL 69.434887 169.612823 \nL 70.934887 168.112823 \nL 69.434887 166.612823 \nL 70.934887 165.112823 \nL 69.434887 163.612823 \nL 67.934887 165.112823 \nL 66.434887 163.612823 \nL 64.934887 165.112823 \nL 66.434887 166.612823 \nL 64.934887 168.112823 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 202.84003 \nL 69.259667 201.34003 \nL 70.759667 202.84003 \nL 72.259667 201.34003 \nL 70.759667 199.84003 \nL 72.259667 198.34003 \nL 70.759667 196.84003 \nL 69.259667 198.34003 \nL 67.759667 196.84003 \nL 66.259667 198.34003 \nL 67.759667 199.84003 \nL 66.259667 201.34003 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 23.79168 \nL 71.909227 22.29168 \nL 73.409227 23.79168 \nL 74.909227 22.29168 \nL 73.409227 20.79168 \nL 74.909227 19.29168 \nL 73.409227 17.79168 \nL 71.909227 19.29168 \nL 70.409227 17.79168 \nL 68.909227 19.29168 \nL 70.409227 20.79168 \nL 68.909227 22.29168 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 89.734258 \nC 67.391916 89.734258 68.155046 89.418159 68.717627 88.855578 \nC 69.280208 88.292997 69.596307 87.529867 69.596307 86.734258 \nC 69.596307 85.938649 69.280208 85.175518 68.717627 84.612937 \nC 68.155046 84.050357 67.391916 83.734258 66.596307 83.734258 \nC 65.800698 83.734258 65.037567 84.050357 64.474986 84.612937 \nC 63.912406 85.175518 63.596307 85.938649 63.596307 86.734258 \nC 63.596307 87.529867 63.912406 88.292997 64.474986 88.855578 \nC 65.037567 89.418159 65.800698 89.734258 66.596307 89.734258 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 211.337088 \nC 67.470115 211.337088 68.233245 211.020989 68.795826 210.458409 \nC 69.358407 209.895828 69.674506 209.132697 69.674506 208.337088 \nC 69.674506 207.541479 69.358407 206.778349 68.795826 206.215768 \nC 68.233245 205.653187 67.470115 205.337088 66.674506 205.337088 \nC 65.878896 205.337088 65.115766 205.653187 64.553185 206.215768 \nC 63.990605 206.778349 63.674506 207.541479 63.674506 208.337088 \nC 63.674506 209.132697 63.990605 209.895828 64.553185 210.458409 \nC 65.115766 211.020989 65.878896 211.337088 66.674506 211.337088 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 200.062524 \nC 67.736911 200.062524 68.500041 199.746425 69.062622 199.183844 \nC 69.625203 198.621264 69.941302 197.858133 69.941302 197.062524 \nC 69.941302 196.266915 69.625203 195.503784 69.062622 194.941204 \nC 68.500041 194.378623 67.736911 194.062524 66.941302 194.062524 \nC 66.145692 194.062524 65.382562 194.378623 64.819981 194.941204 \nC 64.257401 195.503784 63.941302 196.266915 63.941302 197.062524 \nC 63.941302 197.858133 64.257401 198.621264 64.819981 199.183844 \nC 65.382562 199.746425 66.145692 200.062524 66.941302 200.062524 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 172.760909 \nC 68.712096 172.760909 69.475227 172.44481 70.037807 171.88223 \nC 70.600388 171.319649 70.916487 170.556519 70.916487 169.760909 \nC 70.916487 168.9653 70.600388 168.20217 70.037807 167.639589 \nC 69.475227 167.077008 68.712096 166.760909 67.916487 166.760909 \nC 67.120878 166.760909 66.357747 167.077008 65.795167 167.639589 \nC 65.232586 168.20217 64.916487 168.9653 64.916487 169.760909 \nC 64.916487 170.556519 65.232586 171.319649 65.795167 171.88223 \nC 66.357747 172.44481 67.120878 172.760909 67.916487 172.760909 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 172.632441 \nC 72.428841 172.632441 73.191971 172.316342 73.754552 171.753761 \nC 74.317132 171.19118 74.633231 170.42805 74.633231 169.632441 \nC 74.633231 168.836831 74.317132 168.073701 73.754552 167.51112 \nC 73.191971 166.948539 72.428841 166.632441 71.633231 166.632441 \nC 70.837622 166.632441 70.074492 166.948539 69.511911 167.51112 \nC 68.94933 168.073701 68.633231 168.836831 68.633231 169.632441 \nC 68.633231 170.42805 68.94933 171.19118 69.511911 171.753761 \nC 70.074492 172.316342 70.837622 172.632441 71.633231 172.632441 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 164.922077 \nC 86.927823 164.922077 87.690954 164.605978 88.253534 164.043397 \nC 88.816115 163.480817 89.132214 162.717686 89.132214 161.922077 \nC 89.132214 161.126468 88.816115 160.363337 88.253534 159.800757 \nC 87.690954 159.238176 86.927823 158.922077 86.132214 158.922077 \nC 85.336605 158.922077 84.573474 159.238176 84.010894 159.800757 \nC 83.448313 160.363337 83.132214 161.126468 83.132214 161.922077 \nC 83.132214 162.717686 83.448313 163.480817 84.010894 164.043397 \nC 84.573474 164.605978 85.336605 164.922077 86.132214 164.922077 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 157.704167 \nC 144.187765 157.704167 144.950895 157.388068 145.513476 156.825487 \nC 146.076056 156.262906 146.392155 155.499776 146.392155 154.704167 \nC 146.392155 153.908557 146.076056 153.145427 145.513476 152.582846 \nC 144.950895 152.020265 144.187765 151.704167 143.392155 151.704167 \nC 142.596546 151.704167 141.833416 152.020265 141.270835 152.582846 \nC 140.708254 153.145427 140.392155 153.908557 140.392155 154.704167 \nC 140.392155 155.499776 140.708254 156.262906 141.270835 156.825487 \nC 141.833416 157.388068 142.596546 157.704167 143.392155 157.704167 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 148.587344 \nC 371.755552 148.587344 372.518683 148.271245 373.081264 147.708664 \nC 373.643844 147.146083 373.959943 146.382953 373.959943 145.587344 \nC 373.959943 144.791734 373.643844 144.028604 373.081264 143.466023 \nC 372.518683 142.903442 371.755552 142.587344 370.959943 142.587344 \nC 370.164334 142.587344 369.401204 142.903442 368.838623 143.466023 \nC 368.276042 144.028604 367.959943 144.791734 367.959943 145.587344 \nC 367.959943 146.382953 368.276042 147.146083 368.838623 147.708664 \nC 369.401204 148.271245 370.164334 148.587344 370.959943 148.587344 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 152.313388 \nC 67.571313 152.313388 68.334444 151.997289 68.897024 151.434708 \nC 69.459605 150.872127 69.775704 150.108997 69.775704 149.313388 \nC 69.775704 148.517778 69.459605 147.754648 68.897024 147.192067 \nC 68.334444 146.629487 67.571313 146.313388 66.775704 146.313388 \nC 65.980095 146.313388 65.216965 146.629487 64.654384 147.192067 \nC 64.091803 147.754648 63.775704 148.517778 63.775704 149.313388 \nC 63.775704 150.108997 64.091803 150.872127 64.654384 151.434708 \nC 65.216965 151.997289 65.980095 152.313388 66.775704 152.313388 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 174.750049 \nC 67.736911 174.750049 68.500041 174.43395 69.062622 173.87137 \nC 69.625203 173.308789 69.941302 172.545658 69.941302 171.750049 \nC 69.941302 170.95444 69.625203 170.19131 69.062622 169.628729 \nC 68.500041 169.066148 67.736911 168.750049 66.941302 168.750049 \nC 66.145692 168.750049 65.382562 169.066148 64.819981 169.628729 \nC 64.257401 170.19131 63.941302 170.95444 63.941302 171.750049 \nC 63.941302 172.545658 64.257401 173.308789 64.819981 173.87137 \nC 65.382562 174.43395 66.145692 174.750049 66.941302 174.750049 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 200.108313 \nC 68.068106 200.108313 68.831236 199.792214 69.393817 199.229633 \nC 69.956398 198.667052 70.272497 197.903922 70.272497 197.108313 \nC 70.272497 196.312704 69.956398 195.549573 69.393817 194.986993 \nC 68.831236 194.424412 68.068106 194.108313 67.272497 194.108313 \nC 66.476887 194.108313 65.713757 194.424412 65.151176 194.986993 \nC 64.588596 195.549573 64.272497 196.312704 64.272497 197.108313 \nC 64.272497 197.903922 64.588596 198.667052 65.151176 199.229633 \nC 65.713757 199.792214 66.476887 200.108313 67.272497 200.108313 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 167.194199 \nC 68.730496 167.194199 69.493626 166.8781 70.056207 166.315519 \nC 70.618788 165.752938 70.934887 164.989808 70.934887 164.194199 \nC 70.934887 163.398589 70.618788 162.635459 70.056207 162.072878 \nC 69.493626 161.510298 68.730496 161.194199 67.934887 161.194199 \nC 67.139277 161.194199 66.376147 161.510298 65.813566 162.072878 \nC 65.250986 162.635459 64.934887 163.398589 64.934887 164.194199 \nC 64.934887 164.989808 65.250986 165.752938 65.813566 166.315519 \nC 66.376147 166.8781 67.139277 167.194199 67.934887 167.194199 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 20.086218 \nC 70.055276 20.086218 70.818406 19.770119 71.380987 19.207538 \nC 71.943568 18.644957 72.259667 17.881827 72.259667 17.086218 \nC 72.259667 16.290609 71.943568 15.527478 71.380987 14.964897 \nC 70.818406 14.402317 70.055276 14.086218 69.259667 14.086218 \nC 68.464058 14.086218 67.700927 14.402317 67.138347 14.964897 \nC 66.575766 15.527478 66.259667 16.290609 66.259667 17.086218 \nC 66.259667 17.881827 66.575766 18.644957 67.138347 19.207538 \nC 67.700927 19.770119 68.464058 20.086218 69.259667 20.086218 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083708 \nC 72.704836 20.083708 73.467967 19.767609 74.030547 19.205028 \nC 74.593128 18.642447 74.909227 17.879317 74.909227 17.083708 \nC 74.909227 16.288098 74.593128 15.524968 74.030547 14.962387 \nC 73.467967 14.399807 72.704836 14.083708 71.909227 14.083708 \nC 71.113618 14.083708 70.350488 14.399807 69.787907 14.962387 \nC 69.225326 15.524968 68.909227 16.288098 68.909227 17.083708 \nC 68.909227 17.879317 69.225326 18.642447 69.787907 19.205028 \nC 70.350488 19.767609 71.113618 20.083708 71.909227 20.083708 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 158.148146 \nL 66.596307 156.648146 \nL 68.096307 158.148146 \nL 69.596307 156.648146 \nL 68.096307 155.148146 \nL 69.596307 153.648146 \nL 68.096307 152.148146 \nL 66.596307 153.648146 \nL 65.096307 152.148146 \nL 63.596307 153.648146 \nL 65.096307 155.148146 \nL 63.596307 156.648146 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 138.011962 \nL 66.674506 136.511962 \nL 68.174506 138.011962 \nL 69.674506 136.511962 \nL 68.174506 135.011962 \nL 69.674506 133.511962 \nL 68.174506 132.011962 \nL 66.674506 133.511962 \nL 65.174506 132.011962 \nL 63.674506 133.511962 \nL 65.174506 135.011962 \nL 63.674506 136.511962 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 148.647939 \nL 66.941302 147.147939 \nL 68.441302 148.647939 \nL 69.941302 147.147939 \nL 68.441302 145.647939 \nL 69.941302 144.147939 \nL 68.441302 142.647939 \nL 66.941302 144.147939 \nL 65.441302 142.647939 \nL 63.941302 144.147939 \nL 65.441302 145.647939 \nL 63.941302 147.147939 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 181.033683 \nL 67.916487 179.533683 \nL 69.416487 181.033683 \nL 70.916487 179.533683 \nL 69.416487 178.033683 \nL 70.916487 176.533683 \nL 69.416487 175.033683 \nL 67.916487 176.533683 \nL 66.416487 175.033683 \nL 64.916487 176.533683 \nL 66.416487 178.033683 \nL 64.916487 179.533683 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 164.166745 \nL 71.633231 162.666745 \nL 73.133231 164.166745 \nL 74.633231 162.666745 \nL 73.133231 161.166745 \nL 74.633231 159.666745 \nL 73.133231 158.166745 \nL 71.633231 159.666745 \nL 70.133231 158.166745 \nL 68.633231 159.666745 \nL 70.133231 161.166745 \nL 68.633231 162.666745 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 140.324746 \nL 86.132214 138.824746 \nL 87.632214 140.324746 \nL 89.132214 138.824746 \nL 87.632214 137.324746 \nL 89.132214 135.824746 \nL 87.632214 134.324746 \nL 86.132214 135.824746 \nL 84.632214 134.324746 \nL 83.132214 135.824746 \nL 84.632214 137.324746 \nL 83.132214 138.824746 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 180.402058 \nL 143.392155 178.902058 \nL 144.892155 180.402058 \nL 146.392155 178.902058 \nL 144.892155 177.402058 \nL 146.392155 175.902058 \nL 144.892155 174.402058 \nL 143.392155 175.902058 \nL 141.892155 174.402058 \nL 140.392155 175.902058 \nL 141.892155 177.402058 \nL 140.392155 178.902058 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 154.615582 \nL 370.959943 153.115582 \nL 372.459943 154.615582 \nL 373.959943 153.115582 \nL 372.459943 151.615582 \nL 373.959943 150.115582 \nL 372.459943 148.615582 \nL 370.959943 150.115582 \nL 369.459943 148.615582 \nL 367.959943 150.115582 \nL 369.459943 151.615582 \nL 367.959943 153.115582 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 156.548449 \nL 66.775704 155.048449 \nL 68.275704 156.548449 \nL 69.775704 155.048449 \nL 68.275704 153.548449 \nL 69.775704 152.048449 \nL 68.275704 150.548449 \nL 66.775704 152.048449 \nL 65.275704 150.548449 \nL 63.775704 152.048449 \nL 65.275704 153.548449 \nL 63.775704 155.048449 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 142.702229 \nL 66.941302 141.202229 \nL 68.441302 142.702229 \nL 69.941302 141.202229 \nL 68.441302 139.702229 \nL 69.941302 138.202229 \nL 68.441302 136.702229 \nL 66.941302 138.202229 \nL 65.441302 136.702229 \nL 63.941302 138.202229 \nL 65.441302 139.702229 \nL 63.941302 141.202229 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 184.433416 \nL 67.272497 182.933416 \nL 68.772497 184.433416 \nL 70.272497 182.933416 \nL 68.772497 181.433416 \nL 70.272497 179.933416 \nL 68.772497 178.433416 \nL 67.272497 179.933416 \nL 65.772497 178.433416 \nL 64.272497 179.933416 \nL 65.772497 181.433416 \nL 64.272497 182.933416 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 170.720433 \nL 67.934887 169.220433 \nL 69.434887 170.720433 \nL 70.934887 169.220433 \nL 69.434887 167.720433 \nL 70.934887 166.220433 \nL 69.434887 164.720433 \nL 67.934887 166.220433 \nL 66.434887 164.720433 \nL 64.934887 166.220433 \nL 66.434887 167.720433 \nL 64.934887 169.220433 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 150.212746 \nL 69.259667 148.712746 \nL 70.759667 150.212746 \nL 72.259667 148.712746 \nL 70.759667 147.212746 \nL 72.259667 145.712746 \nL 70.759667 144.212746 \nL 69.259667 145.712746 \nL 67.759667 144.212746 \nL 66.259667 145.712746 \nL 67.759667 147.212746 \nL 66.259667 148.712746 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 166.89414 \nL 71.909227 165.39414 \nL 73.409227 166.89414 \nL 74.909227 165.39414 \nL 73.409227 163.89414 \nL 74.909227 162.39414 \nL 73.409227 160.89414 \nL 71.909227 162.39414 \nL 70.409227 160.89414 \nL 68.909227 162.39414 \nL 70.409227 163.89414 \nL 68.909227 165.39414 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 97.046433 \nC 67.391916 97.046433 68.155046 96.730334 68.717627 96.167753 \nC 69.280208 95.605172 69.596307 94.842042 69.596307 94.046433 \nC 69.596307 93.250824 69.280208 92.487693 68.717627 91.925112 \nC 68.155046 91.362532 67.391916 91.046433 66.596307 91.046433 \nC 65.800698 91.046433 65.037567 91.362532 64.474986 91.925112 \nC 63.912406 92.487693 63.596307 93.250824 63.596307 94.046433 \nC 63.596307 94.842042 63.912406 95.605172 64.474986 96.167753 \nC 65.037567 96.730334 65.800698 97.046433 66.596307 97.046433 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 160.107276 \nC 67.470115 160.107276 68.233245 159.791177 68.795826 159.228596 \nC 69.358407 158.666015 69.674506 157.902885 69.674506 157.107276 \nC 69.674506 156.311667 69.358407 155.548536 68.795826 154.985955 \nC 68.233245 154.423375 67.470115 154.107276 66.674506 154.107276 \nC 65.878896 154.107276 65.115766 154.423375 64.553185 154.985955 \nC 63.990605 155.548536 63.674506 156.311667 63.674506 157.107276 \nC 63.674506 157.902885 63.990605 158.666015 64.553185 159.228596 \nC 65.115766 159.791177 65.878896 160.107276 66.674506 160.107276 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 170.405057 \nC 67.736911 170.405057 68.500041 170.088958 69.062622 169.526377 \nC 69.625203 168.963797 69.941302 168.200666 69.941302 167.405057 \nC 69.941302 166.609448 69.625203 165.846317 69.062622 165.283737 \nC 68.500041 164.721156 67.736911 164.405057 66.941302 164.405057 \nC 66.145692 164.405057 65.382562 164.721156 64.819981 165.283737 \nC 64.257401 165.846317 63.941302 166.609448 63.941302 167.405057 \nC 63.941302 168.200666 64.257401 168.963797 64.819981 169.526377 \nC 65.382562 170.088958 66.145692 170.405057 66.941302 170.405057 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 156.031318 \nC 68.712096 156.031318 69.475227 155.715219 70.037807 155.152639 \nC 70.600388 154.590058 70.916487 153.826928 70.916487 153.031318 \nC 70.916487 152.235709 70.600388 151.472579 70.037807 150.909998 \nC 69.475227 150.347417 68.712096 150.031318 67.916487 150.031318 \nC 67.120878 150.031318 66.357747 150.347417 65.795167 150.909998 \nC 65.232586 151.472579 64.916487 152.235709 64.916487 153.031318 \nC 64.916487 153.826928 65.232586 154.590058 65.795167 155.152639 \nC 66.357747 155.715219 67.120878 156.031318 67.916487 156.031318 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 161.543243 \nC 72.428841 161.543243 73.191971 161.227144 73.754552 160.664564 \nC 74.317132 160.101983 74.633231 159.338853 74.633231 158.543243 \nC 74.633231 157.747634 74.317132 156.984504 73.754552 156.421923 \nC 73.191971 155.859342 72.428841 155.543243 71.633231 155.543243 \nC 70.837622 155.543243 70.074492 155.859342 69.511911 156.421923 \nC 68.94933 156.984504 68.633231 157.747634 68.633231 158.543243 \nC 68.633231 159.338853 68.94933 160.101983 69.511911 160.664564 \nC 70.074492 161.227144 70.837622 161.543243 71.633231 161.543243 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 177.344902 \nC 86.927823 177.344902 87.690954 177.028803 88.253534 176.466222 \nC 88.816115 175.903641 89.132214 175.140511 89.132214 174.344902 \nC 89.132214 173.549292 88.816115 172.786162 88.253534 172.223581 \nC 87.690954 171.661001 86.927823 171.344902 86.132214 171.344902 \nC 85.336605 171.344902 84.573474 171.661001 84.010894 172.223581 \nC 83.448313 172.786162 83.132214 173.549292 83.132214 174.344902 \nC 83.132214 175.140511 83.448313 175.903641 84.010894 176.466222 \nC 84.573474 177.028803 85.336605 177.344902 86.132214 177.344902 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 182.725642 \nC 144.187765 182.725642 144.950895 182.409543 145.513476 181.846962 \nC 146.076056 181.284381 146.392155 180.521251 146.392155 179.725642 \nC 146.392155 178.930032 146.076056 178.166902 145.513476 177.604321 \nC 144.950895 177.04174 144.187765 176.725642 143.392155 176.725642 \nC 142.596546 176.725642 141.833416 177.04174 141.270835 177.604321 \nC 140.708254 178.166902 140.392155 178.930032 140.392155 179.725642 \nC 140.392155 180.521251 140.708254 181.284381 141.270835 181.846962 \nC 141.833416 182.409543 142.596546 182.725642 143.392155 182.725642 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 192.476766 \nC 371.755552 192.476766 372.518683 192.160667 373.081264 191.598087 \nC 373.643844 191.035506 373.959943 190.272376 373.959943 189.476766 \nC 373.959943 188.681157 373.643844 187.918027 373.081264 187.355446 \nC 372.518683 186.792865 371.755552 186.476766 370.959943 186.476766 \nC 370.164334 186.476766 369.401204 186.792865 368.838623 187.355446 \nC 368.276042 187.918027 367.959943 188.681157 367.959943 189.476766 \nC 367.959943 190.272376 368.276042 191.035506 368.838623 191.598087 \nC 369.401204 192.160667 370.164334 192.476766 370.959943 192.476766 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 210.562356 \nC 67.571313 210.562356 68.334444 210.246257 68.897024 209.683676 \nC 69.459605 209.121095 69.775704 208.357965 69.775704 207.562356 \nC 69.775704 206.766746 69.459605 206.003616 68.897024 205.441035 \nC 68.334444 204.878455 67.571313 204.562356 66.775704 204.562356 \nC 65.980095 204.562356 65.216965 204.878455 64.654384 205.441035 \nC 64.091803 206.003616 63.775704 206.766746 63.775704 207.562356 \nC 63.775704 208.357965 64.091803 209.121095 64.654384 209.683676 \nC 65.216965 210.246257 65.980095 210.562356 66.775704 210.562356 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 194.753239 \nC 67.736911 194.753239 68.500041 194.43714 69.062622 193.87456 \nC 69.625203 193.311979 69.941302 192.548849 69.941302 191.753239 \nC 69.941302 190.95763 69.625203 190.1945 69.062622 189.631919 \nC 68.500041 189.069338 67.736911 188.753239 66.941302 188.753239 \nC 66.145692 188.753239 65.382562 189.069338 64.819981 189.631919 \nC 64.257401 190.1945 63.941302 190.95763 63.941302 191.753239 \nC 63.941302 192.548849 64.257401 193.311979 64.819981 193.87456 \nC 65.382562 194.43714 66.145692 194.753239 66.941302 194.753239 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 189.887911 \nC 68.068106 189.887911 68.831236 189.571812 69.393817 189.009231 \nC 69.956398 188.446651 70.272497 187.68352 70.272497 186.887911 \nC 70.272497 186.092302 69.956398 185.329171 69.393817 184.766591 \nC 68.831236 184.20401 68.068106 183.887911 67.272497 183.887911 \nC 66.476887 183.887911 65.713757 184.20401 65.151176 184.766591 \nC 64.588596 185.329171 64.272497 186.092302 64.272497 186.887911 \nC 64.272497 187.68352 64.588596 188.446651 65.151176 189.009231 \nC 65.713757 189.571812 66.476887 189.887911 67.272497 189.887911 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 137.779758 \nC 68.730496 137.779758 69.493626 137.463659 70.056207 136.901078 \nC 70.618788 136.338498 70.934887 135.575367 70.934887 134.779758 \nC 70.934887 133.984149 70.618788 133.221018 70.056207 132.658438 \nC 69.493626 132.095857 68.730496 131.779758 67.934887 131.779758 \nC 67.139277 131.779758 66.376147 132.095857 65.813566 132.658438 \nC 65.250986 133.221018 64.934887 133.984149 64.934887 134.779758 \nC 64.934887 135.575367 65.250986 136.338498 65.813566 136.901078 \nC 66.376147 137.463659 67.139277 137.779758 67.934887 137.779758 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 20.085924 \nC 70.055276 20.085924 70.818406 19.769825 71.380987 19.207245 \nC 71.943568 18.644664 72.259667 17.881534 72.259667 17.085924 \nC 72.259667 16.290315 71.943568 15.527185 71.380987 14.964604 \nC 70.818406 14.402023 70.055276 14.085924 69.259667 14.085924 \nC 68.464058 14.085924 67.700927 14.402023 67.138347 14.964604 \nC 66.575766 15.527185 66.259667 16.290315 66.259667 17.085924 \nC 66.259667 17.881534 66.575766 18.644664 67.138347 19.207245 \nC 67.700927 19.769825 68.464058 20.085924 69.259667 20.085924 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083689 \nC 72.704836 20.083689 73.467967 19.76759 74.030547 19.205009 \nC 74.593128 18.642429 74.909227 17.879298 74.909227 17.083689 \nC 74.909227 16.28808 74.593128 15.524949 74.030547 14.962369 \nC 73.467967 14.399788 72.704836 14.083689 71.909227 14.083689 \nC 71.113618 14.083689 70.350488 14.399788 69.787907 14.962369 \nC 69.225326 15.524949 68.909227 16.28808 68.909227 17.083689 \nC 68.909227 17.879298 69.225326 18.642429 69.787907 19.205009 \nC 70.350488 19.76759 71.113618 20.083689 71.909227 20.083689 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 136.112352 \nL 66.596307 134.612352 \nL 68.096307 136.112352 \nL 69.596307 134.612352 \nL 68.096307 133.112352 \nL 69.596307 131.612352 \nL 68.096307 130.112352 \nL 66.596307 131.612352 \nL 65.096307 130.112352 \nL 63.596307 131.612352 \nL 65.096307 133.112352 \nL 63.596307 134.612352 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 184.375401 \nL 66.674506 182.875401 \nL 68.174506 184.375401 \nL 69.674506 182.875401 \nL 68.174506 181.375401 \nL 69.674506 179.875401 \nL 68.174506 178.375401 \nL 66.674506 179.875401 \nL 65.174506 178.375401 \nL 63.674506 179.875401 \nL 65.174506 181.375401 \nL 63.674506 182.875401 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 168.289516 \nL 66.941302 166.789516 \nL 68.441302 168.289516 \nL 69.941302 166.789516 \nL 68.441302 165.289516 \nL 69.941302 163.789516 \nL 68.441302 162.289516 \nL 66.941302 163.789516 \nL 65.441302 162.289516 \nL 63.941302 163.789516 \nL 65.441302 165.289516 \nL 63.941302 166.789516 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 161.991995 \nL 67.916487 160.491995 \nL 69.416487 161.991995 \nL 70.916487 160.491995 \nL 69.416487 158.991995 \nL 70.916487 157.491995 \nL 69.416487 155.991995 \nL 67.916487 157.491995 \nL 66.416487 155.991995 \nL 64.916487 157.491995 \nL 66.416487 158.991995 \nL 64.916487 160.491995 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 152.013553 \nL 71.633231 150.513553 \nL 73.133231 152.013553 \nL 74.633231 150.513553 \nL 73.133231 149.013553 \nL 74.633231 147.513553 \nL 73.133231 146.013553 \nL 71.633231 147.513553 \nL 70.133231 146.013553 \nL 68.633231 147.513553 \nL 70.133231 149.013553 \nL 68.633231 150.513553 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 124.723689 \nL 86.132214 123.223689 \nL 87.632214 124.723689 \nL 89.132214 123.223689 \nL 87.632214 121.723689 \nL 89.132214 120.223689 \nL 87.632214 118.723689 \nL 86.132214 120.223689 \nL 84.632214 118.723689 \nL 83.132214 120.223689 \nL 84.632214 121.723689 \nL 83.132214 123.223689 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 131.126489 \nL 143.392155 129.626489 \nL 144.892155 131.126489 \nL 146.392155 129.626489 \nL 144.892155 128.126489 \nL 146.392155 126.626489 \nL 144.892155 125.126489 \nL 143.392155 126.626489 \nL 141.892155 125.126489 \nL 140.392155 126.626489 \nL 141.892155 128.126489 \nL 140.392155 129.626489 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 138.101429 \nL 370.959943 136.601429 \nL 372.459943 138.101429 \nL 373.959943 136.601429 \nL 372.459943 135.101429 \nL 373.959943 133.601429 \nL 372.459943 132.101429 \nL 370.959943 133.601429 \nL 369.459943 132.101429 \nL 367.959943 133.601429 \nL 369.459943 135.101429 \nL 367.959943 136.601429 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 157.803079 \nL 66.775704 156.303079 \nL 68.275704 157.803079 \nL 69.775704 156.303079 \nL 68.275704 154.803079 \nL 69.775704 153.303079 \nL 68.275704 151.803079 \nL 66.775704 153.303079 \nL 65.275704 151.803079 \nL 63.775704 153.303079 \nL 65.275704 154.803079 \nL 63.775704 156.303079 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 132.326462 \nL 66.941302 130.826462 \nL 68.441302 132.326462 \nL 69.941302 130.826462 \nL 68.441302 129.326462 \nL 69.941302 127.826462 \nL 68.441302 126.326462 \nL 66.941302 127.826462 \nL 65.441302 126.326462 \nL 63.941302 127.826462 \nL 65.441302 129.326462 \nL 63.941302 130.826462 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 160.278412 \nL 67.272497 158.778412 \nL 68.772497 160.278412 \nL 70.272497 158.778412 \nL 68.772497 157.278412 \nL 70.272497 155.778412 \nL 68.772497 154.278412 \nL 67.272497 155.778412 \nL 65.772497 154.278412 \nL 64.272497 155.778412 \nL 65.772497 157.278412 \nL 64.272497 158.778412 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 203.264375 \nL 67.934887 201.764375 \nL 69.434887 203.264375 \nL 70.934887 201.764375 \nL 69.434887 200.264375 \nL 70.934887 198.764375 \nL 69.434887 197.264375 \nL 67.934887 198.764375 \nL 66.434887 197.264375 \nL 64.934887 198.764375 \nL 66.434887 200.264375 \nL 64.934887 201.764375 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 173.650605 \nL 69.259667 172.150605 \nL 70.759667 173.650605 \nL 72.259667 172.150605 \nL 70.759667 170.650605 \nL 72.259667 169.150605 \nL 70.759667 167.650605 \nL 69.259667 169.150605 \nL 67.759667 167.650605 \nL 66.259667 169.150605 \nL 67.759667 170.650605 \nL 66.259667 172.150605 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 187.877494 \nL 71.909227 186.377494 \nL 73.409227 187.877494 \nL 74.909227 186.377494 \nL 73.409227 184.877494 \nL 74.909227 183.377494 \nL 73.409227 181.877494 \nL 71.909227 183.377494 \nL 70.409227 181.877494 \nL 68.909227 183.377494 \nL 70.409227 184.877494 \nL 68.909227 186.377494 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.596307 126.808605 \nC 67.391916 126.808605 68.155046 126.492506 68.717627 125.929925 \nC 69.280208 125.367345 69.596307 124.604214 69.596307 123.808605 \nC 69.596307 123.012996 69.280208 122.249865 68.717627 121.687285 \nC 68.155046 121.124704 67.391916 120.808605 66.596307 120.808605 \nC 65.800698 120.808605 65.037567 121.124704 64.474986 121.687285 \nC 63.912406 122.249865 63.596307 123.012996 63.596307 123.808605 \nC 63.596307 124.604214 63.912406 125.367345 64.474986 125.929925 \nC 65.037567 126.492506 65.800698 126.808605 66.596307 126.808605 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.674506 135.365779 \nC 67.470115 135.365779 68.233245 135.04968 68.795826 134.487099 \nC 69.358407 133.924518 69.674506 133.161388 69.674506 132.365779 \nC 69.674506 131.570169 69.358407 130.807039 68.795826 130.244458 \nC 68.233245 129.681878 67.470115 129.365779 66.674506 129.365779 \nC 65.878896 129.365779 65.115766 129.681878 64.553185 130.244458 \nC 63.990605 130.807039 63.674506 131.570169 63.674506 132.365779 \nC 63.674506 133.161388 63.990605 133.924518 64.553185 134.487099 \nC 65.115766 135.04968 65.878896 135.365779 66.674506 135.365779 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 204.897615 \nC 67.736911 204.897615 68.500041 204.581517 69.062622 204.018936 \nC 69.625203 203.456355 69.941302 202.693225 69.941302 201.897615 \nC 69.941302 201.102006 69.625203 200.338876 69.062622 199.776295 \nC 68.500041 199.213714 67.736911 198.897615 66.941302 198.897615 \nC 66.145692 198.897615 65.382562 199.213714 64.819981 199.776295 \nC 64.257401 200.338876 63.941302 201.102006 63.941302 201.897615 \nC 63.941302 202.693225 64.257401 203.456355 64.819981 204.018936 \nC 65.382562 204.581517 66.145692 204.897615 66.941302 204.897615 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.916487 208.021785 \nC 68.712096 208.021785 69.475227 207.705686 70.037807 207.143105 \nC 70.600388 206.580524 70.916487 205.817394 70.916487 205.021785 \nC 70.916487 204.226176 70.600388 203.463045 70.037807 202.900464 \nC 69.475227 202.337884 68.712096 202.021785 67.916487 202.021785 \nC 67.120878 202.021785 66.357747 202.337884 65.795167 202.900464 \nC 65.232586 203.463045 64.916487 204.226176 64.916487 205.021785 \nC 64.916487 205.817394 65.232586 206.580524 65.795167 207.143105 \nC 66.357747 207.705686 67.120878 208.021785 67.916487 208.021785 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.633231 150.38883 \nC 72.428841 150.38883 73.191971 150.072731 73.754552 149.51015 \nC 74.317132 148.947569 74.633231 148.184439 74.633231 147.38883 \nC 74.633231 146.593221 74.317132 145.83009 73.754552 145.26751 \nC 73.191971 144.704929 72.428841 144.38883 71.633231 144.38883 \nC 70.837622 144.38883 70.074492 144.704929 69.511911 145.26751 \nC 68.94933 145.83009 68.633231 146.593221 68.633231 147.38883 \nC 68.633231 148.184439 68.94933 148.947569 69.511911 149.51015 \nC 70.074492 150.072731 70.837622 150.38883 71.633231 150.38883 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 86.132214 173.489678 \nC 86.927823 173.489678 87.690954 173.173579 88.253534 172.610998 \nC 88.816115 172.048417 89.132214 171.285287 89.132214 170.489678 \nC 89.132214 169.694069 88.816115 168.930938 88.253534 168.368358 \nC 87.690954 167.805777 86.927823 167.489678 86.132214 167.489678 \nC 85.336605 167.489678 84.573474 167.805777 84.010894 168.368358 \nC 83.448313 168.930938 83.132214 169.694069 83.132214 170.489678 \nC 83.132214 171.285287 83.448313 172.048417 84.010894 172.610998 \nC 84.573474 173.173579 85.336605 173.489678 86.132214 173.489678 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 143.392155 194.558776 \nC 144.187765 194.558776 144.950895 194.242677 145.513476 193.680097 \nC 146.076056 193.117516 146.392155 192.354386 146.392155 191.558776 \nC 146.392155 190.763167 146.076056 190.000037 145.513476 189.437456 \nC 144.950895 188.874875 144.187765 188.558776 143.392155 188.558776 \nC 142.596546 188.558776 141.833416 188.874875 141.270835 189.437456 \nC 140.708254 190.000037 140.392155 190.763167 140.392155 191.558776 \nC 140.392155 192.354386 140.708254 193.117516 141.270835 193.680097 \nC 141.833416 194.242677 142.596546 194.558776 143.392155 194.558776 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 370.959943 162.857561 \nC 371.755552 162.857561 372.518683 162.541462 373.081264 161.978882 \nC 373.643844 161.416301 373.959943 160.653171 373.959943 159.857561 \nC 373.959943 159.061952 373.643844 158.298822 373.081264 157.736241 \nC 372.518683 157.17366 371.755552 156.857561 370.959943 156.857561 \nC 370.164334 156.857561 369.401204 157.17366 368.838623 157.736241 \nC 368.276042 158.298822 367.959943 159.061952 367.959943 159.857561 \nC 367.959943 160.653171 368.276042 161.416301 368.838623 161.978882 \nC 369.401204 162.541462 370.164334 162.857561 370.959943 162.857561 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.775704 195.089997 \nC 67.571313 195.089997 68.334444 194.773898 68.897024 194.211318 \nC 69.459605 193.648737 69.775704 192.885607 69.775704 192.089997 \nC 69.775704 191.294388 69.459605 190.531258 68.897024 189.968677 \nC 68.334444 189.406096 67.571313 189.089997 66.775704 189.089997 \nC 65.980095 189.089997 65.216965 189.406096 64.654384 189.968677 \nC 64.091803 190.531258 63.775704 191.294388 63.775704 192.089997 \nC 63.775704 192.885607 64.091803 193.648737 64.654384 194.211318 \nC 65.216965 194.773898 65.980095 195.089997 66.775704 195.089997 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.941302 205.547553 \nC 67.736911 205.547553 68.500041 205.231454 69.062622 204.668873 \nC 69.625203 204.106292 69.941302 203.343162 69.941302 202.547553 \nC 69.941302 201.751943 69.625203 200.988813 69.062622 200.426232 \nC 68.500041 199.863652 67.736911 199.547553 66.941302 199.547553 \nC 66.145692 199.547553 65.382562 199.863652 64.819981 200.426232 \nC 64.257401 200.988813 63.941302 201.751943 63.941302 202.547553 \nC 63.941302 203.343162 64.257401 204.106292 64.819981 204.668873 \nC 65.382562 205.231454 66.145692 205.547553 66.941302 205.547553 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.272497 149.517677 \nC 68.068106 149.517677 68.831236 149.201578 69.393817 148.638997 \nC 69.956398 148.076416 70.272497 147.313286 70.272497 146.517677 \nC 70.272497 145.722068 69.956398 144.958937 69.393817 144.396357 \nC 68.831236 143.833776 68.068106 143.517677 67.272497 143.517677 \nC 66.476887 143.517677 65.713757 143.833776 65.151176 144.396357 \nC 64.588596 144.958937 64.272497 145.722068 64.272497 146.517677 \nC 64.272497 147.313286 64.588596 148.076416 65.151176 148.638997 \nC 65.713757 149.201578 66.476887 149.517677 67.272497 149.517677 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.934887 163.126603 \nC 68.730496 163.126603 69.493626 162.810504 70.056207 162.247924 \nC 70.618788 161.685343 70.934887 160.922213 70.934887 160.126603 \nC 70.934887 159.330994 70.618788 158.567864 70.056207 158.005283 \nC 69.493626 157.442702 68.730496 157.126603 67.934887 157.126603 \nC 67.139277 157.126603 66.376147 157.442702 65.813566 158.005283 \nC 65.250986 158.567864 64.934887 159.330994 64.934887 160.126603 \nC 64.934887 160.922213 65.250986 161.685343 65.813566 162.247924 \nC 66.376147 162.810504 67.139277 163.126603 67.934887 163.126603 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 69.259667 144.269037 \nC 70.055276 144.269037 70.818406 143.952939 71.380987 143.390358 \nC 71.943568 142.827777 72.259667 142.064647 72.259667 141.269037 \nC 72.259667 140.473428 71.943568 139.710298 71.380987 139.147717 \nC 70.818406 138.585136 70.055276 138.269037 69.259667 138.269037 \nC 68.464058 138.269037 67.700927 138.585136 67.138347 139.147717 \nC 66.575766 139.710298 66.259667 140.473428 66.259667 141.269037 \nC 66.259667 142.064647 66.575766 142.827777 67.138347 143.390358 \nC 67.700927 143.952939 68.464058 144.269037 69.259667 144.269037 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 71.909227 20.083649 \nC 72.704836 20.083649 73.467967 19.76755 74.030547 19.204969 \nC 74.593128 18.642388 74.909227 17.879258 74.909227 17.083649 \nC 74.909227 16.288039 74.593128 15.524909 74.030547 14.962328 \nC 73.467967 14.399748 72.704836 14.083649 71.909227 14.083649 \nC 71.113618 14.083649 70.350488 14.399748 69.787907 14.962328 \nC 69.225326 15.524909 68.909227 16.288039 68.909227 17.083649 \nC 68.909227 17.879258 69.225326 18.642388 69.787907 19.204969 \nC 70.350488 19.76755 71.113618 20.083649 71.909227 20.083649 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.096307 90.848465 \nL 66.596307 89.348465 \nL 68.096307 90.848465 \nL 69.596307 89.348465 \nL 68.096307 87.848465 \nL 69.596307 86.348465 \nL 68.096307 84.848465 \nL 66.596307 86.348465 \nL 65.096307 84.848465 \nL 63.596307 86.348465 \nL 65.096307 87.848465 \nL 63.596307 89.348465 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.174506 156.468377 \nL 66.674506 154.968377 \nL 68.174506 156.468377 \nL 69.674506 154.968377 \nL 68.174506 153.468377 \nL 69.674506 151.968377 \nL 68.174506 150.468377 \nL 66.674506 151.968377 \nL 65.174506 150.468377 \nL 63.674506 151.968377 \nL 65.174506 153.468377 \nL 63.674506 154.968377 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 173.612514 \nL 66.941302 172.112514 \nL 68.441302 173.612514 \nL 69.941302 172.112514 \nL 68.441302 170.612514 \nL 69.941302 169.112514 \nL 68.441302 167.612514 \nL 66.941302 169.112514 \nL 65.441302 167.612514 \nL 63.941302 169.112514 \nL 65.441302 170.612514 \nL 63.941302 172.112514 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.416487 154.172448 \nL 67.916487 152.672448 \nL 69.416487 154.172448 \nL 70.916487 152.672448 \nL 69.416487 151.172448 \nL 70.916487 149.672448 \nL 69.416487 148.172448 \nL 67.916487 149.672448 \nL 66.416487 148.172448 \nL 64.916487 149.672448 \nL 66.416487 151.172448 \nL 64.916487 152.672448 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.133231 199.365187 \nL 71.633231 197.865187 \nL 73.133231 199.365187 \nL 74.633231 197.865187 \nL 73.133231 196.365187 \nL 74.633231 194.865187 \nL 73.133231 193.365187 \nL 71.633231 194.865187 \nL 70.133231 193.365187 \nL 68.633231 194.865187 \nL 70.133231 196.365187 \nL 68.633231 197.865187 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 84.632214 156.831235 \nL 86.132214 155.331235 \nL 87.632214 156.831235 \nL 89.132214 155.331235 \nL 87.632214 153.831235 \nL 89.132214 152.331235 \nL 87.632214 150.831235 \nL 86.132214 152.331235 \nL 84.632214 150.831235 \nL 83.132214 152.331235 \nL 84.632214 153.831235 \nL 83.132214 155.331235 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 141.892155 137.925031 \nL 143.392155 136.425031 \nL 144.892155 137.925031 \nL 146.392155 136.425031 \nL 144.892155 134.925031 \nL 146.392155 133.425031 \nL 144.892155 131.925031 \nL 143.392155 133.425031 \nL 141.892155 131.925031 \nL 140.392155 133.425031 \nL 141.892155 134.925031 \nL 140.392155 136.425031 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 369.459943 149.11491 \nL 370.959943 147.61491 \nL 372.459943 149.11491 \nL 373.959943 147.61491 \nL 372.459943 146.11491 \nL 373.959943 144.61491 \nL 372.459943 143.11491 \nL 370.959943 144.61491 \nL 369.459943 143.11491 \nL 367.959943 144.61491 \nL 369.459943 146.11491 \nL 367.959943 147.61491 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.275704 130.99682 \nL 66.775704 129.49682 \nL 68.275704 130.99682 \nL 69.775704 129.49682 \nL 68.275704 127.99682 \nL 69.775704 126.49682 \nL 68.275704 124.99682 \nL 66.775704 126.49682 \nL 65.275704 124.99682 \nL 63.775704 126.49682 \nL 65.275704 127.99682 \nL 63.775704 129.49682 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.441302 127.479653 \nL 66.941302 125.979653 \nL 68.441302 127.479653 \nL 69.941302 125.979653 \nL 68.441302 124.479653 \nL 69.941302 122.979653 \nL 68.441302 121.479653 \nL 66.941302 122.979653 \nL 65.441302 121.479653 \nL 63.941302 122.979653 \nL 65.441302 124.479653 \nL 63.941302 125.979653 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 65.772497 180.191126 \nL 67.272497 178.691126 \nL 68.772497 180.191126 \nL 70.272497 178.691126 \nL 68.772497 177.191126 \nL 70.272497 175.691126 \nL 68.772497 174.191126 \nL 67.272497 175.691126 \nL 65.772497 174.191126 \nL 64.272497 175.691126 \nL 65.772497 177.191126 \nL 64.272497 178.691126 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 66.434887 156.792191 \nL 67.934887 155.292191 \nL 69.434887 156.792191 \nL 70.934887 155.292191 \nL 69.434887 153.792191 \nL 70.934887 152.292191 \nL 69.434887 150.792191 \nL 67.934887 152.292191 \nL 66.434887 150.792191 \nL 64.934887 152.292191 \nL 66.434887 153.792191 \nL 64.934887 155.292191 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 67.759667 167.298587 \nL 69.259667 165.798587 \nL 70.759667 167.298587 \nL 72.259667 165.798587 \nL 70.759667 164.298587 \nL 72.259667 162.798587 \nL 70.759667 161.298587 \nL 69.259667 162.798587 \nL 67.759667 161.298587 \nL 66.259667 162.798587 \nL 67.759667 164.298587 \nL 66.259667 165.798587 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#p22267d1c68)\" d=\"M 70.409227 173.726216 \nL 71.909227 172.226216 \nL 73.409227 173.726216 \nL 74.909227 172.226216 \nL 73.409227 170.726216 \nL 74.909227 169.226216 \nL 73.409227 167.726216 \nL 71.909227 169.226216 \nL 70.409227 167.726216 \nL 68.909227 169.226216 \nL 70.409227 170.726216 \nL 68.909227 172.226216 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n   </g>\n   <g id=\"PathCollection_2\"/>\n   <g id=\"PathCollection_3\"/>\n   <g id=\"PathCollection_4\"/>\n   <g id=\"PathCollection_5\"/>\n   <g id=\"PathCollection_6\"/>\n   <g id=\"PathCollection_7\"/>\n   <g id=\"PathCollection_8\"/>\n   <g id=\"PathCollection_9\"/>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m217eb7140e\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.552607\" xlink:href=\"#m217eb7140e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(63.371357 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"112.551918\" xlink:href=\"#m217eb7140e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 20000 -->\n      <g transform=\"translate(96.645668 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"158.551228\" xlink:href=\"#m217eb7140e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 40000 -->\n      <g transform=\"translate(142.644978 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-34\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"204.550538\" xlink:href=\"#m217eb7140e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 60000 -->\n      <g transform=\"translate(188.644288 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-36\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"250.549849\" xlink:href=\"#m217eb7140e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 80000 -->\n      <g transform=\"translate(234.643599 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-38\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"296.549159\" xlink:href=\"#m217eb7140e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 100000 -->\n      <g transform=\"translate(277.461659 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"342.548469\" xlink:href=\"#m217eb7140e\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 120000 -->\n      <g transform=\"translate(323.460969 239.238437)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"318.115234\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_8\">\n     <!-- parameters -->\n     <g transform=\"translate(189.776563 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"165.869141\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"324.560547\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"386.083984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"425.292969\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"486.816406\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"527.929688\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_8\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m9ebb552f4d\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9ebb552f4d\" y=\"204.401682\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{-4}}$ -->\n      <g transform=\"translate(20.878125 208.2009)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9ebb552f4d\" y=\"173.184301\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{-3}}$ -->\n      <g transform=\"translate(20.878125 176.98352)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9ebb552f4d\" y=\"141.966921\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(20.878125 145.766139)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9ebb552f4d\" y=\"110.74954\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(20.878125 114.548759)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9ebb552f4d\" y=\"79.53216\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(26.778125 83.331378)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9ebb552f4d\" y=\"48.314779\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(26.778125 52.113998)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m9ebb552f4d\" y=\"17.097399\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(26.778125 20.896617)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_15\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m5e4867abad\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"220.724586\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"216.824326\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"213.79905\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"211.327219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"209.237315\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"207.426958\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"205.830111\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"195.004314\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"189.507206\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"185.606946\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"182.581669\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"180.109838\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"178.019935\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"176.209578\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"174.61273\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"163.786933\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"158.289825\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"154.389565\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"151.364289\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"148.892458\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_28\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"146.802554\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_29\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"144.992197\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_30\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"143.39535\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_31\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"132.569553\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_32\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"127.072445\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_33\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"123.172185\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_34\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"120.146908\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_35\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"117.675077\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_36\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"115.585174\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_37\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"113.774817\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_38\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"112.177969\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_39\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"101.352172\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_40\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"95.855064\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_41\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"91.954804\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_42\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"88.929528\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_43\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"86.457697\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_44\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"84.367793\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_45\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"82.557436\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_46\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"80.960589\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_47\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"70.134792\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_48\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"64.637684\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_49\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"60.737424\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_50\">\n     <g id=\"line2d_57\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"57.712147\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_51\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"55.240316\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_52\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"53.150413\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_53\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"51.340056\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_54\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"49.743208\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_55\">\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"38.917411\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_56\">\n     <g id=\"line2d_63\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"33.420303\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_57\">\n     <g id=\"line2d_64\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"29.520043\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_58\">\n     <g id=\"line2d_65\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"26.494767\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_59\">\n     <g id=\"line2d_66\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"24.022936\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_60\">\n     <g id=\"line2d_67\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"21.933032\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_61\">\n     <g id=\"line2d_68\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"20.122675\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_62\">\n     <g id=\"line2d_69\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"18.525828\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_63\">\n     <g id=\"line2d_70\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m5e4867abad\" y=\"7.700031\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_16\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 125.577812)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 224.64 \nL 51.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.178125 224.64 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 7.2 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 183.89375 219.64 \nL 253.6625 219.64 \nQ 255.6625 219.64 255.6625 217.64 \nL 255.6625 71.85875 \nQ 255.6625 69.85875 253.6625 69.85875 \nL 183.89375 69.85875 \nQ 181.89375 69.85875 181.89375 71.85875 \nL 181.89375 217.64 \nQ 181.89375 219.64 183.89375 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_17\">\n     <!-- layers -->\n     <g transform=\"translate(193.89375 81.457187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"209.765625\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"250.878906\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_10\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m9c394832e7\" style=\"stroke:#edd1cb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#edd1cb;stroke:#edd1cb;\" x=\"195.89375\" xlink:href=\"#m9c394832e7\" y=\"93.510312\"/>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- 1 -->\n     <g transform=\"translate(213.89375 96.135312)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_11\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mb9693596aa\" style=\"stroke:#ebcac5;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ebcac5;stroke:#ebcac5;\" x=\"195.89375\" xlink:href=\"#mb9693596aa\" y=\"108.188437\"/>\n     </g>\n    </g>\n    <g id=\"text_19\">\n     <!-- 2 -->\n     <g transform=\"translate(213.89375 110.813437)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_12\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m49ed9231de\" style=\"stroke:#e5bcbb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#e5bcbb;stroke:#e5bcbb;\" x=\"195.89375\" xlink:href=\"#m49ed9231de\" y=\"122.866562\"/>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- 4 -->\n     <g transform=\"translate(213.89375 125.491562)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_13\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m432914d68d\" style=\"stroke:#d89faa;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#d89faa;stroke:#d89faa;\" x=\"195.89375\" xlink:href=\"#m432914d68d\" y=\"137.544688\"/>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- 8 -->\n     <g transform=\"translate(213.89375 140.169688)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-38\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_14\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m166955d06d\" style=\"stroke:#ae6b91;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ae6b91;stroke:#ae6b91;\" x=\"195.89375\" xlink:href=\"#m166955d06d\" y=\"152.222812\"/>\n     </g>\n    </g>\n    <g id=\"text_22\">\n     <!-- 16 -->\n     <g transform=\"translate(213.89375 154.847812)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_15\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m83bd3c1373\" style=\"stroke:#2d1e3e;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#2d1e3e;stroke:#2d1e3e;\" x=\"195.89375\" xlink:href=\"#m83bd3c1373\" y=\"166.900937\"/>\n     </g>\n    </g>\n    <g id=\"text_23\">\n     <!-- 32 -->\n     <g transform=\"translate(213.89375 169.525937)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"text_24\">\n     <!-- model -->\n     <g transform=\"translate(193.89375 184.204062)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_16\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m61110c5f84\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"195.89375\" xlink:href=\"#m61110c5f84\" y=\"196.257187\"/>\n     </g>\n    </g>\n    <g id=\"text_25\">\n     <!-- FFNN -->\n     <g transform=\"translate(213.89375 198.882187)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_17\">\n     <defs>\n      <path d=\"M -1.5 3 \nL 0 1.5 \nL 1.5 3 \nL 3 1.5 \nL 1.5 0 \nL 3 -1.5 \nL 1.5 -3 \nL 0 -1.5 \nL -1.5 -3 \nL -3 -1.5 \nL -1.5 0 \nL -3 1.5 \nz\n\" id=\"mbc62f63a53\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"195.89375\" xlink:href=\"#mbc62f63a53\" y=\"210.935312\"/>\n     </g>\n    </g>\n    <g id=\"text_26\">\n     <!-- ResNET -->\n     <g transform=\"translate(213.89375 213.560312)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 3578 4666 \nL 3578 4134 \nL 1259 4134 \nL 1259 2753 \nL 3481 2753 \nL 3481 2222 \nL 1259 2222 \nL 1259 531 \nL 3634 531 \nL 3634 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-45\" transform=\"scale(0.015625)\"/>\n       <path d=\"M -19 4666 \nL 3928 4666 \nL 3928 4134 \nL 2272 4134 \nL 2272 0 \nL 1638 0 \nL 1638 4134 \nL -19 4134 \nL -19 4666 \nz\n\" id=\"DejaVuSans-54\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-45\"/>\n      <use x=\"316.59375\" xlink:href=\"#DejaVuSans-54\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p22267d1c68\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"51.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261 280\n"
     ]
    }
   ],
   "source": [
    "print(len(d_results[d_results.loss < 2]), len(d_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}