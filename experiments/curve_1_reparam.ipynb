{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from neural_reparam.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    ")\n",
    "from neural_reparam.ResNet import ResNet\n",
    "import experiments.curves as c1\n",
    "\n",
    "# make reproducible\n",
    "seed = torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_1/\"\n",
    "SET_NAME = \"eks_7\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "\n",
    "FOLDS = 1\n",
    "N = 128  # training points internal\n",
    "lr_scheduler = lambda optim: torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='min', factor=0.1, patience=5,\n",
    "                                                                        verbose=True)\n",
    "loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=1e3, verbose=False)\n",
    "no_penalty_loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=0, verbose=False)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [2],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 8, 16, 32],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"model\": [FFNN, ResNet],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N // 2],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [100],\n",
    "    \"learning_rate\": [1],\n",
    "    \"lr_scheduler\": [lr_scheduler]\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = c1.q(x_train.detach())\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.27251244\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.20082574\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00316497\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00215863\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00039829\n",
      "################################  20  ################################\n",
      "Training Loss:  6.937e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  4.854e-05\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  4.366e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  4.94e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  5.012e-05\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.723e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  2.376e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  2.958e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  2.336e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  2.656e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  2.381e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  2.379e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  2.375e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  2.375e-05\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  2.375e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.375e-05\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  2.375e-05\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67980641\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00343569\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00498004\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00357281\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00294619\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00246807\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0031574\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00248825\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00258705\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00258686\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00258699\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00258757\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00258757\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00258757\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00258757\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00258757\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00258757\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00258757\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00258757\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00258757\n",
      "Final training Loss:  0.00258757\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03939177\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00055107\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00063743\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00150405\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00051086\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00038656\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00048185\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00039996\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00039838\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0003981\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00039792\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00039792\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00039792\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00039792\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00039792\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00039792\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00039792\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00039792\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00039792\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00039792\n",
      "Final training Loss:  0.00039792\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06652411\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00420337\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0001417\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00026655\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  9.911e-05\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  9.716e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  9.699e-05\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  7.972e-05\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  7.962e-05\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  7.962e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  7.962e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  7.962e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  7.962e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  7.962e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  7.962e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  7.962e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  7.962e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  7.962e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  7.962e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  7.962e-05\n",
      "Final training Loss:  7.962e-05\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11462717\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01273614\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0115579\n",
      "################################  15  ################################\n",
      "Training Loss:  0.01007835\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00510956\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00209447\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00170625\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00142023\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00114363\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00115916\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00120203\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00076908\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00036639\n",
      "################################  65  ################################\n",
      "Training Loss:  3.8e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  1.877e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  1.081e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  1.439e-05\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  85  ################################\n",
      "Training Loss:  7.83e-06\n",
      "################################  90  ################################\n",
      "Training Loss:  7.97e-06\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  95  ################################\n",
      "Training Loss:  7.96e-06\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Final training Loss:  7.98e-06\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.34813336\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00154474\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00140116\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00140221\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00136784\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00127108\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00128318\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00133227\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00150336\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0010528\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00105834\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00105673\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00109206\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00109232\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00109245\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00109246\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00109247\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00109247\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00109247\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00109247\n",
      "Final training Loss:  0.00109247\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.44989777\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00767456\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00026723\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00035239\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00033824\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00033249\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0003275\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00032747\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00031752\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00031752\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00031752\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00031752\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00031752\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00031752\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00031752\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00031752\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00031752\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00031752\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00031752\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00031752\n",
      "Final training Loss:  0.00031752\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.21664052\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00040571\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00017047\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00025729\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00018182\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00013475\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00014079\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00013392\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00013397\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00013395\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00013395\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00013395\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00013394\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00013394\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00013394\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00013394\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00013394\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00013394\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00013394\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00013394\n",
      "Final training Loss:  0.00013394\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02095323\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0007627\n",
      "################################  10  ################################\n",
      "Training Loss:  9.313e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  4.288e-05\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  4.563e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00010392\n",
      "################################  30  ################################\n",
      "Training Loss:  3.291e-05\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  2.972e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  3.045e-05\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  2.945e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  2.946e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  2.941e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  2.941e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  2.941e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  2.941e-05\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  2.941e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  2.941e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  2.941e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  2.941e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.941e-05\n",
      "Final training Loss:  2.941e-05\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.28491446\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00080329\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00018498\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00019051\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00015927\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00010491\n",
      "################################  30  ################################\n",
      "Training Loss:  5.412e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  4.151e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  5.284e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  6.215e-05\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  4.353e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  3.92e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  3.716e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  3.76e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  3.7e-05\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  3.694e-05\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  3.694e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  3.694e-05\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  3.694e-05\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  3.694e-05\n",
      "Final training Loss:  3.694e-05\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  48.04779816\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02668666\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02597088\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0194554\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00942211\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00749914\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00705658\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00547564\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00174806\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00162327\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00161409\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00139964\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00134045\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00143957\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00131627\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00131595\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0013154\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00131557\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00131553\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00131553\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Final training Loss:  0.00131552\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10144043\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10137939\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10137939\n",
      "Final training Loss:  600.10137939\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  74.29833984\n",
      "################################  5  ################################\n",
      "Training Loss:  0.03098167\n",
      "################################  10  ################################\n",
      "Training Loss:  0.03125468\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.03201033\n",
      "################################  20  ################################\n",
      "Training Loss:  0.01435275\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00707855\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00586269\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00445449\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00680088\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0042166\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00429467\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00455704\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0041826\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00742655\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00426585\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00425473\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00425295\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00425295\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00425295\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00425295\n",
      "Final training Loss:  0.00425295\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05528329\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00033069\n",
      "################################  10  ################################\n",
      "Training Loss:  3.059e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  1.71e-05\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  3.216e-05\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  1.605e-05\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  1.599e-05\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.599e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  1.6e-05\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  1.599e-05\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  1.599e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  1.599e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.599e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  1.599e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  1.599e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  1.599e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  1.599e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  1.599e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  1.599e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  1.599e-05\n",
      "Final training Loss:  1.599e-05\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.2781893\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00111229\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00028037\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00028515\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00021824\n",
      "################################  25  ################################\n",
      "Training Loss:  8.314e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  4.362e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  4.216e-05\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  4.774e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  6.519e-05\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  3.969e-05\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  4.015e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  4.012e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  4.012e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  4.014e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  4.014e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  4.014e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  4.014e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  4.014e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  4.014e-05\n",
      "Final training Loss:  4.014e-05\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.40955472\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02661819\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02019377\n",
      "################################  15  ################################\n",
      "Training Loss:  0.01336167\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00210424\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00238608\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00200511\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0024959\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00277523\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00204242\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00188963\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00185087\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00185086\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00185355\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0018501\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00185057\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0018505\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0018505\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0018505\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0018505\n",
      "Final training Loss:  0.0018505\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  112.31997681\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0058391\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00569658\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00062399\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00105344\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00060784\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00065752\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00056277\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00054446\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00054184\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00054174\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00054177\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00054177\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00054176\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00054176\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00054176\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00054176\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00054176\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00054176\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00054176\n",
      "Final training Loss:  0.00054176\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.08880838\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00448303\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00081613\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0005481\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00056621\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00056699\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00056271\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00145848\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00059281\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0006044\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00060156\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00060181\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00060169\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00060171\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.0006017\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0006017\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0006017\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0006017\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0006017\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0006017\n",
      "Final training Loss:  0.0006017\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06804516\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02054194\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00930532\n",
      "################################  15  ################################\n",
      "Training Loss:  0.01140518\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00889589\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00835741\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00852307\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00852213\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00852179\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00852396\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00852397\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00852447\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00852447\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00852447\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00852447\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00852447\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00852447\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00852447\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00852447\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00852447\n",
      "Final training Loss:  0.00852447\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  54.19946671\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00325075\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00197497\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00017936\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00017031\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00018302\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00019366\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00025716\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00018976\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00016578\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00016158\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00016152\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00016143\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00016142\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00016142\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00016142\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00016142\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00016142\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00016142\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00016142\n",
      "Final training Loss:  0.00016142\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.292831\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00065253\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00026604\n",
      "################################  15  ################################\n",
      "Training Loss:  9.002e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00011263\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00010748\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00012234\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00010338\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00010179\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00010168\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00010159\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00010159\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00010159\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00010159\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00010159\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00010159\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00010159\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00010159\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00010159\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00010159\n",
      "Final training Loss:  0.00010159\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.50124091\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00206711\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00153565\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00193976\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00016754\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0003247\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00023365\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00012616\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00010556\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00011523\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  8.537e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  8.744e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.494e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  8.5e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  8.491e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  8.487e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  8.487e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  8.487e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  8.487e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  8.487e-05\n",
      "Final training Loss:  8.487e-05\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  99.66110229\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00034967\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00036545\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00019417\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00020814\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0001873\n",
      "################################  30  ################################\n",
      "Training Loss:  9.584e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00012814\n",
      "################################  40  ################################\n",
      "Training Loss:  6.32e-05\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  7.851e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  5.092e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  4.931e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  4.519e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  4.547e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  4.562e-05\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  4.564e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  4.566e-05\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  4.56e-05\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  4.561e-05\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  4.561e-05\n",
      "Epoch   100: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  4.561e-05\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.16768321\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00224739\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00024513\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0002047\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00026047\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00025972\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00018874\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00021696\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00018942\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00018935\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00018929\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00018929\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00018929\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00018929\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00018929\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00018929\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00018929\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00018929\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00018929\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00018929\n",
      "Final training Loss:  0.00018929\n",
      "\n",
      "Running model (trial=1, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.96087909\n",
      "################################  5  ################################\n",
      "Training Loss:  2.9710927\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  2.9652226\n",
      "################################  15  ################################\n",
      "Training Loss:  2.9799695\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  2.96915603\n",
      "################################  25  ################################\n",
      "Training Loss:  2.91808987\n",
      "################################  30  ################################\n",
      "Training Loss:  2.89980841\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  2.8987751\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  2.90136051\n",
      "################################  45  ################################\n",
      "Training Loss:  2.90127897\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  2.90127897\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  2.90127897\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  2.90127897\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  2.90127897\n",
      "################################  70  ################################\n",
      "Training Loss:  2.90127897\n",
      "################################  75  ################################\n",
      "Training Loss:  2.90127897\n",
      "################################  80  ################################\n",
      "Training Loss:  2.90127897\n",
      "################################  85  ################################\n",
      "Training Loss:  2.90127897\n",
      "################################  90  ################################\n",
      "Training Loss:  2.90127897\n",
      "################################  95  ################################\n",
      "Training Loss:  2.90127897\n",
      "Final training Loss:  2.90127897\n",
      "\n",
      "Running model (trial=1, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.25209782\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0002711\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00024794\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00024861\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00031447\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00027985\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00021987\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00023109\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00021094\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00024145\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0002057\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00020567\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00020564\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00020567\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00020567\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00020567\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00020567\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00020567\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00020567\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00020567\n",
      "Final training Loss:  0.00020567\n",
      "\n",
      "Running model (trial=1, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.34577164\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00419267\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00432812\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00500643\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00362931\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00803958\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00450486\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00694091\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00289392\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00364192\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00283592\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00283447\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00272619\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0027173\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0027177\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00271778\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0027177\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.0027177\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0027177\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0027177\n",
      "Final training Loss:  0.0027177\n",
      "\n",
      "Running model (trial=1, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.14161457\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00044526\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00026419\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00030239\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00021101\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00023383\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00019571\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00022817\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00020426\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00020409\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00020406\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.0002041\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00020411\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00020411\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00020411\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00020411\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00020411\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00020411\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00020411\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00020411\n",
      "Final training Loss:  0.00020411\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.044773\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00402363\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00115406\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00060459\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00048799\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00023309\n",
      "################################  30  ################################\n",
      "Training Loss:  8.087e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  6.296e-05\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  9.597e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  6.239e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  6.766e-05\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  6.135e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  6.244e-05\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  6.338e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  6.327e-05\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  6.319e-05\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  6.318e-05\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  6.318e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  6.318e-05\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  6.318e-05\n",
      "Final training Loss:  6.318e-05\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.40714064\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01529138\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01401715\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00225095\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00224266\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0020962\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00100091\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00098495\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00095815\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0014869\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00082018\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00086832\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00086836\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00086826\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0008684\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0008684\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00086839\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00086839\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00086839\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00086839\n",
      "Final training Loss:  0.00086839\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.34720075\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00140859\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00100597\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00140465\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00121751\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00154768\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00064305\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00043254\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0010501\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00038509\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00062552\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00036935\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00035536\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00034782\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00034791\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0003479\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00034788\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00034788\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00034788\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00034788\n",
      "Final training Loss:  0.00034788\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.09372691\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00370709\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00368457\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00337848\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0031073\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00250546\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00123118\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00111114\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00141809\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00080565\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00078885\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00063707\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00073524\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00065104\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00065102\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00065102\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00065102\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00065105\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00065105\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00065105\n",
      "Final training Loss:  0.00065105\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.19108173\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00110537\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0002611\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00028321\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00019199\n",
      "################################  25  ################################\n",
      "Training Loss:  8.397e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  5.401e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  6.159e-05\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  9.359e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  7.934e-05\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  5.873e-05\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  6.137e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  5.527e-05\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  5.378e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  5.379e-05\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  5.378e-05\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  5.377e-05\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  5.377e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  5.377e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  5.377e-05\n",
      "Final training Loss:  5.377e-05\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  9.29674625\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00435679\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00323554\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0032689\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00142375\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00100843\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00084581\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00095607\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00071277\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00068319\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00095356\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00076821\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00070303\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00067736\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00068003\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00071181\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00071298\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00071449\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00071439\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00071434\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Final training Loss:  0.00071434\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02133079\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00144873\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00028327\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00031241\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00021378\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00019668\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00017163\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00012793\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00011567\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00010347\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00010028\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00010502\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00010103\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00010102\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00010102\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00010102\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00010102\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00010102\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00010102\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00010102\n",
      "Final training Loss:  0.00010102\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  599.94378662\n",
      "################################  5  ################################\n",
      "Training Loss:  0.03979296\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01751126\n",
      "################################  15  ################################\n",
      "Training Loss:  0.02036162\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00855724\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00714716\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00684471\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00954519\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0071293\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0066413\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00763927\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00694137\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00694068\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00694068\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.006944\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00694325\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00694325\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00694325\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00694325\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00694325\n",
      "Final training Loss:  0.00694325\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10125732\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10125732\n",
      "Final training Loss:  600.10125732\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.99832702\n",
      "################################  5  ################################\n",
      "Training Loss:  0.03327148\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.02696131\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0265997\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.0292237\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0260831\n",
      "################################  30  ################################\n",
      "Training Loss:  0.02588735\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.02552519\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.02604564\n",
      "################################  45  ################################\n",
      "Training Loss:  0.02603963\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.02603948\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.02603948\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.02603948\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.02603948\n",
      "################################  70  ################################\n",
      "Training Loss:  0.02603948\n",
      "################################  75  ################################\n",
      "Training Loss:  0.02603948\n",
      "################################  80  ################################\n",
      "Training Loss:  0.02603948\n",
      "################################  85  ################################\n",
      "Training Loss:  0.02603948\n",
      "################################  90  ################################\n",
      "Training Loss:  0.02603948\n",
      "################################  95  ################################\n",
      "Training Loss:  0.02603948\n",
      "Final training Loss:  0.02603948\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.20472391\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00167936\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00140023\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00321054\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00142697\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00123077\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00133291\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00100753\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00095226\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00138352\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00099552\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00097093\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00097132\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00097115\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00097121\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00097121\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00097121\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00097121\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00097121\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00097121\n",
      "Final training Loss:  0.00097121\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68921441\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01105527\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00518543\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00325765\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00148516\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00044164\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00027962\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00028132\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0004333\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00037136\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00036731\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00026194\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00023331\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0002606\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00023401\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00023411\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00023415\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.000234\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.000234\n",
      "################################  95  ################################\n",
      "Training Loss:  0.000234\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  0.000234\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  42.06087875\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0096814\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00158081\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00127177\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.0015055\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00161002\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00101809\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00135936\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00101239\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00102986\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00099389\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00099396\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00099431\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00099382\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00099411\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.000994\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00099406\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00099406\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00099406\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00099406\n",
      "Final training Loss:  0.00099406\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06437837\n",
      "################################  5  ################################\n",
      "Training Loss:  0.03038027\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00168777\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00015427\n",
      "################################  20  ################################\n",
      "Training Loss:  9.545e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  4.781e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00012071\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  5.643e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  4.575e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  6.907e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  4.624e-05\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  5.014e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  4.54e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  4.536e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  4.538e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  4.535e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  4.535e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  4.535e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  4.535e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  4.535e-05\n",
      "Final training Loss:  4.535e-05\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0929938\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01057051\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00493741\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00277095\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00271166\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00324943\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00276776\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00331817\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00274004\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00265451\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00244798\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00256034\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00244904\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00244891\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00244846\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00244875\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00244875\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00244875\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00244875\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00244875\n",
      "Final training Loss:  0.00244875\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.18454278\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01208492\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00439866\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00359504\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00376851\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00364604\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00422413\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00356336\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00344435\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00587135\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00360531\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00328971\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00328959\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00329215\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00328974\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00328974\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00328977\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00328977\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00328977\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00328977\n",
      "Final training Loss:  0.00328977\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03702252\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02299777\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02877343\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.02261006\n",
      "################################  20  ################################\n",
      "Training Loss:  0.02198943\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00280827\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00108648\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00082482\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00132331\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00127901\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00116045\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00086389\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00080675\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00080681\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00080679\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00080672\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00080672\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00080672\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00080672\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00080672\n",
      "Final training Loss:  0.00080672\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.23697585\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0417986\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00760611\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00422888\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00471944\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00410835\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00309961\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00317152\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00320261\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00359289\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00327116\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00305597\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00303364\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00287954\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00278241\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0033133\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00292845\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00292801\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00292801\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00292789\n",
      "Final training Loss:  0.00292822\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.51145089\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00504234\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00272264\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0005055\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00024466\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00015708\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00026737\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00024957\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00015796\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00017749\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00016433\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00016428\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00016427\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00016426\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00016426\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00016426\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00016426\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00016426\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00016426\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00016426\n",
      "Final training Loss:  0.00016426\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04436193\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00298272\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00293752\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00045116\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00045104\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00037332\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00035483\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00037644\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00035428\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00034427\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00034431\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00034427\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0003442\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00034418\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00034418\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00034418\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00034418\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00034418\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00034418\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00034418\n",
      "Final training Loss:  0.00034418\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.12409259\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00126292\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00024184\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00018761\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00013983\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00010076\n",
      "################################  30  ################################\n",
      "Training Loss:  6.719e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  5.225e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  5.24e-05\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  8.237e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  5.189e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  5.187e-05\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  4.652e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  4.553e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  4.543e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  75  ################################\n",
      "Training Loss:  4.813e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  80  ################################\n",
      "Training Loss:  4.788e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  4.79e-05\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  4.79e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  4.79e-05\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Final training Loss:  4.79e-05\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03553935\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00114205\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00065063\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00015598\n",
      "################################  20  ################################\n",
      "Training Loss:  8.711e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00014017\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  9.536e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00015925\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  8.016e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  7.769e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  7.714e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  7.662e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  8.129e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  8.074e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  8.091e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  8.08e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  8.08e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  8.08e-05\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  8.08e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  8.08e-05\n",
      "Final training Loss:  8.08e-05\n",
      "\n",
      "Running model (trial=2, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.21201897\n",
      "################################  5  ################################\n",
      "Training Loss:  4.05782461\n",
      "################################  10  ################################\n",
      "Training Loss:  3.66649246\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  3.6636405\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.65955114\n",
      "################################  25  ################################\n",
      "Training Loss:  3.635041\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  3.63226867\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  3.63227606\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  3.63305092\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  3.63503194\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  3.63503194\n",
      "################################  55  ################################\n",
      "Training Loss:  3.63503194\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.63503194\n",
      "################################  65  ################################\n",
      "Training Loss:  3.63503194\n",
      "################################  70  ################################\n",
      "Training Loss:  3.63503194\n",
      "################################  75  ################################\n",
      "Training Loss:  3.63503194\n",
      "################################  80  ################################\n",
      "Training Loss:  3.63503194\n",
      "################################  85  ################################\n",
      "Training Loss:  3.63503194\n",
      "################################  90  ################################\n",
      "Training Loss:  3.63503194\n",
      "################################  95  ################################\n",
      "Training Loss:  3.63503194\n",
      "Final training Loss:  3.63503194\n",
      "\n",
      "Running model (trial=2, mod=53, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.20492393\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00372344\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00020311\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00018966\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00023778\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00017861\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00017601\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00017823\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00017824\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00017828\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00017824\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00017824\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00017827\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00017827\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00017827\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00017827\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00017827\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00017827\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00017827\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00017827\n",
      "Final training Loss:  0.00017827\n",
      "\n",
      "Running model (trial=2, mod=54, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05507888\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00120599\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00044809\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00038934\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0003833\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00037817\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.000367\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00039569\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00046015\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00036492\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00035427\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00035425\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00035425\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.000354\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00035399\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00035399\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00035399\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00035399\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00035399\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00035399\n",
      "Final training Loss:  0.00035399\n",
      "\n",
      "Running model (trial=2, mod=55, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.25097728\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00076927\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00035946\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00037607\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00018353\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00020524\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00010706\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00023165\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00011206\n",
      "################################  45  ################################\n",
      "Training Loss:  9.656e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  9.379e-05\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  9.382e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  9.379e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  9.382e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  9.382e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  9.382e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  9.382e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  9.382e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  9.382e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  9.382e-05\n",
      "Final training Loss:  9.382e-05\n",
      "\n",
      "Running model (trial=2, mod=56, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.16047992\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00276429\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00139683\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00095375\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00098342\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00238342\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00087946\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00085182\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0008505\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00087667\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00081871\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00081121\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00080519\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00079803\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00079761\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00079776\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0007977\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0007977\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00079769\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00079769\n",
      "Final training Loss:  0.00079769\n",
      "\n",
      "Running model (trial=2, mod=57, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.52257419\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00517884\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0025882\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00251587\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00235447\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00340763\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0024563\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00286973\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00239666\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00241735\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00240633\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00240702\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00240566\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00240523\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00240523\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00240523\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00240523\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00240523\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00240523\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00240523\n",
      "Final training Loss:  0.00240523\n",
      "\n",
      "Running model (trial=2, mod=58, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03165586\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00077455\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00105478\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00081881\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00032239\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00039837\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00024418\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00024653\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00024652\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00024654\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00024653\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00024651\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00024653\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00024653\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00024653\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00024653\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00024653\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00024653\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00024653\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00024653\n",
      "Final training Loss:  0.00024653\n",
      "\n",
      "Running model (trial=2, mod=59, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.09502289\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00110636\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00079417\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00108961\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00063901\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00056396\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00043908\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00050326\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00050255\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00044467\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00044408\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00044407\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00044404\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00044404\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00044404\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00044404\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00044404\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00044404\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00044404\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00044404\n",
      "Final training Loss:  0.00044404\n",
      "\n",
      "Running model (trial=2, mod=60, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=61, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05638524\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00147455\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00075139\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00054248\n",
      "################################  20  ################################\n",
      "Training Loss:  8.048e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  4.208e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  3.209e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  7.467e-05\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  3.821e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  3.926e-05\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  3.096e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  3.131e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.12e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  3.119e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  3.118e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  3.118e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  3.118e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  3.118e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  3.118e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  3.118e-05\n",
      "Final training Loss:  3.118e-05\n",
      "\n",
      "Running model (trial=2, mod=62, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.52488321\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00733997\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00221356\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00152955\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00052278\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00051299\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00093519\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0005274\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00049017\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00047972\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00048726\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00049563\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00049053\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00049038\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.0004904\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0004904\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0004904\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0004904\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0004904\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0004904\n",
      "Final training Loss:  0.0004904\n",
      "\n",
      "Running model (trial=2, mod=63, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.07672119\n",
      "################################  5  ################################\n",
      "Training Loss:  0.20212585\n",
      "################################  10  ################################\n",
      "Training Loss:  0.17408459\n",
      "################################  15  ################################\n",
      "Training Loss:  0.10875564\n",
      "################################  20  ################################\n",
      "Training Loss:  0.06768962\n",
      "################################  25  ################################\n",
      "Training Loss:  0.05013271\n",
      "################################  30  ################################\n",
      "Training Loss:  0.03438365\n",
      "################################  35  ################################\n",
      "Training Loss:  0.02153765\n",
      "################################  40  ################################\n",
      "Training Loss:  0.02977551\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.02036431\n",
      "################################  50  ################################\n",
      "Training Loss:  0.01868231\n",
      "################################  55  ################################\n",
      "Training Loss:  0.02107612\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.02086599\n",
      "################################  65  ################################\n",
      "Training Loss:  0.01861329\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.01834587\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.0181774\n",
      "################################  80  ################################\n",
      "Training Loss:  0.01815995\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.01818147\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.01817197\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.01817197\n",
      "Epoch   100: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  0.01817197\n",
      "\n",
      "Running model (trial=2, mod=64, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10162354\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10131836\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10131836\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10131836\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10125732\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10125732\n",
      "Final training Loss:  600.10125732\n",
      "\n",
      "Running model (trial=2, mod=65, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  13.97535133\n",
      "################################  5  ################################\n",
      "Training Loss:  5.9772644\n",
      "################################  10  ################################\n",
      "Training Loss:  5.72785473\n",
      "################################  15  ################################\n",
      "Training Loss:  8.82219315\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  5.84841204\n",
      "################################  25  ################################\n",
      "Training Loss:  5.66526413\n",
      "################################  30  ################################\n",
      "Training Loss:  6.06789112\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  5.65596151\n",
      "################################  40  ################################\n",
      "Training Loss:  5.68012524\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  5.72625017\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  5.65470695\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  5.65476561\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  5.65465975\n",
      "################################  65  ################################\n",
      "Training Loss:  5.65465593\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  5.65486145\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  5.6548624\n",
      "################################  80  ################################\n",
      "Training Loss:  5.6548624\n",
      "################################  85  ################################\n",
      "Training Loss:  5.6548624\n",
      "################################  90  ################################\n",
      "Training Loss:  5.6548624\n",
      "################################  95  ################################\n",
      "Training Loss:  5.6548624\n",
      "Final training Loss:  5.6548624\n",
      "\n",
      "Running model (trial=2, mod=66, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  25.01382637\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02619642\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0016634\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00141728\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00150107\n",
      "################################  25  ################################\n",
      "Training Loss:  0.000974\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00065608\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00066189\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00065787\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00070659\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00077218\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00085541\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00058208\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00057919\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00057884\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00058321\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00058229\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00058229\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00058229\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00058229\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  0.00058229\n",
      "\n",
      "Running model (trial=2, mod=67, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3049047\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00067365\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00036763\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00034098\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0004963\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00047761\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00032612\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00039501\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00030445\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00030564\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00030513\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00030491\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00030491\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00030491\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00030491\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00030491\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00030491\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00030491\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00030491\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00030491\n",
      "Final training Loss:  0.00030491\n",
      "\n",
      "Running model (trial=2, mod=68, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10.15435791\n",
      "################################  5  ################################\n",
      "Training Loss:  0.03331246\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00846331\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00627492\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0029146\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0037909\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00318719\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00289561\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00355262\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00288178\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0036092\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00307631\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00286089\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00282591\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00276218\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00276128\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00276137\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00276115\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00276115\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00276115\n",
      "Final training Loss:  0.00276115\n",
      "\n",
      "Running model (trial=2, mod=69, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.08871174\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01760317\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00239805\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00110765\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00099147\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00236448\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00061151\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00044875\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00040549\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0004666\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00032658\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00031738\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0003639\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0004432\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00082986\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00088724\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00088726\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00088724\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00088723\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00088723\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Final training Loss:  0.00088723\n",
      "\n",
      "Running model (trial=2, mod=70, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.42149624\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01249077\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00532927\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00196334\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00044281\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00042824\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0001723\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00019017\n",
      "################################  40  ################################\n",
      "Training Loss:  9.897e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00014504\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00012178\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00010602\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  9.486e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00010425\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00010409\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00010412\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00010421\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00010411\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00010411\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00010411\n",
      "Final training Loss:  0.00010411\n",
      "\n",
      "Running model (trial=2, mod=71, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11697968\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0064126\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00289206\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00256066\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0028765\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00188596\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00108163\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00084955\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00069549\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00062847\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00058012\n",
      "################################  55  ################################\n",
      "Training Loss:  0.000574\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00061713\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00060874\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0006085\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00060865\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00060868\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00060868\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00060868\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00060868\n",
      "Final training Loss:  0.00060868\n",
      "\n",
      "Running model (trial=2, mod=72, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  48.55285645\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02114708\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01265256\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00889863\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00875744\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00865323\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00843397\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00796539\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0076577\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00762186\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00762208\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00762209\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00762209\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00762209\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00762209\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00762209\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00762209\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00762209\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00762209\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00762209\n",
      "Final training Loss:  0.00762209\n",
      "\n",
      "Running model (trial=2, mod=73, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.28909868\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00063666\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00013806\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.0001164\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00012229\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  9.935e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00011725\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00010067\n",
      "################################  40  ################################\n",
      "Training Loss:  9.871e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  9.865e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  9.862e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  9.874e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  9.801e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  9.786e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  9.787e-05\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  9.787e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  9.786e-05\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  9.787e-05\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  9.787e-05\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  9.787e-05\n",
      "Final training Loss:  9.787e-05\n",
      "\n",
      "Running model (trial=2, mod=74, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.19124234\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00106415\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00117491\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00074557\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00019018\n",
      "################################  25  ################################\n",
      "Training Loss:  3.385e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  2.993e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  2.542e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  2.953e-05\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  2.698e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  2.35e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  2.391e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.424e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  2.424e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  2.421e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  2.42e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  2.421e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  2.421e-05\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  2.421e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.421e-05\n",
      "Final training Loss:  2.421e-05\n",
      "\n",
      "Running model (trial=2, mod=75, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  102.68159485\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01073522\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00198243\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00251192\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00200414\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00265944\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0030995\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00175211\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00175965\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00173107\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0017325\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00173285\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00173272\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0017327\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0017327\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0017327\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0017327\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0017327\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0017327\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0017327\n",
      "Final training Loss:  0.0017327\n",
      "\n",
      "Running model (trial=2, mod=76, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.55034655\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00124779\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00123028\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00319909\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00073717\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00031956\n",
      "################################  30  ################################\n",
      "Training Loss:  0.000109\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00011159\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  9.16e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  9.127e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  8.804e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00010626\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00011463\n",
      "################################  65  ################################\n",
      "Training Loss:  8.092e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  8.308e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  75  ################################\n",
      "Training Loss:  8.715e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  80  ################################\n",
      "Training Loss:  8.7e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  8.706e-05\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  8.708e-05\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  8.708e-05\n",
      "Final training Loss:  8.708e-05\n",
      "\n",
      "Running model (trial=2, mod=77, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.1046445\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00268174\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00095191\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00054001\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00021477\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00025684\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00027451\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00020876\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00017852\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00017461\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00016791\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00016775\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00016786\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00016786\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00016786\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00016786\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00016786\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00016786\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00016786\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00016786\n",
      "Final training Loss:  0.00016786\n",
      "\n",
      "Running model (trial=3, mod=78, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  58.0396347\n",
      "################################  5  ################################\n",
      "Training Loss:  57.26129532\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  57.65090561\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  58.19037628\n",
      "################################  20  ################################\n",
      "Training Loss:  55.5827179\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  56.19340897\n",
      "################################  30  ################################\n",
      "Training Loss:  55.86961365\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  55.48363113\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  55.48300934\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  55.48318481\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  55.48318481\n",
      "################################  55  ################################\n",
      "Training Loss:  55.48318481\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  55.48318481\n",
      "################################  65  ################################\n",
      "Training Loss:  55.48318481\n",
      "################################  70  ################################\n",
      "Training Loss:  55.48318481\n",
      "################################  75  ################################\n",
      "Training Loss:  55.48318481\n",
      "################################  80  ################################\n",
      "Training Loss:  55.48318481\n",
      "################################  85  ################################\n",
      "Training Loss:  55.48318481\n",
      "################################  90  ################################\n",
      "Training Loss:  55.48318481\n",
      "################################  95  ################################\n",
      "Training Loss:  55.48318481\n",
      "Final training Loss:  55.48318481\n",
      "\n",
      "Running model (trial=3, mod=79, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04179618\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0039941\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00038114\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00035526\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00059451\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00041741\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00032959\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00027583\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00043017\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00028856\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00032042\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00032041\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00032046\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00032028\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00032026\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00032026\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00032026\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00032026\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00032026\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00032026\n",
      "Final training Loss:  0.00032026\n",
      "\n",
      "Running model (trial=3, mod=80, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07278227\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00139741\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00103814\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00089264\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00023959\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00012343\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00012158\n",
      "################################  35  ################################\n",
      "Training Loss:  9.474e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  8.969e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  8.846e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  8.784e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  8.859e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.839e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  8.839e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  8.826e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  8.826e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  8.826e-05\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  8.826e-05\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  8.826e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  8.826e-05\n",
      "Final training Loss:  8.826e-05\n",
      "\n",
      "Running model (trial=3, mod=81, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.32792318\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00046396\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00013101\n",
      "################################  15  ################################\n",
      "Training Loss:  6.501e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  2.67e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  2.566e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  2.86e-05\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  2.753e-05\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.473e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  2.475e-05\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  2.476e-05\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  2.476e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  2.474e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  2.474e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  2.474e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  2.474e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  2.474e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  2.474e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  2.474e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.474e-05\n",
      "Final training Loss:  2.474e-05\n",
      "\n",
      "Running model (trial=3, mod=82, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06546155\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00264595\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00025087\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00017146\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00036787\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00026608\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00021531\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00020432\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00019617\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00019617\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00019617\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00019609\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00019609\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00019609\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00019609\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00019609\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00019609\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00019609\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00019609\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00019609\n",
      "Final training Loss:  0.00019609\n",
      "\n",
      "Running model (trial=3, mod=83, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.15624282\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00867532\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00377547\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00081297\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00100032\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00078874\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00060479\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00061908\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00061888\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00062791\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00062785\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00062775\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00062776\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00062776\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00062776\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00062776\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00062776\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00062776\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00062776\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00062776\n",
      "Final training Loss:  0.00062776\n",
      "\n",
      "Running model (trial=3, mod=84, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.07798672\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00219104\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00110091\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00082143\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00081861\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00039006\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00046174\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00050034\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00045998\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00034656\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00036517\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00037241\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00034038\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00032537\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00032537\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00032537\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00032544\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00032545\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00032545\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00032545\n",
      "Final training Loss:  0.00032545\n",
      "\n",
      "Running model (trial=3, mod=85, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01601647\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00886747\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00330873\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00336501\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00313496\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00401378\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00402632\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00304757\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00315352\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00315326\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00315458\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00315458\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00315455\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00315455\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00315455\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00315455\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00315455\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00315455\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00315455\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00315455\n",
      "Final training Loss:  0.00315455\n",
      "\n",
      "Running model (trial=3, mod=86, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03764448\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00218127\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00016849\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00015929\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00032399\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00014634\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  9.064e-05\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  8.773e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  8.448e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  8.442e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  8.442e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  8.435e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  8.43e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  8.424e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  8.425e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  8.422e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  8.422e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  8.422e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  8.42e-05\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  8.419e-05\n",
      "Final training Loss:  8.419e-05\n",
      "\n",
      "Running model (trial=3, mod=87, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00671882\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00028746\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00015327\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00021465\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00026703\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00017411\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0001484\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00014347\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00013663\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00013514\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00013964\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00013951\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0001394\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.0001394\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0001394\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0001394\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0001394\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0001394\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0001394\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0001394\n",
      "Final training Loss:  0.0001394\n",
      "\n",
      "Running model (trial=3, mod=88, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11047148\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00233972\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00055387\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00023955\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00033921\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00022763\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0002366\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0002386\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00023389\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00023457\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00023405\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00023031\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00023026\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00023026\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00023025\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00023025\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00023025\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00023025\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00023025\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00023025\n",
      "Final training Loss:  0.00023025\n",
      "\n",
      "Running model (trial=3, mod=89, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  20.73682594\n",
      "################################  5  ################################\n",
      "Training Loss:  0.11963281\n",
      "################################  10  ################################\n",
      "Training Loss:  0.09843396\n",
      "################################  15  ################################\n",
      "Training Loss:  0.02677665\n",
      "################################  20  ################################\n",
      "Training Loss:  0.01204736\n",
      "################################  25  ################################\n",
      "Training Loss:  0.01504066\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00859743\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00895685\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0089436\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00839402\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00816831\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00812508\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00812332\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00812332\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00813286\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00813295\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00813295\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00813295\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00813295\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00813295\n",
      "Final training Loss:  0.00813295\n",
      "\n",
      "Running model (trial=3, mod=90, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10180664\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10180664\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10180664\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10180664\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10180664\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10180664\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10180664\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10180664\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10180664\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10180664\n",
      "Final training Loss:  600.10180664\n",
      "\n",
      "Running model (trial=3, mod=91, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  102.98725128\n",
      "################################  5  ################################\n",
      "Training Loss:  2.31916285\n",
      "################################  10  ################################\n",
      "Training Loss:  0.15630922\n",
      "################################  15  ################################\n",
      "Training Loss:  0.15361412\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.15279751\n",
      "################################  25  ################################\n",
      "Training Loss:  0.14268094\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.14050034\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.14156476\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.13975902\n",
      "################################  45  ################################\n",
      "Training Loss:  0.1399705\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.13996698\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.1399671\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.1399671\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.1399671\n",
      "################################  70  ################################\n",
      "Training Loss:  0.1399671\n",
      "################################  75  ################################\n",
      "Training Loss:  0.1399671\n",
      "################################  80  ################################\n",
      "Training Loss:  0.1399671\n",
      "################################  85  ################################\n",
      "Training Loss:  0.1399671\n",
      "################################  90  ################################\n",
      "Training Loss:  0.1399671\n",
      "################################  95  ################################\n",
      "Training Loss:  0.1399671\n",
      "Final training Loss:  0.1399671\n",
      "\n",
      "Running model (trial=3, mod=92, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03498603\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00098721\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0008609\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.0008236\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00079762\n",
      "################################  25  ################################\n",
      "Training Loss:  8.6e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00013187\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  8.793e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  8.574e-05\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00010691\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00010674\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00010644\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00010644\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00010644\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00010644\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00010644\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00010644\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00010644\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00010644\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00010644\n",
      "Final training Loss:  0.00010644\n",
      "\n",
      "Running model (trial=3, mod=93, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7617833\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00102868\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00028067\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00044252\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00028449\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00027034\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00024149\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00021697\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00020892\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00020874\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0002086\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00020856\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00020856\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00020856\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00020856\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00020856\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00020856\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00020856\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00020856\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00020856\n",
      "Final training Loss:  0.00020856\n",
      "\n",
      "Running model (trial=3, mod=94, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=3, mod=95, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12.11508179\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00291747\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00317067\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00139262\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00126727\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00111653\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00164322\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00103852\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00097884\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00097749\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00097816\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00097812\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00097807\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00097807\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00097807\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00097807\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00097807\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00097807\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00097807\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00097807\n",
      "Final training Loss:  0.00097807\n",
      "\n",
      "Running model (trial=3, mod=96, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59533006\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01301038\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00550978\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00170059\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00309587\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00151482\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00154269\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00156035\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0015026\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00144139\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00144129\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.0014415\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00144145\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00144145\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00144145\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00144145\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00144145\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00144145\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00144145\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00144145\n",
      "Final training Loss:  0.00144145\n",
      "\n",
      "Running model (trial=3, mod=97, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  96.99710083\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00027638\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00032627\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0003273\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00029671\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00027405\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.000274\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00027384\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00027384\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00027385\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00027385\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00027385\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00027385\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00027385\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00027385\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00027385\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00027385\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00027385\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00027385\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00027385\n",
      "Final training Loss:  0.00027385\n",
      "\n",
      "Running model (trial=3, mod=98, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.79538333\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02028164\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02208149\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.02073596\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0190187\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.01937628\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.01895636\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.01895636\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.01895743\n",
      "################################  45  ################################\n",
      "Training Loss:  0.01895743\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.01895743\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.01895743\n",
      "################################  60  ################################\n",
      "Training Loss:  0.01895743\n",
      "################################  65  ################################\n",
      "Training Loss:  0.01895743\n",
      "################################  70  ################################\n",
      "Training Loss:  0.01895743\n",
      "################################  75  ################################\n",
      "Training Loss:  0.01895743\n",
      "################################  80  ################################\n",
      "Training Loss:  0.01895743\n",
      "################################  85  ################################\n",
      "Training Loss:  0.01895743\n",
      "################################  90  ################################\n",
      "Training Loss:  0.01895743\n",
      "################################  95  ################################\n",
      "Training Loss:  0.01895743\n",
      "Final training Loss:  0.01895743\n",
      "\n",
      "Running model (trial=3, mod=99, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  81.67745972\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00964988\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00695195\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00623417\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00720869\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00415775\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00511162\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00432083\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00431949\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00431865\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00431859\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00431859\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00431859\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00431859\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00431859\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00431859\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00431859\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00431859\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00431859\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00431859\n",
      "Final training Loss:  0.00431859\n",
      "\n",
      "Running model (trial=3, mod=100, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68822849\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0006245\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00016443\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00015037\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  9.789e-05\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00014461\n",
      "################################  30  ################################\n",
      "Training Loss:  7.979e-05\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  7.791e-05\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  7.775e-05\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  7.781e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  7.781e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  7.781e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  7.781e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  7.781e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  7.781e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  7.781e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  7.781e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  7.781e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  7.781e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  7.781e-05\n",
      "Final training Loss:  7.781e-05\n",
      "\n",
      "Running model (trial=3, mod=101, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11265247\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00241037\n",
      "################################  10  ################################\n",
      "Training Loss:  7.574e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  5.702e-05\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  4.423e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  4.19e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  5.179e-05\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  4.526e-05\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  4.05e-05\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  4.041e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  4.043e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  4.043e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  4.043e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  4.043e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  4.043e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  4.043e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  4.043e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  4.043e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  4.043e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  4.043e-05\n",
      "Final training Loss:  4.043e-05\n",
      "\n",
      "Running model (trial=3, mod=102, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5482446\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00046632\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00017258\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00016013\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00017797\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00027651\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00015089\n",
      "################################  35  ################################\n",
      "Training Loss:  9.419e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  5.821e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  5.434e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  50  ################################\n",
      "Training Loss:  5.894e-05\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  5.136e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  4.996e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  5.228e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  4.971e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  5.514e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  80  ################################\n",
      "Training Loss:  5.778e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  85  ################################\n",
      "Training Loss:  5.57e-05\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  5.568e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  5.567e-05\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Final training Loss:  5.569e-05\n",
      "\n",
      "Running model (trial=3, mod=103, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.14468981\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0010542\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00137715\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00147884\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00177677\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00104411\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00098074\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00098066\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00098066\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00098053\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00098054\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00098054\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00098054\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00098054\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00098054\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00098054\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00098054\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00098054\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00098054\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00098054\n",
      "Final training Loss:  0.00098054\n",
      "\n",
      "Running model (trial=4, mod=104, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.25296736\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0270651\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02186363\n",
      "################################  15  ################################\n",
      "Training Loss:  0.02172452\n",
      "################################  20  ################################\n",
      "Training Loss:  0.02203748\n",
      "################################  25  ################################\n",
      "Training Loss:  0.02479494\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.02110986\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0237299\n",
      "################################  40  ################################\n",
      "Training Loss:  0.02818058\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.02073587\n",
      "################################  50  ################################\n",
      "Training Loss:  0.02060771\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.02052792\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0205269\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.0205263\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0205263\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0205263\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0205263\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0205263\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0205263\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0205263\n",
      "Final training Loss:  0.0205263\n",
      "\n",
      "Running model (trial=4, mod=105, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.23980397\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01645795\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01389246\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00673378\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00325072\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00243923\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00258205\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00247596\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0033232\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0025102\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00244992\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00244973\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00244955\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00244955\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00244949\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00244949\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00244949\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00244949\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00244949\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00244949\n",
      "Final training Loss:  0.00244949\n",
      "\n",
      "Running model (trial=4, mod=106, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  102.54922485\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00741643\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00455282\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00330047\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00295912\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00335149\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00303034\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00298742\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00299863\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00291051\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0028309\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00282987\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00282978\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00282957\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00283018\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00282999\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00282999\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00282997\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00282997\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00282997\n",
      "Final training Loss:  0.00282997\n",
      "\n",
      "Running model (trial=4, mod=107, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03355177\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00062475\n",
      "################################  10  ################################\n",
      "Training Loss:  4.876e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  4.924e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00010363\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  7.131e-05\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.406e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  3.796e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  3.854e-05\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  3.929e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  3.927e-05\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  3.928e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  3.927e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  3.927e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  3.927e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  3.927e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  3.927e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  3.927e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  3.927e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  3.927e-05\n",
      "Final training Loss:  3.927e-05\n",
      "\n",
      "Running model (trial=4, mod=108, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.86402607\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00666836\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0059854\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00325219\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00354621\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00210945\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00140573\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00169945\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00143951\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00124582\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.0012487\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00124868\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00124876\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0012488\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00124879\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00124879\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00124879\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00124879\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00124879\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00124879\n",
      "Final training Loss:  0.00124879\n",
      "\n",
      "Running model (trial=4, mod=109, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.23876163\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00408993\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00461386\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0012644\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00123906\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00168315\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00169561\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00146009\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00121186\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0013562\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00124148\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00123741\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00123732\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00123717\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00123717\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00123717\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00123717\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00123717\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00123717\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00123717\n",
      "Final training Loss:  0.00123717\n",
      "\n",
      "Running model (trial=4, mod=110, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  112.83636475\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=111, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.17631698\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00066286\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00052294\n",
      "################################  15  ################################\n",
      "Training Loss:  6.385e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  7.057e-05\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  6.696e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  7.904e-05\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  6.249e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  6.254e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  6.236e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  6.315e-05\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  6.358e-05\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  6.3e-05\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  6.304e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  6.304e-05\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  6.304e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  6.304e-05\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  6.304e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  6.304e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  6.304e-05\n",
      "Final training Loss:  6.304e-05\n",
      "\n",
      "Running model (trial=4, mod=112, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00956154\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00028611\n",
      "################################  10  ################################\n",
      "Training Loss:  8.071e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  6.319e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  3.624e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  3.538e-05\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  3.223e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  3.464e-05\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  2.989e-05\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  2.982e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  2.976e-05\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  2.975e-05\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  2.975e-05\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  2.975e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  2.975e-05\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  2.975e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  2.975e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  2.975e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  2.975e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.975e-05\n",
      "Final training Loss:  2.975e-05\n",
      "\n",
      "Running model (trial=4, mod=113, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0471004\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00072392\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00026352\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00019017\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00018906\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00016154\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00022506\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00015517\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00072866\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0001595\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00014257\n",
      "################################  55  ################################\n",
      "Training Loss:  0.0001394\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00036304\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00036285\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00036292\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00036292\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00036292\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00036292\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00036292\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00036292\n",
      "Final training Loss:  0.00036292\n",
      "\n",
      "Running model (trial=4, mod=114, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03094493\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00270101\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00038924\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0001635\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00010427\n",
      "################################  25  ################################\n",
      "Training Loss:  3.319e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  3.419e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  3.023e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  1.848e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  2.152e-05\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  50  ################################\n",
      "Training Loss:  2.14e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  1.735e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  1.977e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  2.189e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  70  ################################\n",
      "Training Loss:  1.661e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  1.853e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  80  ################################\n",
      "Training Loss:  1.661e-05\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  85  ################################\n",
      "Training Loss:  1.661e-05\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  90  ################################\n",
      "Training Loss:  1.661e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  1.66e-05\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Final training Loss:  1.66e-05\n",
      "\n",
      "Running model (trial=4, mod=115, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  945.1048584\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00400496\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00389789\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00154724\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00032324\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00033255\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00030625\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00026246\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00026975\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00026536\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00026532\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00026524\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0002652\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0002652\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.0002652\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0002652\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0002652\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0002652\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0002652\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0002652\n",
      "Final training Loss:  0.0002652\n",
      "\n",
      "Running model (trial=4, mod=116, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10070801\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10070801\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10070801\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10070801\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10070801\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10070801\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10070801\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10070801\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10070801\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10070801\n",
      "Final training Loss:  600.10070801\n",
      "\n",
      "Running model (trial=4, mod=117, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.31313944\n",
      "################################  5  ################################\n",
      "Training Loss:  0.33556527\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.35228068\n",
      "################################  15  ################################\n",
      "Training Loss:  0.38688055\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.34111631\n",
      "################################  25  ################################\n",
      "Training Loss:  0.3358461\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.33992887\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.3399201\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.34013402\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.34013402\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.34013402\n",
      "################################  55  ################################\n",
      "Training Loss:  0.34013402\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.34013402\n",
      "################################  65  ################################\n",
      "Training Loss:  0.34013402\n",
      "################################  70  ################################\n",
      "Training Loss:  0.34013402\n",
      "################################  75  ################################\n",
      "Training Loss:  0.34013402\n",
      "################################  80  ################################\n",
      "Training Loss:  0.34013402\n",
      "################################  85  ################################\n",
      "Training Loss:  0.34013402\n",
      "################################  90  ################################\n",
      "Training Loss:  0.34013402\n",
      "################################  95  ################################\n",
      "Training Loss:  0.34013402\n",
      "Final training Loss:  0.34013402\n",
      "\n",
      "Running model (trial=4, mod=118, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.5248735\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02129454\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00789036\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00073882\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00064576\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00098684\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00071116\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00071661\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00063366\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00063372\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00063356\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00063356\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00063356\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00063356\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00063356\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00063356\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00063356\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00063356\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00063356\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00063356\n",
      "Final training Loss:  0.00063356\n",
      "\n",
      "Running model (trial=4, mod=119, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.16428995\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02399097\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0065989\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00525197\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00228611\n",
      "################################  25  ################################\n",
      "Training Loss:  0.001236\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0012578\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00108174\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00117941\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00102874\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00097143\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00100207\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0010128\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00101278\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00101278\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00101278\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00101278\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00101278\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00101278\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00101278\n",
      "Final training Loss:  0.00101278\n",
      "\n",
      "Running model (trial=4, mod=120, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.99507761\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00236612\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00090695\n",
      "################################  15  ################################\n",
      "Training Loss:  7.266e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  5.29e-05\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  3.296e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  5.081e-05\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  4.245e-05\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.034e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  3.004e-05\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  3.002e-05\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  3.001e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.002e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  3.002e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  3.002e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  3.002e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  3.002e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  3.002e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  3.002e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  3.002e-05\n",
      "Final training Loss:  3.002e-05\n",
      "\n",
      "Running model (trial=4, mod=121, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  91.61161804\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0966711\n",
      "################################  10  ################################\n",
      "Training Loss:  0.08956778\n",
      "################################  15  ################################\n",
      "Training Loss:  0.06445397\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.06675045\n",
      "################################  25  ################################\n",
      "Training Loss:  0.06314351\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.0650209\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0593273\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.06168627\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.05966149\n",
      "################################  50  ################################\n",
      "Training Loss:  0.05961227\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.0596186\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.05961557\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.05961151\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.05961157\n",
      "################################  75  ################################\n",
      "Training Loss:  0.05961157\n",
      "################################  80  ################################\n",
      "Training Loss:  0.05961157\n",
      "################################  85  ################################\n",
      "Training Loss:  0.05961157\n",
      "################################  90  ################################\n",
      "Training Loss:  0.05961157\n",
      "################################  95  ################################\n",
      "Training Loss:  0.05961157\n",
      "Final training Loss:  0.05961157\n",
      "\n",
      "Running model (trial=4, mod=122, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  43.66670609\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0549449\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02200015\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0131263\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0121935\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00412593\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00251948\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00181429\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00190129\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00199758\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00212154\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00166596\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00171645\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00178863\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00177238\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00174499\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00174654\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00174575\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00174578\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00174578\n",
      "Epoch   100: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  0.00174578\n",
      "\n",
      "Running model (trial=4, mod=123, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.15038949\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00110165\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00041733\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00032665\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00029115\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00029358\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00035242\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00030245\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00019324\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00034345\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00025103\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00015991\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00015084\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00015867\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0001588\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00015869\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00015868\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00015871\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00015871\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00015871\n",
      "Final training Loss:  0.00015871\n",
      "\n",
      "Running model (trial=4, mod=124, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.95009643\n",
      "################################  5  ################################\n",
      "Training Loss:  0.35739928\n",
      "################################  10  ################################\n",
      "Training Loss:  0.35223934\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00098143\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00108555\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00096155\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00077111\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00059983\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00059147\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00083819\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00057067\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00056231\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00056239\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00056395\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00056389\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00056389\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00056389\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00056389\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00056389\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00056389\n",
      "Final training Loss:  0.00056389\n",
      "\n",
      "Running model (trial=4, mod=125, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.09111159\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0110934\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00266931\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00238486\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00161966\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00115841\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00081318\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00069152\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00090527\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0006013\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00059323\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00059615\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00059586\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00059602\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.0005959\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00059589\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00059589\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00059589\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00059589\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00059589\n",
      "Final training Loss:  0.00059589\n",
      "\n",
      "Running model (trial=4, mod=126, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.21108837\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0021743\n",
      "################################  10  ################################\n",
      "Training Loss:  7.728e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  6.527e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  3.595e-05\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  4.113e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  3.361e-05\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.801e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  3.583e-05\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  3.533e-05\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  3.54e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  3.565e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.566e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  3.566e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  3.566e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  3.566e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  3.566e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  3.566e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  3.566e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  3.566e-05\n",
      "Final training Loss:  3.566e-05\n",
      "\n",
      "Running model (trial=4, mod=127, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.12144117\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0014613\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00032498\n",
      "################################  15  ################################\n",
      "Training Loss:  5.749e-05\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.0001303\n",
      "################################  25  ################################\n",
      "Training Loss:  7.167e-05\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  6.45e-05\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  7.181e-05\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  7.183e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  7.18e-05\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  7.178e-05\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  7.179e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  7.179e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  7.179e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  7.179e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  7.179e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  7.179e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  7.179e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  7.179e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  7.179e-05\n",
      "Final training Loss:  7.179e-05\n",
      "\n",
      "Running model (trial=4, mod=128, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  110.54692841\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=4, mod=129, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02994406\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00062769\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00055218\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0002982\n",
      "################################  20  ################################\n",
      "Training Loss:  6.25e-05\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  6.062e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  6.628e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  4.827e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  5.677e-05\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  4.733e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  4.707e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  4.863e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  4.859e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  4.855e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  4.855e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  4.854e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  4.854e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  4.854e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  4.854e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  4.854e-05\n",
      "Final training Loss:  4.854e-05\n",
      "\n",
      "Running model (trial=5, mod=130, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  118.40390778\n",
      "################################  5  ################################\n",
      "Training Loss:  126.8032608\n",
      "################################  10  ################################\n",
      "Training Loss:  8.20998478\n",
      "################################  15  ################################\n",
      "Training Loss:  3.24104404\n",
      "################################  20  ################################\n",
      "Training Loss:  0.503986\n",
      "################################  25  ################################\n",
      "Training Loss:  0.44998574\n",
      "################################  30  ################################\n",
      "Training Loss:  0.44339475\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.42136067\n",
      "################################  40  ################################\n",
      "Training Loss:  0.45477164\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.44209704\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.43223915\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.4309468\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.42182904\n",
      "################################  65  ################################\n",
      "Training Loss:  0.42205375\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.42205375\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.42205375\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.42205375\n",
      "################################  85  ################################\n",
      "Training Loss:  0.42205375\n",
      "################################  90  ################################\n",
      "Training Loss:  0.42205375\n",
      "################################  95  ################################\n",
      "Training Loss:  0.42205375\n",
      "Final training Loss:  0.42205375\n",
      "\n",
      "Running model (trial=5, mod=131, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.1393204\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0080735\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00352635\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00111771\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00105199\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00068195\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00101066\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00081415\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00072159\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00099171\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00068003\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00065898\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00065835\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00065835\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00065835\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00065835\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00065835\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00065835\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00065835\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00065835\n",
      "Final training Loss:  0.00065835\n",
      "\n",
      "Running model (trial=5, mod=132, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.96100426\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01415682\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01910469\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.01190041\n",
      "################################  20  ################################\n",
      "Training Loss:  0.01065752\n",
      "################################  25  ################################\n",
      "Training Loss:  0.01021115\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00908428\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00942911\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00875104\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00875019\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00875046\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.0087504\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00875044\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00875038\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00875038\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00875038\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00875038\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00875038\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00875038\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00875038\n",
      "Final training Loss:  0.00875038\n",
      "\n",
      "Running model (trial=5, mod=133, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03592431\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00250048\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0006471\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00019606\n",
      "################################  20  ################################\n",
      "Training Loss:  8.379e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  1.166e-05\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  2.963e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  1.955e-05\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  2.123e-05\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  2.175e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  2.171e-05\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  2.167e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  2.167e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  2.167e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  2.167e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  2.167e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  2.167e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  2.167e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  2.167e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.167e-05\n",
      "Final training Loss:  2.167e-05\n",
      "\n",
      "Running model (trial=5, mod=134, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.10489698\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00113995\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00083867\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00082951\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00076086\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00075285\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00079147\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00082791\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00082787\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00082722\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00082722\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00082722\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00082722\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00082722\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00082722\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00082722\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00082722\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00082722\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00082722\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00082722\n",
      "Final training Loss:  0.00082722\n",
      "\n",
      "Running model (trial=5, mod=135, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07453814\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00554155\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0009523\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00099294\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00069581\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00053905\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00076528\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00057831\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00052673\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00052334\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0005636\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00056357\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00056354\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00056348\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00056345\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00056339\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00056339\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00056339\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00056339\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00056339\n",
      "Final training Loss:  0.00056339\n",
      "\n",
      "Running model (trial=5, mod=136, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.94935358\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00296059\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00111267\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00092914\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00081045\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0010001\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00074261\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00071649\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00066709\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00066684\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00066678\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00066689\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00066722\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00066722\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00066722\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00066722\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00066722\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00066722\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00066722\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00066722\n",
      "Final training Loss:  0.00066722\n",
      "\n",
      "Running model (trial=5, mod=137, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.43156886\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01429966\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01540899\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.01384464\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0132393\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0137663\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.01311223\n",
      "################################  35  ################################\n",
      "Training Loss:  0.01303723\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.01303648\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.01303649\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.01303649\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.01303649\n",
      "################################  60  ################################\n",
      "Training Loss:  0.01303649\n",
      "################################  65  ################################\n",
      "Training Loss:  0.01303649\n",
      "################################  70  ################################\n",
      "Training Loss:  0.01303649\n",
      "################################  75  ################################\n",
      "Training Loss:  0.01303649\n",
      "################################  80  ################################\n",
      "Training Loss:  0.01303649\n",
      "################################  85  ################################\n",
      "Training Loss:  0.01303649\n",
      "################################  90  ################################\n",
      "Training Loss:  0.01303649\n",
      "################################  95  ################################\n",
      "Training Loss:  0.01303649\n",
      "Final training Loss:  0.01303649\n",
      "\n",
      "Running model (trial=5, mod=138, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  22.17390633\n",
      "################################  5  ################################\n",
      "Training Loss:  0.04008239\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01752112\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00105728\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00455988\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00102063\n",
      "################################  30  ################################\n",
      "Training Loss:  8.333e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  3.251e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  3.654e-05\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  3.579e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  1.976e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  2.324e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.954e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  1.956e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  1.888e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  1.959e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  80  ################################\n",
      "Training Loss:  1.93e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  85  ################################\n",
      "Training Loss:  1.932e-05\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  90  ################################\n",
      "Training Loss:  1.932e-05\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  1.932e-05\n",
      "Final training Loss:  1.932e-05\n",
      "\n",
      "Running model (trial=5, mod=139, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00217555\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00033692\n",
      "################################  10  ################################\n",
      "Training Loss:  8.811e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  2.668e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  3.402e-05\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  2.338e-05\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.21e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  2.47e-05\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  2.636e-05\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  2.158e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.157e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  2.157e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  2.157e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  2.157e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  2.157e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  2.157e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  2.157e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  2.157e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  2.157e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.157e-05\n",
      "Final training Loss:  2.157e-05\n",
      "\n",
      "Running model (trial=5, mod=140, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01097303\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00016164\n",
      "################################  10  ################################\n",
      "Training Loss:  9.77e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  9.281e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00010212\n",
      "################################  25  ################################\n",
      "Training Loss:  2.431e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  2.581e-05\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  2.766e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  1.41e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  2.357e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  1.234e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  1.28e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.191e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  1.106e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  1.114e-05\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.114e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  1.115e-05\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  1.114e-05\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  1.114e-05\n",
      "Epoch    94: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  1.114e-05\n",
      "Epoch   100: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  1.114e-05\n",
      "\n",
      "Running model (trial=5, mod=141, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  192.48257446\n",
      "################################  5  ################################\n",
      "Training Loss:  91.39264679\n",
      "################################  10  ################################\n",
      "Training Loss:  90.51413727\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  25113.51171875\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  7407.80078125\n",
      "################################  25  ################################\n",
      "Training Loss:  764.89050293\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  114.20774841\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  108.90133667\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  108.89913177\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  108.89919281\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  108.89919281\n",
      "################################  55  ################################\n",
      "Training Loss:  108.89919281\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  108.89919281\n",
      "################################  65  ################################\n",
      "Training Loss:  108.89919281\n",
      "################################  70  ################################\n",
      "Training Loss:  108.89919281\n",
      "################################  75  ################################\n",
      "Training Loss:  108.89919281\n",
      "################################  80  ################################\n",
      "Training Loss:  108.89919281\n",
      "################################  85  ################################\n",
      "Training Loss:  108.89919281\n",
      "################################  90  ################################\n",
      "Training Loss:  108.89919281\n",
      "################################  95  ################################\n",
      "Training Loss:  108.89919281\n",
      "Final training Loss:  108.89919281\n",
      "\n",
      "Running model (trial=5, mod=142, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10113525\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10113525\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10113525\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10119629\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10119629\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10119629\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10119629\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10119629\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10119629\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10119629\n",
      "Final training Loss:  600.10119629\n",
      "\n",
      "Running model (trial=5, mod=143, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.19904307\n",
      "################################  5  ################################\n",
      "Training Loss:  0.15015991\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.14159329\n",
      "################################  15  ################################\n",
      "Training Loss:  0.14422821\n",
      "################################  20  ################################\n",
      "Training Loss:  0.12129167\n",
      "################################  25  ################################\n",
      "Training Loss:  0.13599114\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.1169994\n",
      "################################  35  ################################\n",
      "Training Loss:  0.16848247\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.11899105\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.11888363\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.11888542\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.11887416\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.11887415\n",
      "################################  65  ################################\n",
      "Training Loss:  0.11887608\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.11887608\n",
      "################################  75  ################################\n",
      "Training Loss:  0.11887608\n",
      "################################  80  ################################\n",
      "Training Loss:  0.11887608\n",
      "################################  85  ################################\n",
      "Training Loss:  0.11887608\n",
      "################################  90  ################################\n",
      "Training Loss:  0.11887608\n",
      "################################  95  ################################\n",
      "Training Loss:  0.11887608\n",
      "Final training Loss:  0.11887608\n",
      "\n",
      "Running model (trial=5, mod=144, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.12214484\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00361218\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00064923\n",
      "################################  15  ################################\n",
      "Training Loss:  4.038e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  5.74e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  7.19e-05\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  2.887e-05\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  3.778e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  2.301e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  2.296e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  2.463e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  2.412e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  2.407e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  2.408e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  2.408e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  2.408e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  2.408e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  2.408e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  2.408e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.408e-05\n",
      "Final training Loss:  2.408e-05\n",
      "\n",
      "Running model (trial=5, mod=145, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.90011477\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00228167\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00216325\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00237571\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00047251\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00033436\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00031769\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00040584\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00031786\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00034562\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00032089\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00029821\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00030956\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00030045\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00030037\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00030065\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00030064\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00030064\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00030064\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00030064\n",
      "Final training Loss:  0.00030064\n",
      "\n",
      "Running model (trial=5, mod=146, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.31100062\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0273584\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01152422\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00385774\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00444719\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0017947\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0019521\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00196448\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00234805\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00225285\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.002239\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00223945\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00223913\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00223913\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00223913\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00223913\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00223913\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00223913\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00223913\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00223913\n",
      "Final training Loss:  0.00223913\n",
      "\n",
      "Running model (trial=5, mod=147, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.3930254\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00324341\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00125198\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0004024\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00061635\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00042458\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00074847\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00065162\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00034814\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00045822\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00033847\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00033826\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00033823\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00033826\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0003383\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0003383\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00033831\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00033831\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00033831\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00033831\n",
      "Final training Loss:  0.00033831\n",
      "\n",
      "Running model (trial=5, mod=148, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03365541\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00733839\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.0082591\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00410326\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0041186\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00410862\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00365941\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00384034\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00381331\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00381333\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00381331\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00381331\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00381332\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00381332\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00381332\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00381332\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00381332\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00381332\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00381332\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00381332\n",
      "Final training Loss:  0.00381332\n",
      "\n",
      "Running model (trial=5, mod=149, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  31.71477127\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00350645\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00374841\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00361313\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00131014\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00123398\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00170544\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00121108\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00134346\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00124747\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00111912\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.0011194\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00111882\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00111881\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00111882\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00111882\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00111882\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00111882\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00111882\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00111882\n",
      "Final training Loss:  0.00111882\n",
      "\n",
      "Running model (trial=5, mod=150, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.38667464\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01347148\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00880299\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0101838\n",
      "################################  20  ################################\n",
      "Training Loss:  0.01422583\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00903596\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.01098306\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00927305\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0085659\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00825913\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00825942\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00826007\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00826007\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00826294\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00826294\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00826294\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00826294\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00826294\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00826294\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00826294\n",
      "Final training Loss:  0.00826294\n",
      "\n",
      "Running model (trial=5, mod=151, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.494084\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01852835\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00970317\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00823995\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00801203\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00749874\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00763505\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00654006\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00642388\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00642089\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00642002\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00641985\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00641985\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00641985\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00641985\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00641985\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00641985\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00641985\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00641985\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00641985\n",
      "Final training Loss:  0.00641985\n",
      "\n",
      "Running model (trial=5, mod=152, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  236.7086792\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00216388\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00070159\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00094694\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00011626\n",
      "################################  25  ################################\n",
      "Training Loss:  4.347e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  2.983e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  3e-05\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  3.467e-05\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  2.922e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  2.898e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  2.874e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  2.793e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  2.809e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  2.803e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  2.802e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  2.8e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  2.8e-05\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  2.8e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.8e-05\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  2.8e-05\n",
      "\n",
      "Running model (trial=5, mod=153, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.16347361\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00036406\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00016142\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00012488\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00011124\n",
      "################################  25  ################################\n",
      "Training Loss:  6.76e-05\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  8.633e-05\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  9.52e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  6.247e-05\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  6.388e-05\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  6.138e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  6.14e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  6.139e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  6.139e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  6.139e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  6.139e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  6.139e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  6.139e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  6.139e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  6.139e-05\n",
      "Final training Loss:  6.139e-05\n",
      "\n",
      "Running model (trial=5, mod=154, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.24452953\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00286393\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00061779\n",
      "################################  15  ################################\n",
      "Training Loss:  4.376e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  3.675e-05\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  3.109e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  3.469e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  2.642e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  2.685e-05\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  2.71e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  2.631e-05\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  2.632e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  2.62e-05\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  2.632e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  2.632e-05\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  2.632e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  2.632e-05\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  2.632e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  2.632e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  2.632e-05\n",
      "Final training Loss:  2.632e-05\n",
      "\n",
      "Running model (trial=5, mod=155, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.40408337\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00998214\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00693153\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00761701\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00665943\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0071835\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00512486\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00509762\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00481779\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00481658\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00481548\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00481549\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00481579\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00481572\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00481573\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00481573\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00481573\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00481573\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00481573\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00481573\n",
      "Final training Loss:  0.00481573\n",
      "\n",
      "Running model (trial=6, mod=156, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.66797948\n",
      "################################  5  ################################\n",
      "Training Loss:  0.66418135\n",
      "################################  10  ################################\n",
      "Training Loss:  0.57424861\n",
      "################################  15  ################################\n",
      "Training Loss:  0.59672856\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.64055336\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.57893556\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.57831252\n",
      "################################  35  ################################\n",
      "Training Loss:  0.57831317\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.57871634\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.57871634\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.57871634\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.57871634\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.57871634\n",
      "################################  65  ################################\n",
      "Training Loss:  0.57871634\n",
      "################################  70  ################################\n",
      "Training Loss:  0.57871634\n",
      "################################  75  ################################\n",
      "Training Loss:  0.57871634\n",
      "################################  80  ################################\n",
      "Training Loss:  0.57871634\n",
      "################################  85  ################################\n",
      "Training Loss:  0.57871634\n",
      "################################  90  ################################\n",
      "Training Loss:  0.57871634\n",
      "################################  95  ################################\n",
      "Training Loss:  0.57871634\n",
      "Final training Loss:  0.57871634\n",
      "\n",
      "Running model (trial=6, mod=157, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.08911593\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00149682\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00148533\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00197121\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00010776\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00010794\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00010374\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00010599\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00010171\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  9.741e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00011171\n",
      "################################  55  ################################\n",
      "Training Loss:  9.914e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  9.221e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  9.22e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  9.22e-05\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  9.22e-05\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  9.22e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  9.22e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  9.22e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  9.22e-05\n",
      "Final training Loss:  9.22e-05\n",
      "\n",
      "Running model (trial=6, mod=158, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  16.75671196\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00978669\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00688191\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00033252\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00022136\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0001291\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00011111\n",
      "################################  35  ################################\n",
      "Training Loss:  8.659e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  9.73e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00011068\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00013519\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00011641\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  7.763e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  9.37e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  9.242e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  9.223e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  9.202e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  9.202e-05\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  9.202e-05\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  9.202e-05\n",
      "Final training Loss:  9.202e-05\n",
      "\n",
      "Running model (trial=6, mod=159, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.83712864\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00349707\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00041134\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00025529\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00045912\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00024125\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00038767\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0003701\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00027442\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00027441\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00027468\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.0002749\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0002749\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0002749\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0002749\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0002749\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0002749\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0002749\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0002749\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0002749\n",
      "Final training Loss:  0.0002749\n",
      "\n",
      "Running model (trial=6, mod=160, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.16236466\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00175239\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00063575\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00063226\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00147656\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00063995\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00077972\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00057822\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00057652\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00057658\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00057658\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00057652\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00057642\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00057643\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00057643\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00057643\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00057643\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00057643\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00057643\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00057643\n",
      "Final training Loss:  0.00057643\n",
      "\n",
      "Running model (trial=6, mod=161, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  21.61328888\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01167039\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00413554\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00138725\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00185223\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00173574\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00108802\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00140407\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00142276\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00106105\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00130865\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00128509\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00128509\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00128538\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0012854\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.0012854\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0012854\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0012854\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0012854\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0012854\n",
      "Final training Loss:  0.0012854\n",
      "\n",
      "Running model (trial=6, mod=162, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.33702961\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02897256\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00170413\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00139829\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0015381\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00098531\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0012966\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00176525\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00096768\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00085316\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00118078\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00096723\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00089674\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00089675\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00089686\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00089686\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00089686\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00089686\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00089686\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00089686\n",
      "Final training Loss:  0.00089686\n",
      "\n",
      "Running model (trial=6, mod=163, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  72.29646301\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0135982\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00028748\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00029718\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00030995\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00055141\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00027494\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0002825\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00032709\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00026166\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0002599\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00026033\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00026033\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00026033\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00026033\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00026033\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00026033\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00026033\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00026033\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00026033\n",
      "Final training Loss:  0.00026033\n",
      "\n",
      "Running model (trial=6, mod=164, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01980845\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00012625\n",
      "################################  10  ################################\n",
      "Training Loss:  5.215e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  2.307e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  1.461e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  1.399e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  1.652e-05\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  1.455e-05\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  1.446e-05\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  1.426e-05\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  1.425e-05\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  1.425e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  1.425e-05\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  1.425e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  1.425e-05\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  1.425e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  1.425e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  1.425e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  1.425e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  1.425e-05\n",
      "Final training Loss:  1.425e-05\n",
      "\n",
      "Running model (trial=6, mod=165, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.77506202\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00024671\n",
      "################################  10  ################################\n",
      "Training Loss:  6.967e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  8.984e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  5.025e-05\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  4.122e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  4.827e-05\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  5.255e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  6.507e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  3.803e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  6.513e-05\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  7.481e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  7.482e-05\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  7.481e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  7.481e-05\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  7.481e-05\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  7.481e-05\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  7.481e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  7.481e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  7.481e-05\n",
      "Final training Loss:  7.481e-05\n",
      "\n",
      "Running model (trial=6, mod=166, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69461197\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00133963\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00052656\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00017061\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00010221\n",
      "################################  25  ################################\n",
      "Training Loss:  4.838e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  4.037e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  4.461e-05\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  4.444e-05\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  4.205e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  6.263e-05\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  5.53e-05\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  5.531e-05\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  5.529e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  5.53e-05\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  5.53e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  5.53e-05\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  5.53e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  5.53e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  5.53e-05\n",
      "Final training Loss:  5.53e-05\n",
      "\n",
      "Running model (trial=6, mod=167, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  232.13337708\n",
      "################################  5  ################################\n",
      "Training Loss:  188.46861267\n",
      "Epoch    10: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  600.10125732\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  600.10125732\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10125732\n",
      "Final training Loss:  600.10125732\n",
      "\n",
      "Running model (trial=6, mod=168, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10144043\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  168.76074219\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  124.34475708\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10980225\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  600.10852051\n",
      "################################  30  ################################\n",
      "Training Loss:  600.10852051\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10852051\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10852051\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10852051\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10852051\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10852051\n",
      "Final training Loss:  600.10852051\n",
      "\n",
      "Running model (trial=6, mod=169, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  30.22134399\n",
      "################################  5  ################################\n",
      "Training Loss:  7.66986752\n",
      "Epoch     9: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  6.71000576\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  5.70800543\n",
      "################################  20  ################################\n",
      "Training Loss:  5.89029217\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  6.22873449\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  5.88860846\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  35  ################################\n",
      "Training Loss:  5.78147221\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  5.78182507\n",
      "################################  45  ################################\n",
      "Training Loss:  5.78182459\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  5.78182459\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  5.78182459\n",
      "################################  60  ################################\n",
      "Training Loss:  5.78182459\n",
      "################################  65  ################################\n",
      "Training Loss:  5.78182459\n",
      "################################  70  ################################\n",
      "Training Loss:  5.78182459\n",
      "################################  75  ################################\n",
      "Training Loss:  5.78182459\n",
      "################################  80  ################################\n",
      "Training Loss:  5.78182459\n",
      "################################  85  ################################\n",
      "Training Loss:  5.78182459\n",
      "################################  90  ################################\n",
      "Training Loss:  5.78182459\n",
      "################################  95  ################################\n",
      "Training Loss:  5.78182459\n",
      "Final training Loss:  5.78182459\n",
      "\n",
      "Running model (trial=6, mod=170, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  36.75469589\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00125416\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00119913\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.0027729\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00114334\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00109648\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00116824\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00105522\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0007746\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00092116\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00076409\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00073728\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00073732\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00073747\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00073747\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00073747\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00073747\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00073747\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00073747\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00073747\n",
      "Final training Loss:  0.00073747\n",
      "\n",
      "Running model (trial=6, mod=171, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.62032819\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00738348\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00050242\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00035466\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00027368\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00031794\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00012947\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00014728\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00024218\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00020214\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00016674\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00012562\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00013152\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00012143\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.0001215\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00012074\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00012079\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00012079\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00012079\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00012079\n",
      "Final training Loss:  0.00012079\n",
      "\n",
      "Running model (trial=6, mod=172, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  19.67819595\n",
      "################################  5  ################################\n",
      "Training Loss:  0.16435045\n",
      "################################  10  ################################\n",
      "Training Loss:  0.05983318\n",
      "################################  15  ################################\n",
      "Training Loss:  0.03270651\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00340122\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00125989\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00107042\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00111301\n",
      "################################  40  ################################\n",
      "Training Loss:  0.001197\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00114447\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0009317\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00095142\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00095598\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00095555\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00095541\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00095501\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00095526\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00095526\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00095526\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00095526\n",
      "Final training Loss:  0.00095526\n",
      "\n",
      "Running model (trial=6, mod=173, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  156.27166748\n",
      "################################  5  ################################\n",
      "Training Loss:  nan\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=6, mod=174, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.19061905\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00322142\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00085472\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0002271\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0002238\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00039627\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00022397\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00021782\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0002273\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00022571\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00021507\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00021496\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00021527\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00021527\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00021527\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00021527\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00021527\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00021527\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00021527\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00021527\n",
      "Final training Loss:  0.00021527\n",
      "\n",
      "Running model (trial=6, mod=175, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.98258352\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02928797\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01200822\n",
      "################################  15  ################################\n",
      "Training Loss:  0.01074339\n",
      "################################  20  ################################\n",
      "Training Loss:  0.01092328\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0046569\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0031735\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0029699\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00289266\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00324654\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00366261\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.0034162\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00279184\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0025983\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00259815\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00259825\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00259803\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00259803\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00259803\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00259803\n",
      "Final training Loss:  0.00259803\n",
      "\n",
      "Running model (trial=6, mod=176, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06827665\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00244474\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00122152\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00107593\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00112026\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00112439\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00105161\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00103555\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00103535\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0010352\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00103575\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00103579\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00103579\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00103579\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00103579\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00103579\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00103579\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00103579\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00103579\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00103579\n",
      "Final training Loss:  0.00103579\n",
      "\n",
      "Running model (trial=6, mod=177, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  46.89876175\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00082805\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00191591\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00084936\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00082027\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00081919\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00068277\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00072638\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00081562\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00071898\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00057356\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00056853\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00060266\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00057325\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00060377\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00058252\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00058252\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00058274\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00058242\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00058242\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  0.00058242\n",
      "\n",
      "Running model (trial=6, mod=178, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.3064853\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0067925\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00317291\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0004621\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00046369\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00048056\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00047707\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00037875\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00038007\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00040604\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00034791\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00034741\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00034746\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00034734\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00034733\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00034733\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00034733\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00034733\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00034733\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00034733\n",
      "Final training Loss:  0.00034733\n",
      "\n",
      "Running model (trial=6, mod=179, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.22866338\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00024135\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00019514\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00026132\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00021761\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00021644\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00020007\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00019117\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0001889\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00018892\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00018893\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00018885\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00018889\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00018889\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00018889\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00018889\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00018889\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00018889\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00018889\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00018889\n",
      "Final training Loss:  0.00018889\n",
      "\n",
      "Running model (trial=6, mod=180, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0164997\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00055287\n",
      "################################  10  ################################\n",
      "Training Loss:  4.952e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  8.291e-05\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  6.782e-05\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  4.559e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  5.051e-05\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  5.342e-05\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  5.324e-05\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  5.33e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  5.326e-05\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  5.33e-05\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  5.33e-05\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  5.33e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  5.33e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  5.33e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  5.33e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  5.33e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  5.33e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  5.33e-05\n",
      "Final training Loss:  5.33e-05\n",
      "\n",
      "Running model (trial=6, mod=181, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.20908265\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00162419\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00051462\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00021236\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00019817\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00015793\n",
      "################################  30  ################################\n",
      "Training Loss:  7.539e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  8.022e-05\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00010041\n",
      "################################  45  ################################\n",
      "Training Loss:  6.29e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  7.215e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  7.235e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  4.997e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  5.263e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  5.263e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  5.263e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  5.263e-05\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  5.263e-05\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  5.263e-05\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  5.263e-05\n",
      "Final training Loss:  5.263e-05\n",
      "\n",
      "Running model (trial=7, mod=182, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  5.33633137\n",
      "################################  5  ################################\n",
      "Training Loss:  4.00253582\n",
      "################################  10  ################################\n",
      "Training Loss:  4.63696957\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  4.03517103\n",
      "################################  20  ################################\n",
      "Training Loss:  4.70219612\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  4.09088802\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  4.12684107\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  4.12715435\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  4.12715435\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  4.1271596\n",
      "################################  50  ################################\n",
      "Training Loss:  4.1271596\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  4.1271596\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  4.1271596\n",
      "################################  65  ################################\n",
      "Training Loss:  4.1271596\n",
      "################################  70  ################################\n",
      "Training Loss:  4.1271596\n",
      "################################  75  ################################\n",
      "Training Loss:  4.1271596\n",
      "################################  80  ################################\n",
      "Training Loss:  4.1271596\n",
      "################################  85  ################################\n",
      "Training Loss:  4.1271596\n",
      "################################  90  ################################\n",
      "Training Loss:  4.1271596\n",
      "################################  95  ################################\n",
      "Training Loss:  4.1271596\n",
      "Final training Loss:  4.1271596\n",
      "\n",
      "Running model (trial=7, mod=183, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.36403343\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00160492\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00038179\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0004802\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00038409\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00013685\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00021381\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00010716\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00018713\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00010421\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00010284\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.0001028\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00010275\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00010269\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00010269\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00010269\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00010269\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00010269\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00010269\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00010269\n",
      "Final training Loss:  0.00010269\n",
      "\n",
      "Running model (trial=7, mod=184, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05141447\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00015\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00058216\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00019095\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00014049\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0001633\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00013452\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00013454\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00013457\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00013454\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00013452\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00013453\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00013453\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00013453\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00013453\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00013453\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00013453\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00013453\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00013453\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00013453\n",
      "Final training Loss:  0.00013453\n",
      "\n",
      "Running model (trial=7, mod=185, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.09800091\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00492044\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0003379\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00025129\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00029985\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00021023\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00022012\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00043036\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00033056\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00021814\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00021642\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.0002144\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00021439\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00021439\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00021439\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00021439\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00021439\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00021439\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00021439\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00021439\n",
      "Final training Loss:  0.00021439\n",
      "\n",
      "Running model (trial=7, mod=186, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.68323815\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01609042\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00238369\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00085816\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00046356\n",
      "################################  25  ################################\n",
      "Training Loss:  0.000493\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00062834\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00070972\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0004463\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00047037\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00045576\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00043188\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00043086\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00042959\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00042958\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00042961\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00042961\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00042961\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00042961\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00042961\n",
      "Final training Loss:  0.00042961\n",
      "\n",
      "Running model (trial=7, mod=187, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11430954\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00452852\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00421201\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00101881\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00085243\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00026138\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00016121\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00021503\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.000195\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00013851\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00015468\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00015425\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00015427\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00015427\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00015425\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00015425\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00015425\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00015425\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00015425\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00015425\n",
      "Final training Loss:  0.00015425\n",
      "\n",
      "Running model (trial=7, mod=188, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.34061468\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00305449\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00097175\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00035157\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0001397\n",
      "################################  25  ################################\n",
      "Training Loss:  9.728e-05\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00011329\n",
      "################################  35  ################################\n",
      "Training Loss:  8.957e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00014888\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00010219\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  9.958e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  9.95e-05\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  9.949e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  9.948e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  9.948e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  9.949e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  9.949e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  9.949e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  9.949e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  9.949e-05\n",
      "Final training Loss:  9.949e-05\n",
      "\n",
      "Running model (trial=7, mod=189, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.23496906\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01320666\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00187352\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00109364\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00080687\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00095938\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00076954\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00096537\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00083528\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00074381\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00084377\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00070156\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00070803\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00070773\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00070257\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00070273\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00070273\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00070273\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00070273\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00070273\n",
      "Final training Loss:  0.00070273\n",
      "\n",
      "Running model (trial=7, mod=190, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01313682\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00621507\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00060037\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00040576\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00033626\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00045598\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00039391\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00031931\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00033726\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00035756\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00035742\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00035742\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00035747\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00035747\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00035747\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00035747\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00035747\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00035747\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00035747\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00035747\n",
      "Final training Loss:  0.00035747\n",
      "\n",
      "Running model (trial=7, mod=191, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.14165275\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00159908\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00021296\n",
      "################################  15  ################################\n",
      "Training Loss:  5.233e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  2.475e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  3.077e-05\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  1.464e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  1.968e-05\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  1.876e-05\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  1.365e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.366e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  1.366e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.364e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  1.365e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  1.365e-05\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  1.365e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  1.365e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  1.365e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  1.365e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  1.365e-05\n",
      "Final training Loss:  1.365e-05\n",
      "\n",
      "Running model (trial=7, mod=192, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0512302\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00026355\n",
      "################################  10  ################################\n",
      "Training Loss:  9.703e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  8.545e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  4.93e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  5.203e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  4.227e-05\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  5.208e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  3.965e-05\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  3.762e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  3.417e-05\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.72e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  3.704e-05\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  3.702e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  3.702e-05\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  3.701e-05\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  3.702e-05\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  3.702e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  3.702e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  3.702e-05\n",
      "Final training Loss:  3.702e-05\n",
      "\n",
      "Running model (trial=7, mod=193, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2506.08544922\n",
      "################################  5  ################################\n",
      "Training Loss:  118.16477203\n",
      "################################  10  ################################\n",
      "Training Loss:  600.0814209\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.0814209\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.0814209\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.0814209\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  30  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.0814209\n",
      "################################  40  ################################\n",
      "Training Loss:  600.0814209\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.0814209\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.0814209\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  55  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  60  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  65  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  70  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  75  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  80  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  85  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  90  ################################\n",
      "Training Loss:  600.0814209\n",
      "################################  95  ################################\n",
      "Training Loss:  600.0814209\n",
      "Final training Loss:  600.0814209\n",
      "\n",
      "Running model (trial=7, mod=194, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10137939\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10137939\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10137939\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10137939\n",
      "Final training Loss:  600.10137939\n",
      "\n",
      "Running model (trial=7, mod=195, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  10.00613976\n",
      "################################  5  ################################\n",
      "Training Loss:  0.33116591\n",
      "################################  10  ################################\n",
      "Training Loss:  0.34279111\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.33559069\n",
      "################################  20  ################################\n",
      "Training Loss:  0.33491713\n",
      "################################  25  ################################\n",
      "Training Loss:  0.29741979\n",
      "################################  30  ################################\n",
      "Training Loss:  0.27980608\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.28718972\n",
      "################################  40  ################################\n",
      "Training Loss:  0.26937583\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.27302217\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.27643976\n",
      "################################  55  ################################\n",
      "Training Loss:  0.27638403\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.27638403\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.27638403\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.27638403\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.27638403\n",
      "################################  80  ################################\n",
      "Training Loss:  0.27638403\n",
      "################################  85  ################################\n",
      "Training Loss:  0.27638403\n",
      "################################  90  ################################\n",
      "Training Loss:  0.27638403\n",
      "################################  95  ################################\n",
      "Training Loss:  0.27638403\n",
      "Final training Loss:  0.27638403\n",
      "\n",
      "Running model (trial=7, mod=196, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.49552277\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00826292\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00326514\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00282509\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00262913\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00255317\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00259405\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00131425\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00128909\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00137639\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00128389\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00125194\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00128528\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00125232\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00126148\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00126137\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00126147\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0012613\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0012613\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0012613\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  0.0012613\n",
      "\n",
      "Running model (trial=7, mod=197, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06622667\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00039642\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00012819\n",
      "################################  15  ################################\n",
      "Training Loss:  9.243e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  9.261e-05\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  8.485e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  7.844e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  7.999e-05\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  7.55e-05\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  7.937e-05\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  7.688e-05\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  7.69e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  7.691e-05\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  7.691e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  7.691e-05\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  7.691e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  7.691e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  7.691e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  7.691e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  7.691e-05\n",
      "Final training Loss:  7.691e-05\n",
      "\n",
      "Running model (trial=7, mod=198, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.44751477\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00454283\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00703464\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00529314\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00408097\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00527518\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00390472\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00392263\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00391984\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00391976\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.0039204\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00392058\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00392058\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00392058\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00392058\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00392058\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00392058\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00392058\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00392058\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00392058\n",
      "Final training Loss:  0.00392058\n",
      "\n",
      "Running model (trial=7, mod=199, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.13544929\n",
      "################################  5  ################################\n",
      "Training Loss:  0.03505528\n",
      "################################  10  ################################\n",
      "Training Loss:  0.03755874\n",
      "################################  15  ################################\n",
      "Training Loss:  0.01396729\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00360478\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0009288\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00132858\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.0009467\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00080117\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00074888\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00073533\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00082292\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00069594\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00076009\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00069698\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0006951\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00069531\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00069531\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00069521\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00069523\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Final training Loss:  0.00069523\n",
      "\n",
      "Running model (trial=7, mod=200, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.75063068\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00347842\n",
      "################################  10  ################################\n",
      "Training Loss:  0.0021706\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00226545\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00188331\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00211699\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00170517\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00180986\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00178196\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0018692\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00182789\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00182543\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00182577\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00182536\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00182514\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00182514\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00182514\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00182514\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00182514\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00182514\n",
      "Final training Loss:  0.00182514\n",
      "\n",
      "Running model (trial=7, mod=201, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.10650766\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01333283\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00540225\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00332647\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00467397\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00255941\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00322272\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0048842\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00261127\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00252022\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00237532\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00227343\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00227344\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00227424\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00227405\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00227389\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00227388\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00227388\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00227388\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00227388\n",
      "Final training Loss:  0.00227388\n",
      "\n",
      "Running model (trial=7, mod=202, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.23267721\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01885836\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01502444\n",
      "################################  15  ################################\n",
      "Training Loss:  0.01574059\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00166033\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00030941\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00025085\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00041028\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00032343\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00025608\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00023944\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00023034\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.0002296\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00022973\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00023085\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.000231\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00023104\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00023104\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00023104\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00023104\n",
      "Final training Loss:  0.00023104\n",
      "\n",
      "Running model (trial=7, mod=203, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.30303892\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00923149\n",
      "################################  10  ################################\n",
      "Training Loss:  0.01073843\n",
      "################################  15  ################################\n",
      "Training Loss:  0.000938\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00034009\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00030369\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00013987\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00015923\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00011508\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00020035\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00010659\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00011718\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00011713\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00011689\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0001169\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0001169\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0001169\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0001169\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0001169\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0001169\n",
      "Final training Loss:  0.0001169\n",
      "\n",
      "Running model (trial=7, mod=204, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  781.49749756\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0089576\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00502162\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00159327\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00098092\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00087834\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00063555\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00052591\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00050109\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00092369\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00049744\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00029259\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00037958\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0002511\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00025174\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00025129\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00025103\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00025102\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0002511\n",
      "Epoch    95: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0002511\n",
      "Final training Loss:  0.0002511\n",
      "\n",
      "Running model (trial=7, mod=205, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.28403687\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00393658\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00024568\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0001564\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00012743\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00015439\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00012482\n",
      "Epoch    35: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00013548\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00011292\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00011083\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00011585\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00011577\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00011577\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00011577\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00011578\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00011579\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00011579\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00011579\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00011579\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00011579\n",
      "Final training Loss:  0.00011579\n",
      "\n",
      "Running model (trial=7, mod=206, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.28679705\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00055825\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00021052\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00018057\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00014405\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00017952\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00013565\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00015256\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00015128\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00014839\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00014835\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00014828\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00014828\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00014828\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00014828\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00014828\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00014828\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00014828\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00014828\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00014828\n",
      "Final training Loss:  0.00014828\n",
      "\n",
      "Running model (trial=7, mod=207, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.08327676\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00081707\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00039108\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00021922\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00031184\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00030621\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00020335\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0003342\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00019282\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00018576\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00026303\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00017939\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00017906\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00017904\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00017904\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00017904\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00017904\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00017904\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00017904\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00017904\n",
      "Final training Loss:  0.00017904\n",
      "\n",
      "Running model (trial=8, mod=208, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.42618632\n",
      "################################  5  ################################\n",
      "Training Loss:  0.29102075\n",
      "################################  10  ################################\n",
      "Training Loss:  0.28877941\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.29836437\n",
      "################################  20  ################################\n",
      "Training Loss:  0.28835493\n",
      "################################  25  ################################\n",
      "Training Loss:  0.31291547\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.29136443\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.30759823\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.29409528\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.29410264\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.29407808\n",
      "################################  55  ################################\n",
      "Training Loss:  0.29407808\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.29407808\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.29407808\n",
      "################################  70  ################################\n",
      "Training Loss:  0.29407808\n",
      "################################  75  ################################\n",
      "Training Loss:  0.29407808\n",
      "################################  80  ################################\n",
      "Training Loss:  0.29407808\n",
      "################################  85  ################################\n",
      "Training Loss:  0.29407808\n",
      "################################  90  ################################\n",
      "Training Loss:  0.29407808\n",
      "################################  95  ################################\n",
      "Training Loss:  0.29407808\n",
      "Final training Loss:  0.29407808\n",
      "\n",
      "Running model (trial=8, mod=209, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00893601\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00221556\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00221753\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00224904\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00168712\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00154509\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00149695\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00161295\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00537779\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00154284\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00150087\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00150087\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00150069\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00150069\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00150069\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00150069\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00150069\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00150069\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00150069\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00150069\n",
      "Final training Loss:  0.00150069\n",
      "\n",
      "Running model (trial=8, mod=210, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02029165\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00075747\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00032749\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00036001\n",
      "Epoch    20: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00033867\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00032196\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0003921\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00037097\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00037067\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00037066\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00037066\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00037067\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00037067\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00037067\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00037067\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00037067\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00037067\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00037067\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00037067\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00037067\n",
      "Final training Loss:  0.00037067\n",
      "\n",
      "Running model (trial=8, mod=211, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.19544008\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00039974\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00022342\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00018344\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00041486\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00021155\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00010154\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00013301\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  8.099e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00010777\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  8.41e-05\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  8.41e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.41e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  8.41e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  8.412e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  8.412e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  8.412e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  8.412e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  8.412e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  8.412e-05\n",
      "Final training Loss:  8.412e-05\n",
      "\n",
      "Running model (trial=8, mod=212, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.04942644\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00280805\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00098247\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00046743\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00056231\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00058643\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00057598\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00039327\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00033412\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0003339\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00033409\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00033429\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00033415\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00033414\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00033414\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00033414\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00033414\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00033414\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00033414\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00033414\n",
      "Final training Loss:  0.00033414\n",
      "\n",
      "Running model (trial=8, mod=213, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  8.65235138\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00562827\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00611648\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00535909\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00480509\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00482162\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.0052943\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00442849\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00487448\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00424992\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00428046\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00419837\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00419235\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00419576\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00419629\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00419596\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00419601\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00419601\n",
      "Epoch    88: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00419601\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00419601\n",
      "Final training Loss:  0.00419601\n",
      "\n",
      "Running model (trial=8, mod=214, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02362521\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00260787\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00086773\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00080817\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00076166\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00072104\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00070961\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00060032\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00059332\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0005901\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00065082\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00065001\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00064997\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00064993\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00065004\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00065004\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00065004\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00065004\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00065004\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00065004\n",
      "Final training Loss:  0.00065004\n",
      "\n",
      "Running model (trial=8, mod=215, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  18.55058289\n",
      "################################  5  ################################\n",
      "Training Loss:  0.03423516\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02711675\n",
      "################################  15  ################################\n",
      "Training Loss:  0.02639056\n",
      "################################  20  ################################\n",
      "Training Loss:  0.03802247\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.02786176\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.02713913\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.02794221\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.02794221\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.02794221\n",
      "################################  50  ################################\n",
      "Training Loss:  0.02794257\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.02794264\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.02794379\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.02794379\n",
      "################################  70  ################################\n",
      "Training Loss:  0.02794379\n",
      "################################  75  ################################\n",
      "Training Loss:  0.02794379\n",
      "################################  80  ################################\n",
      "Training Loss:  0.02794379\n",
      "################################  85  ################################\n",
      "Training Loss:  0.02794379\n",
      "################################  90  ################################\n",
      "Training Loss:  0.02794379\n",
      "################################  95  ################################\n",
      "Training Loss:  0.02794379\n",
      "Final training Loss:  0.02794379\n",
      "\n",
      "Running model (trial=8, mod=216, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.14829747\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00378704\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00219774\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00929245\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00060221\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00014034\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00013456\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  7.522e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00019628\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  9.662e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  7.232e-05\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  6.509e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  6.78e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  6.78e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  6.775e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  6.775e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  6.775e-05\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  6.775e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  6.775e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  6.775e-05\n",
      "Final training Loss:  6.775e-05\n",
      "\n",
      "Running model (trial=8, mod=217, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06870348\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00035071\n",
      "################################  10  ################################\n",
      "Training Loss:  9.787e-05\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00010325\n",
      "################################  20  ################################\n",
      "Training Loss:  2.096e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  1.265e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  9.05e-06\n",
      "################################  35  ################################\n",
      "Training Loss:  1.304e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  7.28e-06\n",
      "################################  45  ################################\n",
      "Training Loss:  8.74e-06\n",
      "################################  50  ################################\n",
      "Training Loss:  7.95e-06\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  55  ################################\n",
      "Training Loss:  7.68e-06\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.74e-06\n",
      "################################  65  ################################\n",
      "Training Loss:  6.99e-06\n",
      "################################  70  ################################\n",
      "Training Loss:  6.96e-06\n",
      "################################  75  ################################\n",
      "Training Loss:  6.86e-06\n",
      "################################  80  ################################\n",
      "Training Loss:  7.74e-06\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  6.92e-06\n",
      "################################  90  ################################\n",
      "Training Loss:  6.91e-06\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  95  ################################\n",
      "Training Loss:  6.91e-06\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Final training Loss:  6.91e-06\n",
      "\n",
      "Running model (trial=8, mod=218, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.1310441\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00020068\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00086366\n",
      "################################  15  ################################\n",
      "Training Loss:  4.456e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  5.789e-05\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  3.882e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  4.806e-05\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  3.87e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  3.823e-05\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  3.84e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  3.836e-05\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  3.838e-05\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.84e-05\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  3.84e-05\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  3.84e-05\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  3.84e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  3.84e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  3.84e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  3.84e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  3.84e-05\n",
      "Final training Loss:  3.84e-05\n",
      "\n",
      "Running model (trial=8, mod=219, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.7687993\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0656833\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02124981\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00055319\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00061772\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00010584\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00014585\n",
      "################################  35  ################################\n",
      "Training Loss:  9.534e-05\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0001428\n",
      "################################  45  ################################\n",
      "Training Loss:  8.345e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  9.581e-05\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  7.113e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  7.129e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  65  ################################\n",
      "Training Loss:  7.273e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  7.288e-05\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  7.333e-05\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  7.288e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  7.288e-05\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  7.288e-05\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  7.288e-05\n",
      "Final training Loss:  7.288e-05\n",
      "\n",
      "Running model (trial=8, mod=220, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10150146\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10144043\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10150146\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10150146\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10150146\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10150146\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10150146\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10150146\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10150146\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10150146\n",
      "Final training Loss:  600.10150146\n",
      "\n",
      "Running model (trial=8, mod=221, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.26164246\n",
      "################################  5  ################################\n",
      "Training Loss:  0.31982917\n",
      "################################  10  ################################\n",
      "Training Loss:  0.36248356\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.32208839\n",
      "################################  20  ################################\n",
      "Training Loss:  0.32779598\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.31815627\n",
      "################################  30  ################################\n",
      "Training Loss:  0.31283504\n",
      "################################  35  ################################\n",
      "Training Loss:  0.32742038\n",
      "################################  40  ################################\n",
      "Training Loss:  0.07873873\n",
      "################################  45  ################################\n",
      "Training Loss:  0.06570033\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.06609537\n",
      "################################  55  ################################\n",
      "Training Loss:  0.06644875\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.06295172\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.06295261\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.06295232\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.06295232\n",
      "################################  80  ################################\n",
      "Training Loss:  0.06295232\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.06295232\n",
      "################################  90  ################################\n",
      "Training Loss:  0.06295232\n",
      "################################  95  ################################\n",
      "Training Loss:  0.06295232\n",
      "Final training Loss:  0.06295232\n",
      "\n",
      "Running model (trial=8, mod=222, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.14785874\n",
      "################################  5  ################################\n",
      "Training Loss:  0.26054317\n",
      "################################  10  ################################\n",
      "Training Loss:  0.04911568\n",
      "################################  15  ################################\n",
      "Training Loss:  0.03595367\n",
      "################################  20  ################################\n",
      "Training Loss:  0.03583396\n",
      "################################  25  ################################\n",
      "Training Loss:  0.03713675\n",
      "Epoch    27: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.04403136\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.04312174\n",
      "################################  40  ################################\n",
      "Training Loss:  0.04491912\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.03815661\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.03588503\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.03393959\n",
      "################################  60  ################################\n",
      "Training Loss:  0.033939\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.033939\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.03393863\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.03393863\n",
      "################################  80  ################################\n",
      "Training Loss:  0.03393863\n",
      "################################  85  ################################\n",
      "Training Loss:  0.03393863\n",
      "################################  90  ################################\n",
      "Training Loss:  0.03393863\n",
      "################################  95  ################################\n",
      "Training Loss:  0.03393863\n",
      "Final training Loss:  0.03393863\n",
      "\n",
      "Running model (trial=8, mod=223, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  11.7034626\n",
      "################################  5  ################################\n",
      "Training Loss:  0.10067935\n",
      "################################  10  ################################\n",
      "Training Loss:  0.04763187\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00717534\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00588899\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.0060047\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00499482\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00533778\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0047308\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00464592\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00489164\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00465257\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00465057\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00465015\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00465013\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00465013\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00465013\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00465013\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00465013\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00465013\n",
      "Final training Loss:  0.00465013\n",
      "\n",
      "Running model (trial=8, mod=224, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.25957274\n",
      "################################  5  ################################\n",
      "Training Loss:  0.07135458\n",
      "################################  10  ################################\n",
      "Training Loss:  0.05079391\n",
      "################################  15  ################################\n",
      "Training Loss:  0.03809989\n",
      "################################  20  ################################\n",
      "Training Loss:  0.02920228\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00328609\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00083213\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00045477\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00032496\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00028845\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00033988\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00030478\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00035773\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00031586\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00024509\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00024839\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00024657\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00024624\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00024584\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.0002458\n",
      "Final training Loss:  0.0002458\n",
      "\n",
      "Running model (trial=8, mod=225, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  40.09843826\n",
      "################################  5  ################################\n",
      "Training Loss:  0.018327\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00834437\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00253617\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00139894\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00017235\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0001438\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00018358\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00013567\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00011559\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0001232\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00014422\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00013874\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00013882\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00013885\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00013882\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00013882\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00013882\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00013882\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00013882\n",
      "Final training Loss:  0.00013882\n",
      "\n",
      "Running model (trial=8, mod=226, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.08110429\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01100195\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00794658\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.01183909\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00984596\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0081336\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00824671\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00757754\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00741197\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00766212\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00770944\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00752562\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00752747\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00752696\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00752672\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00752672\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00752672\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00752672\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00752672\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00752672\n",
      "Final training Loss:  0.00752672\n",
      "\n",
      "Running model (trial=8, mod=227, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04833527\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00578951\n",
      "################################  10  ################################\n",
      "Epoch    11: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00643465\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00673997\n",
      "Epoch    17: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00617262\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00553893\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00574193\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00585238\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00585173\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00585173\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0058517\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00585181\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00585181\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00585181\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00585181\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00585181\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00585181\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00585181\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00585181\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00585181\n",
      "Final training Loss:  0.00585181\n",
      "\n",
      "Running model (trial=8, mod=228, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.43644819\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0290844\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00895147\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00744942\n",
      "################################  20  ################################\n",
      "Training Loss:  0.01436329\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00898473\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00966173\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.00917248\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0078382\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00713685\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00722392\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00722594\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00722451\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00722494\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00722494\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00722494\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00722494\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00722494\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00722494\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00722494\n",
      "Final training Loss:  0.00722494\n",
      "\n",
      "Running model (trial=8, mod=229, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.16053855\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00023266\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00025416\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00017986\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00012837\n",
      "################################  25  ################################\n",
      "Training Loss:  0.0002312\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00010645\n",
      "################################  35  ################################\n",
      "Training Loss:  6.193e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  9.222e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  9.679e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00011382\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  7.668e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  7.748e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  7.689e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  7.688e-05\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  7.688e-05\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  7.688e-05\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  7.688e-05\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  7.688e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  7.688e-05\n",
      "Final training Loss:  7.688e-05\n",
      "\n",
      "Running model (trial=8, mod=230, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.06251788\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02961051\n",
      "################################  10  ################################\n",
      "Training Loss:  0.06175187\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00395761\n",
      "################################  20  ################################\n",
      "Epoch    21: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00388302\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00431591\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0053776\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00317893\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0028111\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0020884\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00121775\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00104582\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00103024\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00103021\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00103018\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00103008\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00103005\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00102995\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00102988\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00102988\n",
      "Epoch    97: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  0.00102988\n",
      "\n",
      "Running model (trial=8, mod=231, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.09336106\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00202871\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00136981\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00086778\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00074356\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00056598\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00047636\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00044338\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00137645\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00038361\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0003999\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00041997\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00040616\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00040612\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00040607\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00040607\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00040607\n",
      "Epoch    82: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00040607\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00040607\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00040607\n",
      "Final training Loss:  0.00040607\n",
      "\n",
      "Running model (trial=8, mod=232, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11501233\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00468268\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00015635\n",
      "################################  15  ################################\n",
      "Training Loss:  8.995e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00012181\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  8.985e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  9.565e-05\n",
      "Epoch    32: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  35  ################################\n",
      "Training Loss:  8.306e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  8.303e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00015465\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  8.718e-05\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  8.72e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  8.716e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  8.715e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  8.715e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  8.715e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  8.715e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  8.715e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  8.715e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  8.715e-05\n",
      "Final training Loss:  8.715e-05\n",
      "\n",
      "Running model (trial=8, mod=233, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.50093663\n",
      "################################  5  ################################\n",
      "Training Loss:  0.000181\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00018546\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00012297\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00017603\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00014875\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00012057\n",
      "################################  35  ################################\n",
      "Training Loss:  9.885e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00011809\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  9.281e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  9.277e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  9.065e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  9.052e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  9.051e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  9.049e-05\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  9.049e-05\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  9.049e-05\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  9.049e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  9.049e-05\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  95  ################################\n",
      "Training Loss:  9.049e-05\n",
      "Final training Loss:  9.049e-05\n",
      "\n",
      "Running model (trial=9, mod=234, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.7137078\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02877912\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02571539\n",
      "################################  15  ################################\n",
      "Training Loss:  0.02313098\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.02134733\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.02192316\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.02248218\n",
      "################################  35  ################################\n",
      "Training Loss:  0.02229813\n",
      "Epoch    38: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.02038335\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.02040647\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.02040647\n",
      "################################  55  ################################\n",
      "Epoch    56: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.02040647\n",
      "################################  60  ################################\n",
      "Training Loss:  0.02040647\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.02040647\n",
      "################################  70  ################################\n",
      "Training Loss:  0.02040647\n",
      "################################  75  ################################\n",
      "Training Loss:  0.02040647\n",
      "################################  80  ################################\n",
      "Training Loss:  0.02040647\n",
      "################################  85  ################################\n",
      "Training Loss:  0.02040647\n",
      "################################  90  ################################\n",
      "Training Loss:  0.02040647\n",
      "################################  95  ################################\n",
      "Training Loss:  0.02040647\n",
      "Final training Loss:  0.02040647\n",
      "\n",
      "Running model (trial=9, mod=235, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.22446026\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00234269\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00229795\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00195631\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00179835\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00175402\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.0016347\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00168526\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00172112\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00171969\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00171971\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00171971\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00171971\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00171971\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00171971\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00171971\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00171971\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00171971\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00171971\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00171971\n",
      "Final training Loss:  0.00171971\n",
      "\n",
      "Running model (trial=9, mod=236, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.2714777\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00017679\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00017044\n",
      "################################  15  ################################\n",
      "Training Loss:  5.044e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  2.794e-05\n",
      "################################  25  ################################\n",
      "Training Loss:  3.41e-05\n",
      "################################  30  ################################\n",
      "Training Loss:  3.32e-05\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  2.127e-05\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  1.865e-05\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  1.98e-05\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  1.981e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  1.978e-05\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  1.979e-05\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  1.98e-05\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  1.98e-05\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  1.98e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  1.98e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  1.98e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  1.98e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  1.98e-05\n",
      "Final training Loss:  1.98e-05\n",
      "\n",
      "Running model (trial=9, mod=237, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.14312841\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00118682\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00113419\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00059133\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00062675\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00044839\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00053414\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00036535\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00012972\n",
      "################################  45  ################################\n",
      "Training Loss:  9.149e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  8.243e-05\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  7.805e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  5.644e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  5.981e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  5.335e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  5.333e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  5.332e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  85  ################################\n",
      "Training Loss:  5.339e-05\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  90  ################################\n",
      "Training Loss:  5.339e-05\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  5.339e-05\n",
      "Final training Loss:  5.339e-05\n",
      "\n",
      "Running model (trial=9, mod=238, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.98361474\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00661563\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00398208\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00450797\n",
      "################################  20  ################################\n",
      "Training Loss:  0.0034736\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00346054\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00316155\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0030107\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00306467\n",
      "Epoch    42: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00304761\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00304708\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.0030474\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0030474\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00304739\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00304739\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00304739\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00304739\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00304739\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00304739\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00304739\n",
      "Final training Loss:  0.00304739\n",
      "\n",
      "Running model (trial=9, mod=239, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.5756321\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00204544\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00207558\n",
      "Epoch    12: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00207756\n",
      "Epoch    18: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00219285\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00209973\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00191028\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00184806\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00189948\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.0018504\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00185005\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00185017\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.0018502\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.0018502\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0018502\n",
      "################################  75  ################################\n",
      "Training Loss:  0.0018502\n",
      "################################  80  ################################\n",
      "Training Loss:  0.0018502\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0018502\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0018502\n",
      "################################  95  ################################\n",
      "Training Loss:  0.0018502\n",
      "Final training Loss:  0.0018502\n",
      "\n",
      "Running model (trial=9, mod=240, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06645257\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00367804\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00167276\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00124117\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00127036\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00111495\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00138712\n",
      "Epoch    34: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00096351\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00086362\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0009002\n",
      "Epoch    47: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00086496\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00086152\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00086132\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00086169\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.0008613\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00086119\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00086122\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00086122\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00086122\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00086122\n",
      "Final training Loss:  0.00086122\n",
      "\n",
      "Running model (trial=9, mod=241, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05208102\n",
      "################################  5  ################################\n",
      "Training Loss:  0.04017704\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  0.03463767\n",
      "################################  15  ################################\n",
      "Training Loss:  0.04517788\n",
      "################################  20  ################################\n",
      "Training Loss:  0.03494022\n",
      "################################  25  ################################\n",
      "Epoch    26: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  0.03407484\n",
      "################################  30  ################################\n",
      "Training Loss:  0.03501155\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.03350092\n",
      "################################  40  ################################\n",
      "Training Loss:  0.03286668\n",
      "################################  45  ################################\n",
      "Training Loss:  0.03713816\n",
      "################################  50  ################################\n",
      "Training Loss:  0.03470819\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.03276956\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.03276917\n",
      "################################  65  ################################\n",
      "Training Loss:  0.03277106\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.03277044\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.03276528\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.03276528\n",
      "################################  85  ################################\n",
      "Training Loss:  0.03276528\n",
      "################################  90  ################################\n",
      "Training Loss:  0.03276528\n",
      "################################  95  ################################\n",
      "Training Loss:  0.03276528\n",
      "Final training Loss:  0.03276528\n",
      "\n",
      "Running model (trial=9, mod=242, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02879108\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00079109\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00016504\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00023149\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00012392\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00010904\n",
      "################################  30  ################################\n",
      "Training Loss:  7.238e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00013199\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  7.817e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  5.072e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  6.604e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  4.826e-05\n",
      "################################  60  ################################\n",
      "Training Loss:  5.491e-05\n",
      "Epoch    62: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  65  ################################\n",
      "Training Loss:  6.434e-05\n",
      "Epoch    68: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  70  ################################\n",
      "Training Loss:  5.276e-05\n",
      "Epoch    74: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  75  ################################\n",
      "Training Loss:  5.274e-05\n",
      "Epoch    80: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  80  ################################\n",
      "Training Loss:  5.271e-05\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  5.271e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  5.271e-05\n",
      "Epoch    92: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  95  ################################\n",
      "Training Loss:  5.271e-05\n",
      "Epoch    98: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Final training Loss:  5.271e-05\n",
      "\n",
      "Running model (trial=9, mod=243, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04182966\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00385564\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00109893\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00033423\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00015216\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00011697\n",
      "Epoch    29: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  7.714e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  8.87e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  6.816e-05\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  45  ################################\n",
      "Training Loss:  7.91e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  5.431e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  7.171e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  5.325e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  5.331e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  5.336e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  5.336e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  5.336e-05\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  5.336e-05\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  5.336e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  5.336e-05\n",
      "Final training Loss:  5.336e-05\n",
      "\n",
      "Running model (trial=9, mod=244, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.36978844\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00056562\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00055068\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00019976\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00010264\n",
      "################################  25  ################################\n",
      "Training Loss:  4.344e-05\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  5.755e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  5.006e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  3.466e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  3.518e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  3.585e-05\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  4.988e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.471e-05\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  65  ################################\n",
      "Training Loss:  3.461e-05\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  3.467e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  3.466e-05\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  3.466e-05\n",
      "Epoch    83: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  3.466e-05\n",
      "Epoch    89: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  90  ################################\n",
      "Training Loss:  3.466e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  3.466e-05\n",
      "Final training Loss:  3.466e-05\n",
      "\n",
      "Running model (trial=9, mod=245, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  116.76744843\n",
      "################################  5  ################################\n",
      "Training Loss:  108.91028595\n",
      "Epoch     8: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  118.94959259\n",
      "Epoch    14: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=246, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10131836\n",
      "################################  5  ################################\n",
      "Training Loss:  600.10119629\n",
      "Epoch     7: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  10  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    13: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  15  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    19: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  20  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  25  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  30  ################################\n",
      "Epoch    31: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  600.10125732\n",
      "################################  35  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  40  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  45  ################################\n",
      "Training Loss:  600.10125732\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  50  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  55  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  60  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  65  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  70  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  75  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  80  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  85  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  90  ################################\n",
      "Training Loss:  600.10125732\n",
      "################################  95  ################################\n",
      "Training Loss:  600.10125732\n",
      "Final training Loss:  600.10125732\n",
      "\n",
      "Running model (trial=9, mod=247, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  12.88355732\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00573393\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00358559\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00181712\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00141695\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00127853\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00121775\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00162916\n",
      "Epoch    40: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00288579\n",
      "################################  45  ################################\n",
      "Epoch    46: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00135327\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00206367\n",
      "Epoch    52: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00193265\n",
      "Epoch    58: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00193238\n",
      "Epoch    64: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00193266\n",
      "Epoch    70: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00193266\n",
      "################################  75  ################################\n",
      "Epoch    76: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.00193266\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00193266\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00193266\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00193266\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00193266\n",
      "Final training Loss:  0.00193266\n",
      "\n",
      "Running model (trial=9, mod=248, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.44088554\n",
      "################################  5  ################################\n",
      "Training Loss:  0.11125948\n",
      "################################  10  ################################\n",
      "Training Loss:  0.07881781\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.09713455\n",
      "################################  20  ################################\n",
      "Training Loss:  0.06361878\n",
      "Epoch    25: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.06138693\n",
      "################################  30  ################################\n",
      "Training Loss:  0.05797348\n",
      "################################  35  ################################\n",
      "Epoch    36: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.05632931\n",
      "################################  40  ################################\n",
      "Training Loss:  0.05587227\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0562268\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.05622766\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.05623296\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.05623474\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.05623474\n",
      "################################  70  ################################\n",
      "Training Loss:  0.05623474\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.05623474\n",
      "################################  80  ################################\n",
      "Training Loss:  0.05623474\n",
      "################################  85  ################################\n",
      "Training Loss:  0.05623474\n",
      "################################  90  ################################\n",
      "Training Loss:  0.05623474\n",
      "################################  95  ################################\n",
      "Training Loss:  0.05623474\n",
      "Final training Loss:  0.05623474\n",
      "\n",
      "Running model (trial=9, mod=249, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  18.50183487\n",
      "################################  5  ################################\n",
      "Training Loss:  0.11971647\n",
      "################################  10  ################################\n",
      "Training Loss:  0.06393046\n",
      "################################  15  ################################\n",
      "Training Loss:  0.07030215\n",
      "################################  20  ################################\n",
      "Training Loss:  0.04403384\n",
      "################################  25  ################################\n",
      "Training Loss:  0.03128314\n",
      "################################  30  ################################\n",
      "Training Loss:  0.02657623\n",
      "################################  35  ################################\n",
      "Training Loss:  0.08409138\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0241659\n",
      "Epoch    44: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.02540902\n",
      "Epoch    50: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.01862668\n",
      "################################  55  ################################\n",
      "Training Loss:  0.02431107\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.02105659\n",
      "################################  65  ################################\n",
      "Training Loss:  0.01870613\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.01870006\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.01868228\n",
      "Epoch    79: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.01868228\n",
      "Epoch    85: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.01868228\n",
      "################################  90  ################################\n",
      "Epoch    91: reducing learning rate of group 0 to 1.0000e-08.\n",
      "Training Loss:  0.01868228\n",
      "################################  95  ################################\n",
      "Training Loss:  0.01868228\n",
      "Final training Loss:  0.01868228\n",
      "\n",
      "Running model (trial=9, mod=250, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  24.05250549\n",
      "################################  5  ################################\n",
      "Training Loss:  0.10512416\n",
      "################################  10  ################################\n",
      "Training Loss:  0.02937459\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00773974\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00694246\n",
      "Epoch    23: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00666046\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00597374\n",
      "################################  35  ################################\n",
      "Training Loss:  0.0071195\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0059953\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00578532\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00592508\n",
      "Epoch    53: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00587566\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00587513\n",
      "Epoch    65: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00587445\n",
      "################################  70  ################################\n",
      "Epoch    71: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  0.00587446\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00587446\n",
      "Epoch    77: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00587446\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00587446\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00587446\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00587446\n",
      "Final training Loss:  0.00587446\n",
      "\n",
      "Running model (trial=9, mod=251, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.73758507\n",
      "################################  5  ################################\n",
      "Training Loss:  0.02034684\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00047072\n",
      "################################  15  ################################\n",
      "Epoch    16: reducing learning rate of group 0 to 1.0000e-01.\n",
      "Training Loss:  0.00063617\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00054614\n",
      "Epoch    22: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00038729\n",
      "Epoch    28: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00037783\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00036291\n",
      "################################  40  ################################\n",
      "Epoch    41: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00036275\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00036276\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00036263\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.0003627\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-06.\n",
      "Training Loss:  0.00036267\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00036267\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00036267\n",
      "Epoch    73: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00036267\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00036267\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00036267\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00036267\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00036267\n",
      "Final training Loss:  0.00036267\n",
      "\n",
      "Running model (trial=9, mod=252, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.26444671\n",
      "################################  5  ################################\n",
      "Training Loss:  0.00642477\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00725942\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00589943\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00698065\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00395953\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00303873\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00235715\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00186345\n",
      "################################  45  ################################\n",
      "Training Loss:  0.0018085\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00186083\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00208522\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00177644\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00096853\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00099142\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00077299\n",
      "################################  80  ################################\n",
      "Training Loss:  0.000801\n",
      "################################  85  ################################\n",
      "Epoch    86: reducing learning rate of group 0 to 1.0000e-03.\n",
      "Training Loss:  0.00100012\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00074782\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00074773\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Final training Loss:  0.00074773\n",
      "\n",
      "Running model (trial=9, mod=253, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=254, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 1, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  2.04532123\n",
      "################################  5  ################################\n",
      "Training Loss:  0.04521941\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00347837\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00294979\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00115945\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00012807\n",
      "################################  30  ################################\n",
      "Training Loss:  6.792e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  4.643e-05\n",
      "################################  40  ################################\n",
      "Training Loss:  3.665e-05\n",
      "################################  45  ################################\n",
      "Training Loss:  2.736e-05\n",
      "################################  50  ################################\n",
      "Training Loss:  3.131e-05\n",
      "################################  55  ################################\n",
      "Training Loss:  4.866e-05\n",
      "Epoch    59: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  60  ################################\n",
      "Training Loss:  3.105e-05\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-02.\n",
      "Training Loss:  2.097e-05\n",
      "################################  70  ################################\n",
      "Training Loss:  1.933e-05\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  75  ################################\n",
      "Training Loss:  1.93e-05\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  80  ################################\n",
      "Training Loss:  1.941e-05\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  85  ################################\n",
      "Training Loss:  1.942e-05\n",
      "Epoch    90: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  90  ################################\n",
      "Training Loss:  1.934e-05\n",
      "################################  95  ################################\n",
      "Epoch    96: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  1.934e-05\n",
      "Final training Loss:  1.934e-05\n",
      "\n",
      "Running model (trial=9, mod=255, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  1.98465419\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0009493\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00020702\n",
      "################################  15  ################################\n",
      "Training Loss:  7.702e-05\n",
      "################################  20  ################################\n",
      "Training Loss:  7.587e-05\n",
      "Epoch    24: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  25  ################################\n",
      "Training Loss:  8.359e-05\n",
      "Epoch    30: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  30  ################################\n",
      "Training Loss:  6.006e-05\n",
      "################################  35  ################################\n",
      "Training Loss:  6.161e-05\n",
      "Epoch    37: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  40  ################################\n",
      "Training Loss:  6.159e-05\n",
      "Epoch    43: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  45  ################################\n",
      "Training Loss:  6.161e-05\n",
      "Epoch    49: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  50  ################################\n",
      "Training Loss:  6.158e-05\n",
      "Epoch    55: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  55  ################################\n",
      "Training Loss:  6.158e-05\n",
      "################################  60  ################################\n",
      "Epoch    61: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Training Loss:  6.157e-05\n",
      "################################  65  ################################\n",
      "Training Loss:  6.157e-05\n",
      "Epoch    67: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  70  ################################\n",
      "Training Loss:  6.157e-05\n",
      "################################  75  ################################\n",
      "Training Loss:  6.157e-05\n",
      "################################  80  ################################\n",
      "Training Loss:  6.157e-05\n",
      "################################  85  ################################\n",
      "Training Loss:  6.157e-05\n",
      "################################  90  ################################\n",
      "Training Loss:  6.157e-05\n",
      "################################  95  ################################\n",
      "Training Loss:  6.157e-05\n",
      "Final training Loss:  6.157e-05\n",
      "\n",
      "Running model (trial=9, mod=256, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.41083813\n",
      "################################  5  ################################\n",
      "Training Loss:  0.03082949\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00519044\n",
      "Epoch    15: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00259778\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00098953\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00083473\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00070809\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00064311\n",
      "################################  40  ################################\n",
      "Training Loss:  0.0006749\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00061934\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.0005735\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00057398\n",
      "Epoch    60: reducing learning rate of group 0 to 1.0000e-04.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00057369\n",
      "################################  65  ################################\n",
      "Epoch    66: reducing learning rate of group 0 to 1.0000e-05.\n",
      "Training Loss:  0.00057369\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00057423\n",
      "Epoch    72: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00057425\n",
      "Epoch    78: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00057423\n",
      "Epoch    84: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00057429\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00057429\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00057429\n",
      "Final training Loss:  0.00057429\n",
      "\n",
      "Running model (trial=9, mod=257, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Final training Loss:  nan\n",
      "\n",
      "Running model (trial=9, mod=258, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.35218954\n",
      "################################  5  ################################\n",
      "Training Loss:  0.01281237\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00670981\n",
      "################################  15  ################################\n",
      "Training Loss:  0.0059176\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00376014\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00159946\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00081734\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00058787\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00061883\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00060129\n",
      "Epoch    48: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  50  ################################\n",
      "Training Loss:  0.00076739\n",
      "Epoch    54: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00055351\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00057015\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00057388\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00062676\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00054048\n",
      "################################  80  ################################\n",
      "Epoch    81: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00054097\n",
      "################################  85  ################################\n",
      "Training Loss:  0.0005408\n",
      "Epoch    87: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  90  ################################\n",
      "Training Loss:  0.0005408\n",
      "Epoch    93: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00054081\n",
      "Epoch    99: reducing learning rate of group 0 to 1.0000e-07.\n",
      "Final training Loss:  0.00054081\n",
      "\n",
      "Running model (trial=9, mod=259, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'neural_reparam.ResNet.ResNet'>}, {'batch_size': 64, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x12a3945e0>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x12addb310>, 'optimizer': 'strong_wolfe', 'num_epochs': 100, 'learning_rate': 1, 'lr_scheduler': <function <lambda> at 0x15b7a3a60>})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.08955582\n",
      "################################  5  ################################\n",
      "Training Loss:  0.0079049\n",
      "################################  10  ################################\n",
      "Training Loss:  0.00132538\n",
      "################################  15  ################################\n",
      "Training Loss:  0.00066473\n",
      "################################  20  ################################\n",
      "Training Loss:  0.00032083\n",
      "################################  25  ################################\n",
      "Training Loss:  0.00016582\n",
      "################################  30  ################################\n",
      "Training Loss:  0.00026146\n",
      "Epoch    33: reducing learning rate of group 0 to 1.0000e-01.\n",
      "################################  35  ################################\n",
      "Training Loss:  0.00025691\n",
      "Epoch    39: reducing learning rate of group 0 to 1.0000e-02.\n",
      "################################  40  ################################\n",
      "Training Loss:  0.00018136\n",
      "Epoch    45: reducing learning rate of group 0 to 1.0000e-03.\n",
      "################################  45  ################################\n",
      "Training Loss:  0.00017461\n",
      "################################  50  ################################\n",
      "Epoch    51: reducing learning rate of group 0 to 1.0000e-04.\n",
      "Training Loss:  0.00017465\n",
      "################################  55  ################################\n",
      "Training Loss:  0.00017465\n",
      "Epoch    57: reducing learning rate of group 0 to 1.0000e-05.\n",
      "################################  60  ################################\n",
      "Training Loss:  0.00017465\n",
      "Epoch    63: reducing learning rate of group 0 to 1.0000e-06.\n",
      "################################  65  ################################\n",
      "Training Loss:  0.00017464\n",
      "Epoch    69: reducing learning rate of group 0 to 1.0000e-07.\n",
      "################################  70  ################################\n",
      "Training Loss:  0.00017464\n",
      "Epoch    75: reducing learning rate of group 0 to 1.0000e-08.\n",
      "################################  75  ################################\n",
      "Training Loss:  0.00017464\n",
      "################################  80  ################################\n",
      "Training Loss:  0.00017464\n",
      "################################  85  ################################\n",
      "Training Loss:  0.00017464\n",
      "################################  90  ################################\n",
      "Training Loss:  0.00017464\n",
      "################################  95  ################################\n",
      "Training Loss:  0.00017464\n",
      "Final training Loss:  0.00017464\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "x_train_ = x_train.detach()\n",
    "x_sorted, indices = torch.sort(x_train_, dim=0)\n",
    "plot_kwargs = {\n",
    "    \"x_test\": x_sorted,\n",
    "    \"x_train\": x_sorted,\n",
    "    \"y_train\": c1.ksi(x_sorted),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"Analytical solution\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: divide by zero encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: overflow encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n",
      "/usr/local/lib/python3.9/site-packages/numpy/lib/function_base.py:2197: RuntimeWarning: invalid value encountered in <lambda> (vectorized)\n",
      "  outputs = ufunc(*inputs)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers )(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: no_penalty_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "\n",
    "# make data frame\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type})\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 393.378125 262.19625\" width=\"393.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T04:11:12.784746</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 393.378125 262.19625 \nL 393.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \nL 386.178125 7.2 \nL 51.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m06e5b8a2b5\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"184.381387\" xlink:href=\"#m06e5b8a2b5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(175.581387 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"352.893739\" xlink:href=\"#m06e5b8a2b5\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(344.093739 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"m114e9091ca\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"66.596307\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"96.269859\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"117.32358\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"133.654114\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"146.997132\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"158.278493\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"168.050852\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"176.670684\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"235.108659\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"264.782212\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"285.835932\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_14\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"302.166466\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_15\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"315.509484\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_16\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"326.790845\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_17\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"336.563205\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_18\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"345.183037\" xlink:href=\"#m114e9091ca\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Neurons -->\n     <g transform=\"translate(198.016406 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 544 1381 \nL 544 3500 \nL 1119 3500 \nL 1119 1403 \nQ 1119 906 1312 657 \nQ 1506 409 1894 409 \nQ 2359 409 2629 706 \nQ 2900 1003 2900 1516 \nL 2900 3500 \nL 3475 3500 \nL 3475 0 \nL 2900 0 \nL 2900 538 \nQ 2691 219 2414 64 \nQ 2138 -91 1772 -91 \nQ 1169 -91 856 284 \nQ 544 659 544 1381 \nz\nM 1991 3584 \nL 1991 3584 \nz\n\" id=\"DejaVuSans-75\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"74.804688\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"136.328125\" xlink:href=\"#DejaVuSans-75\"/>\n      <use x=\"199.707031\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"238.570312\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"299.751953\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"363.130859\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_19\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m640f8801fd\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m640f8801fd\" y=\"213.359675\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{-4}}$ -->\n      <g transform=\"translate(20.878125 217.158894)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m640f8801fd\" y=\"174.503562\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{-3}}$ -->\n      <g transform=\"translate(20.878125 178.302781)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m640f8801fd\" y=\"135.64745\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(20.878125 139.446669)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m640f8801fd\" y=\"96.791338\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(20.878125 100.590556)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m640f8801fd\" y=\"57.935225\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(26.778125 61.734444)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m640f8801fd\" y=\"19.079113\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(26.778125 22.878332)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_25\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m3d18ef475c\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"221.979855\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"219.378563\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"217.125221\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"215.137633\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"201.662819\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"194.820598\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"189.965964\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"186.200418\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"183.123742\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"180.52245\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"178.269109\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"176.281521\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"162.806707\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"155.964485\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"151.109852\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"147.344305\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"144.26763\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"141.666338\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"139.412996\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"137.425408\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"123.950595\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_28\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"117.108373\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_29\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"112.253739\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_30\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"108.488193\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_31\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"105.411518\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_32\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"102.810226\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_33\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"100.556884\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_34\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"98.569296\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_35\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"85.094482\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_36\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"78.252261\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_37\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"73.397627\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_38\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"69.632081\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_39\">\n     <g id=\"line2d_57\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"66.555405\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_40\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"63.954113\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_41\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"61.700772\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_42\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"59.713183\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_43\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"46.23837\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_44\">\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"39.396148\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_45\">\n     <g id=\"line2d_63\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"34.541515\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_46\">\n     <g id=\"line2d_64\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"30.775968\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_47\">\n     <g id=\"line2d_65\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"27.699293\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_48\">\n     <g id=\"line2d_66\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"25.098001\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_49\">\n     <g id=\"line2d_67\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"22.844659\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_50\">\n     <g id=\"line2d_68\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"20.857071\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_51\">\n     <g id=\"line2d_69\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m3d18ef475c\" y=\"7.382257\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_10\">\n     <!-- $E$ -->\n     <g transform=\"translate(14.798438 119.12)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 66.596307 56.233437 \nL 66.596307 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 117.32358 189.570646 \nL 117.32358 173.636386 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 168.050852 189.855155 \nL 168.050852 166.766061 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 218.778125 214.756364 \nL 218.778125 202.41114 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 269.505398 188.447514 \nL 269.505398 172.257938 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 320.23267 176.677459 \nL 320.23267 164.422988 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 370.959943 188.391378 \nL 370.959943 180.203886 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 66.596307 89.898484 \nL 66.596307 45.425853 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 117.32358 153.228814 \nL 117.32358 126.406067 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 168.050852 176.450901 \nL 168.050852 154.151071 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 218.778125 170.751863 \nL 218.778125 155.970601 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 269.505398 184.772096 \nL 269.505398 130.581538 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 320.23267 171.727894 \nL 320.23267 156.798214 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 370.959943 173.13017 \nL 370.959943 158.114383 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_70\">\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 66.596307 27.473314 \nL 117.32358 179.988342 \nL 168.050852 174.767077 \nL 218.778125 207.697209 \nL 269.505398 179.185829 \nL 320.23267 169.871299 \nL 370.959943 183.876996 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_71\">\n    <path clip-path=\"url(#p8b99a3b08a)\" d=\"M 66.596307 55.591452 \nL 117.32358 136.610557 \nL 168.050852 162.523546 \nL 218.778125 161.873821 \nL 269.505398 141.650818 \nL 320.23267 162.587741 \nL 370.959943 164.165385 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_72\"/>\n   <g id=\"line2d_73\"/>\n   <g id=\"line2d_74\"/>\n   <g id=\"line2d_75\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 224.64 \nL 51.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.178125 224.64 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 7.2 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 311.7625 59.234375 \nL 379.178125 59.234375 \nQ 381.178125 59.234375 381.178125 57.234375 \nL 381.178125 14.2 \nQ 381.178125 12.2 379.178125 12.2 \nL 311.7625 12.2 \nQ 309.7625 12.2 309.7625 14.2 \nL 309.7625 57.234375 \nQ 309.7625 59.234375 311.7625 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_11\">\n     <!-- model -->\n     <g transform=\"translate(329.900781 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_76\">\n     <path d=\"M 313.7625 34.976562 \nL 333.7625 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_77\"/>\n    <g id=\"text_12\">\n     <!-- FFNN -->\n     <g transform=\"translate(341.7625 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_78\">\n     <path d=\"M 313.7625 49.654687 \nL 333.7625 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_79\"/>\n    <g id=\"text_13\">\n     <!-- ResNet -->\n     <g transform=\"translate(341.7625 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p8b99a3b08a\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"51.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "'../figures/curve_1/eks_7'"
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "6 160\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 1]), len(d_results_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 393.378125 262.19625\" width=\"393.378125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T04:08:19.685624</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 393.378125 262.19625 \nL 393.378125 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \nL 386.178125 7.2 \nL 51.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"m7d02146bc9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.596307\" xlink:href=\"#m7d02146bc9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(57.796307 239.238437)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"268.81113\" xlink:href=\"#m7d02146bc9\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(260.01113 239.238437)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 2 \n\" id=\"md84bcb7400\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"57.343464\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"127.469034\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"163.077297\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"188.341761\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"207.938402\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"223.950024\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_9\">\n     <g id=\"line2d_9\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"237.487657\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_10\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"249.214489\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_11\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"259.558287\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_12\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"329.683857\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_13\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"365.29212\" xlink:href=\"#md84bcb7400\" y=\"224.64\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_3\">\n     <!-- Hidden layers -->\n     <g transform=\"translate(184.296875 252.916562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 1259 4666 \nL 1259 2753 \nL 3553 2753 \nL 3553 4666 \nL 4184 4666 \nL 4184 0 \nL 3553 0 \nL 3553 2222 \nL 1259 2222 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-48\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 3500 \nL 1178 3500 \nL 1178 0 \nL 603 0 \nL 603 3500 \nz\nM 603 4863 \nL 1178 4863 \nL 1178 4134 \nL 603 4134 \nL 603 4863 \nz\n\" id=\"DejaVuSans-69\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3513 2113 \nL 3513 0 \nL 2938 0 \nL 2938 2094 \nQ 2938 2591 2744 2837 \nQ 2550 3084 2163 3084 \nQ 1697 3084 1428 2787 \nQ 1159 2491 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1366 3272 1645 3428 \nQ 1925 3584 2291 3584 \nQ 2894 3584 3203 3211 \nQ 3513 2838 3513 2113 \nz\n\" id=\"DejaVuSans-6e\" transform=\"scale(0.015625)\"/>\n       <path id=\"DejaVuSans-20\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-48\"/>\n      <use x=\"75.195312\" xlink:href=\"#DejaVuSans-69\"/>\n      <use x=\"102.978516\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"166.455078\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"229.931641\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"291.455078\" xlink:href=\"#DejaVuSans-6e\"/>\n      <use x=\"354.833984\" xlink:href=\"#DejaVuSans-20\"/>\n      <use x=\"386.621094\" xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"414.404297\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"475.683594\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"534.863281\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"596.386719\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"637.5\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_14\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"mb954f4ea31\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#mb954f4ea31\" y=\"204.97816\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- $\\mathdefault{10^{-4}}$ -->\n      <g transform=\"translate(20.878125 208.777379)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#mb954f4ea31\" y=\"173.664703\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- $\\mathdefault{10^{-3}}$ -->\n      <g transform=\"translate(20.878125 177.463921)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#mb954f4ea31\" y=\"142.351245\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(20.878125 146.150464)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_17\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#mb954f4ea31\" y=\"111.037787\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(20.878125 114.837006)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#mb954f4ea31\" y=\"79.72433\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(26.778125 83.523548)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#mb954f4ea31\" y=\"48.410872\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(26.778125 52.210091)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#mb954f4ea31\" y=\"17.097414\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(26.778125 20.896633)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_21\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"mf52ecaaff5\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"221.351302\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"217.439038\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"214.40445\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"211.925012\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"209.828676\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"208.012748\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"206.410986\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"195.55187\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"190.037844\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"186.12558\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"183.090993\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"180.611554\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"178.515219\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"176.69929\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"175.097528\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"164.238413\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"158.724386\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"154.812123\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"151.777535\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"149.298096\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_28\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"147.201761\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_29\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"145.385833\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_30\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"143.78407\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_31\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"132.924955\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_32\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"127.410929\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_33\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"123.498665\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_34\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"120.464077\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_35\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"117.984639\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_36\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"115.888303\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_37\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"114.072375\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_38\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"112.470613\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_39\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"101.611497\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_40\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"96.097471\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_41\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"92.185207\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_42\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"89.15062\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_43\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"86.671181\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_44\">\n     <g id=\"line2d_57\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"84.574846\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_45\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"82.758917\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_46\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"81.157155\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_47\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"70.29804\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_48\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"64.784013\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_49\">\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"60.871749\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_50\">\n     <g id=\"line2d_63\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"57.837162\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_51\">\n     <g id=\"line2d_64\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"55.357723\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_52\">\n     <g id=\"line2d_65\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"53.261388\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_53\">\n     <g id=\"line2d_66\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"51.445459\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_54\">\n     <g id=\"line2d_67\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"49.843697\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_55\">\n     <g id=\"line2d_68\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"38.984582\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_56\">\n     <g id=\"line2d_69\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"33.470556\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_57\">\n     <g id=\"line2d_70\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"29.558292\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_58\">\n     <g id=\"line2d_71\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"26.523704\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_59\">\n     <g id=\"line2d_72\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"24.044266\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_60\">\n     <g id=\"line2d_73\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"21.94793\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_61\">\n     <g id=\"line2d_74\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"20.132002\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_62\">\n     <g id=\"line2d_75\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"18.530239\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_63\">\n     <g id=\"line2d_76\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#mf52ecaaff5\" y=\"7.671124\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_11\">\n     <!-- $E$ -->\n     <g transform=\"translate(14.798438 119.12)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1081 4666 \nL 4031 4666 \nL 3928 4134 \nL 1606 4134 \nL 1338 2753 \nL 3566 2753 \nL 3463 2222 \nL 1234 2222 \nL 909 531 \nL 3284 531 \nL 3181 0 \nL 172 0 \nL 1081 4666 \nz\n\" id=\"DejaVuSans-Oblique-45\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use transform=\"translate(0 0.09375)\" xlink:href=\"#DejaVuSans-Oblique-45\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"LineCollection_1\">\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 66.596307 157.07459 \nL 66.596307 138.682263 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 127.469034 186.490805 \nL 127.469034 166.774361 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 188.341761 212.032247 \nL 188.341761 193.202397 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 249.214489 214.756364 \nL 249.214489 198.45861 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 310.087216 46.955426 \nL 310.087216 24.860858 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 370.959943 17.083812 \nL 370.959943 17.083636 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"LineCollection_2\">\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 66.596307 159.53991 \nL 66.596307 145.827444 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 127.469034 174.485582 \nL 127.469034 156.908339 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 188.341761 199.639953 \nL 188.341761 186.525381 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 249.214489 200.541522 \nL 249.214489 181.906886 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 310.087216 211.555232 \nL 310.087216 196.800194 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 370.959943 198.129825 \nL 370.959943 170.942576 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_77\">\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 66.596307 145.560272 \nL 127.469034 173.877064 \nL 188.341761 200.097539 \nL 249.214489 204.451486 \nL 310.087216 31.664619 \nL 370.959943 17.083718 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_78\">\n    <path clip-path=\"url(#p08bb5c4a09)\" d=\"M 66.596307 150.954658 \nL 127.469034 164.010218 \nL 188.341761 191.936662 \nL 249.214489 188.6731 \nL 310.087216 202.396005 \nL 370.959943 178.991436 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n   </g>\n   <g id=\"line2d_79\"/>\n   <g id=\"line2d_80\"/>\n   <g id=\"line2d_81\"/>\n   <g id=\"line2d_82\"/>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 224.64 \nL 51.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.178125 224.64 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 7.2 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 58.378125 59.234375 \nL 125.79375 59.234375 \nQ 127.79375 59.234375 127.79375 57.234375 \nL 127.79375 14.2 \nQ 127.79375 12.2 125.79375 12.2 \nL 58.378125 12.2 \nQ 56.378125 12.2 56.378125 14.2 \nL 56.378125 57.234375 \nQ 56.378125 59.234375 58.378125 59.234375 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_12\">\n     <!-- model -->\n     <g transform=\"translate(76.516406 23.798437)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"line2d_83\">\n     <path d=\"M 60.378125 34.976562 \nL 80.378125 34.976562 \n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_84\"/>\n    <g id=\"text_13\">\n     <!-- FFNN -->\n     <g transform=\"translate(88.378125 38.476562)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"line2d_85\">\n     <path d=\"M 60.378125 49.654687 \nL 80.378125 49.654687 \n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\n    </g>\n    <g id=\"line2d_86\"/>\n    <g id=\"text_14\">\n     <!-- ResNet -->\n     <g transform=\"translate(88.378125 53.154687)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"p08bb5c4a09\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"51.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Hidden layers\", ylabel=\"Error\")\n",
    "fig_layers.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "faled, total:\n",
      "13 140\n"
     ]
    }
   ],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 1]), len(d_results_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg height=\"262.19625pt\" version=\"1.1\" viewBox=\"0 0 404.936323 262.19625\" width=\"404.936323pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n <metadata>\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2022-01-14T04:08:21.197934</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.4.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M 0 262.19625 \nL 404.936323 262.19625 \nL 404.936323 0 \nL 0 0 \nz\n\" style=\"fill:none;\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \nL 386.178125 7.2 \nL 51.378125 7.2 \nz\n\" style=\"fill:#ffffff;\"/>\n   </g>\n   <g id=\"PathCollection_1\">\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 203.019771 \nC 67.752451 203.019771 68.515581 202.703672 69.078162 202.141091 \nC 69.640743 201.578511 69.956842 200.81538 69.956842 200.019771 \nC 69.956842 199.224162 69.640743 198.461032 69.078162 197.898451 \nC 68.515581 197.33587 67.752451 197.019771 66.956842 197.019771 \nC 66.161233 197.019771 65.398102 197.33587 64.835521 197.898451 \nC 64.272941 198.461032 63.956842 199.224162 63.956842 200.019771 \nC 63.956842 200.81538 64.272941 201.578511 64.835521 202.141091 \nC 65.398102 202.703672 66.161233 203.019771 66.956842 203.019771 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 147.137263 \nC 68.906163 147.137263 69.669293 146.821164 70.231874 146.258583 \nC 70.794455 145.696002 71.110554 144.932872 71.110554 144.137263 \nC 71.110554 143.341653 70.794455 142.578523 70.231874 142.015942 \nC 69.669293 141.453361 68.906163 141.137263 68.110554 141.137263 \nC 67.314944 141.137263 66.551814 141.453361 65.989233 142.015942 \nC 65.426653 142.578523 65.110554 143.341653 65.110554 144.137263 \nC 65.110554 144.932872 65.426653 145.696002 65.989233 146.258583 \nC 66.551814 146.821164 67.314944 147.137263 68.110554 147.137263 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 169.390848 \nC 72.944155 169.390848 73.707285 169.074749 74.269866 168.512168 \nC 74.832447 167.949587 75.148546 167.186457 75.148546 166.390848 \nC 75.148546 165.595239 74.832447 164.832108 74.269866 164.269528 \nC 73.707285 163.706947 72.944155 163.390848 72.148546 163.390848 \nC 71.352936 163.390848 70.589806 163.706947 70.027225 164.269528 \nC 69.464645 164.832108 69.148546 165.595239 69.148546 166.390848 \nC 69.148546 167.186457 69.464645 167.949587 70.027225 168.512168 \nC 70.589806 169.074749 71.352936 169.390848 72.148546 169.390848 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 188.576678 \nC 87.94241 188.576678 88.705541 188.26058 89.268121 187.697999 \nC 89.830702 187.135418 90.146801 186.372288 90.146801 185.576678 \nC 90.146801 184.781069 89.830702 184.017939 89.268121 183.455358 \nC 88.705541 182.892777 87.94241 182.576678 87.146801 182.576678 \nC 86.351192 182.576678 85.588061 182.892777 85.025481 183.455358 \nC 84.4629 184.017939 84.146801 184.781069 84.146801 185.576678 \nC 84.146801 186.372288 84.4629 187.135418 85.025481 187.697999 \nC 85.588061 188.26058 86.351192 188.576678 87.146801 188.576678 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 216.060979 \nC 145.628008 216.060979 146.391139 215.74488 146.953719 215.182299 \nC 147.5163 214.619719 147.832399 213.856588 147.832399 213.060979 \nC 147.832399 212.26537 147.5163 211.502239 146.953719 210.939659 \nC 146.391139 210.377078 145.628008 210.060979 144.832399 210.060979 \nC 144.03679 210.060979 143.273659 210.377078 142.711079 210.939659 \nC 142.148498 211.502239 141.832399 212.26537 141.832399 213.060979 \nC 141.832399 213.856588 142.148498 214.619719 142.711079 215.182299 \nC 143.273659 215.74488 144.03679 216.060979 144.832399 216.060979 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 157.391319 \nC 371.755552 157.391319 372.518683 157.075221 373.081264 156.51264 \nC 373.643844 155.950059 373.959943 155.186929 373.959943 154.391319 \nC 373.959943 153.59571 373.643844 152.83258 373.081264 152.269999 \nC 372.518683 151.707418 371.755552 151.391319 370.959943 151.391319 \nC 370.164334 151.391319 369.401204 151.707418 368.838623 152.269999 \nC 368.276042 152.83258 367.959943 153.59571 367.959943 154.391319 \nC 367.959943 155.186929 368.276042 155.950059 368.838623 156.51264 \nC 369.401204 157.075221 370.164334 157.391319 370.959943 157.391319 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 172.189329 \nC 67.608237 172.189329 68.371367 171.87323 68.933948 171.310649 \nC 69.496529 170.748069 69.812628 169.984938 69.812628 169.189329 \nC 69.812628 168.39372 69.496529 167.630589 68.933948 167.068009 \nC 68.371367 166.505428 67.608237 166.189329 66.812628 166.189329 \nC 66.017019 166.189329 65.253888 166.505428 64.691307 167.068009 \nC 64.128727 167.630589 63.812628 168.39372 63.812628 169.189329 \nC 63.812628 169.984938 64.128727 170.748069 64.691307 171.310649 \nC 65.253888 171.87323 66.017019 172.189329 66.812628 172.189329 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 182.439308 \nC 68.906163 182.439308 69.669293 182.123209 70.231874 181.560629 \nC 70.794455 180.998048 71.110554 180.234918 71.110554 179.439308 \nC 71.110554 178.643699 70.794455 177.880569 70.231874 177.317988 \nC 69.669293 176.755407 68.906163 176.439308 68.110554 176.439308 \nC 67.314944 176.439308 66.551814 176.755407 65.989233 177.317988 \nC 65.426653 177.880569 65.110554 178.643699 65.110554 179.439308 \nC 65.110554 180.234918 65.426653 180.998048 65.989233 181.560629 \nC 66.551814 182.123209 67.314944 182.439308 68.110554 182.439308 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 200.619151 \nC 71.502015 200.619151 72.265145 200.303052 72.827726 199.740471 \nC 73.390307 199.177891 73.706406 198.41476 73.706406 197.619151 \nC 73.706406 196.823542 73.390307 196.060412 72.827726 195.497831 \nC 72.265145 194.93525 71.502015 194.619151 70.706406 194.619151 \nC 69.910796 194.619151 69.147666 194.93525 68.585085 195.497831 \nC 68.022505 196.060412 67.706406 196.823542 67.706406 197.619151 \nC 67.706406 198.41476 68.022505 199.177891 68.585085 199.740471 \nC 69.147666 200.303052 69.910796 200.619151 70.706406 200.619151 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 197.768735 \nC 76.693719 197.768735 77.456849 197.452636 78.01943 196.890055 \nC 78.582011 196.327474 78.898109 195.564344 78.898109 194.768735 \nC 78.898109 193.973126 78.582011 193.209995 78.01943 192.647414 \nC 77.456849 192.084834 76.693719 191.768735 75.898109 191.768735 \nC 75.1025 191.768735 74.33937 192.084834 73.776789 192.647414 \nC 73.214208 193.209995 72.898109 193.973126 72.898109 194.768735 \nC 72.898109 195.564344 73.214208 196.327474 73.776789 196.890055 \nC 74.33937 197.452636 75.1025 197.768735 75.898109 197.768735 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 155.196566 \nC 87.077126 155.196566 87.840257 154.880467 88.402837 154.317886 \nC 88.965418 153.755305 89.281517 152.992175 89.281517 152.196566 \nC 89.281517 151.400956 88.965418 150.637826 88.402837 150.075245 \nC 87.840257 149.512665 87.077126 149.196566 86.281517 149.196566 \nC 85.485908 149.196566 84.722778 149.512665 84.160197 150.075245 \nC 83.597616 150.637826 83.281517 151.400956 83.281517 152.196566 \nC 83.281517 152.992175 83.597616 153.755305 84.160197 154.317886 \nC 84.722778 154.880467 85.485908 155.196566 86.281517 155.196566 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.003913 \nC 107.843942 21.003913 108.607072 20.687814 109.169653 20.125233 \nC 109.732233 19.562653 110.048332 18.799522 110.048332 18.003913 \nC 110.048332 17.208304 109.732233 16.445173 109.169653 15.882593 \nC 108.607072 15.320012 107.843942 15.003913 107.048332 15.003913 \nC 106.252723 15.003913 105.489593 15.320012 104.927012 15.882593 \nC 104.364431 16.445173 104.048332 17.208304 104.048332 18.003913 \nC 104.048332 18.799522 104.364431 19.562653 104.927012 20.125233 \nC 105.489593 20.687814 106.252723 21.003913 107.048332 21.003913 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 141.222491 \nL 66.596307 139.722491 \nL 68.096307 141.222491 \nL 69.596307 139.722491 \nL 68.096307 138.222491 \nL 69.596307 136.722491 \nL 68.096307 135.222491 \nL 66.596307 136.722491 \nL 65.096307 135.222491 \nL 63.596307 136.722491 \nL 65.096307 138.222491 \nL 63.596307 139.722491 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 207.999897 \nL 66.956842 206.499897 \nL 68.456842 207.999897 \nL 69.956842 206.499897 \nL 68.456842 204.999897 \nL 69.956842 203.499897 \nL 68.456842 201.999897 \nL 66.956842 203.499897 \nL 65.456842 201.999897 \nL 63.956842 203.499897 \nL 65.456842 204.999897 \nL 63.956842 206.499897 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 196.747937 \nL 68.110554 195.247937 \nL 69.610554 196.747937 \nL 71.110554 195.247937 \nL 69.610554 193.747937 \nL 71.110554 192.247937 \nL 69.610554 190.747937 \nL 68.110554 192.247937 \nL 66.610554 190.747937 \nL 65.110554 192.247937 \nL 66.610554 193.747937 \nL 65.110554 195.247937 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 151.045188 \nL 87.146801 149.545188 \nL 88.646801 151.045188 \nL 90.146801 149.545188 \nL 88.646801 148.045188 \nL 90.146801 146.545188 \nL 88.646801 145.045188 \nL 87.146801 146.545188 \nL 85.646801 145.045188 \nL 84.146801 146.545188 \nL 85.646801 148.045188 \nL 84.146801 149.545188 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 165.717545 \nL 144.832399 164.217545 \nL 146.332399 165.717545 \nL 147.832399 164.217545 \nL 146.332399 162.717545 \nL 147.832399 161.217545 \nL 146.332399 159.717545 \nL 144.832399 161.217545 \nL 143.332399 159.717545 \nL 141.832399 161.217545 \nL 143.332399 162.717545 \nL 141.832399 164.217545 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 164.519274 \nL 370.959943 163.019274 \nL 372.459943 164.519274 \nL 373.959943 163.019274 \nL 372.459943 161.519274 \nL 373.959943 160.019274 \nL 372.459943 158.519274 \nL 370.959943 160.019274 \nL 369.459943 158.519274 \nL 367.959943 160.019274 \nL 369.459943 161.519274 \nL 367.959943 163.019274 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 132.827508 \nL 66.812628 131.327508 \nL 68.312628 132.827508 \nL 69.812628 131.327508 \nL 68.312628 129.827508 \nL 69.812628 128.327508 \nL 68.312628 126.827508 \nL 66.812628 128.327508 \nL 65.312628 126.827508 \nL 63.812628 128.327508 \nL 65.312628 129.827508 \nL 63.812628 131.327508 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 180.265635 \nL 68.110554 178.765635 \nL 69.610554 180.265635 \nL 71.110554 178.765635 \nL 69.610554 177.265635 \nL 71.110554 175.765635 \nL 69.610554 174.265635 \nL 68.110554 175.765635 \nL 66.610554 174.265635 \nL 65.110554 175.765635 \nL 66.610554 177.265635 \nL 65.110554 178.765635 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 185.677009 \nL 70.706406 184.177009 \nL 72.206406 185.677009 \nL 73.706406 184.177009 \nL 72.206406 182.677009 \nL 73.706406 181.177009 \nL 72.206406 179.677009 \nL 70.706406 181.177009 \nL 69.206406 179.677009 \nL 67.706406 181.177009 \nL 69.206406 182.677009 \nL 67.706406 184.177009 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 187.817934 \nL 75.898109 186.317934 \nL 77.398109 187.817934 \nL 78.898109 186.317934 \nL 77.398109 184.817934 \nL 78.898109 183.317934 \nL 77.398109 181.817934 \nL 75.898109 183.317934 \nL 74.398109 181.817934 \nL 72.898109 183.317934 \nL 74.398109 184.817934 \nL 72.898109 186.317934 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 195.233745 \nL 86.281517 193.733745 \nL 87.781517 195.233745 \nL 89.281517 193.733745 \nL 87.781517 192.233745 \nL 89.281517 190.733745 \nL 87.781517 189.233745 \nL 86.281517 190.733745 \nL 84.781517 189.233745 \nL 83.281517 190.733745 \nL 84.781517 192.233745 \nL 83.281517 193.733745 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 178.384955 \nL 107.048332 176.884955 \nL 108.548332 178.384955 \nL 110.048332 176.884955 \nL 108.548332 175.384955 \nL 110.048332 173.884955 \nL 108.548332 172.384955 \nL 107.048332 173.884955 \nL 105.548332 172.384955 \nL 104.048332 173.884955 \nL 105.548332 175.384955 \nL 104.048332 176.884955 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 63.418303 \nC 67.391916 63.418303 68.155046 63.102204 68.717627 62.539624 \nC 69.280208 61.977043 69.596307 61.213913 69.596307 60.418303 \nC 69.596307 59.622694 69.280208 58.859564 68.717627 58.296983 \nC 68.155046 57.734402 67.391916 57.418303 66.596307 57.418303 \nC 65.800698 57.418303 65.037567 57.734402 64.474986 58.296983 \nC 63.912406 58.859564 63.596307 59.622694 63.596307 60.418303 \nC 63.596307 61.213913 63.912406 61.977043 64.474986 62.539624 \nC 65.037567 63.102204 65.800698 63.418303 66.596307 63.418303 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 177.255564 \nC 67.752451 177.255564 68.515581 176.939465 69.078162 176.376884 \nC 69.640743 175.814304 69.956842 175.051173 69.956842 174.255564 \nC 69.956842 173.459955 69.640743 172.696824 69.078162 172.134244 \nC 68.515581 171.571663 67.752451 171.255564 66.956842 171.255564 \nC 66.161233 171.255564 65.398102 171.571663 64.835521 172.134244 \nC 64.272941 172.696824 63.956842 173.459955 63.956842 174.255564 \nC 63.956842 175.051173 64.272941 175.814304 64.835521 176.376884 \nC 65.398102 176.939465 66.161233 177.255564 66.956842 177.255564 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 146.49792 \nC 68.906163 146.49792 69.669293 146.181821 70.231874 145.61924 \nC 70.794455 145.056659 71.110554 144.293529 71.110554 143.49792 \nC 71.110554 142.70231 70.794455 141.93918 70.231874 141.376599 \nC 69.669293 140.814018 68.906163 140.49792 68.110554 140.49792 \nC 67.314944 140.49792 66.551814 140.814018 65.989233 141.376599 \nC 65.426653 141.93918 65.110554 142.70231 65.110554 143.49792 \nC 65.110554 144.293529 65.426653 145.056659 65.989233 145.61924 \nC 66.551814 146.181821 67.314944 146.49792 68.110554 146.49792 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 177.417646 \nC 72.944155 177.417646 73.707285 177.101547 74.269866 176.538966 \nC 74.832447 175.976385 75.148546 175.213255 75.148546 174.417646 \nC 75.148546 173.622036 74.832447 172.858906 74.269866 172.296325 \nC 73.707285 171.733744 72.944155 171.417646 72.148546 171.417646 \nC 71.352936 171.417646 70.589806 171.733744 70.027225 172.296325 \nC 69.464645 172.858906 69.148546 173.622036 69.148546 174.417646 \nC 69.148546 175.213255 69.464645 175.976385 70.027225 176.538966 \nC 70.589806 177.101547 71.352936 177.417646 72.148546 177.417646 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 191.421729 \nC 87.94241 191.421729 88.705541 191.10563 89.268121 190.543049 \nC 89.830702 189.980469 90.146801 189.217338 90.146801 188.421729 \nC 90.146801 187.62612 89.830702 186.862989 89.268121 186.300409 \nC 88.705541 185.737828 87.94241 185.421729 87.146801 185.421729 \nC 86.351192 185.421729 85.588061 185.737828 85.025481 186.300409 \nC 84.4629 186.862989 84.146801 187.62612 84.146801 188.421729 \nC 84.146801 189.217338 84.4629 189.980469 85.025481 190.543049 \nC 85.588061 191.10563 86.351192 191.421729 87.146801 191.421729 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 160.085833 \nC 145.628008 160.085833 146.391139 159.769734 146.953719 159.207153 \nC 147.5163 158.644572 147.832399 157.881442 147.832399 157.085833 \nC 147.832399 156.290224 147.5163 155.527093 146.953719 154.964513 \nC 146.391139 154.401932 145.628008 154.085833 144.832399 154.085833 \nC 144.03679 154.085833 143.273659 154.401932 142.711079 154.964513 \nC 142.148498 155.527093 141.832399 156.290224 141.832399 157.085833 \nC 141.832399 157.881442 142.148498 158.644572 142.711079 159.207153 \nC 143.273659 159.769734 144.03679 160.085833 144.832399 160.085833 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 171.046346 \nC 371.755552 171.046346 372.518683 170.730247 373.081264 170.167666 \nC 373.643844 169.605085 373.959943 168.841955 373.959943 168.046346 \nC 373.959943 167.250737 373.643844 166.487606 373.081264 165.925025 \nC 372.518683 165.362445 371.755552 165.046346 370.959943 165.046346 \nC 370.164334 165.046346 369.401204 165.362445 368.838623 165.925025 \nC 368.276042 166.487606 367.959943 167.250737 367.959943 168.046346 \nC 367.959943 168.841955 368.276042 169.605085 368.838623 170.167666 \nC 369.401204 170.730247 370.164334 171.046346 370.959943 171.046346 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 163.628541 \nC 67.608237 163.628541 68.371367 163.312442 68.933948 162.749861 \nC 69.496529 162.18728 69.812628 161.42415 69.812628 160.628541 \nC 69.812628 159.832931 69.496529 159.069801 68.933948 158.50722 \nC 68.371367 157.94464 67.608237 157.628541 66.812628 157.628541 \nC 66.017019 157.628541 65.253888 157.94464 64.691307 158.50722 \nC 64.128727 159.069801 63.812628 159.832931 63.812628 160.628541 \nC 63.812628 161.42415 64.128727 162.18728 64.691307 162.749861 \nC 65.253888 163.312442 66.017019 163.628541 66.812628 163.628541 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 193.346561 \nC 68.906163 193.346561 69.669293 193.030462 70.231874 192.467881 \nC 70.794455 191.9053 71.110554 191.14217 71.110554 190.346561 \nC 71.110554 189.550952 70.794455 188.787821 70.231874 188.225241 \nC 69.669293 187.66266 68.906163 187.346561 68.110554 187.346561 \nC 67.314944 187.346561 66.551814 187.66266 65.989233 188.225241 \nC 65.426653 188.787821 65.110554 189.550952 65.110554 190.346561 \nC 65.110554 191.14217 65.426653 191.9053 65.989233 192.467881 \nC 66.551814 193.030462 67.314944 193.346561 68.110554 193.346561 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 162.599701 \nC 71.502015 162.599701 72.265145 162.283602 72.827726 161.721021 \nC 73.390307 161.158441 73.706406 160.39531 73.706406 159.599701 \nC 73.706406 158.804092 73.390307 158.040961 72.827726 157.478381 \nC 72.265145 156.9158 71.502015 156.599701 70.706406 156.599701 \nC 69.910796 156.599701 69.147666 156.9158 68.585085 157.478381 \nC 68.022505 158.040961 67.706406 158.804092 67.706406 159.599701 \nC 67.706406 160.39531 68.022505 161.158441 68.585085 161.721021 \nC 69.147666 162.283602 69.910796 162.599701 70.706406 162.599701 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 185.841313 \nC 76.693719 185.841313 77.456849 185.525214 78.01943 184.962634 \nC 78.582011 184.400053 78.898109 183.636923 78.898109 182.841313 \nC 78.898109 182.045704 78.582011 181.282574 78.01943 180.719993 \nC 77.456849 180.157412 76.693719 179.841313 75.898109 179.841313 \nC 75.1025 179.841313 74.33937 180.157412 73.776789 180.719993 \nC 73.214208 181.282574 72.898109 182.045704 72.898109 182.841313 \nC 72.898109 183.636923 73.214208 184.400053 73.776789 184.962634 \nC 74.33937 185.525214 75.1025 185.841313 75.898109 185.841313 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 135.295084 \nC 87.077126 135.295084 87.840257 134.978985 88.402837 134.416405 \nC 88.965418 133.853824 89.281517 133.090694 89.281517 132.295084 \nC 89.281517 131.499475 88.965418 130.736345 88.402837 130.173764 \nC 87.840257 129.611183 87.077126 129.295084 86.281517 129.295084 \nC 85.485908 129.295084 84.722778 129.611183 84.160197 130.173764 \nC 83.597616 130.736345 83.281517 131.499475 83.281517 132.295084 \nC 83.281517 133.090694 83.597616 133.853824 84.160197 134.416405 \nC 84.722778 134.978985 85.485908 135.295084 86.281517 135.295084 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.003934 \nC 107.843942 21.003934 108.607072 20.687835 109.169653 20.125254 \nC 109.732233 19.562674 110.048332 18.799543 110.048332 18.003934 \nC 110.048332 17.208325 109.732233 16.445194 109.169653 15.882614 \nC 108.607072 15.320033 107.843942 15.003934 107.048332 15.003934 \nC 106.252723 15.003934 105.489593 15.320033 104.927012 15.882614 \nC 104.364431 16.445194 104.048332 17.208325 104.048332 18.003934 \nC 104.048332 18.799543 104.364431 19.562674 104.927012 20.125254 \nC 105.489593 20.687835 106.252723 21.003934 107.048332 21.003934 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 119.497254 \nL 66.596307 117.997254 \nL 68.096307 119.497254 \nL 69.596307 117.997254 \nL 68.096307 116.497254 \nL 69.596307 114.997254 \nL 68.096307 113.497254 \nL 66.596307 114.997254 \nL 65.096307 113.497254 \nL 63.596307 114.997254 \nL 65.096307 116.497254 \nL 63.596307 117.997254 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 158.736005 \nL 66.956842 157.236005 \nL 68.456842 158.736005 \nL 69.956842 157.236005 \nL 68.456842 155.736005 \nL 69.956842 154.236005 \nL 68.456842 152.736005 \nL 66.956842 154.236005 \nL 65.456842 152.736005 \nL 63.956842 154.236005 \nL 65.456842 155.736005 \nL 63.956842 157.236005 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 175.802247 \nL 68.110554 174.302247 \nL 69.610554 175.802247 \nL 71.110554 174.302247 \nL 69.610554 172.802247 \nL 71.110554 171.302247 \nL 69.610554 169.802247 \nL 68.110554 171.302247 \nL 66.610554 169.802247 \nL 65.110554 171.302247 \nL 66.610554 172.802247 \nL 65.110554 174.302247 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.648546 158.60835 \nL 72.148546 157.10835 \nL 73.648546 158.60835 \nL 75.148546 157.10835 \nL 73.648546 155.60835 \nL 75.148546 154.10835 \nL 73.648546 152.60835 \nL 72.148546 154.10835 \nL 70.648546 152.60835 \nL 69.148546 154.10835 \nL 70.648546 155.60835 \nL 69.148546 157.10835 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 195.300618 \nL 87.146801 193.800618 \nL 88.646801 195.300618 \nL 90.146801 193.800618 \nL 88.646801 192.300618 \nL 90.146801 190.800618 \nL 88.646801 189.300618 \nL 87.146801 190.800618 \nL 85.646801 189.300618 \nL 84.146801 190.800618 \nL 85.646801 192.300618 \nL 84.146801 193.800618 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 147.743219 \nL 144.832399 146.243219 \nL 146.332399 147.743219 \nL 147.832399 146.243219 \nL 146.332399 144.743219 \nL 147.832399 143.243219 \nL 146.332399 141.743219 \nL 144.832399 143.243219 \nL 143.332399 141.743219 \nL 141.832399 143.243219 \nL 143.332399 144.743219 \nL 141.832399 146.243219 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 144.329468 \nL 370.959943 142.829468 \nL 372.459943 144.329468 \nL 373.959943 142.829468 \nL 372.459943 141.329468 \nL 373.959943 139.829468 \nL 372.459943 138.329468 \nL 370.959943 139.829468 \nL 369.459943 138.329468 \nL 367.959943 139.829468 \nL 369.459943 141.329468 \nL 367.959943 142.829468 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 161.175408 \nL 66.812628 159.675408 \nL 68.312628 161.175408 \nL 69.812628 159.675408 \nL 68.312628 158.175408 \nL 69.812628 156.675408 \nL 68.312628 155.175408 \nL 66.812628 156.675408 \nL 65.312628 155.175408 \nL 63.812628 156.675408 \nL 65.312628 158.175408 \nL 63.812628 159.675408 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 145.608398 \nL 68.110554 144.108398 \nL 69.610554 145.608398 \nL 71.110554 144.108398 \nL 69.610554 142.608398 \nL 71.110554 141.108398 \nL 69.610554 139.608398 \nL 68.110554 141.108398 \nL 66.610554 139.608398 \nL 65.110554 141.108398 \nL 66.610554 142.608398 \nL 65.110554 144.108398 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 180.015177 \nL 70.706406 178.515177 \nL 72.206406 180.015177 \nL 73.706406 178.515177 \nL 72.206406 177.015177 \nL 73.706406 175.515177 \nL 72.206406 174.015177 \nL 70.706406 175.515177 \nL 69.206406 174.015177 \nL 67.706406 175.515177 \nL 69.206406 177.015177 \nL 67.706406 178.515177 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 171.131092 \nL 75.898109 169.631092 \nL 77.398109 171.131092 \nL 78.898109 169.631092 \nL 77.398109 168.131092 \nL 78.898109 166.631092 \nL 77.398109 165.131092 \nL 75.898109 166.631092 \nL 74.398109 165.131092 \nL 72.898109 166.631092 \nL 74.398109 168.131092 \nL 72.898109 169.631092 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 194.676642 \nL 86.281517 193.176642 \nL 87.781517 194.676642 \nL 89.281517 193.176642 \nL 87.781517 191.676642 \nL 89.281517 190.176642 \nL 87.781517 188.676642 \nL 86.281517 190.176642 \nL 84.781517 188.676642 \nL 83.281517 190.176642 \nL 84.781517 191.676642 \nL 83.281517 193.176642 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 188.411472 \nL 107.048332 186.911472 \nL 108.548332 188.411472 \nL 110.048332 186.911472 \nL 108.548332 185.411472 \nL 110.048332 183.911472 \nL 108.548332 182.411472 \nL 107.048332 183.911472 \nL 105.548332 182.411472 \nL 104.048332 183.911472 \nL 105.548332 185.411472 \nL 104.048332 186.911472 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 62.010285 \nC 67.391916 62.010285 68.155046 61.694186 68.717627 61.131606 \nC 69.280208 60.569025 69.596307 59.805895 69.596307 59.010285 \nC 69.596307 58.214676 69.280208 57.451546 68.717627 56.888965 \nC 68.155046 56.326384 67.391916 56.010285 66.596307 56.010285 \nC 65.800698 56.010285 65.037567 56.326384 64.474986 56.888965 \nC 63.912406 57.451546 63.596307 58.214676 63.596307 59.010285 \nC 63.596307 59.805895 63.912406 60.569025 64.474986 61.131606 \nC 65.037567 61.694186 65.800698 62.010285 66.596307 62.010285 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 178.987893 \nC 67.752451 178.987893 68.515581 178.671794 69.078162 178.109213 \nC 69.640743 177.546632 69.956842 176.783502 69.956842 175.987893 \nC 69.956842 175.192283 69.640743 174.429153 69.078162 173.866572 \nC 68.515581 173.303992 67.752451 172.987893 66.956842 172.987893 \nC 66.161233 172.987893 65.398102 173.303992 64.835521 173.866572 \nC 64.272941 174.429153 63.956842 175.192283 63.956842 175.987893 \nC 63.956842 176.783502 64.272941 177.546632 64.835521 178.109213 \nC 65.398102 178.671794 66.161233 178.987893 66.956842 178.987893 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 170.779918 \nC 68.906163 170.779918 69.669293 170.463819 70.231874 169.901238 \nC 70.794455 169.338657 71.110554 168.575527 71.110554 167.779918 \nC 71.110554 166.984308 70.794455 166.221178 70.231874 165.658597 \nC 69.669293 165.096017 68.906163 164.779918 68.110554 164.779918 \nC 67.314944 164.779918 66.551814 165.096017 65.989233 165.658597 \nC 65.426653 166.221178 65.110554 166.984308 65.110554 167.779918 \nC 65.110554 168.575527 65.426653 169.338657 65.989233 169.901238 \nC 66.551814 170.463819 67.314944 170.779918 68.110554 170.779918 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 186.631575 \nC 72.944155 186.631575 73.707285 186.315477 74.269866 185.752896 \nC 74.832447 185.190315 75.148546 184.427185 75.148546 183.631575 \nC 75.148546 182.835966 74.832447 182.072836 74.269866 181.510255 \nC 73.707285 180.947674 72.944155 180.631575 72.148546 180.631575 \nC 71.352936 180.631575 70.589806 180.947674 70.027225 181.510255 \nC 69.464645 182.072836 69.148546 182.835966 69.148546 183.631575 \nC 69.148546 184.427185 69.464645 185.190315 70.027225 185.752896 \nC 70.589806 186.315477 71.352936 186.631575 72.148546 186.631575 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 161.083267 \nC 87.94241 161.083267 88.705541 160.767168 89.268121 160.204587 \nC 89.830702 159.642007 90.146801 158.878876 90.146801 158.083267 \nC 90.146801 157.287658 89.830702 156.524527 89.268121 155.961947 \nC 88.705541 155.399366 87.94241 155.083267 87.146801 155.083267 \nC 86.351192 155.083267 85.588061 155.399366 85.025481 155.961947 \nC 84.4629 156.524527 84.146801 157.287658 84.146801 158.083267 \nC 84.146801 158.878876 84.4629 159.642007 85.025481 160.204587 \nC 85.588061 160.767168 86.351192 161.083267 87.146801 161.083267 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 147.935733 \nC 145.628008 147.935733 146.391139 147.619634 146.953719 147.057054 \nC 147.5163 146.494473 147.832399 145.731343 147.832399 144.935733 \nC 147.832399 144.140124 147.5163 143.376994 146.953719 142.814413 \nC 146.391139 142.251832 145.628008 141.935733 144.832399 141.935733 \nC 144.03679 141.935733 143.273659 142.251832 142.711079 142.814413 \nC 142.148498 143.376994 141.832399 144.140124 141.832399 144.935733 \nC 141.832399 145.731343 142.148498 146.494473 142.711079 147.057054 \nC 143.273659 147.619634 144.03679 147.935733 144.832399 147.935733 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 175.181175 \nC 371.755552 175.181175 372.518683 174.865076 373.081264 174.302495 \nC 373.643844 173.739914 373.959943 172.976784 373.959943 172.181175 \nC 373.959943 171.385566 373.643844 170.622435 373.081264 170.059854 \nC 372.518683 169.497274 371.755552 169.181175 370.959943 169.181175 \nC 370.164334 169.181175 369.401204 169.497274 368.838623 170.059854 \nC 368.276042 170.622435 367.959943 171.385566 367.959943 172.181175 \nC 367.959943 172.976784 368.276042 173.739914 368.838623 174.302495 \nC 369.401204 174.865076 370.164334 175.181175 370.959943 175.181175 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 168.187577 \nC 67.608237 168.187577 68.371367 167.871478 68.933948 167.308897 \nC 69.496529 166.746317 69.812628 165.983186 69.812628 165.187577 \nC 69.812628 164.391968 69.496529 163.628838 68.933948 163.066257 \nC 68.371367 162.503676 67.608237 162.187577 66.812628 162.187577 \nC 66.017019 162.187577 65.253888 162.503676 64.691307 163.066257 \nC 64.128727 163.628838 63.812628 164.391968 63.812628 165.187577 \nC 63.812628 165.983186 64.128727 166.746317 64.691307 167.308897 \nC 65.253888 167.871478 66.017019 168.187577 66.812628 168.187577 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 199.761892 \nC 71.502015 199.761892 72.265145 199.445793 72.827726 198.883213 \nC 73.390307 198.320632 73.706406 197.557502 73.706406 196.761892 \nC 73.706406 195.966283 73.390307 195.203153 72.827726 194.640572 \nC 72.265145 194.077991 71.502015 193.761892 70.706406 193.761892 \nC 69.910796 193.761892 69.147666 194.077991 68.585085 194.640572 \nC 68.022505 195.203153 67.706406 195.966283 67.706406 196.761892 \nC 67.706406 197.557502 68.022505 198.320632 68.585085 198.883213 \nC 69.147666 199.445793 69.910796 199.761892 70.706406 199.761892 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 167.077833 \nC 76.693719 167.077833 77.456849 166.761734 78.01943 166.199153 \nC 78.582011 165.636572 78.898109 164.873442 78.898109 164.077833 \nC 78.898109 163.282223 78.582011 162.519093 78.01943 161.956512 \nC 77.456849 161.393932 76.693719 161.077833 75.898109 161.077833 \nC 75.1025 161.077833 74.33937 161.393932 73.776789 161.956512 \nC 73.214208 162.519093 72.898109 163.282223 72.898109 164.077833 \nC 72.898109 164.873442 73.214208 165.636572 73.776789 166.199153 \nC 74.33937 166.761734 75.1025 167.077833 75.898109 167.077833 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 123.785448 \nC 87.077126 123.785448 87.840257 123.469349 88.402837 122.906768 \nC 88.965418 122.344188 89.281517 121.581057 89.281517 120.785448 \nC 89.281517 119.989839 88.965418 119.226709 88.402837 118.664128 \nC 87.840257 118.101547 87.077126 117.785448 86.281517 117.785448 \nC 85.485908 117.785448 84.722778 118.101547 84.160197 118.664128 \nC 83.597616 119.226709 83.281517 119.989839 83.281517 120.785448 \nC 83.281517 121.581057 83.597616 122.344188 84.160197 122.906768 \nC 84.722778 123.469349 85.485908 123.785448 86.281517 123.785448 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.003928 \nC 107.843942 21.003928 108.607072 20.687829 109.169653 20.125248 \nC 109.732233 19.562667 110.048332 18.799537 110.048332 18.003928 \nC 110.048332 17.208318 109.732233 16.445188 109.169653 15.882607 \nC 108.607072 15.320027 107.843942 15.003928 107.048332 15.003928 \nC 106.252723 15.003928 105.489593 15.320027 104.927012 15.882607 \nC 104.364431 16.445188 104.048332 17.208318 104.048332 18.003928 \nC 104.048332 18.799537 104.364431 19.562667 104.927012 20.125248 \nC 105.489593 20.687829 106.252723 21.003928 107.048332 21.003928 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 56.708446 \nL 66.596307 55.208446 \nL 68.096307 56.708446 \nL 69.596307 55.208446 \nL 68.096307 53.708446 \nL 69.596307 52.208446 \nL 68.096307 50.708446 \nL 66.596307 52.208446 \nL 65.096307 50.708446 \nL 63.596307 52.208446 \nL 65.096307 53.708446 \nL 63.596307 55.208446 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 164.844799 \nL 66.956842 163.344799 \nL 68.456842 164.844799 \nL 69.956842 163.344799 \nL 68.456842 161.844799 \nL 69.956842 160.344799 \nL 68.456842 158.844799 \nL 66.956842 160.344799 \nL 65.456842 158.844799 \nL 63.956842 160.344799 \nL 65.456842 161.844799 \nL 63.956842 163.344799 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 172.590768 \nL 68.110554 171.090768 \nL 69.610554 172.590768 \nL 71.110554 171.090768 \nL 69.610554 169.590768 \nL 71.110554 168.090768 \nL 69.610554 166.590768 \nL 68.110554 168.090768 \nL 66.610554 166.590768 \nL 65.110554 168.090768 \nL 66.610554 169.590768 \nL 65.110554 171.090768 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.648546 146.493051 \nL 72.148546 144.993051 \nL 73.648546 146.493051 \nL 75.148546 144.993051 \nL 73.648546 143.493051 \nL 75.148546 141.993051 \nL 73.648546 140.493051 \nL 72.148546 141.993051 \nL 70.648546 140.493051 \nL 69.148546 141.993051 \nL 70.648546 143.493051 \nL 69.148546 144.993051 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 160.013202 \nL 87.146801 158.513202 \nL 88.646801 160.013202 \nL 90.146801 158.513202 \nL 88.646801 157.013202 \nL 90.146801 155.513202 \nL 88.646801 154.013202 \nL 87.146801 155.513202 \nL 85.646801 154.013202 \nL 84.146801 155.513202 \nL 85.646801 157.013202 \nL 84.146801 158.513202 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 185.422728 \nL 144.832399 183.922728 \nL 146.332399 185.422728 \nL 147.832399 183.922728 \nL 146.332399 182.422728 \nL 147.832399 180.922728 \nL 146.332399 179.422728 \nL 144.832399 180.922728 \nL 143.332399 179.422728 \nL 141.832399 180.922728 \nL 143.332399 182.422728 \nL 141.832399 183.922728 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 164.342915 \nL 370.959943 162.842915 \nL 372.459943 164.342915 \nL 373.959943 162.842915 \nL 372.459943 161.342915 \nL 373.959943 159.842915 \nL 372.459943 158.342915 \nL 370.959943 159.842915 \nL 369.459943 158.342915 \nL 367.959943 159.842915 \nL 369.459943 161.342915 \nL 367.959943 162.842915 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 134.201089 \nL 66.812628 132.701089 \nL 68.312628 134.201089 \nL 69.812628 132.701089 \nL 68.312628 131.201089 \nL 69.812628 129.701089 \nL 68.312628 128.201089 \nL 66.812628 129.701089 \nL 65.312628 128.201089 \nL 63.812628 129.701089 \nL 65.312628 131.201089 \nL 63.812628 132.701089 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 186.147004 \nL 68.110554 184.647004 \nL 69.610554 186.147004 \nL 71.110554 184.647004 \nL 69.610554 183.147004 \nL 71.110554 181.647004 \nL 69.610554 180.147004 \nL 68.110554 181.647004 \nL 66.610554 180.147004 \nL 65.110554 181.647004 \nL 66.610554 183.147004 \nL 65.110554 184.647004 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 202.853693 \nL 70.706406 201.353693 \nL 72.206406 202.853693 \nL 73.706406 201.353693 \nL 72.206406 199.853693 \nL 73.706406 198.353693 \nL 72.206406 196.853693 \nL 70.706406 198.353693 \nL 69.206406 196.853693 \nL 67.706406 198.353693 \nL 69.206406 199.853693 \nL 67.706406 201.353693 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 152.191357 \nL 75.898109 150.691357 \nL 77.398109 152.191357 \nL 78.898109 150.691357 \nL 77.398109 149.191357 \nL 78.898109 147.691357 \nL 77.398109 146.191357 \nL 75.898109 147.691357 \nL 74.398109 146.191357 \nL 72.898109 147.691357 \nL 74.398109 149.191357 \nL 72.898109 150.691357 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 187.551882 \nL 86.281517 186.051882 \nL 87.781517 187.551882 \nL 89.281517 186.051882 \nL 87.781517 184.551882 \nL 89.281517 183.051882 \nL 87.781517 181.551882 \nL 86.281517 183.051882 \nL 84.781517 181.551882 \nL 83.281517 183.051882 \nL 84.781517 184.551882 \nL 83.281517 186.051882 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 179.776551 \nL 107.048332 178.276551 \nL 108.548332 179.776551 \nL 110.048332 178.276551 \nL 108.548332 176.776551 \nL 110.048332 175.276551 \nL 108.548332 173.776551 \nL 107.048332 175.276551 \nL 105.548332 173.776551 \nL 104.048332 175.276551 \nL 105.548332 176.776551 \nL 104.048332 178.276551 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 30.82082 \nC 67.391916 30.82082 68.155046 30.504721 68.717627 29.942141 \nC 69.280208 29.37956 69.596307 28.61643 69.596307 27.82082 \nC 69.596307 27.025211 69.280208 26.262081 68.717627 25.6995 \nC 68.155046 25.136919 67.391916 24.82082 66.596307 24.82082 \nC 65.800698 24.82082 65.037567 25.136919 64.474986 25.6995 \nC 63.912406 26.262081 63.596307 27.025211 63.596307 27.82082 \nC 63.596307 28.61643 63.912406 29.37956 64.474986 29.942141 \nC 65.037567 30.504721 65.800698 30.82082 66.596307 30.82082 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 171.973067 \nC 67.752451 171.973067 68.515581 171.656968 69.078162 171.094387 \nC 69.640743 170.531806 69.956842 169.768676 69.956842 168.973067 \nC 69.956842 168.177457 69.640743 167.414327 69.078162 166.851746 \nC 68.515581 166.289166 67.752451 165.973067 66.956842 165.973067 \nC 66.161233 165.973067 65.398102 166.289166 64.835521 166.851746 \nC 64.272941 167.414327 63.956842 168.177457 63.956842 168.973067 \nC 63.956842 169.768676 64.272941 170.531806 64.835521 171.094387 \nC 65.398102 171.656968 66.161233 171.973067 66.956842 171.973067 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 187.348272 \nC 68.906163 187.348272 69.669293 187.032173 70.231874 186.469592 \nC 70.794455 185.907011 71.110554 185.143881 71.110554 184.348272 \nC 71.110554 183.552662 70.794455 182.789532 70.231874 182.226951 \nC 69.669293 181.66437 68.906163 181.348272 68.110554 181.348272 \nC 67.314944 181.348272 66.551814 181.66437 65.989233 182.226951 \nC 65.426653 182.789532 65.110554 183.552662 65.110554 184.348272 \nC 65.110554 185.143881 65.426653 185.907011 65.989233 186.469592 \nC 66.551814 187.032173 67.314944 187.348272 68.110554 187.348272 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 202.539846 \nC 72.944155 202.539846 73.707285 202.223747 74.269866 201.661166 \nC 74.832447 201.098586 75.148546 200.335455 75.148546 199.539846 \nC 75.148546 198.744237 74.832447 197.981106 74.269866 197.418526 \nC 73.707285 196.855945 72.944155 196.539846 72.148546 196.539846 \nC 71.352936 196.539846 70.589806 196.855945 70.027225 197.418526 \nC 69.464645 197.981106 69.148546 198.744237 69.148546 199.539846 \nC 69.148546 200.335455 69.464645 201.098586 70.027225 201.661166 \nC 70.589806 202.223747 71.352936 202.539846 72.148546 202.539846 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 178.275661 \nC 87.94241 178.275661 88.705541 177.959562 89.268121 177.396981 \nC 89.830702 176.8344 90.146801 176.07127 90.146801 175.275661 \nC 90.146801 174.480052 89.830702 173.716921 89.268121 173.15434 \nC 88.705541 172.59176 87.94241 172.275661 87.146801 172.275661 \nC 86.351192 172.275661 85.588061 172.59176 85.025481 173.15434 \nC 84.4629 173.716921 84.146801 174.480052 84.146801 175.275661 \nC 84.146801 176.07127 84.4629 176.8344 85.025481 177.396981 \nC 85.588061 177.959562 86.351192 178.275661 87.146801 178.275661 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 163.956432 \nC 145.628008 163.956432 146.391139 163.640333 146.953719 163.077752 \nC 147.5163 162.515172 147.832399 161.752041 147.832399 160.956432 \nC 147.832399 160.160823 147.5163 159.397693 146.953719 158.835112 \nC 146.391139 158.272531 145.628008 157.956432 144.832399 157.956432 \nC 144.03679 157.956432 143.273659 158.272531 142.711079 158.835112 \nC 142.148498 159.397693 141.832399 160.160823 141.832399 160.956432 \nC 141.832399 161.752041 142.148498 162.515172 142.711079 163.077752 \nC 143.273659 163.640333 144.03679 163.956432 144.832399 163.956432 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 171.794288 \nC 371.755552 171.794288 372.518683 171.478189 373.081264 170.915608 \nC 373.643844 170.353027 373.959943 169.589897 373.959943 168.794288 \nC 373.959943 167.998678 373.643844 167.235548 373.081264 166.672967 \nC 372.518683 166.110387 371.755552 165.794288 370.959943 165.794288 \nC 370.164334 165.794288 369.401204 166.110387 368.838623 166.672967 \nC 368.276042 167.235548 367.959943 167.998678 367.959943 168.794288 \nC 367.959943 169.589897 368.276042 170.353027 368.838623 170.915608 \nC 369.401204 171.478189 370.164334 171.794288 370.959943 171.794288 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 144.815675 \nC 67.608237 144.815675 68.371367 144.499577 68.933948 143.936996 \nC 69.496529 143.374415 69.812628 142.611285 69.812628 141.815675 \nC 69.812628 141.020066 69.496529 140.256936 68.933948 139.694355 \nC 68.371367 139.131774 67.608237 138.815675 66.812628 138.815675 \nC 66.017019 138.815675 65.253888 139.131774 64.691307 139.694355 \nC 64.128727 140.256936 63.812628 141.020066 63.812628 141.815675 \nC 63.812628 142.611285 64.128727 143.374415 64.691307 143.936996 \nC 65.253888 144.499577 66.017019 144.815675 66.812628 144.815675 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 187.949923 \nC 68.906163 187.949923 69.669293 187.633824 70.231874 187.071243 \nC 70.794455 186.508662 71.110554 185.745532 71.110554 184.949923 \nC 71.110554 184.154313 70.794455 183.391183 70.231874 182.828602 \nC 69.669293 182.266022 68.906163 181.949923 68.110554 181.949923 \nC 67.314944 181.949923 66.551814 182.266022 65.989233 182.828602 \nC 65.426653 183.391183 65.110554 184.154313 65.110554 184.949923 \nC 65.110554 185.745532 65.426653 186.508662 65.989233 187.071243 \nC 66.551814 187.633824 67.314944 187.949923 68.110554 187.949923 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 181.955828 \nC 71.502015 181.955828 72.265145 181.639729 72.827726 181.077148 \nC 73.390307 180.514568 73.706406 179.751437 73.706406 178.955828 \nC 73.706406 178.160219 73.390307 177.397088 72.827726 176.834508 \nC 72.265145 176.271927 71.502015 175.955828 70.706406 175.955828 \nC 69.910796 175.955828 69.147666 176.271927 68.585085 176.834508 \nC 68.022505 177.397088 67.706406 178.160219 67.706406 178.955828 \nC 67.706406 179.751437 68.022505 180.514568 68.585085 181.077148 \nC 69.147666 181.639729 69.910796 181.955828 70.706406 181.955828 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 176.025249 \nC 76.693719 176.025249 77.456849 175.70915 78.01943 175.146569 \nC 78.582011 174.583988 78.898109 173.820858 78.898109 173.025249 \nC 78.898109 172.22964 78.582011 171.466509 78.01943 170.903928 \nC 77.456849 170.341348 76.693719 170.025249 75.898109 170.025249 \nC 75.1025 170.025249 74.33937 170.341348 73.776789 170.903928 \nC 73.214208 171.466509 72.898109 172.22964 72.898109 173.025249 \nC 72.898109 173.820858 73.214208 174.583988 73.776789 175.146569 \nC 74.33937 175.70915 75.1025 176.025249 75.898109 176.025249 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 133.487118 \nC 87.077126 133.487118 87.840257 133.171019 88.402837 132.608438 \nC 88.965418 132.045857 89.281517 131.282727 89.281517 130.487118 \nC 89.281517 129.691508 88.965418 128.928378 88.402837 128.365797 \nC 87.840257 127.803216 87.077126 127.487118 86.281517 127.487118 \nC 85.485908 127.487118 84.722778 127.803216 84.160197 128.365797 \nC 83.597616 128.928378 83.281517 129.691508 83.281517 130.487118 \nC 83.281517 131.282727 83.597616 132.045857 84.160197 132.608438 \nC 84.722778 133.171019 85.485908 133.487118 86.281517 133.487118 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.003865 \nC 107.843942 21.003865 108.607072 20.687766 109.169653 20.125185 \nC 109.732233 19.562604 110.048332 18.799474 110.048332 18.003865 \nC 110.048332 17.208256 109.732233 16.445125 109.169653 15.882545 \nC 108.607072 15.319964 107.843942 15.003865 107.048332 15.003865 \nC 106.252723 15.003865 105.489593 15.319964 104.927012 15.882545 \nC 104.364431 16.445125 104.048332 17.208256 104.048332 18.003865 \nC 104.048332 18.799474 104.364431 19.562604 104.927012 20.125185 \nC 105.489593 20.687766 106.252723 21.003865 107.048332 21.003865 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 99.789455 \nL 66.596307 98.289455 \nL 68.096307 99.789455 \nL 69.596307 98.289455 \nL 68.096307 96.789455 \nL 69.596307 95.289455 \nL 68.096307 93.789455 \nL 66.596307 95.289455 \nL 65.096307 93.789455 \nL 63.596307 95.289455 \nL 65.096307 96.789455 \nL 63.596307 98.289455 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 185.128273 \nL 66.956842 183.628273 \nL 68.456842 185.128273 \nL 69.956842 183.628273 \nL 68.456842 182.128273 \nL 69.956842 180.628273 \nL 68.456842 179.128273 \nL 66.956842 180.628273 \nL 65.456842 179.128273 \nL 63.956842 180.628273 \nL 65.456842 182.128273 \nL 63.956842 183.628273 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 177.680266 \nL 68.110554 176.180266 \nL 69.610554 177.680266 \nL 71.110554 176.180266 \nL 69.610554 174.680266 \nL 71.110554 173.180266 \nL 69.610554 171.680266 \nL 68.110554 173.180266 \nL 66.610554 171.680266 \nL 65.110554 173.180266 \nL 66.610554 174.680266 \nL 65.110554 176.180266 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 158.747262 \nL 87.146801 157.247262 \nL 88.646801 158.747262 \nL 90.146801 157.247262 \nL 88.646801 155.747262 \nL 90.146801 154.247262 \nL 88.646801 152.747262 \nL 87.146801 154.247262 \nL 85.646801 152.747262 \nL 84.146801 154.247262 \nL 85.646801 155.747262 \nL 84.146801 157.247262 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 154.136113 \nL 144.832399 152.636113 \nL 146.332399 154.136113 \nL 147.832399 152.636113 \nL 146.332399 151.136113 \nL 147.832399 149.636113 \nL 146.332399 148.136113 \nL 144.832399 149.636113 \nL 143.332399 148.136113 \nL 141.832399 149.636113 \nL 143.332399 151.136113 \nL 141.832399 152.636113 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 173.935035 \nL 370.959943 172.435035 \nL 372.459943 173.935035 \nL 373.959943 172.435035 \nL 372.459943 170.935035 \nL 373.959943 169.435035 \nL 372.459943 167.935035 \nL 370.959943 169.435035 \nL 369.459943 167.935035 \nL 367.959943 169.435035 \nL 369.459943 170.935035 \nL 367.959943 172.435035 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 123.286446 \nL 66.812628 121.786446 \nL 68.312628 123.286446 \nL 69.812628 121.786446 \nL 68.312628 120.286446 \nL 69.812628 118.786446 \nL 68.312628 117.286446 \nL 66.812628 118.786446 \nL 65.312628 117.286446 \nL 63.812628 118.786446 \nL 65.312628 120.286446 \nL 63.812628 121.786446 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 141.112402 \nL 68.110554 139.612402 \nL 69.610554 141.112402 \nL 71.110554 139.612402 \nL 69.610554 138.112402 \nL 71.110554 136.612402 \nL 69.610554 135.112402 \nL 68.110554 136.612402 \nL 66.610554 135.112402 \nL 65.110554 136.612402 \nL 66.610554 138.112402 \nL 65.110554 139.612402 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 188.999825 \nL 70.706406 187.499825 \nL 72.206406 188.999825 \nL 73.706406 187.499825 \nL 72.206406 185.999825 \nL 73.706406 184.499825 \nL 72.206406 182.999825 \nL 70.706406 184.499825 \nL 69.206406 182.999825 \nL 67.706406 184.499825 \nL 69.206406 185.999825 \nL 67.706406 187.499825 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 196.690402 \nL 75.898109 195.190402 \nL 77.398109 196.690402 \nL 78.898109 195.190402 \nL 77.398109 193.690402 \nL 78.898109 192.190402 \nL 77.398109 190.690402 \nL 75.898109 192.190402 \nL 74.398109 190.690402 \nL 72.898109 192.190402 \nL 74.398109 193.690402 \nL 72.898109 195.190402 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 192.870074 \nL 86.281517 191.370074 \nL 87.781517 192.870074 \nL 89.281517 191.370074 \nL 87.781517 189.870074 \nL 89.281517 188.370074 \nL 87.781517 186.870074 \nL 86.281517 188.370074 \nL 84.781517 186.870074 \nL 83.281517 188.370074 \nL 84.781517 189.870074 \nL 83.281517 191.370074 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 158.677058 \nL 107.048332 157.177058 \nL 108.548332 158.677058 \nL 110.048332 157.177058 \nL 108.548332 155.677058 \nL 110.048332 154.177058 \nL 108.548332 152.677058 \nL 107.048332 154.177058 \nL 105.548332 152.677058 \nL 104.048332 154.177058 \nL 105.548332 155.677058 \nL 104.048332 157.177058 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 122.346371 \nC 67.391916 122.346371 68.155046 122.030272 68.717627 121.467691 \nC 69.280208 120.905111 69.596307 120.14198 69.596307 119.346371 \nC 69.596307 118.550762 69.280208 117.787631 68.717627 117.225051 \nC 68.155046 116.66247 67.391916 116.346371 66.596307 116.346371 \nC 65.800698 116.346371 65.037567 116.66247 64.474986 117.225051 \nC 63.912406 117.787631 63.596307 118.550762 63.596307 119.346371 \nC 63.596307 120.14198 63.912406 120.905111 64.474986 121.467691 \nC 65.037567 122.030272 65.800698 122.346371 66.596307 122.346371 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 147.773764 \nC 67.752451 147.773764 68.515581 147.457665 69.078162 146.895085 \nC 69.640743 146.332504 69.956842 145.569373 69.956842 144.773764 \nC 69.956842 143.978155 69.640743 143.215025 69.078162 142.652444 \nC 68.515581 142.089863 67.752451 141.773764 66.956842 141.773764 \nC 66.161233 141.773764 65.398102 142.089863 64.835521 142.652444 \nC 64.272941 143.215025 63.956842 143.978155 63.956842 144.773764 \nC 63.956842 145.569373 64.272941 146.332504 64.835521 146.895085 \nC 65.398102 147.457665 66.161233 147.773764 66.956842 147.773764 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 145.97861 \nC 68.906163 145.97861 69.669293 145.662511 70.231874 145.09993 \nC 70.794455 144.537349 71.110554 143.774219 71.110554 142.97861 \nC 71.110554 142.183001 70.794455 141.41987 70.231874 140.857289 \nC 69.669293 140.294709 68.906163 139.97861 68.110554 139.97861 \nC 67.314944 139.97861 66.551814 140.294709 65.989233 140.857289 \nC 65.426653 141.41987 65.110554 142.183001 65.110554 142.97861 \nC 65.110554 143.774219 65.426653 144.537349 65.989233 145.09993 \nC 66.551814 145.662511 67.314944 145.97861 68.110554 145.97861 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 197.1264 \nC 72.944155 197.1264 73.707285 196.810301 74.269866 196.24772 \nC 74.832447 195.68514 75.148546 194.922009 75.148546 194.1264 \nC 75.148546 193.330791 74.832447 192.56766 74.269866 192.00508 \nC 73.707285 191.442499 72.944155 191.1264 72.148546 191.1264 \nC 71.352936 191.1264 70.589806 191.442499 70.027225 192.00508 \nC 69.464645 192.56766 69.148546 193.330791 69.148546 194.1264 \nC 69.148546 194.922009 69.464645 195.68514 70.027225 196.24772 \nC 70.589806 196.810301 71.352936 197.1264 72.148546 197.1264 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 155.81039 \nC 87.94241 155.81039 88.705541 155.494291 89.268121 154.93171 \nC 89.830702 154.369129 90.146801 153.605999 90.146801 152.81039 \nC 90.146801 152.01478 89.830702 151.25165 89.268121 150.689069 \nC 88.705541 150.126489 87.94241 149.81039 87.146801 149.81039 \nC 86.351192 149.81039 85.588061 150.126489 85.025481 150.689069 \nC 84.4629 151.25165 84.146801 152.01478 84.146801 152.81039 \nC 84.146801 153.605999 84.4629 154.369129 85.025481 154.93171 \nC 85.588061 155.494291 86.351192 155.81039 87.146801 155.81039 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 155.858948 \nC 145.628008 155.858948 146.391139 155.542849 146.953719 154.980268 \nC 147.5163 154.417687 147.832399 153.654557 147.832399 152.858948 \nC 147.832399 152.063339 147.5163 151.300208 146.953719 150.737628 \nC 146.391139 150.175047 145.628008 149.858948 144.832399 149.858948 \nC 144.03679 149.858948 143.273659 150.175047 142.711079 150.737628 \nC 142.148498 151.300208 141.832399 152.063339 141.832399 152.858948 \nC 141.832399 153.654557 142.148498 154.417687 142.711079 154.980268 \nC 143.273659 155.542849 144.03679 155.858948 144.832399 155.858948 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 191.395603 \nC 67.608237 191.395603 68.371367 191.079504 68.933948 190.516923 \nC 69.496529 189.954342 69.812628 189.191212 69.812628 188.395603 \nC 69.812628 187.599993 69.496529 186.836863 68.933948 186.274282 \nC 68.371367 185.711701 67.608237 185.395603 66.812628 185.395603 \nC 66.017019 185.395603 65.253888 185.711701 64.691307 186.274282 \nC 64.128727 186.836863 63.812628 187.599993 63.812628 188.395603 \nC 63.812628 189.191212 64.128727 189.954342 64.691307 190.516923 \nC 65.253888 191.079504 66.017019 191.395603 66.812628 191.395603 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 200.4021 \nC 68.906163 200.4021 69.669293 200.086001 70.231874 199.52342 \nC 70.794455 198.960839 71.110554 198.197709 71.110554 197.4021 \nC 71.110554 196.60649 70.794455 195.84336 70.231874 195.280779 \nC 69.669293 194.718199 68.906163 194.4021 68.110554 194.4021 \nC 67.314944 194.4021 66.551814 194.718199 65.989233 195.280779 \nC 65.426653 195.84336 65.110554 196.60649 65.110554 197.4021 \nC 65.110554 198.197709 65.426653 198.960839 65.989233 199.52342 \nC 66.551814 200.086001 67.314944 200.4021 68.110554 200.4021 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 170.493892 \nC 71.502015 170.493892 72.265145 170.177793 72.827726 169.615213 \nC 73.390307 169.052632 73.706406 168.289502 73.706406 167.493892 \nC 73.706406 166.698283 73.390307 165.935153 72.827726 165.372572 \nC 72.265145 164.809991 71.502015 164.493892 70.706406 164.493892 \nC 69.910796 164.493892 69.147666 164.809991 68.585085 165.372572 \nC 68.022505 165.935153 67.706406 166.698283 67.706406 167.493892 \nC 67.706406 168.289502 68.022505 169.052632 68.585085 169.615213 \nC 69.147666 170.177793 69.910796 170.493892 70.706406 170.493892 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 207.364988 \nC 76.693719 207.364988 77.456849 207.048889 78.01943 206.486308 \nC 78.582011 205.923727 78.898109 205.160597 78.898109 204.364988 \nC 78.898109 203.569378 78.582011 202.806248 78.01943 202.243667 \nC 77.456849 201.681087 76.693719 201.364988 75.898109 201.364988 \nC 75.1025 201.364988 74.33937 201.681087 73.776789 202.243667 \nC 73.214208 202.806248 72.898109 203.569378 72.898109 204.364988 \nC 72.898109 205.160597 73.214208 205.923727 73.776789 206.486308 \nC 74.33937 207.048889 75.1025 207.364988 75.898109 207.364988 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 174.286973 \nC 87.077126 174.286973 87.840257 173.970874 88.402837 173.408294 \nC 88.965418 172.845713 89.281517 172.082582 89.281517 171.286973 \nC 89.281517 170.491364 88.965418 169.728234 88.402837 169.165653 \nC 87.840257 168.603072 87.077126 168.286973 86.281517 168.286973 \nC 85.485908 168.286973 84.722778 168.603072 84.160197 169.165653 \nC 83.597616 169.728234 83.281517 170.491364 83.281517 171.286973 \nC 83.281517 172.082582 83.597616 172.845713 84.160197 173.408294 \nC 84.722778 173.970874 85.485908 174.286973 86.281517 174.286973 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.004001 \nC 107.843942 21.004001 108.607072 20.687902 109.169653 20.125322 \nC 109.732233 19.562741 110.048332 18.799611 110.048332 18.004001 \nC 110.048332 17.208392 109.732233 16.445262 109.169653 15.882681 \nC 108.607072 15.3201 107.843942 15.004001 107.048332 15.004001 \nC 106.252723 15.004001 105.489593 15.3201 104.927012 15.882681 \nC 104.364431 16.445262 104.048332 17.208392 104.048332 18.004001 \nC 104.048332 18.799611 104.364431 19.562741 104.927012 20.125322 \nC 105.489593 20.687902 106.252723 21.004001 107.048332 21.004001 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 89.564071 \nL 66.596307 88.064071 \nL 68.096307 89.564071 \nL 69.596307 88.064071 \nL 68.096307 86.564071 \nL 69.596307 85.064071 \nL 68.096307 83.564071 \nL 66.596307 85.064071 \nL 65.096307 83.564071 \nL 63.596307 85.064071 \nL 65.096307 86.564071 \nL 63.596307 88.064071 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 163.830067 \nL 66.956842 162.330067 \nL 68.456842 163.830067 \nL 69.956842 162.330067 \nL 68.456842 160.830067 \nL 69.956842 159.330067 \nL 68.456842 157.830067 \nL 66.956842 159.330067 \nL 65.456842 157.830067 \nL 63.956842 159.330067 \nL 65.456842 160.830067 \nL 63.956842 162.330067 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 158.233904 \nL 68.110554 156.733904 \nL 69.610554 158.233904 \nL 71.110554 156.733904 \nL 69.610554 155.233904 \nL 71.110554 153.733904 \nL 69.610554 152.233904 \nL 68.110554 153.733904 \nL 66.610554 152.233904 \nL 65.110554 153.733904 \nL 66.610554 155.233904 \nL 65.110554 156.733904 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.648546 200.292636 \nL 72.148546 198.792636 \nL 73.648546 200.292636 \nL 75.148546 198.792636 \nL 73.648546 197.292636 \nL 75.148546 195.792636 \nL 73.648546 194.292636 \nL 72.148546 195.792636 \nL 70.648546 194.292636 \nL 69.148546 195.792636 \nL 70.648546 197.292636 \nL 69.148546 198.792636 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 109.97615 \nL 87.146801 108.47615 \nL 88.646801 109.97615 \nL 90.146801 108.47615 \nL 88.646801 106.97615 \nL 90.146801 105.47615 \nL 88.646801 103.97615 \nL 87.146801 105.47615 \nL 85.646801 103.97615 \nL 84.146801 105.47615 \nL 85.646801 106.97615 \nL 84.146801 108.47615 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 151.829313 \nL 144.832399 150.329313 \nL 146.332399 151.829313 \nL 147.832399 150.329313 \nL 146.332399 148.829313 \nL 147.832399 147.329313 \nL 146.332399 145.829313 \nL 144.832399 147.329313 \nL 143.332399 145.829313 \nL 141.832399 147.329313 \nL 143.332399 148.829313 \nL 141.832399 150.329313 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 180.435334 \nL 370.959943 178.935334 \nL 372.459943 180.435334 \nL 373.959943 178.935334 \nL 372.459943 177.435334 \nL 373.959943 175.935334 \nL 372.459943 174.435334 \nL 370.959943 175.935334 \nL 369.459943 174.435334 \nL 367.959943 175.935334 \nL 369.459943 177.435334 \nL 367.959943 178.935334 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 165.406961 \nL 66.812628 163.906961 \nL 68.312628 165.406961 \nL 69.812628 163.906961 \nL 68.312628 162.406961 \nL 69.812628 160.906961 \nL 68.312628 159.406961 \nL 66.812628 160.906961 \nL 65.312628 159.406961 \nL 63.812628 160.906961 \nL 65.312628 162.406961 \nL 63.812628 163.906961 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 164.699122 \nL 68.110554 163.199122 \nL 69.610554 164.699122 \nL 71.110554 163.199122 \nL 69.610554 161.699122 \nL 71.110554 160.199122 \nL 69.610554 158.699122 \nL 68.110554 160.199122 \nL 66.610554 158.699122 \nL 65.110554 160.199122 \nL 66.610554 161.699122 \nL 65.110554 163.199122 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 198.195355 \nL 70.706406 196.695355 \nL 72.206406 198.195355 \nL 73.706406 196.695355 \nL 72.206406 195.195355 \nL 73.706406 193.695355 \nL 72.206406 192.195355 \nL 70.706406 193.695355 \nL 69.206406 192.195355 \nL 67.706406 193.695355 \nL 69.206406 195.195355 \nL 67.706406 196.695355 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 189.904088 \nL 75.898109 188.404088 \nL 77.398109 189.904088 \nL 78.898109 188.404088 \nL 77.398109 186.904088 \nL 78.898109 185.404088 \nL 77.398109 183.904088 \nL 75.898109 185.404088 \nL 74.398109 183.904088 \nL 72.898109 185.404088 \nL 74.398109 186.904088 \nL 72.898109 188.404088 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 194.522776 \nL 107.048332 193.022776 \nL 108.548332 194.522776 \nL 110.048332 193.022776 \nL 108.548332 191.522776 \nL 110.048332 190.022776 \nL 108.548332 188.522776 \nL 107.048332 190.022776 \nL 105.548332 188.522776 \nL 104.048332 190.022776 \nL 105.548332 191.522776 \nL 104.048332 193.022776 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 86.64024 \nC 67.391916 86.64024 68.155046 86.324141 68.717627 85.76156 \nC 69.280208 85.19898 69.596307 84.435849 69.596307 83.64024 \nC 69.596307 82.844631 69.280208 82.0815 68.717627 81.51892 \nC 68.155046 80.956339 67.391916 80.64024 66.596307 80.64024 \nC 65.800698 80.64024 65.037567 80.956339 64.474986 81.51892 \nC 63.912406 82.0815 63.596307 82.844631 63.596307 83.64024 \nC 63.596307 84.435849 63.912406 85.19898 64.474986 85.76156 \nC 65.037567 86.324141 65.800698 86.64024 66.596307 86.64024 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 163.466532 \nC 67.752451 163.466532 68.515581 163.150434 69.078162 162.587853 \nC 69.640743 162.025272 69.956842 161.262142 69.956842 160.466532 \nC 69.956842 159.670923 69.640743 158.907793 69.078162 158.345212 \nC 68.515581 157.782631 67.752451 157.466532 66.956842 157.466532 \nC 66.161233 157.466532 65.398102 157.782631 64.835521 158.345212 \nC 64.272941 158.907793 63.956842 159.670923 63.956842 160.466532 \nC 63.956842 161.262142 64.272941 162.025272 64.835521 162.587853 \nC 65.398102 163.150434 66.161233 163.466532 66.956842 163.466532 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 132.505974 \nC 68.906163 132.505974 69.669293 132.189875 70.231874 131.627294 \nC 70.794455 131.064713 71.110554 130.301583 71.110554 129.505974 \nC 71.110554 128.710364 70.794455 127.947234 70.231874 127.384653 \nC 69.669293 126.822073 68.906163 126.505974 68.110554 126.505974 \nC 67.314944 126.505974 66.551814 126.822073 65.989233 127.384653 \nC 65.426653 127.947234 65.110554 128.710364 65.110554 129.505974 \nC 65.110554 130.301583 65.426653 131.064713 65.989233 131.627294 \nC 66.551814 132.189875 67.314944 132.505974 68.110554 132.505974 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 204.110948 \nC 72.944155 204.110948 73.707285 203.794849 74.269866 203.232269 \nC 74.832447 202.669688 75.148546 201.906558 75.148546 201.110948 \nC 75.148546 200.315339 74.832447 199.552209 74.269866 198.989628 \nC 73.707285 198.427047 72.944155 198.110948 72.148546 198.110948 \nC 71.352936 198.110948 70.589806 198.427047 70.027225 198.989628 \nC 69.464645 199.552209 69.148546 200.315339 69.148546 201.110948 \nC 69.148546 201.906558 69.464645 202.669688 70.027225 203.232269 \nC 70.589806 203.794849 71.352936 204.110948 72.148546 204.110948 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 160.66763 \nC 87.94241 160.66763 88.705541 160.351531 89.268121 159.788951 \nC 89.830702 159.22637 90.146801 158.46324 90.146801 157.66763 \nC 90.146801 156.872021 89.830702 156.108891 89.268121 155.54631 \nC 88.705541 154.983729 87.94241 154.66763 87.146801 154.66763 \nC 86.351192 154.66763 85.588061 154.983729 85.025481 155.54631 \nC 84.4629 156.108891 84.146801 156.872021 84.146801 157.66763 \nC 84.146801 158.46324 84.4629 159.22637 85.025481 159.788951 \nC 85.588061 160.351531 86.351192 160.66763 87.146801 160.66763 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 165.336674 \nC 145.628008 165.336674 146.391139 165.020575 146.953719 164.457995 \nC 147.5163 163.895414 147.832399 163.132284 147.832399 162.336674 \nC 147.832399 161.541065 147.5163 160.777935 146.953719 160.215354 \nC 146.391139 159.652773 145.628008 159.336674 144.832399 159.336674 \nC 144.03679 159.336674 143.273659 159.652773 142.711079 160.215354 \nC 142.148498 160.777935 141.832399 161.541065 141.832399 162.336674 \nC 141.832399 163.132284 142.148498 163.895414 142.711079 164.457995 \nC 143.273659 165.020575 144.03679 165.336674 144.832399 165.336674 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 163.241599 \nC 371.755552 163.241599 372.518683 162.9255 373.081264 162.362919 \nC 373.643844 161.800338 373.959943 161.037208 373.959943 160.241599 \nC 373.959943 159.445989 373.643844 158.682859 373.081264 158.120278 \nC 372.518683 157.557698 371.755552 157.241599 370.959943 157.241599 \nC 370.164334 157.241599 369.401204 157.557698 368.838623 158.120278 \nC 368.276042 158.682859 367.959943 159.445989 367.959943 160.241599 \nC 367.959943 161.037208 368.276042 161.800338 368.838623 162.362919 \nC 369.401204 162.9255 370.164334 163.241599 370.959943 163.241599 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 127.745919 \nC 67.608237 127.745919 68.371367 127.42982 68.933948 126.86724 \nC 69.496529 126.304659 69.812628 125.541529 69.812628 124.745919 \nC 69.812628 123.95031 69.496529 123.18718 68.933948 122.624599 \nC 68.371367 122.062018 67.608237 121.745919 66.812628 121.745919 \nC 66.017019 121.745919 65.253888 122.062018 64.691307 122.624599 \nC 64.128727 123.18718 63.812628 123.95031 63.812628 124.745919 \nC 63.812628 125.541529 64.128727 126.304659 64.691307 126.86724 \nC 65.253888 127.42982 66.017019 127.745919 66.812628 127.745919 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 205.486884 \nC 68.906163 205.486884 69.669293 205.170785 70.231874 204.608204 \nC 70.794455 204.045623 71.110554 203.282493 71.110554 202.486884 \nC 71.110554 201.691274 70.794455 200.928144 70.231874 200.365563 \nC 69.669293 199.802983 68.906163 199.486884 68.110554 199.486884 \nC 67.314944 199.486884 66.551814 199.802983 65.989233 200.365563 \nC 65.426653 200.928144 65.110554 201.691274 65.110554 202.486884 \nC 65.110554 203.282493 65.426653 204.045623 65.989233 204.608204 \nC 66.551814 205.170785 67.314944 205.486884 68.110554 205.486884 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 204.164028 \nC 71.502015 204.164028 72.265145 203.847929 72.827726 203.285349 \nC 73.390307 202.722768 73.706406 201.959638 73.706406 201.164028 \nC 73.706406 200.368419 73.390307 199.605289 72.827726 199.042708 \nC 72.265145 198.480127 71.502015 198.164028 70.706406 198.164028 \nC 69.910796 198.164028 69.147666 198.480127 68.585085 199.042708 \nC 68.022505 199.605289 67.706406 200.368419 67.706406 201.164028 \nC 67.706406 201.959638 68.022505 202.722768 68.585085 203.285349 \nC 69.147666 203.847929 69.910796 204.164028 70.706406 204.164028 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 212.074017 \nC 76.693719 212.074017 77.456849 211.757918 78.01943 211.195337 \nC 78.582011 210.632756 78.898109 209.869626 78.898109 209.074017 \nC 78.898109 208.278407 78.582011 207.515277 78.01943 206.952696 \nC 77.456849 206.390116 76.693719 206.074017 75.898109 206.074017 \nC 75.1025 206.074017 74.33937 206.390116 73.776789 206.952696 \nC 73.214208 207.515277 72.898109 208.278407 72.898109 209.074017 \nC 72.898109 209.869626 73.214208 210.632756 73.776789 211.195337 \nC 74.33937 211.757918 75.1025 212.074017 75.898109 212.074017 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 20.083636 \nC 87.077126 20.083636 87.840257 19.767537 88.402837 19.204957 \nC 88.965418 18.642376 89.281517 17.879246 89.281517 17.083636 \nC 89.281517 16.288027 88.965418 15.524897 88.402837 14.962316 \nC 87.840257 14.399735 87.077126 14.083636 86.281517 14.083636 \nC 85.485908 14.083636 84.722778 14.399735 84.160197 14.962316 \nC 83.597616 15.524897 83.281517 16.288027 83.281517 17.083636 \nC 83.281517 17.879246 83.597616 18.642376 84.160197 19.204957 \nC 84.722778 19.767537 85.485908 20.083636 86.281517 20.083636 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.003941 \nC 107.843942 21.003941 108.607072 20.687842 109.169653 20.125262 \nC 109.732233 19.562681 110.048332 18.799551 110.048332 18.003941 \nC 110.048332 17.208332 109.732233 16.445202 109.169653 15.882621 \nC 108.607072 15.32004 107.843942 15.003941 107.048332 15.003941 \nC 106.252723 15.003941 105.489593 15.32004 104.927012 15.882621 \nC 104.364431 16.445202 104.048332 17.208332 104.048332 18.003941 \nC 104.048332 18.799551 104.364431 19.562681 104.927012 20.125262 \nC 105.489593 20.687842 106.252723 21.003941 107.048332 21.003941 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 101.386407 \nL 66.596307 99.886407 \nL 68.096307 101.386407 \nL 69.596307 99.886407 \nL 68.096307 98.386407 \nL 69.596307 96.886407 \nL 68.096307 95.386407 \nL 66.596307 96.886407 \nL 65.096307 95.386407 \nL 63.596307 96.886407 \nL 65.096307 98.386407 \nL 63.596307 99.886407 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 202.882929 \nL 66.956842 201.382929 \nL 68.456842 202.882929 \nL 69.956842 201.382929 \nL 68.456842 199.882929 \nL 69.956842 198.382929 \nL 68.456842 196.882929 \nL 66.956842 198.382929 \nL 65.456842 196.882929 \nL 63.956842 198.382929 \nL 65.456842 199.882929 \nL 63.956842 201.382929 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 172.748148 \nL 68.110554 171.248148 \nL 69.610554 172.748148 \nL 71.110554 171.248148 \nL 69.610554 169.748148 \nL 71.110554 168.248148 \nL 69.610554 166.748148 \nL 68.110554 168.248148 \nL 66.610554 166.748148 \nL 65.110554 168.248148 \nL 66.610554 169.748148 \nL 65.110554 171.248148 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.648546 148.767421 \nL 72.148546 147.267421 \nL 73.648546 148.767421 \nL 75.148546 147.267421 \nL 73.648546 145.767421 \nL 75.148546 144.267421 \nL 73.648546 142.767421 \nL 72.148546 144.267421 \nL 70.648546 142.767421 \nL 69.148546 144.267421 \nL 70.648546 145.767421 \nL 69.148546 147.267421 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 171.774557 \nL 87.146801 170.274557 \nL 88.646801 171.774557 \nL 90.146801 170.274557 \nL 88.646801 168.774557 \nL 90.146801 167.274557 \nL 88.646801 165.774557 \nL 87.146801 167.274557 \nL 85.646801 165.774557 \nL 84.146801 167.274557 \nL 85.646801 168.774557 \nL 84.146801 170.274557 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 142.528931 \nL 144.832399 141.028931 \nL 146.332399 142.528931 \nL 147.832399 141.028931 \nL 146.332399 139.528931 \nL 147.832399 138.028931 \nL 146.332399 136.528931 \nL 144.832399 138.028931 \nL 143.332399 136.528931 \nL 141.832399 138.028931 \nL 143.332399 139.528931 \nL 141.832399 141.028931 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 157.21628 \nL 370.959943 155.71628 \nL 372.459943 157.21628 \nL 373.959943 155.71628 \nL 372.459943 154.21628 \nL 373.959943 152.71628 \nL 372.459943 151.21628 \nL 370.959943 152.71628 \nL 369.459943 151.21628 \nL 367.959943 152.71628 \nL 369.459943 154.21628 \nL 367.959943 155.71628 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 133.233646 \nL 66.812628 131.733646 \nL 68.312628 133.233646 \nL 69.812628 131.733646 \nL 68.312628 130.233646 \nL 69.812628 128.733646 \nL 68.312628 127.233646 \nL 66.812628 128.733646 \nL 65.312628 127.233646 \nL 63.812628 128.733646 \nL 65.312628 130.233646 \nL 63.812628 131.733646 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 136.247548 \nL 68.110554 134.747548 \nL 69.610554 136.247548 \nL 71.110554 134.747548 \nL 69.610554 133.247548 \nL 71.110554 131.747548 \nL 69.610554 130.247548 \nL 68.110554 131.747548 \nL 66.610554 130.247548 \nL 65.110554 131.747548 \nL 66.610554 133.247548 \nL 65.110554 134.747548 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 201.071345 \nL 70.706406 199.571345 \nL 72.206406 201.071345 \nL 73.706406 199.571345 \nL 72.206406 198.071345 \nL 73.706406 196.571345 \nL 72.206406 195.071345 \nL 70.706406 196.571345 \nL 69.206406 195.071345 \nL 67.706406 196.571345 \nL 69.206406 198.071345 \nL 67.706406 199.571345 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 191.681862 \nL 75.898109 190.181862 \nL 77.398109 191.681862 \nL 78.898109 190.181862 \nL 77.398109 188.681862 \nL 78.898109 187.181862 \nL 77.398109 185.681862 \nL 75.898109 187.181862 \nL 74.398109 185.681862 \nL 72.898109 187.181862 \nL 74.398109 188.681862 \nL 72.898109 190.181862 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 201.883199 \nL 86.281517 200.383199 \nL 87.781517 201.883199 \nL 89.281517 200.383199 \nL 87.781517 198.883199 \nL 89.281517 197.383199 \nL 87.781517 195.883199 \nL 86.281517 197.383199 \nL 84.781517 195.883199 \nL 83.281517 197.383199 \nL 84.781517 198.883199 \nL 83.281517 200.383199 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 139.638382 \nL 107.048332 138.138382 \nL 108.548332 139.638382 \nL 110.048332 138.138382 \nL 108.548332 136.638382 \nL 110.048332 135.138382 \nL 108.548332 133.638382 \nL 107.048332 135.138382 \nL 105.548332 133.638382 \nL 104.048332 135.138382 \nL 105.548332 136.638382 \nL 104.048332 138.138382 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 82.700202 \nC 67.391916 82.700202 68.155046 82.384103 68.717627 81.821522 \nC 69.280208 81.258942 69.596307 80.495811 69.596307 79.700202 \nC 69.596307 78.904593 69.280208 78.141462 68.717627 77.578882 \nC 68.155046 77.016301 67.391916 76.700202 66.596307 76.700202 \nC 65.800698 76.700202 65.037567 77.016301 64.474986 77.578882 \nC 63.912406 78.141462 63.596307 78.904593 63.596307 79.700202 \nC 63.596307 80.495811 63.912406 81.258942 64.474986 81.821522 \nC 65.037567 82.384103 65.800698 82.700202 66.596307 82.700202 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 186.852266 \nC 67.752451 186.852266 68.515581 186.536167 69.078162 185.973586 \nC 69.640743 185.411005 69.956842 184.647875 69.956842 183.852266 \nC 69.956842 183.056656 69.640743 182.293526 69.078162 181.730945 \nC 68.515581 181.168365 67.752451 180.852266 66.956842 180.852266 \nC 66.161233 180.852266 65.398102 181.168365 64.835521 181.730945 \nC 64.272941 182.293526 63.956842 183.056656 63.956842 183.852266 \nC 63.956842 184.647875 64.272941 185.411005 64.835521 185.973586 \nC 65.398102 186.536167 66.161233 186.852266 66.956842 186.852266 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 186.893792 \nC 68.906163 186.893792 69.669293 186.577693 70.231874 186.015112 \nC 70.794455 185.452531 71.110554 184.689401 71.110554 183.893792 \nC 71.110554 183.098182 70.794455 182.335052 70.231874 181.772471 \nC 69.669293 181.20989 68.906163 180.893792 68.110554 180.893792 \nC 67.314944 180.893792 66.551814 181.20989 65.989233 181.772471 \nC 65.426653 182.335052 65.110554 183.098182 65.110554 183.893792 \nC 65.110554 184.689401 65.426653 185.452531 65.989233 186.015112 \nC 66.551814 186.577693 67.314944 186.893792 68.110554 186.893792 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 173.854077 \nC 72.944155 173.854077 73.707285 173.537978 74.269866 172.975398 \nC 74.832447 172.412817 75.148546 171.649687 75.148546 170.854077 \nC 75.148546 170.058468 74.832447 169.295338 74.269866 168.732757 \nC 73.707285 168.170176 72.944155 167.854077 72.148546 167.854077 \nC 71.352936 167.854077 70.589806 168.170176 70.027225 168.732757 \nC 69.464645 169.295338 69.148546 170.058468 69.148546 170.854077 \nC 69.148546 171.649687 69.464645 172.412817 70.027225 172.975398 \nC 70.589806 173.537978 71.352936 173.854077 72.148546 173.854077 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 164.972219 \nC 87.94241 164.972219 88.705541 164.65612 89.268121 164.093539 \nC 89.830702 163.530958 90.146801 162.767828 90.146801 161.972219 \nC 90.146801 161.176609 89.830702 160.413479 89.268121 159.850898 \nC 88.705541 159.288317 87.94241 158.972219 87.146801 158.972219 \nC 86.351192 158.972219 85.588061 159.288317 85.025481 159.850898 \nC 84.4629 160.413479 84.146801 161.176609 84.146801 161.972219 \nC 84.146801 162.767828 84.4629 163.530958 85.025481 164.093539 \nC 85.588061 164.65612 86.351192 164.972219 87.146801 164.972219 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 155.400512 \nC 145.628008 155.400512 146.391139 155.084413 146.953719 154.521832 \nC 147.5163 153.959251 147.832399 153.196121 147.832399 152.400512 \nC 147.832399 151.604902 147.5163 150.841772 146.953719 150.279191 \nC 146.391139 149.716611 145.628008 149.400512 144.832399 149.400512 \nC 144.03679 149.400512 143.273659 149.716611 142.711079 150.279191 \nC 142.148498 150.841772 141.832399 151.604902 141.832399 152.400512 \nC 141.832399 153.196121 142.148498 153.959251 142.711079 154.521832 \nC 143.273659 155.084413 144.03679 155.400512 144.832399 155.400512 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 159.683659 \nC 371.755552 159.683659 372.518683 159.36756 373.081264 158.804979 \nC 373.643844 158.242398 373.959943 157.479268 373.959943 156.683659 \nC 373.959943 155.888049 373.643844 155.124919 373.081264 154.562338 \nC 372.518683 153.999758 371.755552 153.683659 370.959943 153.683659 \nC 370.164334 153.683659 369.401204 153.999758 368.838623 154.562338 \nC 368.276042 155.124919 367.959943 155.888049 367.959943 156.683659 \nC 367.959943 157.479268 368.276042 158.242398 368.838623 158.804979 \nC 369.401204 159.36756 370.164334 159.683659 370.959943 159.683659 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 174.488824 \nC 67.608237 174.488824 68.371367 174.172725 68.933948 173.610144 \nC 69.496529 173.047563 69.812628 172.284433 69.812628 171.488824 \nC 69.812628 170.693214 69.496529 169.930084 68.933948 169.367503 \nC 68.371367 168.804923 67.608237 168.488824 66.812628 168.488824 \nC 66.017019 168.488824 65.253888 168.804923 64.691307 169.367503 \nC 64.128727 169.930084 63.812628 170.693214 63.812628 171.488824 \nC 63.812628 172.284433 64.128727 173.047563 64.691307 173.610144 \nC 65.253888 174.172725 66.017019 174.488824 66.812628 174.488824 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 209.109583 \nC 68.906163 209.109583 69.669293 208.793484 70.231874 208.230903 \nC 70.794455 207.668322 71.110554 206.905192 71.110554 206.109583 \nC 71.110554 205.313973 70.794455 204.550843 70.231874 203.988262 \nC 69.669293 203.425682 68.906163 203.109583 68.110554 203.109583 \nC 67.314944 203.109583 66.551814 203.425682 65.989233 203.988262 \nC 65.426653 204.550843 65.110554 205.313973 65.110554 206.109583 \nC 65.110554 206.905192 65.426653 207.668322 65.989233 208.230903 \nC 66.551814 208.793484 67.314944 209.109583 68.110554 209.109583 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 189.531318 \nC 71.502015 189.531318 72.265145 189.215219 72.827726 188.652638 \nC 73.390307 188.090058 73.706406 187.326927 73.706406 186.531318 \nC 73.706406 185.735709 73.390307 184.972579 72.827726 184.409998 \nC 72.265145 183.847417 71.502015 183.531318 70.706406 183.531318 \nC 69.910796 183.531318 69.147666 183.847417 68.585085 184.409998 \nC 68.022505 184.972579 67.706406 185.735709 67.706406 186.531318 \nC 67.706406 187.326927 68.022505 188.090058 68.585085 188.652638 \nC 69.147666 189.215219 69.910796 189.531318 70.706406 189.531318 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 192.933699 \nC 76.693719 192.933699 77.456849 192.6176 78.01943 192.05502 \nC 78.582011 191.492439 78.898109 190.729309 78.898109 189.933699 \nC 78.898109 189.13809 78.582011 188.37496 78.01943 187.812379 \nC 77.456849 187.249798 76.693719 186.933699 75.898109 186.933699 \nC 75.1025 186.933699 74.33937 187.249798 73.776789 187.812379 \nC 73.214208 188.37496 72.898109 189.13809 72.898109 189.933699 \nC 72.898109 190.729309 73.214208 191.492439 73.776789 192.05502 \nC 74.33937 192.6176 75.1025 192.933699 75.898109 192.933699 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 21.003932 \nC 87.077126 21.003932 87.840257 20.687833 88.402837 20.125253 \nC 88.965418 19.562672 89.281517 18.799541 89.281517 18.003932 \nC 89.281517 17.208323 88.965418 16.445193 88.402837 15.882612 \nC 87.840257 15.320031 87.077126 15.003932 86.281517 15.003932 \nC 85.485908 15.003932 84.722778 15.320031 84.160197 15.882612 \nC 83.597616 16.445193 83.281517 17.208323 83.281517 18.003932 \nC 83.281517 18.799541 83.597616 19.562672 84.160197 20.125253 \nC 84.722778 20.687833 85.485908 21.003932 86.281517 21.003932 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.004596 \nC 107.843942 21.004596 108.607072 20.688497 109.169653 20.125916 \nC 109.732233 19.563336 110.048332 18.800205 110.048332 18.004596 \nC 110.048332 17.208987 109.732233 16.445856 109.169653 15.883276 \nC 108.607072 15.320695 107.843942 15.004596 107.048332 15.004596 \nC 106.252723 15.004596 105.489593 15.320695 104.927012 15.883276 \nC 104.364431 16.445856 104.048332 17.208987 104.048332 18.004596 \nC 104.048332 18.800205 104.364431 19.563336 104.927012 20.125916 \nC 105.489593 20.688497 106.252723 21.004596 107.048332 21.004596 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 55.533348 \nL 66.596307 54.033348 \nL 68.096307 55.533348 \nL 69.596307 54.033348 \nL 68.096307 52.533348 \nL 69.596307 51.033348 \nL 68.096307 49.533348 \nL 66.596307 51.033348 \nL 65.096307 49.533348 \nL 63.596307 51.033348 \nL 65.096307 52.533348 \nL 63.596307 54.033348 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 162.090166 \nL 66.956842 160.590166 \nL 68.456842 162.090166 \nL 69.956842 160.590166 \nL 68.456842 159.090166 \nL 69.956842 157.590166 \nL 68.456842 156.090166 \nL 66.956842 157.590166 \nL 65.456842 156.090166 \nL 63.956842 157.590166 \nL 65.456842 159.090166 \nL 63.956842 160.590166 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 183.675457 \nL 68.110554 182.175457 \nL 69.610554 183.675457 \nL 71.110554 182.175457 \nL 69.610554 180.675457 \nL 71.110554 179.175457 \nL 69.610554 177.675457 \nL 68.110554 179.175457 \nL 66.610554 177.675457 \nL 65.110554 179.175457 \nL 66.610554 180.675457 \nL 65.110554 182.175457 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.648546 159.210804 \nL 72.148546 157.710804 \nL 73.648546 159.210804 \nL 75.148546 157.710804 \nL 73.648546 156.210804 \nL 75.148546 154.710804 \nL 73.648546 153.210804 \nL 72.148546 154.710804 \nL 70.648546 153.210804 \nL 69.148546 154.710804 \nL 70.648546 156.210804 \nL 69.148546 157.710804 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 176.713011 \nL 144.832399 175.213011 \nL 146.332399 176.713011 \nL 147.832399 175.213011 \nL 146.332399 173.713011 \nL 147.832399 172.213011 \nL 146.332399 170.713011 \nL 144.832399 172.213011 \nL 143.332399 170.713011 \nL 141.832399 172.213011 \nL 143.332399 173.713011 \nL 141.832399 175.213011 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 147.002533 \nL 370.959943 145.502533 \nL 372.459943 147.002533 \nL 373.959943 145.502533 \nL 372.459943 144.002533 \nL 373.959943 142.502533 \nL 372.459943 141.002533 \nL 370.959943 142.502533 \nL 369.459943 141.002533 \nL 367.959943 142.502533 \nL 369.459943 144.002533 \nL 367.959943 145.502533 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 158.046361 \nL 66.812628 156.546361 \nL 68.312628 158.046361 \nL 69.812628 156.546361 \nL 68.312628 155.046361 \nL 69.812628 153.546361 \nL 68.312628 152.046361 \nL 66.812628 153.546361 \nL 65.312628 152.046361 \nL 63.812628 153.546361 \nL 65.312628 155.046361 \nL 63.812628 156.546361 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 164.867198 \nL 68.110554 163.367198 \nL 69.610554 164.867198 \nL 71.110554 163.367198 \nL 69.610554 161.867198 \nL 71.110554 160.367198 \nL 69.610554 158.867198 \nL 68.110554 160.367198 \nL 66.610554 158.867198 \nL 65.110554 160.367198 \nL 66.610554 161.867198 \nL 65.110554 163.367198 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 171.180735 \nL 70.706406 169.680735 \nL 72.206406 171.180735 \nL 73.706406 169.680735 \nL 72.206406 168.180735 \nL 73.706406 166.680735 \nL 72.206406 165.180735 \nL 70.706406 166.680735 \nL 69.206406 165.180735 \nL 67.706406 166.680735 \nL 69.206406 168.180735 \nL 67.706406 169.680735 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 178.285212 \nL 75.898109 176.785212 \nL 77.398109 178.285212 \nL 78.898109 176.785212 \nL 77.398109 175.285212 \nL 78.898109 173.785212 \nL 77.398109 172.285212 \nL 75.898109 173.785212 \nL 74.398109 172.285212 \nL 72.898109 173.785212 \nL 74.398109 175.285212 \nL 72.898109 176.785212 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 193.396883 \nL 86.281517 191.896883 \nL 87.781517 193.396883 \nL 89.281517 191.896883 \nL 87.781517 190.396883 \nL 89.281517 188.896883 \nL 87.781517 187.396883 \nL 86.281517 188.896883 \nL 84.781517 187.396883 \nL 83.281517 188.896883 \nL 84.781517 190.396883 \nL 83.281517 191.896883 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 193.584717 \nL 107.048332 192.084717 \nL 108.548332 193.584717 \nL 110.048332 192.084717 \nL 108.548332 190.584717 \nL 110.048332 189.084717 \nL 108.548332 187.584717 \nL 107.048332 189.084717 \nL 105.548332 187.584717 \nL 104.048332 189.084717 \nL 105.548332 190.584717 \nL 104.048332 192.084717 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 61.406279 \nC 67.391916 61.406279 68.155046 61.09018 68.717627 60.527599 \nC 69.280208 59.965019 69.596307 59.201888 69.596307 58.406279 \nC 69.596307 57.61067 69.280208 56.847539 68.717627 56.284959 \nC 68.155046 55.722378 67.391916 55.406279 66.596307 55.406279 \nC 65.800698 55.406279 65.037567 55.722378 64.474986 56.284959 \nC 63.912406 56.847539 63.596307 57.61067 63.596307 58.406279 \nC 63.596307 59.201888 63.912406 59.965019 64.474986 60.527599 \nC 65.037567 61.09018 65.800698 61.406279 66.596307 61.406279 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 185.583794 \nC 67.752451 185.583794 68.515581 185.267695 69.078162 184.705115 \nC 69.640743 184.142534 69.956842 183.379404 69.956842 182.583794 \nC 69.956842 181.788185 69.640743 181.025055 69.078162 180.462474 \nC 68.515581 179.899893 67.752451 179.583794 66.956842 179.583794 \nC 66.161233 179.583794 65.398102 179.899893 64.835521 180.462474 \nC 64.272941 181.025055 63.956842 181.788185 63.956842 182.583794 \nC 63.956842 183.379404 64.272941 184.142534 64.835521 184.705115 \nC 65.398102 185.267695 66.161233 185.583794 66.956842 185.583794 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 182.328242 \nC 68.906163 182.328242 69.669293 182.012143 70.231874 181.449562 \nC 70.794455 180.886981 71.110554 180.123851 71.110554 179.328242 \nC 71.110554 178.532633 70.794455 177.769502 70.231874 177.206922 \nC 69.669293 176.644341 68.906163 176.328242 68.110554 176.328242 \nC 67.314944 176.328242 66.551814 176.644341 65.989233 177.206922 \nC 65.426653 177.769502 65.110554 178.532633 65.110554 179.328242 \nC 65.110554 180.123851 65.426653 180.886981 65.989233 181.449562 \nC 66.551814 182.012143 67.314944 182.328242 68.110554 182.328242 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 176.828359 \nC 72.944155 176.828359 73.707285 176.51226 74.269866 175.949679 \nC 74.832447 175.387099 75.148546 174.623968 75.148546 173.828359 \nC 75.148546 173.03275 74.832447 172.26962 74.269866 171.707039 \nC 73.707285 171.144458 72.944155 170.828359 72.148546 170.828359 \nC 71.352936 170.828359 70.589806 171.144458 70.027225 171.707039 \nC 69.464645 172.26962 69.148546 173.03275 69.148546 173.828359 \nC 69.148546 174.623968 69.464645 175.387099 70.027225 175.949679 \nC 70.589806 176.51226 71.352936 176.828359 72.148546 176.828359 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 168.486698 \nC 87.94241 168.486698 88.705541 168.170599 89.268121 167.608019 \nC 89.830702 167.045438 90.146801 166.282308 90.146801 165.486698 \nC 90.146801 164.691089 89.830702 163.927959 89.268121 163.365378 \nC 88.705541 162.802797 87.94241 162.486698 87.146801 162.486698 \nC 86.351192 162.486698 85.588061 162.802797 85.025481 163.365378 \nC 84.4629 163.927959 84.146801 164.691089 84.146801 165.486698 \nC 84.146801 166.282308 84.4629 167.045438 85.025481 167.608019 \nC 85.588061 168.170599 86.351192 168.486698 87.146801 168.486698 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 180.789649 \nC 145.628008 180.789649 146.391139 180.47355 146.953719 179.91097 \nC 147.5163 179.348389 147.832399 178.585259 147.832399 177.789649 \nC 147.832399 176.99404 147.5163 176.23091 146.953719 175.668329 \nC 146.391139 175.105748 145.628008 174.789649 144.832399 174.789649 \nC 144.03679 174.789649 143.273659 175.105748 142.711079 175.668329 \nC 142.148498 176.23091 141.832399 176.99404 141.832399 177.789649 \nC 141.832399 178.585259 142.148498 179.348389 142.711079 179.91097 \nC 143.273659 180.47355 144.03679 180.789649 144.832399 180.789649 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 186.834967 \nC 371.755552 186.834967 372.518683 186.518868 373.081264 185.956287 \nC 373.643844 185.393706 373.959943 184.630576 373.959943 183.834967 \nC 373.959943 183.039358 373.643844 182.276227 373.081264 181.713647 \nC 372.518683 181.151066 371.755552 180.834967 370.959943 180.834967 \nC 370.164334 180.834967 369.401204 181.151066 368.838623 181.713647 \nC 368.276042 182.276227 367.959943 183.039358 367.959943 183.834967 \nC 367.959943 184.630576 368.276042 185.393706 368.838623 185.956287 \nC 369.401204 186.518868 370.164334 186.834967 370.959943 186.834967 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 162.651793 \nC 67.608237 162.651793 68.371367 162.335694 68.933948 161.773113 \nC 69.496529 161.210532 69.812628 160.447402 69.812628 159.651793 \nC 69.812628 158.856183 69.496529 158.093053 68.933948 157.530472 \nC 68.371367 156.967891 67.608237 156.651793 66.812628 156.651793 \nC 66.017019 156.651793 65.253888 156.967891 64.691307 157.530472 \nC 64.128727 158.093053 63.812628 158.856183 63.812628 159.651793 \nC 63.812628 160.447402 64.128727 161.210532 64.691307 161.773113 \nC 65.253888 162.335694 66.017019 162.651793 66.812628 162.651793 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 170.909081 \nC 68.906163 170.909081 69.669293 170.592982 70.231874 170.030402 \nC 70.794455 169.467821 71.110554 168.704691 71.110554 167.909081 \nC 71.110554 167.113472 70.794455 166.350342 70.231874 165.787761 \nC 69.669293 165.22518 68.906163 164.909081 68.110554 164.909081 \nC 67.314944 164.909081 66.551814 165.22518 65.989233 165.787761 \nC 65.426653 166.350342 65.110554 167.113472 65.110554 167.909081 \nC 65.110554 168.704691 65.426653 169.467821 65.989233 170.030402 \nC 66.551814 170.592982 67.314944 170.909081 68.110554 170.909081 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 209.642662 \nC 71.502015 209.642662 72.265145 209.326563 72.827726 208.763983 \nC 73.390307 208.201402 73.706406 207.438272 73.706406 206.642662 \nC 73.706406 205.847053 73.390307 205.083923 72.827726 204.521342 \nC 72.265145 203.958761 71.502015 203.642662 70.706406 203.642662 \nC 69.910796 203.642662 69.147666 203.958761 68.585085 204.521342 \nC 68.022505 205.083923 67.706406 205.847053 67.706406 206.642662 \nC 67.706406 207.438272 68.022505 208.201402 68.585085 208.763983 \nC 69.147666 209.326563 69.910796 209.642662 70.706406 209.642662 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 197.728834 \nC 76.693719 197.728834 77.456849 197.412736 78.01943 196.850155 \nC 78.582011 196.287574 78.898109 195.524444 78.898109 194.728834 \nC 78.898109 193.933225 78.582011 193.170095 78.01943 192.607514 \nC 77.456849 192.044933 76.693719 191.728834 75.898109 191.728834 \nC 75.1025 191.728834 74.33937 192.044933 73.776789 192.607514 \nC 73.214208 193.170095 72.898109 193.933225 72.898109 194.728834 \nC 72.898109 195.524444 73.214208 196.287574 73.776789 196.850155 \nC 74.33937 197.412736 75.1025 197.728834 75.898109 197.728834 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 21.006355 \nC 87.077126 21.006355 87.840257 20.690256 88.402837 20.127675 \nC 88.965418 19.565095 89.281517 18.801964 89.281517 18.006355 \nC 89.281517 17.210746 88.965418 16.447615 88.402837 15.885035 \nC 87.840257 15.322454 87.077126 15.006355 86.281517 15.006355 \nC 85.485908 15.006355 84.722778 15.322454 84.160197 15.885035 \nC 83.597616 16.447615 83.281517 17.210746 83.281517 18.006355 \nC 83.281517 18.801964 83.597616 19.565095 84.160197 20.127675 \nC 84.722778 20.690256 85.485908 21.006355 86.281517 21.006355 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.003918 \nC 107.843942 21.003918 108.607072 20.687819 109.169653 20.125238 \nC 109.732233 19.562657 110.048332 18.799527 110.048332 18.003918 \nC 110.048332 17.208308 109.732233 16.445178 109.169653 15.882597 \nC 108.607072 15.320017 107.843942 15.003918 107.048332 15.003918 \nC 106.252723 15.003918 105.489593 15.320017 104.927012 15.882597 \nC 104.364431 16.445178 104.048332 17.208308 104.048332 18.003918 \nC 104.048332 18.799527 104.364431 19.562657 104.927012 20.125238 \nC 105.489593 20.687819 106.252723 21.003918 107.048332 21.003918 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 92.088293 \nL 66.596307 90.588293 \nL 68.096307 92.088293 \nL 69.596307 90.588293 \nL 68.096307 89.088293 \nL 69.596307 87.588293 \nL 68.096307 86.088293 \nL 66.596307 87.588293 \nL 65.096307 86.088293 \nL 63.596307 87.588293 \nL 65.096307 89.088293 \nL 63.596307 90.588293 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 155.767005 \nL 66.956842 154.267005 \nL 68.456842 155.767005 \nL 69.956842 154.267005 \nL 68.456842 152.767005 \nL 69.956842 151.267005 \nL 68.456842 149.767005 \nL 66.956842 151.267005 \nL 65.456842 149.767005 \nL 63.956842 151.267005 \nL 65.456842 152.767005 \nL 63.956842 154.267005 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 189.009953 \nL 68.110554 187.509953 \nL 69.610554 189.009953 \nL 71.110554 187.509953 \nL 69.610554 186.009953 \nL 71.110554 184.509953 \nL 69.610554 183.009953 \nL 68.110554 184.509953 \nL 66.610554 183.009953 \nL 65.110554 184.509953 \nL 66.610554 186.009953 \nL 65.110554 187.509953 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.648546 142.091991 \nL 72.148546 140.591991 \nL 73.648546 142.091991 \nL 75.148546 140.591991 \nL 73.648546 139.091991 \nL 75.148546 137.591991 \nL 73.648546 136.091991 \nL 72.148546 137.591991 \nL 70.648546 136.091991 \nL 69.148546 137.591991 \nL 70.648546 139.091991 \nL 69.148546 140.591991 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 162.774591 \nL 87.146801 161.274591 \nL 88.646801 162.774591 \nL 90.146801 161.274591 \nL 88.646801 159.774591 \nL 90.146801 158.274591 \nL 88.646801 156.774591 \nL 87.146801 158.274591 \nL 85.646801 156.774591 \nL 84.146801 158.274591 \nL 85.646801 159.774591 \nL 84.146801 161.274591 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 151.205076 \nL 144.832399 149.705076 \nL 146.332399 151.205076 \nL 147.832399 149.705076 \nL 146.332399 148.205076 \nL 147.832399 146.705076 \nL 146.332399 145.205076 \nL 144.832399 146.705076 \nL 143.332399 145.205076 \nL 141.832399 146.705076 \nL 143.332399 148.205076 \nL 141.832399 149.705076 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 148.650451 \nL 370.959943 147.150451 \nL 372.459943 148.650451 \nL 373.959943 147.150451 \nL 372.459943 145.650451 \nL 373.959943 144.150451 \nL 372.459943 142.650451 \nL 370.959943 144.150451 \nL 369.459943 142.650451 \nL 367.959943 144.150451 \nL 369.459943 145.650451 \nL 367.959943 147.150451 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 175.970597 \nL 66.812628 174.470597 \nL 68.312628 175.970597 \nL 69.812628 174.470597 \nL 68.312628 172.970597 \nL 69.812628 171.470597 \nL 68.312628 169.970597 \nL 66.812628 171.470597 \nL 65.312628 169.970597 \nL 63.812628 171.470597 \nL 65.312628 172.970597 \nL 63.812628 174.470597 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 184.206225 \nL 68.110554 182.706225 \nL 69.610554 184.206225 \nL 71.110554 182.706225 \nL 69.610554 181.206225 \nL 71.110554 179.706225 \nL 69.610554 178.206225 \nL 68.110554 179.706225 \nL 66.610554 178.206225 \nL 65.110554 179.706225 \nL 66.610554 181.206225 \nL 65.110554 182.706225 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 175.509722 \nL 70.706406 174.009722 \nL 72.206406 175.509722 \nL 73.706406 174.009722 \nL 72.206406 172.509722 \nL 73.706406 171.009722 \nL 72.206406 169.509722 \nL 70.706406 171.009722 \nL 69.206406 169.509722 \nL 67.706406 171.009722 \nL 69.206406 172.509722 \nL 67.706406 174.009722 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 184.165671 \nL 75.898109 182.665671 \nL 77.398109 184.165671 \nL 78.898109 182.665671 \nL 77.398109 181.165671 \nL 78.898109 179.665671 \nL 77.398109 178.165671 \nL 75.898109 179.665671 \nL 74.398109 178.165671 \nL 72.898109 179.665671 \nL 74.398109 181.165671 \nL 72.898109 182.665671 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 181.161394 \nL 86.281517 179.661394 \nL 87.781517 181.161394 \nL 89.281517 179.661394 \nL 87.781517 178.161394 \nL 89.281517 176.661394 \nL 87.781517 175.161394 \nL 86.281517 176.661394 \nL 84.781517 175.161394 \nL 83.281517 176.661394 \nL 84.781517 178.161394 \nL 83.281517 179.661394 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 179.200897 \nL 107.048332 177.700897 \nL 108.548332 179.200897 \nL 110.048332 177.700897 \nL 108.548332 176.200897 \nL 110.048332 174.700897 \nL 108.548332 173.200897 \nL 107.048332 174.700897 \nL 105.548332 173.200897 \nL 104.048332 174.700897 \nL 105.548332 176.200897 \nL 104.048332 177.700897 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 91.354259 \nC 67.391916 91.354259 68.155046 91.03816 68.717627 90.475579 \nC 69.280208 89.912999 69.596307 89.149868 69.596307 88.354259 \nC 69.596307 87.55865 69.280208 86.79552 68.717627 86.232939 \nC 68.155046 85.670358 67.391916 85.354259 66.596307 85.354259 \nC 65.800698 85.354259 65.037567 85.670358 64.474986 86.232939 \nC 63.912406 86.79552 63.596307 87.55865 63.596307 88.354259 \nC 63.596307 89.149868 63.912406 89.912999 64.474986 90.475579 \nC 65.037567 91.03816 65.800698 91.354259 66.596307 91.354259 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 153.55249 \nC 67.752451 153.55249 68.515581 153.236391 69.078162 152.673811 \nC 69.640743 152.11123 69.956842 151.3481 69.956842 150.55249 \nC 69.956842 149.756881 69.640743 148.993751 69.078162 148.43117 \nC 68.515581 147.868589 67.752451 147.55249 66.956842 147.55249 \nC 66.161233 147.55249 65.398102 147.868589 64.835521 148.43117 \nC 64.272941 148.993751 63.956842 149.756881 63.956842 150.55249 \nC 63.956842 151.3481 64.272941 152.11123 64.835521 152.673811 \nC 65.398102 153.236391 66.161233 153.55249 66.956842 153.55249 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 170.315272 \nC 68.906163 170.315272 69.669293 169.999173 70.231874 169.436592 \nC 70.794455 168.874012 71.110554 168.110881 71.110554 167.315272 \nC 71.110554 166.519663 70.794455 165.756532 70.231874 165.193952 \nC 69.669293 164.631371 68.906163 164.315272 68.110554 164.315272 \nC 67.314944 164.315272 66.551814 164.631371 65.989233 165.193952 \nC 65.426653 165.756532 65.110554 166.519663 65.110554 167.315272 \nC 65.110554 168.110881 65.426653 168.874012 65.989233 169.436592 \nC 66.551814 169.999173 67.314944 170.315272 68.110554 170.315272 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 188.37733 \nC 72.944155 188.37733 73.707285 188.061231 74.269866 187.49865 \nC 74.832447 186.93607 75.148546 186.172939 75.148546 185.37733 \nC 75.148546 184.581721 74.832447 183.818591 74.269866 183.25601 \nC 73.707285 182.693429 72.944155 182.37733 72.148546 182.37733 \nC 71.352936 182.37733 70.589806 182.693429 70.027225 183.25601 \nC 69.464645 183.818591 69.148546 184.581721 69.148546 185.37733 \nC 69.148546 186.172939 69.464645 186.93607 70.027225 187.49865 \nC 70.589806 188.061231 71.352936 188.37733 72.148546 188.37733 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 171.507399 \nC 87.94241 171.507399 88.705541 171.1913 89.268121 170.62872 \nC 89.830702 170.066139 90.146801 169.303009 90.146801 168.507399 \nC 90.146801 167.71179 89.830702 166.94866 89.268121 166.386079 \nC 88.705541 165.823498 87.94241 165.507399 87.146801 165.507399 \nC 86.351192 165.507399 85.588061 165.823498 85.025481 166.386079 \nC 84.4629 166.94866 84.146801 167.71179 84.146801 168.507399 \nC 84.146801 169.303009 84.4629 170.066139 85.025481 170.62872 \nC 85.588061 171.1913 86.351192 171.507399 87.146801 171.507399 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 141.320232 \nC 145.628008 141.320232 146.391139 141.004133 146.953719 140.441552 \nC 147.5163 139.878971 147.832399 139.115841 147.832399 138.320232 \nC 147.832399 137.524623 147.5163 136.761492 146.953719 136.198911 \nC 146.391139 135.636331 145.628008 135.320232 144.832399 135.320232 \nC 144.03679 135.320232 143.273659 135.636331 142.711079 136.198911 \nC 142.148498 136.761492 141.832399 137.524623 141.832399 138.320232 \nC 141.832399 139.115841 142.148498 139.878971 142.711079 140.441552 \nC 143.273659 141.004133 144.03679 141.320232 144.832399 141.320232 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 163.55316 \nC 371.755552 163.55316 372.518683 163.237061 373.081264 162.67448 \nC 373.643844 162.111899 373.959943 161.348769 373.959943 160.55316 \nC 373.959943 159.75755 373.643844 158.99442 373.081264 158.431839 \nC 372.518683 157.869259 371.755552 157.55316 370.959943 157.55316 \nC 370.164334 157.55316 369.401204 157.869259 368.838623 158.431839 \nC 368.276042 158.99442 367.959943 159.75755 367.959943 160.55316 \nC 367.959943 161.348769 368.276042 162.111899 368.838623 162.67448 \nC 369.401204 163.237061 370.164334 163.55316 370.959943 163.55316 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 118.773693 \nC 67.608237 118.773693 68.371367 118.457594 68.933948 117.895013 \nC 69.496529 117.332433 69.812628 116.569302 69.812628 115.773693 \nC 69.812628 114.978084 69.496529 114.214953 68.933948 113.652373 \nC 68.371367 113.089792 67.608237 112.773693 66.812628 112.773693 \nC 66.017019 112.773693 65.253888 113.089792 64.691307 113.652373 \nC 64.128727 114.214953 63.812628 114.978084 63.812628 115.773693 \nC 63.812628 116.569302 64.128727 117.332433 64.691307 117.895013 \nC 65.253888 118.457594 66.017019 118.773693 66.812628 118.773693 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 191.13711 \nC 68.906163 191.13711 69.669293 190.821011 70.231874 190.25843 \nC 70.794455 189.695849 71.110554 188.932719 71.110554 188.13711 \nC 71.110554 187.3415 70.794455 186.57837 70.231874 186.015789 \nC 69.669293 185.453208 68.906163 185.13711 68.110554 185.13711 \nC 67.314944 185.13711 66.551814 185.453208 65.989233 186.015789 \nC 65.426653 186.57837 65.110554 187.3415 65.110554 188.13711 \nC 65.110554 188.932719 65.426653 189.695849 65.989233 190.25843 \nC 66.551814 190.821011 67.314944 191.13711 68.110554 191.13711 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 217.756364 \nC 71.502015 217.756364 72.265145 217.440265 72.827726 216.877684 \nC 73.390307 216.315103 73.706406 215.551973 73.706406 214.756364 \nC 73.706406 213.960754 73.390307 213.197624 72.827726 212.635043 \nC 72.265145 212.072463 71.502015 211.756364 70.706406 211.756364 \nC 69.910796 211.756364 69.147666 212.072463 68.585085 212.635043 \nC 68.022505 213.197624 67.706406 213.960754 67.706406 214.756364 \nC 67.706406 215.551973 68.022505 216.315103 68.585085 216.877684 \nC 69.147666 217.440265 69.910796 217.756364 70.706406 217.756364 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 197.291523 \nC 76.693719 197.291523 77.456849 196.975424 78.01943 196.412843 \nC 78.582011 195.850262 78.898109 195.087132 78.898109 194.291523 \nC 78.898109 193.495913 78.582011 192.732783 78.01943 192.170202 \nC 77.456849 191.607622 76.693719 191.291523 75.898109 191.291523 \nC 75.1025 191.291523 74.33937 191.607622 73.776789 192.170202 \nC 73.214208 192.732783 72.898109 193.495913 72.898109 194.291523 \nC 72.898109 195.087132 73.214208 195.850262 73.776789 196.412843 \nC 74.33937 196.975424 75.1025 197.291523 75.898109 197.291523 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 86.281517 190.076565 \nC 87.077126 190.076565 87.840257 189.760466 88.402837 189.197885 \nC 88.965418 188.635305 89.281517 187.872174 89.281517 187.076565 \nC 89.281517 186.280956 88.965418 185.517825 88.402837 184.955245 \nC 87.840257 184.392664 87.077126 184.076565 86.281517 184.076565 \nC 85.485908 184.076565 84.722778 184.392664 84.160197 184.955245 \nC 83.597616 185.517825 83.281517 186.280956 83.281517 187.076565 \nC 83.281517 187.872174 83.597616 188.635305 84.160197 189.197885 \nC 84.722778 189.760466 85.485908 190.076565 86.281517 190.076565 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.003906 \nC 107.843942 21.003906 108.607072 20.687807 109.169653 20.125226 \nC 109.732233 19.562645 110.048332 18.799515 110.048332 18.003906 \nC 110.048332 17.208296 109.732233 16.445166 109.169653 15.882585 \nC 108.607072 15.320005 107.843942 15.003906 107.048332 15.003906 \nC 106.252723 15.003906 105.489593 15.320005 104.927012 15.882585 \nC 104.364431 16.445166 104.048332 17.208296 104.048332 18.003906 \nC 104.048332 18.799515 104.364431 19.562645 104.927012 20.125226 \nC 105.489593 20.687807 106.252723 21.003906 107.048332 21.003906 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 109.161114 \nL 66.596307 107.661114 \nL 68.096307 109.161114 \nL 69.596307 107.661114 \nL 68.096307 106.161114 \nL 69.596307 104.661114 \nL 68.096307 103.161114 \nL 66.596307 104.661114 \nL 65.096307 103.161114 \nL 63.596307 104.661114 \nL 65.096307 106.161114 \nL 63.596307 107.661114 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 116.33034 \nL 66.956842 114.83034 \nL 68.456842 116.33034 \nL 69.956842 114.83034 \nL 68.456842 113.33034 \nL 69.956842 111.83034 \nL 68.456842 110.33034 \nL 66.956842 111.83034 \nL 65.456842 110.33034 \nL 63.956842 111.83034 \nL 65.456842 113.33034 \nL 63.956842 114.83034 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 140.045879 \nL 68.110554 138.545879 \nL 69.610554 140.045879 \nL 71.110554 138.545879 \nL 69.610554 137.045879 \nL 71.110554 135.545879 \nL 69.610554 134.045879 \nL 68.110554 135.545879 \nL 66.610554 134.045879 \nL 65.110554 135.545879 \nL 66.610554 137.045879 \nL 65.110554 138.545879 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.648546 175.156362 \nL 72.148546 173.656362 \nL 73.648546 175.156362 \nL 75.148546 173.656362 \nL 73.648546 172.156362 \nL 75.148546 170.656362 \nL 73.648546 169.156362 \nL 72.148546 170.656362 \nL 70.648546 169.156362 \nL 69.148546 170.656362 \nL 70.648546 172.156362 \nL 69.148546 173.656362 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 182.025142 \nL 87.146801 180.525142 \nL 88.646801 182.025142 \nL 90.146801 180.525142 \nL 88.646801 179.025142 \nL 90.146801 177.525142 \nL 88.646801 176.025142 \nL 87.146801 177.525142 \nL 85.646801 176.025142 \nL 84.146801 177.525142 \nL 85.646801 179.025142 \nL 84.146801 180.525142 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 134.408458 \nL 144.832399 132.908458 \nL 146.332399 134.408458 \nL 147.832399 132.908458 \nL 146.332399 131.408458 \nL 147.832399 129.908458 \nL 146.332399 128.408458 \nL 144.832399 129.908458 \nL 143.332399 128.408458 \nL 141.832399 129.908458 \nL 143.332399 131.408458 \nL 141.832399 132.908458 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 369.459943 137.476276 \nL 370.959943 135.976276 \nL 372.459943 137.476276 \nL 373.959943 135.976276 \nL 372.459943 134.476276 \nL 373.959943 132.976276 \nL 372.459943 131.476276 \nL 370.959943 132.976276 \nL 369.459943 131.476276 \nL 367.959943 132.976276 \nL 369.459943 134.476276 \nL 367.959943 135.976276 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 134.798199 \nL 66.812628 133.298199 \nL 68.312628 134.798199 \nL 69.812628 133.298199 \nL 68.312628 131.798199 \nL 69.812628 130.298199 \nL 68.312628 128.798199 \nL 66.812628 130.298199 \nL 65.312628 128.798199 \nL 63.812628 130.298199 \nL 65.312628 131.798199 \nL 63.812628 133.298199 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 189.080823 \nL 68.110554 187.580823 \nL 69.610554 189.080823 \nL 71.110554 187.580823 \nL 69.610554 186.080823 \nL 71.110554 184.580823 \nL 69.610554 183.080823 \nL 68.110554 184.580823 \nL 66.610554 183.080823 \nL 65.110554 184.580823 \nL 66.610554 186.080823 \nL 65.110554 187.580823 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 158.062267 \nL 70.706406 156.562267 \nL 72.206406 158.062267 \nL 73.706406 156.562267 \nL 72.206406 155.062267 \nL 73.706406 153.562267 \nL 72.206406 152.062267 \nL 70.706406 153.562267 \nL 69.206406 152.062267 \nL 67.706406 153.562267 \nL 69.206406 155.062267 \nL 67.706406 156.562267 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 74.398109 169.325854 \nL 75.898109 167.825854 \nL 77.398109 169.325854 \nL 78.898109 167.825854 \nL 77.398109 166.325854 \nL 78.898109 164.825854 \nL 77.398109 163.325854 \nL 75.898109 164.825854 \nL 74.398109 163.325854 \nL 72.898109 164.825854 \nL 74.398109 166.325854 \nL 72.898109 167.825854 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 187.746544 \nL 86.281517 186.246544 \nL 87.781517 187.746544 \nL 89.281517 186.246544 \nL 87.781517 184.746544 \nL 89.281517 183.246544 \nL 87.781517 181.746544 \nL 86.281517 183.246544 \nL 84.781517 181.746544 \nL 83.281517 183.246544 \nL 84.781517 184.746544 \nL 83.281517 186.246544 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 187.120096 \nL 107.048332 185.620096 \nL 108.548332 187.120096 \nL 110.048332 185.620096 \nL 108.548332 184.120096 \nL 110.048332 182.620096 \nL 108.548332 181.120096 \nL 107.048332 182.620096 \nL 105.548332 181.120096 \nL 104.048332 182.620096 \nL 105.548332 184.120096 \nL 104.048332 185.620096 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.596307 122.405584 \nC 67.391916 122.405584 68.155046 122.089485 68.717627 121.526905 \nC 69.280208 120.964324 69.596307 120.201194 69.596307 119.405584 \nC 69.596307 118.609975 69.280208 117.846845 68.717627 117.284264 \nC 68.155046 116.721683 67.391916 116.405584 66.596307 116.405584 \nC 65.800698 116.405584 65.037567 116.721683 64.474986 117.284264 \nC 63.912406 117.846845 63.596307 118.609975 63.596307 119.405584 \nC 63.596307 120.201194 63.912406 120.964324 64.474986 121.526905 \nC 65.037567 122.089485 65.800698 122.405584 66.596307 122.405584 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.956842 151.924188 \nC 67.752451 151.924188 68.515581 151.608089 69.078162 151.045509 \nC 69.640743 150.482928 69.956842 149.719798 69.956842 148.924188 \nC 69.956842 148.128579 69.640743 147.365449 69.078162 146.802868 \nC 68.515581 146.240287 67.752451 145.924188 66.956842 145.924188 \nC 66.161233 145.924188 65.398102 146.240287 64.835521 146.802868 \nC 64.272941 147.365449 63.956842 148.128579 63.956842 148.924188 \nC 63.956842 149.719798 64.272941 150.482928 64.835521 151.045509 \nC 65.398102 151.608089 66.161233 151.924188 66.956842 151.924188 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 205.46894 \nC 68.906163 205.46894 69.669293 205.152841 70.231874 204.590261 \nC 70.794455 204.02768 71.110554 203.26455 71.110554 202.46894 \nC 71.110554 201.673331 70.794455 200.910201 70.231874 200.34762 \nC 69.669293 199.785039 68.906163 199.46894 68.110554 199.46894 \nC 67.314944 199.46894 66.551814 199.785039 65.989233 200.34762 \nC 65.426653 200.910201 65.110554 201.673331 65.110554 202.46894 \nC 65.110554 203.26455 65.426653 204.02768 65.989233 204.590261 \nC 66.551814 205.152841 67.314944 205.46894 68.110554 205.46894 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 72.148546 193.674281 \nC 72.944155 193.674281 73.707285 193.358182 74.269866 192.795601 \nC 74.832447 192.233021 75.148546 191.46989 75.148546 190.674281 \nC 75.148546 189.878672 74.832447 189.115541 74.269866 188.552961 \nC 73.707285 187.99038 72.944155 187.674281 72.148546 187.674281 \nC 71.352936 187.674281 70.589806 187.99038 70.027225 188.552961 \nC 69.464645 189.115541 69.148546 189.878672 69.148546 190.674281 \nC 69.148546 191.46989 69.464645 192.233021 70.027225 192.795601 \nC 70.589806 193.358182 71.352936 193.674281 72.148546 193.674281 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 87.146801 145.09787 \nC 87.94241 145.09787 88.705541 144.781771 89.268121 144.21919 \nC 89.830702 143.656609 90.146801 142.893479 90.146801 142.09787 \nC 90.146801 141.30226 89.830702 140.53913 89.268121 139.976549 \nC 88.705541 139.413969 87.94241 139.09787 87.146801 139.09787 \nC 86.351192 139.09787 85.588061 139.413969 85.025481 139.976549 \nC 84.4629 140.53913 84.146801 141.30226 84.146801 142.09787 \nC 84.146801 142.893479 84.4629 143.656609 85.025481 144.21919 \nC 85.588061 144.781771 86.351192 145.09787 87.146801 145.09787 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 144.832399 151.061558 \nC 145.628008 151.061558 146.391139 150.745459 146.953719 150.182879 \nC 147.5163 149.620298 147.832399 148.857168 147.832399 148.061558 \nC 147.832399 147.265949 147.5163 146.502819 146.953719 145.940238 \nC 146.391139 145.377657 145.628008 145.061558 144.832399 145.061558 \nC 144.03679 145.061558 143.273659 145.377657 142.711079 145.940238 \nC 142.148498 146.502819 141.832399 147.265949 141.832399 148.061558 \nC 141.832399 148.857168 142.148498 149.620298 142.711079 150.182879 \nC 143.273659 150.745459 144.03679 151.061558 144.832399 151.061558 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 370.959943 160.192036 \nC 371.755552 160.192036 372.518683 159.875937 373.081264 159.313357 \nC 373.643844 158.750776 373.959943 157.987646 373.959943 157.192036 \nC 373.959943 156.396427 373.643844 155.633297 373.081264 155.070716 \nC 372.518683 154.508135 371.755552 154.192036 370.959943 154.192036 \nC 370.164334 154.192036 369.401204 154.508135 368.838623 155.070716 \nC 368.276042 155.633297 367.959943 156.396427 367.959943 157.192036 \nC 367.959943 157.987646 368.276042 158.750776 368.838623 159.313357 \nC 369.401204 159.875937 370.164334 160.192036 370.959943 160.192036 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.812628 116.753213 \nC 67.608237 116.753213 68.371367 116.437114 68.933948 115.874533 \nC 69.496529 115.311952 69.812628 114.548822 69.812628 113.753213 \nC 69.812628 112.957603 69.496529 112.194473 68.933948 111.631892 \nC 68.371367 111.069312 67.608237 110.753213 66.812628 110.753213 \nC 66.017019 110.753213 65.253888 111.069312 64.691307 111.631892 \nC 64.128727 112.194473 63.812628 112.957603 63.812628 113.753213 \nC 63.812628 114.548822 64.128727 115.311952 64.691307 115.874533 \nC 65.253888 116.437114 66.017019 116.753213 66.812628 116.753213 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 68.110554 193.505977 \nC 68.906163 193.505977 69.669293 193.189878 70.231874 192.627297 \nC 70.794455 192.064716 71.110554 191.301586 71.110554 190.505977 \nC 71.110554 189.710367 70.794455 188.947237 70.231874 188.384656 \nC 69.669293 187.822075 68.906163 187.505977 68.110554 187.505977 \nC 67.314944 187.505977 66.551814 187.822075 65.989233 188.384656 \nC 65.426653 188.947237 65.110554 189.710367 65.110554 190.505977 \nC 65.110554 191.301586 65.426653 192.064716 65.989233 192.627297 \nC 66.551814 193.189878 67.314944 193.505977 68.110554 193.505977 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.706406 193.513366 \nC 71.502015 193.513366 72.265145 193.197267 72.827726 192.634686 \nC 73.390307 192.072105 73.706406 191.308975 73.706406 190.513366 \nC 73.706406 189.717756 73.390307 188.954626 72.827726 188.392045 \nC 72.265145 187.829464 71.502015 187.513366 70.706406 187.513366 \nC 69.910796 187.513366 69.147666 187.829464 68.585085 188.392045 \nC 68.022505 188.954626 67.706406 189.717756 67.706406 190.513366 \nC 67.706406 191.308975 68.022505 192.072105 68.585085 192.634686 \nC 69.147666 193.197267 69.910796 193.513366 70.706406 193.513366 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 75.898109 198.771396 \nC 76.693719 198.771396 77.456849 198.455297 78.01943 197.892717 \nC 78.582011 197.330136 78.898109 196.567006 78.898109 195.771396 \nC 78.898109 194.975787 78.582011 194.212657 78.01943 193.650076 \nC 77.456849 193.087495 76.693719 192.771396 75.898109 192.771396 \nC 75.1025 192.771396 74.33937 193.087495 73.776789 193.650076 \nC 73.214208 194.212657 72.898109 194.975787 72.898109 195.771396 \nC 72.898109 196.567006 73.214208 197.330136 73.776789 197.892717 \nC 74.33937 198.455297 75.1025 198.771396 75.898109 198.771396 \nz\n\" style=\"fill:#d89faa;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 107.048332 21.003932 \nC 107.843942 21.003932 108.607072 20.687833 109.169653 20.125253 \nC 109.732233 19.562672 110.048332 18.799541 110.048332 18.003932 \nC 110.048332 17.208323 109.732233 16.445193 109.169653 15.882612 \nC 108.607072 15.320031 107.843942 15.003932 107.048332 15.003932 \nC 106.252723 15.003932 105.489593 15.320031 104.927012 15.882612 \nC 104.364431 16.445193 104.048332 17.208323 104.048332 18.003932 \nC 104.048332 18.799541 104.364431 19.562672 104.927012 20.125253 \nC 105.489593 20.687833 106.252723 21.003932 107.048332 21.003932 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.096307 150.674517 \nL 66.596307 149.174517 \nL 68.096307 150.674517 \nL 69.596307 149.174517 \nL 68.096307 147.674517 \nL 69.596307 146.174517 \nL 68.096307 144.674517 \nL 66.596307 146.174517 \nL 65.096307 144.674517 \nL 63.596307 146.174517 \nL 65.096307 147.674517 \nL 63.596307 149.174517 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.456842 110.310429 \nL 66.956842 108.810429 \nL 68.456842 110.310429 \nL 69.956842 108.810429 \nL 68.456842 107.310429 \nL 69.956842 105.810429 \nL 68.456842 104.310429 \nL 66.956842 105.810429 \nL 65.456842 104.310429 \nL 63.956842 105.810429 \nL 65.456842 107.310429 \nL 63.956842 108.810429 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 123.568885 \nL 68.110554 122.068885 \nL 69.610554 123.568885 \nL 71.110554 122.068885 \nL 69.610554 120.568885 \nL 71.110554 119.068885 \nL 69.610554 117.568885 \nL 68.110554 119.068885 \nL 66.610554 117.568885 \nL 65.110554 119.068885 \nL 66.610554 120.568885 \nL 65.110554 122.068885 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 70.648546 137.295157 \nL 72.148546 135.795157 \nL 73.648546 137.295157 \nL 75.148546 135.795157 \nL 73.648546 134.295157 \nL 75.148546 132.795157 \nL 73.648546 131.295157 \nL 72.148546 132.795157 \nL 70.648546 131.295157 \nL 69.148546 132.795157 \nL 70.648546 134.295157 \nL 69.148546 135.795157 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 85.646801 170.493982 \nL 87.146801 168.993982 \nL 88.646801 170.493982 \nL 90.146801 168.993982 \nL 88.646801 167.493982 \nL 90.146801 165.993982 \nL 88.646801 164.493982 \nL 87.146801 165.993982 \nL 85.646801 164.493982 \nL 84.146801 165.993982 \nL 85.646801 167.493982 \nL 84.146801 168.993982 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 143.332399 161.925744 \nL 144.832399 160.425744 \nL 146.332399 161.925744 \nL 147.832399 160.425744 \nL 146.332399 158.925744 \nL 147.832399 157.425744 \nL 146.332399 155.925744 \nL 144.832399 157.425744 \nL 143.332399 155.925744 \nL 141.832399 157.425744 \nL 143.332399 158.925744 \nL 141.832399 160.425744 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 65.312628 206.259683 \nL 66.812628 204.759683 \nL 68.312628 206.259683 \nL 69.812628 204.759683 \nL 68.312628 203.259683 \nL 69.812628 201.759683 \nL 68.312628 200.259683 \nL 66.812628 201.759683 \nL 65.312628 200.259683 \nL 63.812628 201.759683 \nL 65.312628 203.259683 \nL 63.812628 204.759683 \nz\n\" style=\"fill:#edd1cb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 66.610554 191.711546 \nL 68.110554 190.211546 \nL 69.610554 191.711546 \nL 71.110554 190.211546 \nL 69.610554 188.711546 \nL 71.110554 187.211546 \nL 69.610554 185.711546 \nL 68.110554 187.211546 \nL 66.610554 185.711546 \nL 65.110554 187.211546 \nL 66.610554 188.711546 \nL 65.110554 190.211546 \nz\n\" style=\"fill:#ebcac5;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 69.206406 165.03498 \nL 70.706406 163.53498 \nL 72.206406 165.03498 \nL 73.706406 163.53498 \nL 72.206406 162.03498 \nL 73.706406 160.53498 \nL 72.206406 159.03498 \nL 70.706406 160.53498 \nL 69.206406 159.03498 \nL 67.706406 160.53498 \nL 69.206406 162.03498 \nL 67.706406 163.53498 \nz\n\" style=\"fill:#e5bcbb;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 84.781517 165.740381 \nL 86.281517 164.240381 \nL 87.781517 165.740381 \nL 89.281517 164.240381 \nL 87.781517 162.740381 \nL 89.281517 161.240381 \nL 87.781517 159.740381 \nL 86.281517 161.240381 \nL 84.781517 159.740381 \nL 83.281517 161.240381 \nL 84.781517 162.740381 \nL 83.281517 164.240381 \nz\n\" style=\"fill:#ae6b91;stroke:#ffffff;stroke-width:0.48;\"/>\n    <path clip-path=\"url(#pce631d0c5c)\" d=\"M 105.548332 179.354083 \nL 107.048332 177.854083 \nL 108.548332 179.354083 \nL 110.048332 177.854083 \nL 108.548332 176.354083 \nL 110.048332 174.854083 \nL 108.548332 173.354083 \nL 107.048332 174.854083 \nL 105.548332 173.354083 \nL 104.048332 174.854083 \nL 105.548332 176.354083 \nL 104.048332 177.854083 \nz\n\" style=\"fill:#2d1e3e;stroke:#ffffff;stroke-width:0.48;\"/>\n   </g>\n   <g id=\"PathCollection_2\"/>\n   <g id=\"PathCollection_3\"/>\n   <g id=\"PathCollection_4\"/>\n   <g id=\"PathCollection_5\"/>\n   <g id=\"PathCollection_6\"/>\n   <g id=\"PathCollection_7\"/>\n   <g id=\"PathCollection_8\"/>\n   <g id=\"PathCollection_9\"/>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path d=\"M 0 0 \nL 0 3.5 \n\" id=\"md7901d65c7\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"66.361959\" xlink:href=\"#md7901d65c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g transform=\"translate(63.180709 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"111.428833\" xlink:href=\"#md7901d65c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 2500 -->\n      <g transform=\"translate(98.703833 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\n        <path d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"156.495706\" xlink:href=\"#md7901d65c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 5000 -->\n      <g transform=\"translate(143.770706 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"201.562579\" xlink:href=\"#md7901d65c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 7500 -->\n      <g transform=\"translate(188.837579 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 525 4666 \nL 3525 4666 \nL 3525 4397 \nL 1831 0 \nL 1172 0 \nL 2766 4134 \nL 525 4134 \nL 525 4666 \nz\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"246.629453\" xlink:href=\"#md7901d65c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 10000 -->\n      <g transform=\"translate(230.723203 239.238438)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"291.696326\" xlink:href=\"#md7901d65c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 12500 -->\n      <g transform=\"translate(275.790076 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_7\">\n     <g id=\"line2d_7\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"336.7632\" xlink:href=\"#md7901d65c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 15000 -->\n      <g transform=\"translate(320.85695 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_8\">\n     <g id=\"line2d_8\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"381.830073\" xlink:href=\"#md7901d65c7\" y=\"224.64\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 17500 -->\n      <g transform=\"translate(365.923823 239.238438)scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-37\"/>\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-35\"/>\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\n       <use x=\"254.492188\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_9\">\n     <!-- parameters -->\n     <g transform=\"translate(189.776563 252.916563)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 1159 525 \nL 1159 -1331 \nL 581 -1331 \nL 581 3500 \nL 1159 3500 \nL 1159 2969 \nQ 1341 3281 1617 3432 \nQ 1894 3584 2278 3584 \nQ 2916 3584 3314 3078 \nQ 3713 2572 3713 1747 \nQ 3713 922 3314 415 \nQ 2916 -91 2278 -91 \nQ 1894 -91 1617 61 \nQ 1341 213 1159 525 \nz\nM 3116 1747 \nQ 3116 2381 2855 2742 \nQ 2594 3103 2138 3103 \nQ 1681 3103 1420 2742 \nQ 1159 2381 1159 1747 \nQ 1159 1113 1420 752 \nQ 1681 391 2138 391 \nQ 2594 391 2855 752 \nQ 3116 1113 3116 1747 \nz\n\" id=\"DejaVuSans-70\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2194 1759 \nQ 1497 1759 1228 1600 \nQ 959 1441 959 1056 \nQ 959 750 1161 570 \nQ 1363 391 1709 391 \nQ 2188 391 2477 730 \nQ 2766 1069 2766 1631 \nL 2766 1759 \nL 2194 1759 \nz\nM 3341 1997 \nL 3341 0 \nL 2766 0 \nL 2766 531 \nQ 2569 213 2275 61 \nQ 1981 -91 1556 -91 \nQ 1019 -91 701 211 \nQ 384 513 384 1019 \nQ 384 1609 779 1909 \nQ 1175 2209 1959 2209 \nL 2766 2209 \nL 2766 2266 \nQ 2766 2663 2505 2880 \nQ 2244 3097 1772 3097 \nQ 1472 3097 1187 3025 \nQ 903 2953 641 2809 \nL 641 3341 \nQ 956 3463 1253 3523 \nQ 1550 3584 1831 3584 \nQ 2591 3584 2966 3190 \nQ 3341 2797 3341 1997 \nz\n\" id=\"DejaVuSans-61\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2631 2963 \nQ 2534 3019 2420 3045 \nQ 2306 3072 2169 3072 \nQ 1681 3072 1420 2755 \nQ 1159 2438 1159 1844 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1341 3275 1631 3429 \nQ 1922 3584 2338 3584 \nQ 2397 3584 2469 3576 \nQ 2541 3569 2628 3553 \nL 2631 2963 \nz\n\" id=\"DejaVuSans-72\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3328 2828 \nQ 3544 3216 3844 3400 \nQ 4144 3584 4550 3584 \nQ 5097 3584 5394 3201 \nQ 5691 2819 5691 2113 \nL 5691 0 \nL 5113 0 \nL 5113 2094 \nQ 5113 2597 4934 2840 \nQ 4756 3084 4391 3084 \nQ 3944 3084 3684 2787 \nQ 3425 2491 3425 1978 \nL 3425 0 \nL 2847 0 \nL 2847 2094 \nQ 2847 2600 2669 2842 \nQ 2491 3084 2119 3084 \nQ 1678 3084 1418 2786 \nQ 1159 2488 1159 1978 \nL 1159 0 \nL 581 0 \nL 581 3500 \nL 1159 3500 \nL 1159 2956 \nQ 1356 3278 1631 3431 \nQ 1906 3584 2284 3584 \nQ 2666 3584 2933 3390 \nQ 3200 3197 3328 2828 \nz\n\" id=\"DejaVuSans-6d\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 3597 1894 \nL 3597 1613 \nL 953 1613 \nQ 991 1019 1311 708 \nQ 1631 397 2203 397 \nQ 2534 397 2845 478 \nQ 3156 559 3463 722 \nL 3463 178 \nQ 3153 47 2828 -22 \nQ 2503 -91 2169 -91 \nQ 1331 -91 842 396 \nQ 353 884 353 1716 \nQ 353 2575 817 3079 \nQ 1281 3584 2069 3584 \nQ 2775 3584 3186 3129 \nQ 3597 2675 3597 1894 \nz\nM 3022 2063 \nQ 3016 2534 2758 2815 \nQ 2500 3097 2075 3097 \nQ 1594 3097 1305 2825 \nQ 1016 2553 972 2059 \nL 3022 2063 \nz\n\" id=\"DejaVuSans-65\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1172 4494 \nL 1172 3500 \nL 2356 3500 \nL 2356 3053 \nL 1172 3053 \nL 1172 1153 \nQ 1172 725 1289 603 \nQ 1406 481 1766 481 \nL 2356 481 \nL 2356 0 \nL 1766 0 \nQ 1100 0 847 248 \nQ 594 497 594 1153 \nL 594 3053 \nL 172 3053 \nL 172 3500 \nL 594 3500 \nL 594 4494 \nL 1172 4494 \nz\n\" id=\"DejaVuSans-74\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 2834 3397 \nL 2834 2853 \nQ 2591 2978 2328 3040 \nQ 2066 3103 1784 3103 \nQ 1356 3103 1142 2972 \nQ 928 2841 928 2578 \nQ 928 2378 1081 2264 \nQ 1234 2150 1697 2047 \nL 1894 2003 \nQ 2506 1872 2764 1633 \nQ 3022 1394 3022 966 \nQ 3022 478 2636 193 \nQ 2250 -91 1575 -91 \nQ 1294 -91 989 -36 \nQ 684 19 347 128 \nL 347 722 \nQ 666 556 975 473 \nQ 1284 391 1588 391 \nQ 1994 391 2212 530 \nQ 2431 669 2431 922 \nQ 2431 1156 2273 1281 \nQ 2116 1406 1581 1522 \nL 1381 1569 \nQ 847 1681 609 1914 \nQ 372 2147 372 2553 \nQ 372 3047 722 3315 \nQ 1072 3584 1716 3584 \nQ 2034 3584 2315 3537 \nQ 2597 3491 2834 3397 \nz\n\" id=\"DejaVuSans-73\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-70\"/>\n      <use x=\"63.476562\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"124.755859\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"165.869141\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"227.148438\" xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"324.560547\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"386.083984\" xlink:href=\"#DejaVuSans-74\"/>\n      <use x=\"425.292969\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"486.816406\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"527.929688\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_9\">\n      <defs>\n       <path d=\"M 0 0 \nL -3.5 0 \n\" id=\"m7d3a675e6a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m7d3a675e6a\" y=\"210.329615\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- $\\mathdefault{10^{-5}}$ -->\n      <g transform=\"translate(20.878125 214.128833)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 678 2272 \nL 4684 2272 \nL 4684 1741 \nL 678 1741 \nL 678 2272 \nz\n\" id=\"DejaVuSans-2212\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_10\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m7d3a675e6a\" y=\"182.856242\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- $\\mathdefault{10^{-4}}$ -->\n      <g transform=\"translate(20.878125 186.655461)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2419 4116 \nL 825 1625 \nL 2419 1625 \nL 2419 4116 \nz\nM 2253 4666 \nL 3047 4666 \nL 3047 1625 \nL 3713 1625 \nL 3713 1100 \nL 3047 1100 \nL 3047 0 \nL 2419 0 \nL 2419 1100 \nL 313 1100 \nL 313 1709 \nL 2253 4666 \nz\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-34\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_11\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m7d3a675e6a\" y=\"155.38287\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- $\\mathdefault{10^{-3}}$ -->\n      <g transform=\"translate(20.878125 159.182089)scale(0.1 -0.1)\">\n       <defs>\n        <path d=\"M 2597 2516 \nQ 3050 2419 3304 2112 \nQ 3559 1806 3559 1356 \nQ 3559 666 3084 287 \nQ 2609 -91 1734 -91 \nQ 1441 -91 1130 -33 \nQ 819 25 488 141 \nL 488 750 \nQ 750 597 1062 519 \nQ 1375 441 1716 441 \nQ 2309 441 2620 675 \nQ 2931 909 2931 1356 \nQ 2931 1769 2642 2001 \nQ 2353 2234 1838 2234 \nL 1294 2234 \nL 1294 2753 \nL 1863 2753 \nQ 2328 2753 2575 2939 \nQ 2822 3125 2822 3475 \nQ 2822 3834 2567 4026 \nQ 2313 4219 1838 4219 \nQ 1578 4219 1281 4162 \nQ 984 4106 628 3988 \nL 628 4550 \nQ 988 4650 1302 4700 \nQ 1616 4750 1894 4750 \nQ 2613 4750 3031 4423 \nQ 3450 4097 3450 3541 \nQ 3450 3153 3228 2886 \nQ 3006 2619 2597 2516 \nz\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-33\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_12\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m7d3a675e6a\" y=\"127.909498\"/>\n      </g>\n     </g>\n     <g id=\"text_13\">\n      <!-- $\\mathdefault{10^{-2}}$ -->\n      <g transform=\"translate(20.878125 131.708717)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_13\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m7d3a675e6a\" y=\"100.436126\"/>\n      </g>\n     </g>\n     <g id=\"text_14\">\n      <!-- $\\mathdefault{10^{-1}}$ -->\n      <g transform=\"translate(20.878125 104.235345)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-2212\"/>\n       <use transform=\"translate(186.855469 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_14\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m7d3a675e6a\" y=\"72.962754\"/>\n      </g>\n     </g>\n     <g id=\"text_15\">\n      <!-- $\\mathdefault{10^{0}}$ -->\n      <g transform=\"translate(26.778125 76.761973)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_7\">\n     <g id=\"line2d_15\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m7d3a675e6a\" y=\"45.489382\"/>\n      </g>\n     </g>\n     <g id=\"text_16\">\n      <!-- $\\mathdefault{10^{1}}$ -->\n      <g transform=\"translate(26.778125 49.2886)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.684375)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.684375)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 38.965625)scale(0.7)\" xlink:href=\"#DejaVuSans-31\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_8\">\n     <g id=\"line2d_16\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.378125\" xlink:href=\"#m7d3a675e6a\" y=\"18.016009\"/>\n      </g>\n     </g>\n     <g id=\"text_17\">\n      <!-- $\\mathdefault{10^{2}}$ -->\n      <g transform=\"translate(26.778125 21.815228)scale(0.1 -0.1)\">\n       <use transform=\"translate(0 0.765625)\" xlink:href=\"#DejaVuSans-31\"/>\n       <use transform=\"translate(63.623047 0.765625)\" xlink:href=\"#DejaVuSans-30\"/>\n       <use transform=\"translate(128.203125 39.046875)scale(0.7)\" xlink:href=\"#DejaVuSans-32\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_9\">\n     <g id=\"line2d_17\">\n      <defs>\n       <path d=\"M 0 0 \nL -2 0 \n\" id=\"m33bf588f1a\" style=\"stroke:#000000;stroke-width:0.6;\"/>\n      </defs>\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"221.262369\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_10\">\n     <g id=\"line2d_18\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"218.599924\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_11\">\n     <g id=\"line2d_19\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"216.424548\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_12\">\n     <g id=\"line2d_20\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"214.585294\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_13\">\n     <g id=\"line2d_21\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"212.992059\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_14\">\n     <g id=\"line2d_22\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"211.586727\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_15\">\n     <g id=\"line2d_23\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"202.059305\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_16\">\n     <g id=\"line2d_24\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"197.221485\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_17\">\n     <g id=\"line2d_25\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"193.788996\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_18\">\n     <g id=\"line2d_26\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"191.126552\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_19\">\n     <g id=\"line2d_27\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"188.951176\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_20\">\n     <g id=\"line2d_28\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"187.111922\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_21\">\n     <g id=\"line2d_29\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"185.518687\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_22\">\n     <g id=\"line2d_30\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"184.113355\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_23\">\n     <g id=\"line2d_31\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"174.585933\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_24\">\n     <g id=\"line2d_32\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"169.748113\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_25\">\n     <g id=\"line2d_33\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"166.315624\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_26\">\n     <g id=\"line2d_34\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"163.653179\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_27\">\n     <g id=\"line2d_35\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"161.477803\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_28\">\n     <g id=\"line2d_36\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"159.638549\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_29\">\n     <g id=\"line2d_37\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"158.045315\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_30\">\n     <g id=\"line2d_38\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"156.639983\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_31\">\n     <g id=\"line2d_39\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"147.112561\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_32\">\n     <g id=\"line2d_40\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"142.27474\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_33\">\n     <g id=\"line2d_41\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"138.842252\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_34\">\n     <g id=\"line2d_42\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"136.179807\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_35\">\n     <g id=\"line2d_43\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"134.004431\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_36\">\n     <g id=\"line2d_44\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"132.165177\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_37\">\n     <g id=\"line2d_45\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"130.571943\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_38\">\n     <g id=\"line2d_46\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"129.166611\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_39\">\n     <g id=\"line2d_47\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"119.639189\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_40\">\n     <g id=\"line2d_48\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"114.801368\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_41\">\n     <g id=\"line2d_49\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"111.36888\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_42\">\n     <g id=\"line2d_50\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"108.706435\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_43\">\n     <g id=\"line2d_51\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"106.531059\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_44\">\n     <g id=\"line2d_52\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"104.691805\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_45\">\n     <g id=\"line2d_53\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"103.098571\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_46\">\n     <g id=\"line2d_54\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"101.693238\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_47\">\n     <g id=\"line2d_55\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"92.165817\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_48\">\n     <g id=\"line2d_56\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"87.327996\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_49\">\n     <g id=\"line2d_57\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"83.895508\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_50\">\n     <g id=\"line2d_58\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"81.233063\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_51\">\n     <g id=\"line2d_59\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"79.057687\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_52\">\n     <g id=\"line2d_60\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"77.218433\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_53\">\n     <g id=\"line2d_61\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"75.625199\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_54\">\n     <g id=\"line2d_62\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"74.219866\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_55\">\n     <g id=\"line2d_63\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"64.692445\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_56\">\n     <g id=\"line2d_64\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"59.854624\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_57\">\n     <g id=\"line2d_65\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"56.422136\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_58\">\n     <g id=\"line2d_66\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"53.759691\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_59\">\n     <g id=\"line2d_67\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"51.584315\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_60\">\n     <g id=\"line2d_68\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"49.745061\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_61\">\n     <g id=\"line2d_69\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"48.151826\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_62\">\n     <g id=\"line2d_70\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"46.746494\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_63\">\n     <g id=\"line2d_71\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"37.219072\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_64\">\n     <g id=\"line2d_72\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"32.381252\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_65\">\n     <g id=\"line2d_73\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"28.948763\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_66\">\n     <g id=\"line2d_74\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"26.286319\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_67\">\n     <g id=\"line2d_75\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"24.110943\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_68\">\n     <g id=\"line2d_76\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"22.271689\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_69\">\n     <g id=\"line2d_77\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"20.678454\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_70\">\n     <g id=\"line2d_78\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"19.273122\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_71\">\n     <g id=\"line2d_79\">\n      <g>\n       <use style=\"stroke:#000000;stroke-width:0.6;\" x=\"51.378125\" xlink:href=\"#m33bf588f1a\" y=\"9.7457\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"text_18\">\n     <!-- loss -->\n     <g transform=\"translate(14.798438 125.577813)rotate(-90)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 603 4863 \nL 1178 4863 \nL 1178 0 \nL 603 0 \nL 603 4863 \nz\n\" id=\"DejaVuSans-6c\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 1959 3097 \nQ 1497 3097 1228 2736 \nQ 959 2375 959 1747 \nQ 959 1119 1226 758 \nQ 1494 397 1959 397 \nQ 2419 397 2687 759 \nQ 2956 1122 2956 1747 \nQ 2956 2369 2687 2733 \nQ 2419 3097 1959 3097 \nz\nM 1959 3584 \nQ 2709 3584 3137 3096 \nQ 3566 2609 3566 1747 \nQ 3566 888 3137 398 \nQ 2709 -91 1959 -91 \nQ 1206 -91 779 398 \nQ 353 888 353 1747 \nQ 353 2609 779 3096 \nQ 1206 3584 1959 3584 \nz\n\" id=\"DejaVuSans-6f\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"88.964844\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"141.064453\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 51.378125 224.64 \nL 51.378125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 386.178125 224.64 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 51.378125 224.64 \nL 386.178125 224.64 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 51.378125 7.2 \nL 386.178125 7.2 \n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\n   </g>\n   <g id=\"legend_1\">\n    <g id=\"patch_7\">\n     <path d=\"M 185.070312 219.64 \nL 252.485938 219.64 \nQ 254.485938 219.64 254.485938 217.64 \nL 254.485938 71.85875 \nQ 254.485938 69.85875 252.485938 69.85875 \nL 185.070312 69.85875 \nQ 183.070312 69.85875 183.070312 71.85875 \nL 183.070312 217.64 \nQ 183.070312 219.64 185.070312 219.64 \nz\n\" style=\"fill:#ffffff;opacity:0.8;stroke:#cccccc;stroke-linejoin:miter;\"/>\n    </g>\n    <g id=\"text_19\">\n     <!-- layers -->\n     <g transform=\"translate(195.070312 81.457188)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2059 -325 \nQ 1816 -950 1584 -1140 \nQ 1353 -1331 966 -1331 \nL 506 -1331 \nL 506 -850 \nL 844 -850 \nQ 1081 -850 1212 -737 \nQ 1344 -625 1503 -206 \nL 1606 56 \nL 191 3500 \nL 800 3500 \nL 1894 763 \nL 2988 3500 \nL 3597 3500 \nL 2059 -325 \nz\n\" id=\"DejaVuSans-79\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6c\"/>\n      <use x=\"27.783203\" xlink:href=\"#DejaVuSans-61\"/>\n      <use x=\"89.0625\" xlink:href=\"#DejaVuSans-79\"/>\n      <use x=\"148.242188\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"209.765625\" xlink:href=\"#DejaVuSans-72\"/>\n      <use x=\"250.878906\" xlink:href=\"#DejaVuSans-73\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_10\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"me3f5d8a813\" style=\"stroke:#edd1cb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#edd1cb;stroke:#edd1cb;\" x=\"197.070312\" xlink:href=\"#me3f5d8a813\" y=\"93.510313\"/>\n     </g>\n    </g>\n    <g id=\"text_20\">\n     <!-- 1 -->\n     <g transform=\"translate(215.070312 96.135313)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-31\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_11\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m1e61c26442\" style=\"stroke:#ebcac5;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ebcac5;stroke:#ebcac5;\" x=\"197.070312\" xlink:href=\"#m1e61c26442\" y=\"108.188438\"/>\n     </g>\n    </g>\n    <g id=\"text_21\">\n     <!-- 2 -->\n     <g transform=\"translate(215.070312 110.813438)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_12\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mfdac05a06c\" style=\"stroke:#e5bcbb;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#e5bcbb;stroke:#e5bcbb;\" x=\"197.070312\" xlink:href=\"#mfdac05a06c\" y=\"122.866563\"/>\n     </g>\n    </g>\n    <g id=\"text_22\">\n     <!-- 4 -->\n     <g transform=\"translate(215.070312 125.491563)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-34\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_13\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"md5ff581df7\" style=\"stroke:#d89faa;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#d89faa;stroke:#d89faa;\" x=\"197.070312\" xlink:href=\"#md5ff581df7\" y=\"137.544688\"/>\n     </g>\n    </g>\n    <g id=\"text_23\">\n     <!-- 8 -->\n     <g transform=\"translate(215.070312 140.169688)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2034 2216 \nQ 1584 2216 1326 1975 \nQ 1069 1734 1069 1313 \nQ 1069 891 1326 650 \nQ 1584 409 2034 409 \nQ 2484 409 2743 651 \nQ 3003 894 3003 1313 \nQ 3003 1734 2745 1975 \nQ 2488 2216 2034 2216 \nz\nM 1403 2484 \nQ 997 2584 770 2862 \nQ 544 3141 544 3541 \nQ 544 4100 942 4425 \nQ 1341 4750 2034 4750 \nQ 2731 4750 3128 4425 \nQ 3525 4100 3525 3541 \nQ 3525 3141 3298 2862 \nQ 3072 2584 2669 2484 \nQ 3125 2378 3379 2068 \nQ 3634 1759 3634 1313 \nQ 3634 634 3220 271 \nQ 2806 -91 2034 -91 \nQ 1263 -91 848 271 \nQ 434 634 434 1313 \nQ 434 1759 690 2068 \nQ 947 2378 1403 2484 \nz\nM 1172 3481 \nQ 1172 3119 1398 2916 \nQ 1625 2713 2034 2713 \nQ 2441 2713 2670 2916 \nQ 2900 3119 2900 3481 \nQ 2900 3844 2670 4047 \nQ 2441 4250 2034 4250 \nQ 1625 4250 1398 4047 \nQ 1172 3844 1172 3481 \nz\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-38\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_14\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"m67d7ea63e3\" style=\"stroke:#ae6b91;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#ae6b91;stroke:#ae6b91;\" x=\"197.070312\" xlink:href=\"#m67d7ea63e3\" y=\"152.222813\"/>\n     </g>\n    </g>\n    <g id=\"text_24\">\n     <!-- 16 -->\n     <g transform=\"translate(215.070312 154.847813)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2113 2584 \nQ 1688 2584 1439 2293 \nQ 1191 2003 1191 1497 \nQ 1191 994 1439 701 \nQ 1688 409 2113 409 \nQ 2538 409 2786 701 \nQ 3034 994 3034 1497 \nQ 3034 2003 2786 2293 \nQ 2538 2584 2113 2584 \nz\nM 3366 4563 \nL 3366 3988 \nQ 3128 4100 2886 4159 \nQ 2644 4219 2406 4219 \nQ 1781 4219 1451 3797 \nQ 1122 3375 1075 2522 \nQ 1259 2794 1537 2939 \nQ 1816 3084 2150 3084 \nQ 2853 3084 3261 2657 \nQ 3669 2231 3669 1497 \nQ 3669 778 3244 343 \nQ 2819 -91 2113 -91 \nQ 1303 -91 875 529 \nQ 447 1150 447 2328 \nQ 447 3434 972 4092 \nQ 1497 4750 2381 4750 \nQ 2619 4750 2861 4703 \nQ 3103 4656 3366 4563 \nz\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-31\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-36\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_15\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"ma9cc06d9a9\" style=\"stroke:#2d1e3e;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#2d1e3e;stroke:#2d1e3e;\" x=\"197.070312\" xlink:href=\"#ma9cc06d9a9\" y=\"166.900938\"/>\n     </g>\n    </g>\n    <g id=\"text_25\">\n     <!-- 32 -->\n     <g transform=\"translate(215.070312 169.525938)scale(0.1 -0.1)\">\n      <use xlink:href=\"#DejaVuSans-33\"/>\n      <use x=\"63.623047\" xlink:href=\"#DejaVuSans-32\"/>\n     </g>\n    </g>\n    <g id=\"text_26\">\n     <!-- model -->\n     <g transform=\"translate(195.070312 184.204063)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2906 2969 \nL 2906 4863 \nL 3481 4863 \nL 3481 0 \nL 2906 0 \nL 2906 525 \nQ 2725 213 2448 61 \nQ 2172 -91 1784 -91 \nQ 1150 -91 751 415 \nQ 353 922 353 1747 \nQ 353 2572 751 3078 \nQ 1150 3584 1784 3584 \nQ 2172 3584 2448 3432 \nQ 2725 3281 2906 2969 \nz\nM 947 1747 \nQ 947 1113 1208 752 \nQ 1469 391 1925 391 \nQ 2381 391 2643 752 \nQ 2906 1113 2906 1747 \nQ 2906 2381 2643 2742 \nQ 2381 3103 1925 3103 \nQ 1469 3103 1208 2742 \nQ 947 2381 947 1747 \nz\n\" id=\"DejaVuSans-64\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-6d\"/>\n      <use x=\"97.412109\" xlink:href=\"#DejaVuSans-6f\"/>\n      <use x=\"158.59375\" xlink:href=\"#DejaVuSans-64\"/>\n      <use x=\"222.070312\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"283.59375\" xlink:href=\"#DejaVuSans-6c\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_16\">\n     <defs>\n      <path d=\"M 0 3 \nC 0.795609 3 1.55874 2.683901 2.12132 2.12132 \nC 2.683901 1.55874 3 0.795609 3 0 \nC 3 -0.795609 2.683901 -1.55874 2.12132 -2.12132 \nC 1.55874 -2.683901 0.795609 -3 0 -3 \nC -0.795609 -3 -1.55874 -2.683901 -2.12132 -2.12132 \nC -2.683901 -1.55874 -3 -0.795609 -3 0 \nC -3 0.795609 -2.683901 1.55874 -2.12132 2.12132 \nC -1.55874 2.683901 -0.795609 3 0 3 \nz\n\" id=\"mda5ecb6452\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"197.070312\" xlink:href=\"#mda5ecb6452\" y=\"196.257188\"/>\n     </g>\n    </g>\n    <g id=\"text_27\">\n     <!-- FFNN -->\n     <g transform=\"translate(215.070312 198.882188)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 628 4666 \nL 3309 4666 \nL 3309 4134 \nL 1259 4134 \nL 1259 2759 \nL 3109 2759 \nL 3109 2228 \nL 1259 2228 \nL 1259 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-46\" transform=\"scale(0.015625)\"/>\n       <path d=\"M 628 4666 \nL 1478 4666 \nL 3547 763 \nL 3547 4666 \nL 4159 4666 \nL 4159 0 \nL 3309 0 \nL 1241 3903 \nL 1241 0 \nL 628 0 \nL 628 4666 \nz\n\" id=\"DejaVuSans-4e\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"57.519531\" xlink:href=\"#DejaVuSans-46\"/>\n      <use x=\"115.039062\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"189.84375\" xlink:href=\"#DejaVuSans-4e\"/>\n     </g>\n    </g>\n    <g id=\"PathCollection_17\">\n     <defs>\n      <path d=\"M -1.5 3 \nL 0 1.5 \nL 1.5 3 \nL 3 1.5 \nL 1.5 0 \nL 3 -1.5 \nL 1.5 -3 \nL 0 -1.5 \nL -1.5 -3 \nL -3 -1.5 \nL -1.5 0 \nL -3 1.5 \nz\n\" id=\"m18f73be8e1\" style=\"stroke:#333333;\"/>\n     </defs>\n     <g>\n      <use style=\"fill:#333333;stroke:#333333;\" x=\"197.070312\" xlink:href=\"#m18f73be8e1\" y=\"210.935313\"/>\n     </g>\n    </g>\n    <g id=\"text_28\">\n     <!-- ResNet -->\n     <g transform=\"translate(215.070312 213.560313)scale(0.1 -0.1)\">\n      <defs>\n       <path d=\"M 2841 2188 \nQ 3044 2119 3236 1894 \nQ 3428 1669 3622 1275 \nL 4263 0 \nL 3584 0 \nL 2988 1197 \nQ 2756 1666 2539 1819 \nQ 2322 1972 1947 1972 \nL 1259 1972 \nL 1259 0 \nL 628 0 \nL 628 4666 \nL 2053 4666 \nQ 2853 4666 3247 4331 \nQ 3641 3997 3641 3322 \nQ 3641 2881 3436 2590 \nQ 3231 2300 2841 2188 \nz\nM 1259 4147 \nL 1259 2491 \nL 2053 2491 \nQ 2509 2491 2742 2702 \nQ 2975 2913 2975 3322 \nQ 2975 3731 2742 3939 \nQ 2509 4147 2053 4147 \nL 1259 4147 \nz\n\" id=\"DejaVuSans-52\" transform=\"scale(0.015625)\"/>\n      </defs>\n      <use xlink:href=\"#DejaVuSans-52\"/>\n      <use x=\"64.982422\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"126.505859\" xlink:href=\"#DejaVuSans-73\"/>\n      <use x=\"178.605469\" xlink:href=\"#DejaVuSans-4e\"/>\n      <use x=\"253.410156\" xlink:href=\"#DejaVuSans-65\"/>\n      <use x=\"314.933594\" xlink:href=\"#DejaVuSans-74\"/>\n     </g>\n    </g>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pce631d0c5c\">\n   <rect height=\"217.44\" width=\"334.8\" x=\"51.378125\" y=\"7.2\"/>\n  </clipPath>\n </defs>\n</svg>\n",
      "application/pdf": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "236 260\n"
     ]
    }
   ],
   "source": [
    "print(len(d_results[d_results.loss < 10]), len(d_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [],
   "source": [
    "d_results.to_csv(f\"{PATH_FIGURES}/d_results.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "outputs": [],
   "source": [
    "resnet = d_results[d_results.model==\"ResNet\"]\n",
    "resnet_small = resnet[resnet.parameters == 33]\n",
    "resnet_large = resnet[resnet.parameters == 2257]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "outputs": [
    {
     "data": {
      "text/plain": "loss             0.000676\nneurons          8.000000\nlayers          32.000000\nparameters    2257.000000\ndtype: float64"
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_large.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "outputs": [
    {
     "data": {
      "text/plain": "loss           0.009445\nneurons        4.000000\nlayers         2.000000\nparameters    33.000000\ndtype: float64"
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_small.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [
    {
     "data": {
      "text/plain": "loss          0.002212\nneurons       0.000000\nlayers        0.000000\nparameters    0.000000\ndtype: float64"
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_small.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "outputs": [
    {
     "data": {
      "text/plain": "loss             0.002026\nneurons         64.000000\nlayers           2.000000\nparameters    4353.000000\ndtype: float64"
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet_large.mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%resnet_small.std(resnet_large.mean()\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}