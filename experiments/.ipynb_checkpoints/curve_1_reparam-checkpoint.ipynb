{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x111f7c570>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from itertools import chain\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib_inline.backend_inline import set_matplotlib_formats\n",
    "\n",
    "set_matplotlib_formats('pdf', 'svg')\n",
    "\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from deep_reparametrization.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    "    get_elastic_error_func,\n",
    ")\n",
    "from deep_reparametrization.ResNet import ResNet, init_zero\n",
    "import experiments.curves as c1\n",
    "\n",
    "# make reproducible\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "########\n",
    "DIR = \"../figures/curve_1/\"\n",
    "SET_NAME = \"eks_5\"\n",
    "PATH_FIGURES = os.path.join(DIR, SET_NAME)\n",
    "if not os.path.exists(PATH_FIGURES):\n",
    "    os.makedirs(PATH_FIGURES)\n",
    "########\n",
    "\n",
    "\n",
    "FOLDS = 1\n",
    "N = 128  # training points internal\n",
    "# lr_scheduler  = lambda optim:  torch.optim.lr_scheduler.ReduceLROnPlateau(optim , mode='min', factor=0.8, patience=10, verbose=True)\n",
    "loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=1e3, verbose=False)\n",
    "no_penalty_loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=0, verbose=False)\n",
    "\n",
    "MODEL_PARAMS = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [2],\n",
    "    \"neurons\": [2, 4, 8, 16, 32, 64, 128]\n",
    "}\n",
    "MODEL_PARAMS_2 = {\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [2, 4, 8, 16, 32, 64],\n",
    "    \"neurons\": [8],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"model\": [FFNN, ResNet],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [10],\n",
    "    \"learning_rate\": [1],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# create iterators\n",
    "model_params_iter_1 = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_params_iter_2 = create_subdictionary_iterator(MODEL_PARAMS_2)\n",
    "model_params_iter = chain.from_iterable((model_params_iter_1, model_params_iter_2))\n",
    "\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = c1.q(x_train.detach())\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.93418568\n",
      "Training Loss:  0.93418568\n",
      "Training Loss:  0.93418568\n",
      "Training Loss:  0.93388301\n",
      "Training Loss:  0.93388301\n",
      "Training Loss:  0.93388301\n",
      "Training Loss:  0.93386149\n",
      "Training Loss:  0.93384498\n",
      "Training Loss:  0.93384498\n",
      "Training Loss:  0.93384498\n",
      "Final training Loss:  0.93384498\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00478804\n",
      "Training Loss:  0.00478804\n",
      "Training Loss:  0.00478804\n",
      "Training Loss:  0.00478804\n",
      "Training Loss:  0.00128755\n",
      "Training Loss:  0.00075672\n",
      "Training Loss:  0.00075653\n",
      "Training Loss:  0.00075645\n",
      "Training Loss:  0.00075645\n",
      "Training Loss:  0.00075645\n",
      "Final training Loss:  0.00075645\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Training Loss:  4.663e-05\n",
      "Final training Loss:  4.663e-05\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00015439\n",
      "Training Loss:  0.00015439\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Training Loss:  0.00014744\n",
      "Final training Loss:  0.00014744\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01005564\n",
      "Training Loss:  0.01005564\n",
      "Training Loss:  0.01005564\n",
      "Training Loss:  0.00020929\n",
      "Training Loss:  0.00020923\n",
      "Training Loss:  0.00020918\n",
      "Training Loss:  0.00020918\n",
      "Training Loss:  0.00020918\n",
      "Training Loss:  0.00020918\n",
      "Training Loss:  0.00020918\n",
      "Final training Loss:  0.00020918\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00272022\n",
      "Training Loss:  0.00272022\n",
      "Training Loss:  0.00272022\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272016\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272015\n",
      "Training Loss:  0.00272015\n",
      "Final training Loss:  0.00272015\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00868353\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Training Loss:  0.00477595\n",
      "Final training Loss:  0.00477595\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00140426\n",
      "Training Loss:  0.00140426\n",
      "Training Loss:  0.00140426\n",
      "Training Loss:  0.00140397\n",
      "Training Loss:  0.00140397\n",
      "Training Loss:  0.00140397\n",
      "Training Loss:  0.00140391\n",
      "Training Loss:  0.00140391\n",
      "Training Loss:  0.00140391\n",
      "Training Loss:  0.00140391\n",
      "Final training Loss:  0.00140391\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Training Loss:  0.00126206\n",
      "Final training Loss:  0.00126206\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00014642\n",
      "Training Loss:  0.00014629\n",
      "Training Loss:  0.00014626\n",
      "Training Loss:  0.00014626\n",
      "Training Loss:  0.00014624\n",
      "Training Loss:  0.00014624\n",
      "Training Loss:  0.00014624\n",
      "Training Loss:  0.00014624\n",
      "Training Loss:  0.00014624\n",
      "Training Loss:  0.00014624\n",
      "Final training Loss:  0.00014624\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.08893653\n",
      "Training Loss:  0.08893653\n",
      "Training Loss:  0.08893653\n",
      "Training Loss:  0.08891516\n",
      "Training Loss:  0.08891516\n",
      "Training Loss:  0.08891516\n",
      "Training Loss:  0.08814941\n",
      "Training Loss:  0.08814941\n",
      "Training Loss:  0.08814941\n",
      "Training Loss:  0.08814941\n",
      "Final training Loss:  0.08814941\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Final training Loss:  600.10137939\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 64, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Final training Loss:  600.1015625\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03845739\n",
      "Training Loss:  0.03845739\n",
      "Training Loss:  0.01620127\n",
      "Training Loss:  0.01620127\n",
      "Training Loss:  0.00310275\n",
      "Training Loss:  0.00310275\n",
      "Training Loss:  0.00310257\n",
      "Training Loss:  0.00310257\n",
      "Training Loss:  0.00310257\n",
      "Training Loss:  0.00310253\n",
      "Final training Loss:  0.00310253\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01048491\n",
      "Training Loss:  0.01048047\n",
      "Training Loss:  0.01047976\n",
      "Training Loss:  0.01047468\n",
      "Training Loss:  0.01047468\n",
      "Training Loss:  0.01047468\n",
      "Training Loss:  0.01047468\n",
      "Training Loss:  0.01047359\n",
      "Training Loss:  0.01047359\n",
      "Training Loss:  0.01047359\n",
      "Final training Loss:  0.01047359\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00045784\n",
      "Training Loss:  0.00045784\n",
      "Training Loss:  0.00045784\n",
      "Training Loss:  0.0004578\n",
      "Training Loss:  0.0004578\n",
      "Training Loss:  0.0004578\n",
      "Training Loss:  0.0004578\n",
      "Training Loss:  0.0004578\n",
      "Training Loss:  0.0004578\n",
      "Training Loss:  0.0004578\n",
      "Final training Loss:  0.0004578\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00118775\n",
      "Training Loss:  0.00118775\n",
      "Training Loss:  0.00118775\n",
      "Training Loss:  0.00118775\n",
      "Training Loss:  0.00118749\n",
      "Training Loss:  0.00118749\n",
      "Training Loss:  0.00118749\n",
      "Training Loss:  0.00118749\n",
      "Training Loss:  0.00118749\n",
      "Training Loss:  0.00118749\n",
      "Final training Loss:  0.00118749\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  21.81729698\n",
      "Training Loss:  21.81729698\n",
      "Training Loss:  21.81729698\n",
      "Training Loss:  21.81729507\n",
      "Training Loss:  21.81729507\n",
      "Training Loss:  21.81729507\n",
      "Training Loss:  21.81729507\n",
      "Training Loss:  21.81729507\n",
      "Training Loss:  21.81729507\n",
      "Training Loss:  21.81729507\n",
      "Final training Loss:  21.81729507\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00530378\n",
      "Training Loss:  0.0053037\n",
      "Training Loss:  0.0053037\n",
      "Training Loss:  0.0053037\n",
      "Training Loss:  0.0053037\n",
      "Training Loss:  0.0053037\n",
      "Training Loss:  0.0053037\n",
      "Training Loss:  0.0053037\n",
      "Training Loss:  0.0053037\n",
      "Training Loss:  0.0053037\n",
      "Final training Loss:  0.0053037\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00438328\n",
      "Training Loss:  0.00101974\n",
      "Training Loss:  0.00101972\n",
      "Training Loss:  0.00101972\n",
      "Training Loss:  0.00101972\n",
      "Training Loss:  0.00101963\n",
      "Training Loss:  0.00101963\n",
      "Training Loss:  0.00101963\n",
      "Training Loss:  0.00101963\n",
      "Training Loss:  0.00101963\n",
      "Final training Loss:  0.00101963\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.32210016\n",
      "Training Loss:  0.32210016\n",
      "Training Loss:  0.32209924\n",
      "Training Loss:  0.32209924\n",
      "Training Loss:  0.32209164\n",
      "Training Loss:  0.32209164\n",
      "Training Loss:  0.32208493\n",
      "Training Loss:  0.28667158\n",
      "Training Loss:  0.28667158\n",
      "Training Loss:  0.28667158\n",
      "Final training Loss:  0.28667158\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07085595\n",
      "Training Loss:  0.07085595\n",
      "Training Loss:  0.07085595\n",
      "Training Loss:  0.07085595\n",
      "Training Loss:  0.07085241\n",
      "Training Loss:  0.07085241\n",
      "Training Loss:  0.07085241\n",
      "Training Loss:  0.07085241\n",
      "Training Loss:  0.07085241\n",
      "Training Loss:  0.07085241\n",
      "Final training Loss:  0.07085241\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00091033\n",
      "Training Loss:  0.00080585\n",
      "Training Loss:  0.00036293\n",
      "Training Loss:  0.00036293\n",
      "Training Loss:  0.00036293\n",
      "Training Loss:  0.00036293\n",
      "Training Loss:  0.0003629\n",
      "Training Loss:  0.0003629\n",
      "Training Loss:  0.0003629\n",
      "Training Loss:  0.0003629\n",
      "Final training Loss:  0.0003629\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01907366\n",
      "Training Loss:  0.01907194\n",
      "Training Loss:  0.01907194\n",
      "Training Loss:  0.01907194\n",
      "Training Loss:  0.01907194\n",
      "Training Loss:  0.01907194\n",
      "Training Loss:  0.01907194\n",
      "Training Loss:  0.01907194\n",
      "Training Loss:  0.01907194\n",
      "Training Loss:  0.01907194\n",
      "Final training Loss:  0.01907194\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  105.43851471\n",
      "Training Loss:  105.43822479\n",
      "Training Loss:  105.43822479\n",
      "Training Loss:  105.43822479\n",
      "Training Loss:  105.43822479\n",
      "Training Loss:  105.43822479\n",
      "Training Loss:  105.43822479\n",
      "Training Loss:  105.43822479\n",
      "Training Loss:  105.43822479\n",
      "Training Loss:  105.43822479\n",
      "Final training Loss:  105.43822479\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 64, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00334121\n",
      "Training Loss:  0.00333608\n",
      "Training Loss:  0.00333256\n",
      "Training Loss:  0.00333256\n",
      "Training Loss:  0.00332828\n",
      "Training Loss:  0.00332841\n",
      "Training Loss:  0.00332828\n",
      "Training Loss:  0.00332828\n",
      "Training Loss:  0.00332828\n",
      "Training Loss:  0.00332828\n",
      "Final training Loss:  0.00332828\n",
      "\n",
      "Running model (trial=1, mod=26, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.45620385\n",
      "Training Loss:  0.45620385\n",
      "Training Loss:  0.45619783\n",
      "Training Loss:  0.45619783\n",
      "Training Loss:  0.45619783\n",
      "Training Loss:  0.45619783\n",
      "Training Loss:  0.45619783\n",
      "Training Loss:  0.45619783\n",
      "Training Loss:  0.45619783\n",
      "Training Loss:  0.45619783\n",
      "Final training Loss:  0.45619783\n",
      "\n",
      "Running model (trial=1, mod=27, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00254416\n",
      "Training Loss:  0.00254413\n",
      "Training Loss:  0.00254413\n",
      "Training Loss:  0.00232095\n",
      "Training Loss:  0.00033905\n",
      "Training Loss:  0.00033799\n",
      "Training Loss:  0.00033792\n",
      "Training Loss:  0.00033782\n",
      "Training Loss:  0.00033782\n",
      "Training Loss:  0.00033782\n",
      "Final training Loss:  0.00033782\n",
      "\n",
      "Running model (trial=1, mod=28, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00156527\n",
      "Training Loss:  0.00156502\n",
      "Training Loss:  0.00156502\n",
      "Training Loss:  0.00156502\n",
      "Training Loss:  0.00156502\n",
      "Training Loss:  0.00156502\n",
      "Training Loss:  0.00156502\n",
      "Training Loss:  0.00156502\n",
      "Training Loss:  0.00156502\n",
      "Training Loss:  0.00156502\n",
      "Final training Loss:  0.00156502\n",
      "\n",
      "Running model (trial=1, mod=29, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Training Loss:  0.00052231\n",
      "Final training Loss:  0.00052231\n",
      "\n",
      "Running model (trial=1, mod=30, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00037814\n",
      "Training Loss:  0.00037809\n",
      "Training Loss:  0.00037809\n",
      "Training Loss:  0.00037809\n",
      "Training Loss:  0.00037809\n",
      "Training Loss:  0.00037804\n",
      "Training Loss:  0.00037784\n",
      "Training Loss:  0.00037784\n",
      "Training Loss:  0.00037784\n",
      "Training Loss:  0.00037784\n",
      "Final training Loss:  0.00037784\n",
      "\n",
      "Running model (trial=1, mod=31, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00114693\n",
      "Training Loss:  0.00114693\n",
      "Training Loss:  0.00114693\n",
      "Training Loss:  0.00114693\n",
      "Training Loss:  0.00114692\n",
      "Training Loss:  0.00114676\n",
      "Training Loss:  0.00114676\n",
      "Training Loss:  0.00114676\n",
      "Training Loss:  0.00114676\n",
      "Training Loss:  0.00114676\n",
      "Final training Loss:  0.00114676\n",
      "\n",
      "Running model (trial=1, mod=32, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0463154\n",
      "Training Loss:  0.04631519\n",
      "Training Loss:  0.04631519\n",
      "Training Loss:  0.0463152\n",
      "Training Loss:  0.0463152\n",
      "Training Loss:  0.00134495\n",
      "Training Loss:  0.00134495\n",
      "Training Loss:  0.00134495\n",
      "Training Loss:  0.00134495\n",
      "Training Loss:  0.00134477\n",
      "Final training Loss:  0.00134477\n",
      "\n",
      "Running model (trial=1, mod=33, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00029743\n",
      "Training Loss:  0.00029743\n",
      "Training Loss:  0.00029743\n",
      "Training Loss:  0.0001569\n",
      "Training Loss:  0.0001569\n",
      "Training Loss:  0.00015687\n",
      "Training Loss:  5.265e-05\n",
      "Training Loss:  5.265e-05\n",
      "Training Loss:  5.264e-05\n",
      "Training Loss:  5.264e-05\n",
      "Final training Loss:  5.264e-05\n",
      "\n",
      "Running model (trial=1, mod=34, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00072447\n",
      "Training Loss:  0.00067937\n",
      "Training Loss:  0.00067937\n",
      "Training Loss:  0.00067935\n",
      "Training Loss:  0.00067935\n",
      "Training Loss:  0.00067935\n",
      "Training Loss:  0.00067935\n",
      "Training Loss:  0.00067932\n",
      "Training Loss:  0.00067932\n",
      "Training Loss:  0.00067932\n",
      "Final training Loss:  0.00067932\n",
      "\n",
      "Running model (trial=1, mod=35, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00409778\n",
      "Training Loss:  0.00409778\n",
      "Training Loss:  0.00409778\n",
      "Training Loss:  0.00409778\n",
      "Training Loss:  0.00409768\n",
      "Training Loss:  0.00409768\n",
      "Training Loss:  0.003627\n",
      "Training Loss:  0.003627\n",
      "Training Loss:  0.003627\n",
      "Training Loss:  0.003627\n",
      "Final training Loss:  0.003627\n",
      "\n",
      "Running model (trial=1, mod=36, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  7.34243059\n",
      "Training Loss:  7.34222317\n",
      "Training Loss:  7.34222317\n",
      "Training Loss:  7.34222317\n",
      "Training Loss:  1.74681854\n",
      "Training Loss:  1.72337425\n",
      "Training Loss:  1.72337425\n",
      "Training Loss:  0.23609421\n",
      "Training Loss:  0.23609421\n",
      "Training Loss:  0.23609421\n",
      "Final training Loss:  0.23609421\n",
      "\n",
      "Running model (trial=1, mod=37, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10137939\n",
      "Training Loss:  600.10131836\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Training Loss:  600.10125732\n",
      "Final training Loss:  600.10125732\n",
      "\n",
      "Running model (trial=1, mod=38, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 64, 'neurons': 8, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Training Loss:  600.1015625\n",
      "Final training Loss:  600.1015625\n",
      "\n",
      "Running model (trial=1, mod=39, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.2597048\n",
      "Training Loss:  0.25970462\n",
      "Training Loss:  0.25969836\n",
      "Training Loss:  0.25969836\n",
      "Training Loss:  0.25969717\n",
      "Training Loss:  0.25969717\n",
      "Training Loss:  0.25969717\n",
      "Training Loss:  0.25969717\n",
      "Training Loss:  0.25969717\n",
      "Training Loss:  0.25969717\n",
      "Final training Loss:  0.25969717\n",
      "\n",
      "Running model (trial=1, mod=40, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 4, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00111495\n",
      "Training Loss:  0.00111495\n",
      "Training Loss:  0.00111421\n",
      "Training Loss:  0.00111406\n",
      "Training Loss:  0.00111406\n",
      "Training Loss:  0.00111406\n",
      "Training Loss:  0.00111405\n",
      "Training Loss:  0.00111405\n",
      "Training Loss:  0.00111405\n",
      "Training Loss:  0.00111405\n",
      "Final training Loss:  0.00111405\n",
      "\n",
      "Running model (trial=1, mod=41, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  3.158e-05\n",
      "Training Loss:  3.155e-05\n",
      "Training Loss:  3.154e-05\n",
      "Training Loss:  3.154e-05\n",
      "Training Loss:  3.154e-05\n",
      "Training Loss:  3.151e-05\n",
      "Training Loss:  3.151e-05\n",
      "Training Loss:  3.151e-05\n",
      "Training Loss:  3.151e-05\n",
      "Training Loss:  3.151e-05\n",
      "Final training Loss:  3.151e-05\n",
      "\n",
      "Running model (trial=1, mod=42, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 16, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00278979\n",
      "Training Loss:  0.00278976\n",
      "Training Loss:  0.00278957\n",
      "Training Loss:  0.00278957\n",
      "Training Loss:  0.00278957\n",
      "Training Loss:  0.00278948\n",
      "Training Loss:  0.00278948\n",
      "Training Loss:  0.00278935\n",
      "Training Loss:  0.00278935\n",
      "Training Loss:  0.00278935\n",
      "Final training Loss:  0.00278935\n",
      "\n",
      "Running model (trial=1, mod=43, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 32, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05465783\n",
      "Training Loss:  0.01829607\n",
      "Training Loss:  0.01829607\n",
      "Training Loss:  0.01829607\n",
      "Training Loss:  0.01829607\n",
      "Training Loss:  0.01829607\n",
      "Training Loss:  0.01829607\n",
      "Training Loss:  0.01829607\n",
      "Training Loss:  0.01744718\n",
      "Training Loss:  0.01736536\n",
      "Final training Loss:  0.01736536\n",
      "\n",
      "Running model (trial=1, mod=44, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 64, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Training Loss:  0.00561647\n",
      "Final training Loss:  0.00561647\n",
      "\n",
      "Running model (trial=1, mod=45, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 128, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00988567\n",
      "Training Loss:  0.00258081\n",
      "Training Loss:  0.00258076\n",
      "Training Loss:  0.00258076\n",
      "Training Loss:  0.00258062\n",
      "Training Loss:  0.00258062\n",
      "Training Loss:  0.00258062\n",
      "Training Loss:  0.00258062\n",
      "Training Loss:  0.00258055\n",
      "Training Loss:  0.00258055\n",
      "Final training Loss:  0.00258055\n",
      "\n",
      "Running model (trial=1, mod=46, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00319587\n",
      "Training Loss:  0.00319587\n",
      "Training Loss:  0.00034529\n",
      "Training Loss:  0.00034529\n",
      "Training Loss:  0.00034523\n",
      "Training Loss:  0.00034523\n",
      "Training Loss:  0.00023422\n",
      "Training Loss:  0.00023422\n",
      "Training Loss:  0.00023422\n",
      "Training Loss:  0.00023421\n",
      "Final training Loss:  0.00023421\n",
      "\n",
      "Running model (trial=1, mod=47, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 4, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00394357\n",
      "Training Loss:  0.00394357\n",
      "Training Loss:  0.00394357\n",
      "Training Loss:  0.00394357\n",
      "Training Loss:  0.00394357\n",
      "Training Loss:  0.00394331\n",
      "Training Loss:  0.00394331\n",
      "Training Loss:  0.00394331\n",
      "Training Loss:  0.00394331\n",
      "Training Loss:  0.00394331\n",
      "Final training Loss:  0.00394331\n",
      "\n",
      "Running model (trial=1, mod=48, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 8, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00212552\n",
      "Training Loss:  0.00212535\n",
      "Training Loss:  0.00197502\n",
      "Training Loss:  0.00192674\n",
      "Training Loss:  0.00141666\n",
      "Training Loss:  0.0014162\n",
      "Training Loss:  0.00139573\n",
      "Training Loss:  0.00139573\n",
      "Training Loss:  0.00139573\n",
      "Training Loss:  0.00139573\n",
      "Final training Loss:  0.00139573\n",
      "\n",
      "Running model (trial=1, mod=49, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 16, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Training Loss:  0.00077342\n",
      "Final training Loss:  0.00077342\n",
      "\n",
      "Running model (trial=1, mod=50, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 32, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Training Loss:  0.00152119\n",
      "Final training Loss:  0.00152119\n",
      "\n",
      "Running model (trial=1, mod=51, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 64, 'neurons': 8, 'model': <class 'deep_reparametrization.ResNet.ResNet'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Training Loss:  2366.91918945\n",
      "Final training Loss:  2366.91918945\n",
      "\n",
      "Running model (trial=2, mod=52, k=0):\n",
      "Parameters: ({'input_dimension': 1, 'output_dimension': 1, 'activation': 'tanh', 'n_hidden_layers': 2, 'neurons': 2, 'model': <class 'deepthermal.FFNN_model.FFNN'>}, {'batch_size': 128, 'regularization_param': 0, 'compute_loss': <function compute_loss_reparam at 0x122dd5f70>, 'loss_func': <function get_elastic_metric_loss.<locals>.elastic_metric_loss at 0x1030988b0>, 'optimizer': 'strong_wolfe', 'num_epochs': 10, 'learning_rate': 1})\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-8ce513209ae6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m cv_results = k_fold_cv_grid(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_model_params_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mfit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfit_FFNN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtraining_params\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexp_training_params_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/git_skole/deepthermal/deepthermal/validation.py\u001b[0m in \u001b[0;36mk_fold_cv_grid\u001b[0;34m(model_params, training_params, data, val_data, fit, folds, trials, partial, verbose, get_error)\u001b[0m\n\u001b[1;32m     99\u001b[0m             \u001b[0mmodel_param_k\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_param\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m             \u001b[0mmodel_instance\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_param_k\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_param_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m             model_instance, loss_history_train, loss_history_val = fit(\n\u001b[0m\u001b[1;32m    102\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_instance\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0;34m**\u001b[0m\u001b[0mtraining_param\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/git_skole/deepthermal/deepthermal/FFNN_model.py\u001b[0m in \u001b[0;36mfit_FFNN\u001b[0;34m(model, data, num_epochs, batch_size, optimizer, init, regularization_param, regularization_exp, data_val, track_history, track_epoch, verbose, verbose_interval, learning_rate, init_weight_seed, lr_scheduler, loss_func, compute_loss, max_nan_steps, **kwargs)\u001b[0m\n\u001b[1;32m    230\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 232\u001b[0;31m             \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    233\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtrack_epoch\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtrack_history\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlr_scheduler\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    423\u001b[0m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0m\u001b[1;32m    426\u001b[0m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mg_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mgtd_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgtd_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mls_func_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git_repos/git_skole/deepthermal/deepthermal/FFNN_model.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m    187\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    188\u001b[0m                 \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 189\u001b[0;31m                 \u001b[0moptimizer_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    190\u001b[0m                 \u001b[0;31m# forward + backward + optimize\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m                 loss_u = compute_loss(\n",
      "\u001b[0;32m/usr/local/lib/python3.9/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[0;34m(self, set_to_none)\u001b[0m\n\u001b[1;32m    214\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m                                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m                             \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# plotting\n",
    "\n",
    "x_train_ = x_train.detach()\n",
    "x_sorted, indices = torch.sort(x_train_, dim=0)\n",
    "plot_kwargs = {\n",
    "    \"x_test\": x_sorted,\n",
    "    \"x_train\": x_sorted,\n",
    "    \"y_train\": c1.ksi(x_sorted),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\varphi(t)$\",\n",
    "    \"compare_label\": \"Analytical solution\"\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    plot_name=SET_NAME,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models).flatten()\n",
    "model_type = np.vectorize(str)(models).flatten()\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models).flatten()\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models).flatten()\n",
    "loss_array = np.vectorize(lambda model: no_penalty_loss_func(model, x_train, q_train).detach())(models).flatten()\n",
    "\n",
    "# make data frame\n",
    "d_results = pd.DataFrame(\n",
    "    {\"loss\": loss_array, \"neurons\": neurons, \"layers\": layers, \"parameters\": parameters, \"model\": model_type})\n",
    "\n",
    "d_results_layer = d_results[d_results.neurons == 8]\n",
    "d_results_neurons = d_results[d_results.layers == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig_neurons = sns.lineplot(data=d_results_neurons, y=\"loss\", x=\"neurons\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_neurons.set(xscale=\"log\", yscale=\"log\", xlabel=\"Neurons\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/neurons_error.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_neurons[d_results_neurons.loss > 1]), len(d_results_neurons))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig_layers = sns.lineplot(data=d_results_layer, y=\"loss\", x=\"layers\", hue=\"model\", ci=80, err_style=\"bars\")\n",
    "fig_layers.set(yscale=\"log\", xscale=\"log\", xlabel=\"Layers\", ylabel=\"Error\")\n",
    "fig_layers.set(ylabel=\"$E$\")\n",
    "plt.savefig(f\"{PATH_FIGURES}/layer_error.pdf\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"faled, total:\")\n",
    "print(len(d_results_layer[d_results_layer.loss\n",
    "                          > 1]), len(d_results_layer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig_scatter = sns.scatterplot(data=d_results, y=\"loss\", x=\"parameters\", hue=\"layers\", style=\"model\")\n",
    "fig_neurons.set(ylabel=\"$E$\")\n",
    "fig_scatter.set(yscale=\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(d_results[d_results.loss < 2]), len(d_results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
