{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Test reparam of curves that are not equivqalent \"\"\"\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from deep_reparametrization.plotting import plot_reparametrization\n",
    "from deep_reparametrization.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    "    get_elastic_error_func,\n",
    ")\n",
    "from deep_reparametrization.ResNET import ResNET\n",
    "import test.curves_2 as c2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "########\n",
    "PATH_FIGURES = \"../figures/curve_2\"\n",
    "########\n",
    "\n",
    "\n",
    "SET_NAME = \"curve_2_exp_1\"\n",
    "\n",
    "FOLDS = 1\n",
    "N = 128  # training points internal\n",
    "\n",
    "# lr_scheduler =  lambda optimizer : optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=30,\n",
    "#                                                                         factor=0.8, verbose=True)\n",
    "loss_func = get_elastic_metric_loss(r=c2.r, constrain_cost=1e3, verbose=False)\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": [ResNET, FFNN],\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 16, 32, 64],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"neurons\": [4, 8, 16, 32, 64, 128],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N],\n",
    "    \"regularization_param\": [0],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [1],\n",
    "    \"learning_rate\": [ 0.01],\n",
    "}\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = c2.q(x_train.detach())\n",
    "\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n",
    "\n",
    "model_params_iter = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72648269\n",
      "Validation Loss:  0.72648263\n",
      "Final Training Loss:  0.72648269\n",
      "Final Validation Loss:  0.72648263\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.67634803\n",
      "Validation Loss:  0.67634803\n",
      "Final Training Loss:  0.67634803\n",
      "Final Validation Loss:  0.67634803\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70437694\n",
      "Validation Loss:  0.70437694\n",
      "Final Training Loss:  0.70437694\n",
      "Final Validation Loss:  0.70437694\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63440239\n",
      "Validation Loss:  0.63440239\n",
      "Final Training Loss:  0.63440239\n",
      "Final Validation Loss:  0.63440239\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.75155056\n",
      "Validation Loss:  0.75155056\n",
      "Final Training Loss:  0.75155056\n",
      "Final Validation Loss:  0.75155056\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  910285.75\n",
      "Validation Loss:  910285.8125\n",
      "Final Training Loss:  910285.75\n",
      "Final Validation Loss:  910285.8125\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71993434\n",
      "Validation Loss:  0.71993428\n",
      "Final Training Loss:  0.71993434\n",
      "Final Validation Loss:  0.71993428\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7101934\n",
      "Validation Loss:  0.7101934\n",
      "Final Training Loss:  0.7101934\n",
      "Final Validation Loss:  0.7101934\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60403877\n",
      "Validation Loss:  0.60403872\n",
      "Final Training Loss:  0.60403877\n",
      "Final Validation Loss:  0.60403872\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99337769\n",
      "Validation Loss:  500.99337769\n",
      "Final Training Loss:  500.99337769\n",
      "Final Validation Loss:  500.99337769\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00006104\n",
      "Validation Loss:  501.00006104\n",
      "Final Training Loss:  501.00006104\n",
      "Final Validation Loss:  501.00006104\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00106812\n",
      "Validation Loss:  501.00106812\n",
      "Final Training Loss:  501.00106812\n",
      "Final Validation Loss:  501.00106812\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73948562\n",
      "Validation Loss:  0.73948556\n",
      "Final Training Loss:  0.73948562\n",
      "Final Validation Loss:  0.73948556\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.74069422\n",
      "Validation Loss:  0.74069422\n",
      "Final Training Loss:  0.74069422\n",
      "Final Validation Loss:  0.74069422\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73182654\n",
      "Validation Loss:  0.7318266\n",
      "Final Training Loss:  0.73182654\n",
      "Final Validation Loss:  0.7318266\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61706138\n",
      "Validation Loss:  0.61706144\n",
      "Final Training Loss:  0.61706138\n",
      "Final Validation Loss:  0.61706144\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.64557618\n",
      "Validation Loss:  0.64557618\n",
      "Final Training Loss:  0.64557618\n",
      "Final Validation Loss:  0.64557618\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  575620.5\n",
      "Validation Loss:  575620.5625\n",
      "Final Training Loss:  575620.5\n",
      "Final Validation Loss:  575620.5625\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71165383\n",
      "Validation Loss:  0.71165383\n",
      "Final Training Loss:  0.71165383\n",
      "Final Validation Loss:  0.71165383\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70592123\n",
      "Validation Loss:  0.70592123\n",
      "Final Training Loss:  0.70592123\n",
      "Final Validation Loss:  0.70592123\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70310879\n",
      "Validation Loss:  0.70310873\n",
      "Final Training Loss:  0.70310879\n",
      "Final Validation Loss:  0.70310873\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00354004\n",
      "Validation Loss:  501.00354004\n",
      "Final Training Loss:  501.00354004\n",
      "Final Validation Loss:  501.00354004\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00344849\n",
      "Validation Loss:  501.00344849\n",
      "Final Training Loss:  501.00344849\n",
      "Final Validation Loss:  501.00344849\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00262451\n",
      "Validation Loss:  501.00262451\n",
      "Final Training Loss:  501.00262451\n",
      "Final Validation Loss:  501.00262451\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71949703\n",
      "Validation Loss:  0.71949697\n",
      "Final Training Loss:  0.71949703\n",
      "Final Validation Loss:  0.71949697\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71604115\n",
      "Validation Loss:  0.71604115\n",
      "Final Training Loss:  0.71604115\n",
      "Final Validation Loss:  0.71604115\n",
      "\n",
      "Running model (trial=0, mod=26, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71558815\n",
      "Validation Loss:  0.71558827\n",
      "Final Training Loss:  0.71558815\n",
      "Final Validation Loss:  0.71558827\n",
      "\n",
      "Running model (trial=0, mod=27, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71311104\n",
      "Validation Loss:  0.71311104\n",
      "Final Training Loss:  0.71311104\n",
      "Final Validation Loss:  0.71311104\n",
      "\n",
      "Running model (trial=0, mod=28, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.88478172\n",
      "Validation Loss:  0.88478172\n",
      "Final Training Loss:  0.88478172\n",
      "Final Validation Loss:  0.88478172\n",
      "\n",
      "Running model (trial=0, mod=29, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  8714.60058594\n",
      "Validation Loss:  8714.60058594\n",
      "Final Training Loss:  8714.60058594\n",
      "Final Validation Loss:  8714.60058594\n",
      "\n",
      "Running model (trial=0, mod=30, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71749818\n",
      "Validation Loss:  0.71749818\n",
      "Final Training Loss:  0.71749818\n",
      "Final Validation Loss:  0.71749818\n",
      "\n",
      "Running model (trial=0, mod=31, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71822459\n",
      "Validation Loss:  0.71822459\n",
      "Final Training Loss:  0.71822459\n",
      "Final Validation Loss:  0.71822459\n",
      "\n",
      "Running model (trial=0, mod=32, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69940531\n",
      "Validation Loss:  0.69940525\n",
      "Final Training Loss:  0.69940531\n",
      "Final Validation Loss:  0.69940525\n",
      "\n",
      "Running model (trial=0, mod=33, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99099731\n",
      "Validation Loss:  500.99099731\n",
      "Final Training Loss:  500.99099731\n",
      "Final Validation Loss:  500.99099731\n",
      "\n",
      "Running model (trial=0, mod=34, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0017395\n",
      "Validation Loss:  501.0017395\n",
      "Final Training Loss:  501.0017395\n",
      "Final Validation Loss:  501.0017395\n",
      "\n",
      "Running model (trial=0, mod=35, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0015564\n",
      "Validation Loss:  501.0015564\n",
      "Final Training Loss:  501.0015564\n",
      "Final Validation Loss:  501.0015564\n",
      "\n",
      "Running model (trial=0, mod=36, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73896062\n",
      "Validation Loss:  0.73896062\n",
      "Final Training Loss:  0.73896062\n",
      "Final Validation Loss:  0.73896062\n",
      "\n",
      "Running model (trial=0, mod=37, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72660071\n",
      "Validation Loss:  0.72660071\n",
      "Final Training Loss:  0.72660071\n",
      "Final Validation Loss:  0.72660071\n",
      "\n",
      "Running model (trial=0, mod=38, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73162037\n",
      "Validation Loss:  0.73162031\n",
      "Final Training Loss:  0.73162037\n",
      "Final Validation Loss:  0.73162031\n",
      "\n",
      "Running model (trial=0, mod=39, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62522656\n",
      "Validation Loss:  0.62522656\n",
      "Final Training Loss:  0.62522656\n",
      "Final Validation Loss:  0.62522656\n",
      "\n",
      "Running model (trial=0, mod=40, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.80001467\n",
      "Validation Loss:  0.80001467\n",
      "Final Training Loss:  0.80001467\n",
      "Final Validation Loss:  0.80001467\n",
      "\n",
      "Running model (trial=0, mod=41, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1412.95800781\n",
      "Validation Loss:  1412.95800781\n",
      "Final Training Loss:  1412.95800781\n",
      "Final Validation Loss:  1412.95800781\n",
      "\n",
      "Running model (trial=0, mod=42, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73427218\n",
      "Validation Loss:  0.73427224\n",
      "Final Training Loss:  0.73427218\n",
      "Final Validation Loss:  0.73427224\n",
      "\n",
      "Running model (trial=0, mod=43, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70239669\n",
      "Validation Loss:  0.70239663\n",
      "Final Training Loss:  0.70239669\n",
      "Final Validation Loss:  0.70239663\n",
      "\n",
      "Running model (trial=0, mod=44, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70611095\n",
      "Validation Loss:  0.70611101\n",
      "Final Training Loss:  0.70611095\n",
      "Final Validation Loss:  0.70611101\n",
      "\n",
      "Running model (trial=0, mod=45, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.94454956\n",
      "Validation Loss:  500.94454956\n",
      "Final Training Loss:  500.94454956\n",
      "Final Validation Loss:  500.94454956\n",
      "\n",
      "Running model (trial=0, mod=46, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00308228\n",
      "Validation Loss:  501.00308228\n",
      "Final Training Loss:  501.00308228\n",
      "Final Validation Loss:  501.00308228\n",
      "\n",
      "Running model (trial=0, mod=47, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00167847\n",
      "Validation Loss:  501.00167847\n",
      "Final Training Loss:  501.00167847\n",
      "Final Validation Loss:  501.00167847\n",
      "\n",
      "Running model (trial=0, mod=48, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73063284\n",
      "Validation Loss:  0.73063278\n",
      "Final Training Loss:  0.73063284\n",
      "Final Validation Loss:  0.73063278\n",
      "\n",
      "Running model (trial=0, mod=49, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73622888\n",
      "Validation Loss:  0.73622882\n",
      "Final Training Loss:  0.73622888\n",
      "Final Validation Loss:  0.73622882\n",
      "\n",
      "Running model (trial=0, mod=50, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72364855\n",
      "Validation Loss:  0.72364861\n",
      "Final Training Loss:  0.72364855\n",
      "Final Validation Loss:  0.72364861\n",
      "\n",
      "Running model (trial=0, mod=51, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.88440233\n",
      "Validation Loss:  0.88440239\n",
      "Final Training Loss:  0.88440233\n",
      "Final Validation Loss:  0.88440239\n",
      "\n",
      "Running model (trial=0, mod=52, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.80195957\n",
      "Validation Loss:  0.80195957\n",
      "Final Training Loss:  0.80195957\n",
      "Final Validation Loss:  0.80195957\n",
      "\n",
      "Running model (trial=0, mod=53, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1258.7677002\n",
      "Validation Loss:  1258.7677002\n",
      "Final Training Loss:  1258.7677002\n",
      "Final Validation Loss:  1258.7677002\n",
      "\n",
      "Running model (trial=0, mod=54, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73058283\n",
      "Validation Loss:  0.73058283\n",
      "Final Training Loss:  0.73058283\n",
      "Final Validation Loss:  0.73058283\n",
      "\n",
      "Running model (trial=0, mod=55, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72453654\n",
      "Validation Loss:  0.72453648\n",
      "Final Training Loss:  0.72453654\n",
      "Final Validation Loss:  0.72453648\n",
      "\n",
      "Running model (trial=0, mod=56, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71325344\n",
      "Validation Loss:  0.71325338\n",
      "Final Training Loss:  0.71325344\n",
      "Final Validation Loss:  0.71325338\n",
      "\n",
      "Running model (trial=0, mod=57, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.95263672\n",
      "Validation Loss:  500.95263672\n",
      "Final Training Loss:  500.95263672\n",
      "Final Validation Loss:  500.95263672\n",
      "\n",
      "Running model (trial=0, mod=58, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00088501\n",
      "Validation Loss:  501.00088501\n",
      "Final Training Loss:  501.00088501\n",
      "Final Validation Loss:  501.00088501\n",
      "\n",
      "Running model (trial=0, mod=59, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0022583\n",
      "Validation Loss:  501.0022583\n",
      "Final Training Loss:  501.0022583\n",
      "Final Validation Loss:  501.0022583\n",
      "\n",
      "Running model (trial=0, mod=60, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73047769\n",
      "Validation Loss:  0.73047769\n",
      "Final Training Loss:  0.73047769\n",
      "Final Validation Loss:  0.73047769\n",
      "\n",
      "Running model (trial=0, mod=61, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72178191\n",
      "Validation Loss:  0.72178185\n",
      "Final Training Loss:  0.72178191\n",
      "Final Validation Loss:  0.72178185\n",
      "\n",
      "Running model (trial=0, mod=62, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7350992\n",
      "Validation Loss:  0.73509914\n",
      "Final Training Loss:  0.7350992\n",
      "Final Validation Loss:  0.73509914\n",
      "\n",
      "Running model (trial=0, mod=63, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62163275\n",
      "Validation Loss:  0.6216327\n",
      "Final Training Loss:  0.62163275\n",
      "Final Validation Loss:  0.6216327\n",
      "\n",
      "Running model (trial=0, mod=64, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.91634214\n",
      "Validation Loss:  0.9163422\n",
      "Final Training Loss:  0.91634214\n",
      "Final Validation Loss:  0.9163422\n",
      "\n",
      "Running model (trial=0, mod=65, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  3765.63623047\n",
      "Validation Loss:  3765.63623047\n",
      "Final Training Loss:  3765.63623047\n",
      "Final Validation Loss:  3765.63623047\n",
      "\n",
      "Running model (trial=0, mod=66, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72675061\n",
      "Validation Loss:  0.72675061\n",
      "Final Training Loss:  0.72675061\n",
      "Final Validation Loss:  0.72675061\n",
      "\n",
      "Running model (trial=0, mod=67, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72417688\n",
      "Validation Loss:  0.72417688\n",
      "Final Training Loss:  0.72417688\n",
      "Final Validation Loss:  0.72417688\n",
      "\n",
      "Running model (trial=0, mod=68, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72044843\n",
      "Validation Loss:  0.72044843\n",
      "Final Training Loss:  0.72044843\n",
      "Final Validation Loss:  0.72044843\n",
      "\n",
      "Running model (trial=0, mod=69, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99362183\n",
      "Validation Loss:  500.99362183\n",
      "Final Training Loss:  500.99362183\n",
      "Final Validation Loss:  500.99362183\n",
      "\n",
      "Running model (trial=0, mod=70, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00149536\n",
      "Validation Loss:  501.00149536\n",
      "Final Training Loss:  501.00149536\n",
      "Final Validation Loss:  501.00149536\n",
      "\n",
      "Running model (trial=0, mod=71, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00189209\n",
      "Validation Loss:  501.00189209\n",
      "Final Training Loss:  501.00189209\n",
      "Final Validation Loss:  501.00189209\n",
      "\n",
      "Running model (trial=1, mod=72, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72267437\n",
      "Validation Loss:  0.72267443\n",
      "Final Training Loss:  0.72267437\n",
      "Final Validation Loss:  0.72267443\n",
      "\n",
      "Running model (trial=1, mod=73, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.74592048\n",
      "Validation Loss:  0.74592048\n",
      "Final Training Loss:  0.74592048\n",
      "Final Validation Loss:  0.74592048\n",
      "\n",
      "Running model (trial=1, mod=74, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.61003685\n",
      "Validation Loss:  0.61003691\n",
      "Final Training Loss:  0.61003685\n",
      "Final Validation Loss:  0.61003691\n",
      "\n",
      "Running model (trial=1, mod=75, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59293729\n",
      "Validation Loss:  0.59293729\n",
      "Final Training Loss:  0.59293729\n",
      "Final Validation Loss:  0.59293729\n",
      "\n",
      "Running model (trial=1, mod=76, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  26.15698814\n",
      "Validation Loss:  26.15698814\n",
      "Final Training Loss:  26.15698814\n",
      "Final Validation Loss:  26.15698814\n",
      "\n",
      "Running model (trial=1, mod=77, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24494827\n",
      "Validation Loss:  1.24494827\n",
      "Final Training Loss:  1.24494827\n",
      "Final Validation Loss:  1.24494827\n",
      "\n",
      "Running model (trial=1, mod=78, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72109222\n",
      "Validation Loss:  0.72109216\n",
      "Final Training Loss:  0.72109222\n",
      "Final Validation Loss:  0.72109216\n",
      "\n",
      "Running model (trial=1, mod=79, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71011478\n",
      "Validation Loss:  0.71011472\n",
      "Final Training Loss:  0.71011478\n",
      "Final Validation Loss:  0.71011472\n",
      "\n",
      "Running model (trial=1, mod=80, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60226852\n",
      "Validation Loss:  0.60226852\n",
      "Final Training Loss:  0.60226852\n",
      "Final Validation Loss:  0.60226852\n",
      "\n",
      "Running model (trial=1, mod=81, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0012207\n",
      "Validation Loss:  501.0012207\n",
      "Final Training Loss:  501.0012207\n",
      "Final Validation Loss:  501.0012207\n",
      "\n",
      "Running model (trial=1, mod=82, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00048828\n",
      "Validation Loss:  501.00048828\n",
      "Final Training Loss:  501.00048828\n",
      "Final Validation Loss:  501.00048828\n",
      "\n",
      "Running model (trial=1, mod=83, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00256348\n",
      "Validation Loss:  501.00256348\n",
      "Final Training Loss:  501.00256348\n",
      "Final Validation Loss:  501.00256348\n",
      "\n",
      "Running model (trial=1, mod=84, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72346532\n",
      "Validation Loss:  0.72346532\n",
      "Final Training Loss:  0.72346532\n",
      "Final Validation Loss:  0.72346532\n",
      "\n",
      "Running model (trial=1, mod=85, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.74113065\n",
      "Validation Loss:  0.74113065\n",
      "Final Training Loss:  0.74113065\n",
      "Final Validation Loss:  0.74113065\n",
      "\n",
      "Running model (trial=1, mod=86, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.63470095\n",
      "Validation Loss:  0.63470095\n",
      "Final Training Loss:  0.63470095\n",
      "Final Validation Loss:  0.63470095\n",
      "\n",
      "Running model (trial=1, mod=87, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1.04708934\n",
      "Validation Loss:  1.04708934\n",
      "Final Training Loss:  1.04708934\n",
      "Final Validation Loss:  1.04708934\n",
      "\n",
      "Running model (trial=1, mod=88, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.68767762\n",
      "Validation Loss:  0.68767756\n",
      "Final Training Loss:  0.68767762\n",
      "Final Validation Loss:  0.68767756\n",
      "\n",
      "Running model (trial=1, mod=89, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6014173\n",
      "Validation Loss:  0.6014173\n",
      "Final Training Loss:  0.6014173\n",
      "Final Validation Loss:  0.6014173\n",
      "\n",
      "Running model (trial=1, mod=90, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72352034\n",
      "Validation Loss:  0.72352028\n",
      "Final Training Loss:  0.72352034\n",
      "Final Validation Loss:  0.72352028\n",
      "\n",
      "Running model (trial=1, mod=91, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70875764\n",
      "Validation Loss:  0.70875764\n",
      "Final Training Loss:  0.70875764\n",
      "Final Validation Loss:  0.70875764\n",
      "\n",
      "Running model (trial=1, mod=92, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59573627\n",
      "Validation Loss:  0.59573627\n",
      "Final Training Loss:  0.59573627\n",
      "Final Validation Loss:  0.59573627\n",
      "\n",
      "Running model (trial=1, mod=93, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.99841309\n",
      "Validation Loss:  500.99841309\n",
      "Final Training Loss:  500.99841309\n",
      "Final Validation Loss:  500.99841309\n",
      "\n",
      "Running model (trial=1, mod=94, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00085449\n",
      "Validation Loss:  501.00085449\n",
      "Final Training Loss:  501.00085449\n",
      "Final Validation Loss:  501.00085449\n",
      "\n",
      "Running model (trial=1, mod=95, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00213623\n",
      "Validation Loss:  501.00213623\n",
      "Final Training Loss:  501.00213623\n",
      "Final Validation Loss:  501.00213623\n",
      "\n",
      "Running model (trial=1, mod=96, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7250697\n",
      "Validation Loss:  0.72506964\n",
      "Final Training Loss:  0.7250697\n",
      "Final Validation Loss:  0.72506964\n",
      "\n",
      "Running model (trial=1, mod=97, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73770481\n",
      "Validation Loss:  0.73770481\n",
      "Final Training Loss:  0.73770481\n",
      "Final Validation Loss:  0.73770481\n",
      "\n",
      "Running model (trial=1, mod=98, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72671396\n",
      "Validation Loss:  0.72671396\n",
      "Final Training Loss:  0.72671396\n",
      "Final Validation Loss:  0.72671396\n",
      "\n",
      "Running model (trial=1, mod=99, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60041571\n",
      "Validation Loss:  0.60041571\n",
      "Final Training Loss:  0.60041571\n",
      "Final Validation Loss:  0.60041571\n",
      "\n",
      "Running model (trial=1, mod=100, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  76.72756958\n",
      "Validation Loss:  76.72756958\n",
      "Final Training Loss:  76.72756958\n",
      "Final Validation Loss:  76.72756958\n",
      "\n",
      "Running model (trial=1, mod=101, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7407667\n",
      "Validation Loss:  0.74076682\n",
      "Final Training Loss:  0.7407667\n",
      "Final Validation Loss:  0.74076682\n",
      "\n",
      "Running model (trial=1, mod=102, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73225456\n",
      "Validation Loss:  0.73225451\n",
      "Final Training Loss:  0.73225456\n",
      "Final Validation Loss:  0.73225451\n",
      "\n",
      "Running model (trial=1, mod=103, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70436907\n",
      "Validation Loss:  0.70436901\n",
      "Final Training Loss:  0.70436907\n",
      "Final Validation Loss:  0.70436901\n",
      "\n",
      "Running model (trial=1, mod=104, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70233411\n",
      "Validation Loss:  0.70233417\n",
      "Final Training Loss:  0.70233411\n",
      "Final Validation Loss:  0.70233417\n",
      "\n",
      "Running model (trial=1, mod=105, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.93563843\n",
      "Validation Loss:  500.93563843\n",
      "Final Training Loss:  500.93563843\n",
      "Final Validation Loss:  500.93563843\n",
      "\n",
      "Running model (trial=1, mod=106, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00198364\n",
      "Validation Loss:  501.00198364\n",
      "Final Training Loss:  501.00198364\n",
      "Final Validation Loss:  501.00198364\n",
      "\n",
      "Running model (trial=1, mod=107, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00094604\n",
      "Validation Loss:  501.00094604\n",
      "Final Training Loss:  501.00094604\n",
      "Final Validation Loss:  501.00094604\n",
      "\n",
      "Running model (trial=1, mod=108, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7244556\n",
      "Validation Loss:  0.7244556\n",
      "Final Training Loss:  0.7244556\n",
      "Final Validation Loss:  0.7244556\n",
      "\n",
      "Running model (trial=1, mod=109, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72588283\n",
      "Validation Loss:  0.72588283\n",
      "Final Training Loss:  0.72588283\n",
      "Final Validation Loss:  0.72588283\n",
      "\n",
      "Running model (trial=1, mod=110, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72085017\n",
      "Validation Loss:  0.72085017\n",
      "Final Training Loss:  0.72085017\n",
      "Final Validation Loss:  0.72085017\n",
      "\n",
      "Running model (trial=1, mod=111, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6518051\n",
      "Validation Loss:  0.65180504\n",
      "Final Training Loss:  0.6518051\n",
      "Final Validation Loss:  0.65180504\n",
      "\n",
      "Running model (trial=1, mod=112, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  700.72674561\n",
      "Validation Loss:  700.72674561\n",
      "Final Training Loss:  700.72674561\n",
      "Final Validation Loss:  700.72674561\n",
      "\n",
      "Running model (trial=1, mod=113, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1226.43676758\n",
      "Validation Loss:  1226.43676758\n",
      "Final Training Loss:  1226.43676758\n",
      "Final Validation Loss:  1226.43676758\n",
      "\n",
      "Running model (trial=1, mod=114, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72577864\n",
      "Validation Loss:  0.7257787\n",
      "Final Training Loss:  0.72577864\n",
      "Final Validation Loss:  0.7257787\n",
      "\n",
      "Running model (trial=1, mod=115, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71857947\n",
      "Validation Loss:  0.71857941\n",
      "Final Training Loss:  0.71857947\n",
      "Final Validation Loss:  0.71857941\n",
      "\n",
      "Running model (trial=1, mod=116, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71935052\n",
      "Validation Loss:  0.71935052\n",
      "Final Training Loss:  0.71935052\n",
      "Final Validation Loss:  0.71935052\n",
      "\n",
      "Running model (trial=1, mod=117, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.98391724\n",
      "Validation Loss:  500.98391724\n",
      "Final Training Loss:  500.98391724\n",
      "Final Validation Loss:  500.98391724\n",
      "\n",
      "Running model (trial=1, mod=118, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00201416\n",
      "Validation Loss:  501.00201416\n",
      "Final Training Loss:  501.00201416\n",
      "Final Validation Loss:  501.00201416\n",
      "\n",
      "Running model (trial=1, mod=119, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00125122\n",
      "Validation Loss:  501.00125122\n",
      "Final Training Loss:  501.00125122\n",
      "Final Validation Loss:  501.00125122\n",
      "\n",
      "Running model (trial=1, mod=120, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72799993\n",
      "Validation Loss:  0.72799999\n",
      "Final Training Loss:  0.72799993\n",
      "Final Validation Loss:  0.72799999\n",
      "\n",
      "Running model (trial=1, mod=121, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73796457\n",
      "Validation Loss:  0.73796457\n",
      "Final Training Loss:  0.73796457\n",
      "Final Validation Loss:  0.73796457\n",
      "\n",
      "Running model (trial=1, mod=122, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73044801\n",
      "Validation Loss:  0.73044801\n",
      "Final Training Loss:  0.73044801\n",
      "Final Validation Loss:  0.73044801\n",
      "\n",
      "Running model (trial=1, mod=123, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.69685674\n",
      "Validation Loss:  0.69685674\n",
      "Final Training Loss:  0.69685674\n",
      "Final Validation Loss:  0.69685674\n",
      "\n",
      "Running model (trial=1, mod=124, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  170.27272034\n",
      "Validation Loss:  170.27272034\n",
      "Final Training Loss:  170.27272034\n",
      "Final Validation Loss:  170.27272034\n",
      "\n",
      "Running model (trial=1, mod=125, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  17166.66210938\n",
      "Validation Loss:  17166.66210938\n",
      "Final Training Loss:  17166.66210938\n",
      "Final Validation Loss:  17166.66210938\n",
      "\n",
      "Running model (trial=1, mod=126, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7217598\n",
      "Validation Loss:  0.72175986\n",
      "Final Training Loss:  0.7217598\n",
      "Final Validation Loss:  0.72175986\n",
      "\n",
      "Running model (trial=1, mod=127, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73007065\n",
      "Validation Loss:  0.73007065\n",
      "Final Training Loss:  0.73007065\n",
      "Final Validation Loss:  0.73007065\n",
      "\n",
      "Running model (trial=1, mod=128, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.70373726\n",
      "Validation Loss:  0.70373726\n",
      "Final Training Loss:  0.70373726\n",
      "Final Validation Loss:  0.70373726\n",
      "\n",
      "Running model (trial=1, mod=129, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.98529053\n",
      "Validation Loss:  500.98529053\n",
      "Final Training Loss:  500.98529053\n",
      "Final Validation Loss:  500.98529053\n",
      "\n",
      "Running model (trial=1, mod=130, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00109863\n",
      "Validation Loss:  501.00109863\n",
      "Final Training Loss:  501.00109863\n",
      "Final Validation Loss:  501.00109863\n",
      "\n",
      "Running model (trial=1, mod=131, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00167847\n",
      "Validation Loss:  501.00167847\n",
      "Final Training Loss:  501.00167847\n",
      "Final Validation Loss:  501.00167847\n",
      "\n",
      "Running model (trial=1, mod=132, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72594738\n",
      "Validation Loss:  0.72594744\n",
      "Final Training Loss:  0.72594738\n",
      "Final Validation Loss:  0.72594744\n",
      "\n",
      "Running model (trial=1, mod=133, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73252863\n",
      "Validation Loss:  0.73252863\n",
      "Final Training Loss:  0.73252863\n",
      "Final Validation Loss:  0.73252863\n",
      "\n",
      "Running model (trial=1, mod=134, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7245214\n",
      "Validation Loss:  0.7245214\n",
      "Final Training Loss:  0.7245214\n",
      "Final Validation Loss:  0.7245214\n",
      "\n",
      "Running model (trial=1, mod=135, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62279946\n",
      "Validation Loss:  0.62279952\n",
      "Final Training Loss:  0.62279946\n",
      "Final Validation Loss:  0.62279952\n",
      "\n",
      "Running model (trial=1, mod=136, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1.10817587\n",
      "Validation Loss:  1.10817599\n",
      "Final Training Loss:  1.10817587\n",
      "Final Validation Loss:  1.10817599\n",
      "\n",
      "Running model (trial=1, mod=137, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  2265.5\n",
      "Validation Loss:  2265.5\n",
      "Final Training Loss:  2265.5\n",
      "Final Validation Loss:  2265.5\n",
      "\n",
      "Running model (trial=1, mod=138, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72672051\n",
      "Validation Loss:  0.72672051\n",
      "Final Training Loss:  0.72672051\n",
      "Final Validation Loss:  0.72672051\n",
      "\n",
      "Running model (trial=1, mod=139, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72336733\n",
      "Validation Loss:  0.72336733\n",
      "Final Training Loss:  0.72336733\n",
      "Final Validation Loss:  0.72336733\n",
      "\n",
      "Running model (trial=1, mod=140, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7129398\n",
      "Validation Loss:  0.7129398\n",
      "Final Training Loss:  0.7129398\n",
      "Final Validation Loss:  0.7129398\n",
      "\n",
      "Running model (trial=1, mod=141, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.9480896\n",
      "Validation Loss:  500.9480896\n",
      "Final Training Loss:  500.9480896\n",
      "Final Validation Loss:  500.9480896\n",
      "\n",
      "Running model (trial=1, mod=142, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00109863\n",
      "Validation Loss:  501.00109863\n",
      "Final Training Loss:  501.00109863\n",
      "Final Validation Loss:  501.00109863\n",
      "\n",
      "Running model (trial=1, mod=143, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00161743\n",
      "Validation Loss:  501.00161743\n",
      "Final Training Loss:  501.00161743\n",
      "Final Validation Loss:  501.00161743\n",
      "\n",
      "Running model (trial=2, mod=144, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.74199474\n",
      "Validation Loss:  0.74199474\n",
      "Final Training Loss:  0.74199474\n",
      "Final Validation Loss:  0.74199474\n",
      "\n",
      "Running model (trial=2, mod=145, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73807496\n",
      "Validation Loss:  0.73807496\n",
      "Final Training Loss:  0.73807496\n",
      "Final Validation Loss:  0.73807496\n",
      "\n",
      "Running model (trial=2, mod=146, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.6915217\n",
      "Validation Loss:  0.6915217\n",
      "Final Training Loss:  0.6915217\n",
      "Final Validation Loss:  0.6915217\n",
      "\n",
      "Running model (trial=2, mod=147, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62526762\n",
      "Validation Loss:  0.62526768\n",
      "Final Training Loss:  0.62526762\n",
      "Final Validation Loss:  0.62526768\n",
      "\n",
      "Running model (trial=2, mod=148, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66763532\n",
      "Validation Loss:  0.66763526\n",
      "Final Training Loss:  0.66763532\n",
      "Final Validation Loss:  0.66763526\n",
      "\n",
      "Running model (trial=2, mod=149, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.74436784\n",
      "Validation Loss:  0.74436784\n",
      "Final Training Loss:  0.74436784\n",
      "Final Validation Loss:  0.74436784\n",
      "\n",
      "Running model (trial=2, mod=150, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60178733\n",
      "Validation Loss:  0.60178733\n",
      "Final Training Loss:  0.60178733\n",
      "Final Validation Loss:  0.60178733\n",
      "\n",
      "Running model (trial=2, mod=151, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73070443\n",
      "Validation Loss:  0.73070443\n",
      "Final Training Loss:  0.73070443\n",
      "Final Validation Loss:  0.73070443\n",
      "\n",
      "Running model (trial=2, mod=152, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60503751\n",
      "Validation Loss:  0.60503757\n",
      "Final Training Loss:  0.60503751\n",
      "Final Validation Loss:  0.60503757\n",
      "\n",
      "Running model (trial=2, mod=153, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00973511\n",
      "Validation Loss:  501.00973511\n",
      "Final Training Loss:  501.00973511\n",
      "Final Validation Loss:  501.00973511\n",
      "\n",
      "Running model (trial=2, mod=154, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00198364\n",
      "Validation Loss:  501.00198364\n",
      "Final Training Loss:  501.00198364\n",
      "Final Validation Loss:  501.00198364\n",
      "\n",
      "Running model (trial=2, mod=155, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00262451\n",
      "Validation Loss:  501.00262451\n",
      "Final Training Loss:  501.00262451\n",
      "Final Validation Loss:  501.00262451\n",
      "\n",
      "Running model (trial=2, mod=156, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71931934\n",
      "Validation Loss:  0.71931928\n",
      "Final Training Loss:  0.71931934\n",
      "Final Validation Loss:  0.71931928\n",
      "\n",
      "Running model (trial=2, mod=157, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72030151\n",
      "Validation Loss:  0.72030151\n",
      "Final Training Loss:  0.72030151\n",
      "Final Validation Loss:  0.72030151\n",
      "\n",
      "Running model (trial=2, mod=158, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60002482\n",
      "Validation Loss:  0.60002482\n",
      "Final Training Loss:  0.60002482\n",
      "Final Validation Loss:  0.60002482\n",
      "\n",
      "Running model (trial=2, mod=159, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.76026887\n",
      "Validation Loss:  0.76026887\n",
      "Final Training Loss:  0.76026887\n",
      "Final Validation Loss:  0.76026887\n",
      "\n",
      "Running model (trial=2, mod=160, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62825972\n",
      "Validation Loss:  0.62825972\n",
      "Final Training Loss:  0.62825972\n",
      "Final Validation Loss:  0.62825972\n",
      "\n",
      "Running model (trial=2, mod=161, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  2721572.25\n",
      "Validation Loss:  2721572.5\n",
      "Final Training Loss:  2721572.25\n",
      "Final Validation Loss:  2721572.5\n",
      "\n",
      "Running model (trial=2, mod=162, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.74864417\n",
      "Validation Loss:  0.74864423\n",
      "Final Training Loss:  0.74864417\n",
      "Final Validation Loss:  0.74864423\n",
      "\n",
      "Running model (trial=2, mod=163, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71118593\n",
      "Validation Loss:  0.71118593\n",
      "Final Training Loss:  0.71118593\n",
      "Final Validation Loss:  0.71118593\n",
      "\n",
      "Running model (trial=2, mod=164, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.59736544\n",
      "Validation Loss:  0.59736544\n",
      "Final Training Loss:  0.59736544\n",
      "Final Validation Loss:  0.59736544\n",
      "\n",
      "Running model (trial=2, mod=165, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00064087\n",
      "Validation Loss:  501.00064087\n",
      "Final Training Loss:  501.00064087\n",
      "Final Validation Loss:  501.00064087\n",
      "\n",
      "Running model (trial=2, mod=166, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00125122\n",
      "Validation Loss:  501.00125122\n",
      "Final Training Loss:  501.00125122\n",
      "Final Validation Loss:  501.00125122\n",
      "\n",
      "Running model (trial=2, mod=167, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00189209\n",
      "Validation Loss:  501.00189209\n",
      "Final Training Loss:  501.00189209\n",
      "Final Validation Loss:  501.00189209\n",
      "\n",
      "Running model (trial=2, mod=168, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72369909\n",
      "Validation Loss:  0.72369909\n",
      "Final Training Loss:  0.72369909\n",
      "Final Validation Loss:  0.72369909\n",
      "\n",
      "Running model (trial=2, mod=169, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.7109924\n",
      "Validation Loss:  0.71099246\n",
      "Final Training Loss:  0.7109924\n",
      "Final Validation Loss:  0.71099246\n",
      "\n",
      "Running model (trial=2, mod=170, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62358981\n",
      "Validation Loss:  0.62358981\n",
      "Final Training Loss:  0.62358981\n",
      "Final Validation Loss:  0.62358981\n",
      "\n",
      "Running model (trial=2, mod=171, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  51.92425156\n",
      "Validation Loss:  51.92425156\n",
      "Final Training Loss:  51.92425156\n",
      "Final Validation Loss:  51.92425156\n",
      "\n",
      "Running model (trial=2, mod=172, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1.23481655\n",
      "Validation Loss:  1.23481667\n",
      "Final Training Loss:  1.23481655\n",
      "Final Validation Loss:  1.23481667\n",
      "\n",
      "Running model (trial=2, mod=173, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  63550.515625\n",
      "Validation Loss:  63550.515625\n",
      "Final Training Loss:  63550.515625\n",
      "Final Validation Loss:  63550.515625\n",
      "\n",
      "Running model (trial=2, mod=174, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73014182\n",
      "Validation Loss:  0.73014182\n",
      "Final Training Loss:  0.73014182\n",
      "Final Validation Loss:  0.73014182\n",
      "\n",
      "Running model (trial=2, mod=175, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72272301\n",
      "Validation Loss:  0.72272301\n",
      "Final Training Loss:  0.72272301\n",
      "Final Validation Loss:  0.72272301\n",
      "\n",
      "Running model (trial=2, mod=176, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60508198\n",
      "Validation Loss:  0.60508186\n",
      "Final Training Loss:  0.60508198\n",
      "Final Validation Loss:  0.60508186\n",
      "\n",
      "Running model (trial=2, mod=177, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00091553\n",
      "Validation Loss:  501.00091553\n",
      "Final Training Loss:  501.00091553\n",
      "Final Validation Loss:  501.00091553\n",
      "\n",
      "Running model (trial=2, mod=178, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00268555\n",
      "Validation Loss:  501.00268555\n",
      "Final Training Loss:  501.00268555\n",
      "Final Validation Loss:  501.00268555\n",
      "\n",
      "Running model (trial=2, mod=179, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.0007019\n",
      "Validation Loss:  501.0007019\n",
      "Final Training Loss:  501.0007019\n",
      "Final Validation Loss:  501.0007019\n",
      "\n",
      "Running model (trial=2, mod=180, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71741641\n",
      "Validation Loss:  0.71741641\n",
      "Final Training Loss:  0.71741641\n",
      "Final Validation Loss:  0.71741641\n",
      "\n",
      "Running model (trial=2, mod=181, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73475605\n",
      "Validation Loss:  0.73475605\n",
      "Final Training Loss:  0.73475605\n",
      "Final Validation Loss:  0.73475605\n",
      "\n",
      "Running model (trial=2, mod=182, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72716171\n",
      "Validation Loss:  0.72716165\n",
      "Final Training Loss:  0.72716171\n",
      "Final Validation Loss:  0.72716165\n",
      "\n",
      "Running model (trial=2, mod=183, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.66300231\n",
      "Validation Loss:  0.66300231\n",
      "Final Training Loss:  0.66300231\n",
      "Final Validation Loss:  0.66300231\n",
      "\n",
      "Running model (trial=2, mod=184, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73985481\n",
      "Validation Loss:  0.73985487\n",
      "Final Training Loss:  0.73985481\n",
      "Final Validation Loss:  0.73985487\n",
      "\n",
      "Running model (trial=2, mod=185, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  3637.24316406\n",
      "Validation Loss:  3637.24316406\n",
      "Final Training Loss:  3637.24316406\n",
      "Final Validation Loss:  3637.24316406\n",
      "\n",
      "Running model (trial=2, mod=186, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71763253\n",
      "Validation Loss:  0.71763259\n",
      "Final Training Loss:  0.71763253\n",
      "Final Validation Loss:  0.71763259\n",
      "\n",
      "Running model (trial=2, mod=187, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73410434\n",
      "Validation Loss:  0.73410439\n",
      "Final Training Loss:  0.73410434\n",
      "Final Validation Loss:  0.73410439\n",
      "\n",
      "Running model (trial=2, mod=188, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71361256\n",
      "Validation Loss:  0.71361256\n",
      "Final Training Loss:  0.71361256\n",
      "Final Validation Loss:  0.71361256\n",
      "\n",
      "Running model (trial=2, mod=189, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.92779541\n",
      "Validation Loss:  500.92779541\n",
      "Final Training Loss:  500.92779541\n",
      "Final Validation Loss:  500.92779541\n",
      "\n",
      "Running model (trial=2, mod=190, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00009155\n",
      "Validation Loss:  501.00009155\n",
      "Final Training Loss:  501.00009155\n",
      "Final Validation Loss:  501.00009155\n",
      "\n",
      "Running model (trial=2, mod=191, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00106812\n",
      "Validation Loss:  501.00106812\n",
      "Final Training Loss:  501.00106812\n",
      "Final Validation Loss:  501.00106812\n",
      "\n",
      "Running model (trial=2, mod=192, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72751319\n",
      "Validation Loss:  0.72751319\n",
      "Final Training Loss:  0.72751319\n",
      "Final Validation Loss:  0.72751319\n",
      "\n",
      "Running model (trial=2, mod=193, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73292488\n",
      "Validation Loss:  0.732925\n",
      "Final Training Loss:  0.73292488\n",
      "Final Validation Loss:  0.732925\n",
      "\n",
      "Running model (trial=2, mod=194, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72842491\n",
      "Validation Loss:  0.72842497\n",
      "Final Training Loss:  0.72842491\n",
      "Final Validation Loss:  0.72842497\n",
      "\n",
      "Running model (trial=2, mod=195, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73365909\n",
      "Validation Loss:  0.73365915\n",
      "Final Training Loss:  0.73365909\n",
      "Final Validation Loss:  0.73365915\n",
      "\n",
      "Running model (trial=2, mod=196, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  110.14409637\n",
      "Validation Loss:  110.14409637\n",
      "Final Training Loss:  110.14409637\n",
      "Final Validation Loss:  110.14409637\n",
      "\n",
      "Running model (trial=2, mod=197, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  4025.9375\n",
      "Validation Loss:  4025.9375\n",
      "Final Training Loss:  4025.9375\n",
      "Final Validation Loss:  4025.9375\n",
      "\n",
      "Running model (trial=2, mod=198, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72624797\n",
      "Validation Loss:  0.72624791\n",
      "Final Training Loss:  0.72624797\n",
      "Final Validation Loss:  0.72624791\n",
      "\n",
      "Running model (trial=2, mod=199, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72768307\n",
      "Validation Loss:  0.72768307\n",
      "Final Training Loss:  0.72768307\n",
      "Final Validation Loss:  0.72768307\n",
      "\n",
      "Running model (trial=2, mod=200, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71942246\n",
      "Validation Loss:  0.71942246\n",
      "Final Training Loss:  0.71942246\n",
      "Final Validation Loss:  0.71942246\n",
      "\n",
      "Running model (trial=2, mod=201, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  500.95291138\n",
      "Validation Loss:  500.95291138\n",
      "Final Training Loss:  500.95291138\n",
      "Final Validation Loss:  500.95291138\n",
      "\n",
      "Running model (trial=2, mod=202, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00201416\n",
      "Validation Loss:  501.00201416\n",
      "Final Training Loss:  501.00201416\n",
      "Final Validation Loss:  501.00201416\n",
      "\n",
      "Running model (trial=2, mod=203, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00201416\n",
      "Validation Loss:  501.00201416\n",
      "Final Training Loss:  501.00201416\n",
      "Final Validation Loss:  501.00201416\n",
      "\n",
      "Running model (trial=2, mod=204, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73149472\n",
      "Validation Loss:  0.73149472\n",
      "Final Training Loss:  0.73149472\n",
      "Final Validation Loss:  0.73149472\n",
      "\n",
      "Running model (trial=2, mod=205, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72335434\n",
      "Validation Loss:  0.72335428\n",
      "Final Training Loss:  0.72335434\n",
      "Final Validation Loss:  0.72335428\n",
      "\n",
      "Running model (trial=2, mod=206, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72542524\n",
      "Validation Loss:  0.72542524\n",
      "Final Training Loss:  0.72542524\n",
      "Final Validation Loss:  0.72542524\n",
      "\n",
      "Running model (trial=2, mod=207, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.73303348\n",
      "Validation Loss:  0.73303342\n",
      "Final Training Loss:  0.73303348\n",
      "Final Validation Loss:  0.73303342\n",
      "\n",
      "Running model (trial=2, mod=208, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.76293594\n",
      "Validation Loss:  0.76293594\n",
      "Final Training Loss:  0.76293594\n",
      "Final Validation Loss:  0.76293594\n",
      "\n",
      "Running model (trial=2, mod=209, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  9139.203125\n",
      "Validation Loss:  9139.203125\n",
      "Final Training Loss:  9139.203125\n",
      "Final Validation Loss:  9139.203125\n",
      "\n",
      "Running model (trial=2, mod=210, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72947788\n",
      "Validation Loss:  0.72947776\n",
      "Final Training Loss:  0.72947788\n",
      "Final Validation Loss:  0.72947776\n",
      "\n",
      "Running model (trial=2, mod=211, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.72448999\n",
      "Validation Loss:  0.72449005\n",
      "Final Training Loss:  0.72448999\n",
      "Final Validation Loss:  0.72449005\n",
      "\n",
      "Running model (trial=2, mod=212, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.71760583\n",
      "Validation Loss:  0.71760583\n",
      "Final Training Loss:  0.71760583\n",
      "Final Validation Loss:  0.71760583\n",
      "\n",
      "Running model (trial=2, mod=213, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.60729742\n",
      "Validation Loss:  0.60729742\n",
      "Final Training Loss:  0.60729742\n",
      "Final Validation Loss:  0.60729742\n",
      "\n",
      "Running model (trial=2, mod=214, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00231934\n",
      "Validation Loss:  501.00231934\n",
      "Final Training Loss:  501.00231934\n",
      "Final Validation Loss:  501.00231934\n",
      "\n",
      "Running model (trial=2, mod=215, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  501.00189209\n",
      "Validation Loss:  501.00189209\n",
      "Final Training Loss:  501.00189209\n",
      "Final Validation Loss:  501.00189209\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=3\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [],
   "source": [
    "# plotting\n",
    "x_train_ = x_train.detach()\n",
    "x_sorted, indices = torch.sort(x_train_, dim=0)\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": x_sorted,\n",
    "    \"x_train\": x_sorted,\n",
    "    \"y_train\": c2.ksi(x_sorted),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\xi(t)$\",\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    plot_name=SET_NAME,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models)\n",
    "model_type = np.vectorize(lambda model: \"ResNET\" if isinstance(model,ResNET) else \"FFNN\")(models)\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models)\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models)\n",
    "loss_array = np.vectorize(lambda model: loss_func(model, x_train, q_train).detach())(models)\n",
    "loss_array -= c2.DIST_R_Q\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'c' argument must be a color, a sequence of colors, or a sequence of numbers, not [['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']]",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001B[0m in \u001B[0;36m_parse_scatter_color_args\u001B[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001B[0m\n\u001B[1;32m   4328\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Is 'c' acceptable as PathCollection facecolors?\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4329\u001B[0;31m                 \u001B[0mcolors\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmcolors\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_rgba_array\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   4330\u001B[0m             \u001B[0;32mexcept\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mTypeError\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0merr\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/colors.py\u001B[0m in \u001B[0;36mto_rgba_array\u001B[0;34m(c, alpha)\u001B[0m\n\u001B[1;32m    384\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 385\u001B[0;31m         \u001B[0mrgba\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mto_rgba\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcc\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mc\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    386\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/colors.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    384\u001B[0m     \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 385\u001B[0;31m         \u001B[0mrgba\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mto_rgba\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcc\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mcc\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mc\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    386\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/colors.py\u001B[0m in \u001B[0;36mto_rgba\u001B[0;34m(c, alpha)\u001B[0m\n\u001B[1;32m    205\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mrgba\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m  \u001B[0;31m# Suppress exception chaining of cache lookup failure.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 206\u001B[0;31m         \u001B[0mrgba\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_to_rgba_no_colorcycle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0malpha\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    207\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/colors.py\u001B[0m in \u001B[0;36m_to_rgba_no_colorcycle\u001B[0;34m(c, alpha)\u001B[0m\n\u001B[1;32m    285\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mlen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mc\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32min\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m4\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 286\u001B[0;31m         \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"RGBA sequence should have length 3 or 4\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    287\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mall\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mNumber\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mc\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: RGBA sequence should have length 3 or 4",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-159-1cefad439eb8>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0mfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0max\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msubplots\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 2\u001B[0;31m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mscatter\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mlayers\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mloss_array\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mc\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mmodel_type\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      3\u001B[0m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_yscale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'log'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_xscale\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'log'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0max\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_ylabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Error\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/__init__.py\u001B[0m in \u001B[0;36minner\u001B[0;34m(ax, data, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1350\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0minner\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0max\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1351\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mdata\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1352\u001B[0;31m             \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0max\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mmap\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msanitize_sequence\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0margs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1353\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1354\u001B[0m         \u001B[0mbound\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnew_sig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mbind\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0max\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001B[0m in \u001B[0;36mscatter\u001B[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001B[0m\n\u001B[1;32m   4493\u001B[0m             \u001B[0morig_edgecolor\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'edgecolor'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4494\u001B[0m         \u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcolors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0medgecolors\u001B[0m \u001B[0;34m=\u001B[0m\u001B[0;31m \u001B[0m\u001B[0;31m\\\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4495\u001B[0;31m             self._parse_scatter_color_args(\n\u001B[0m\u001B[1;32m   4496\u001B[0m                 \u001B[0mc\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0medgecolors\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mkwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mx\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4497\u001B[0m                 get_next_color_func=self._get_patches_for_fill.get_next_color)\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/axes/_axes.py\u001B[0m in \u001B[0;36m_parse_scatter_color_args\u001B[0;34m(c, edgecolors, kwargs, xsize, get_next_color_func)\u001B[0m\n\u001B[1;32m   4336\u001B[0m                     \u001B[0;31m# Both the mapping *and* the RGBA conversion failed: pretty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4337\u001B[0m                     \u001B[0;31m# severe failure => one may appreciate a verbose feedback.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 4338\u001B[0;31m                     raise ValueError(\n\u001B[0m\u001B[1;32m   4339\u001B[0m                         \u001B[0;34mf\"'c' argument must be a color, a sequence of colors, \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   4340\u001B[0m                         f\"or a sequence of numbers, not {c}\") from err\n",
      "\u001B[0;31mValueError\u001B[0m: 'c' argument must be a color, a sequence of colors, or a sequence of numbers, not [['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['ResNET']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']\n ['FFNN']]"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(layers,loss_array, c = model_type)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"Layers\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(parameters,loss_array)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"Number of parameters\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(neurons,loss_array)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"Neurons per layer\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fmri = sns.load_dataset(\"fmri\")\n",
    "sns.relplot(x=parameters.ravel(), y=loss_array.ravel(), kind=\"line\");\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}