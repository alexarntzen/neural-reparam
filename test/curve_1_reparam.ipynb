{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Test reparam of curves that are not equivqalent \"\"\"\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "# torch.set_default_dtype(torch.float64)\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from deepthermal.FFNN_model import fit_FFNN, FFNN, init_xavier\n",
    "from deepthermal.validation import create_subdictionary_iterator, k_fold_cv_grid, add_dictionary_iterators\n",
    "\n",
    "from deepthermal.plotting import plot_result, plot_model_1d\n",
    "\n",
    "from deep_reparametrization.plotting import plot_reparametrization\n",
    "from deep_reparametrization.reparametrization import (\n",
    "    get_elastic_metric_loss,\n",
    "    compute_loss_reparam,\n",
    "    get_elastic_error_func,\n",
    ")\n",
    "from deep_reparametrization.ResNET import ResNET\n",
    "import test.curves as c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "########\n",
    "PATH_FIGURES = \"../figures/curve_1\"\n",
    "########\n",
    "\n",
    "SET_NAME = \"curve_1_exp_1\"\n",
    "\n",
    "FOLDS = 1\n",
    "N = 128  # training points internal\n",
    "\n",
    "loss_func = get_elastic_metric_loss(r=c1.r, constrain_cost=1e3, verbose=False)\n",
    "MODEL_PARAMS = {\n",
    "    \"model\": [ResNET, FFNN],\n",
    "    \"input_dimension\": [1],\n",
    "    \"output_dimension\": [1],\n",
    "    \"activation\": [\"tanh\"],\n",
    "    \"n_hidden_layers\": [1, 2, 4, 16, 32, 64],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "MODEL_PARAMS_EXPERIMENT = {\n",
    "    \"neurons\": [4, 8, 16, 32, 64, 128],\n",
    "}\n",
    "TRAINING_PARAMS = {\n",
    "    \"batch_size\": [N],\n",
    "    \"regularization_param\": [1e-8],\n",
    "    \"compute_loss\": [compute_loss_reparam],\n",
    "    \"loss_func\": [loss_func],\n",
    "}\n",
    "# extend the previous dict with the zip of this\n",
    "TRAINING_PARAMS_EXPERIMENT = {\n",
    "    \"optimizer\": [\"strong_wolfe\"],\n",
    "    \"num_epochs\": [1],\n",
    "    \"learning_rate\": [ 0.01],\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Load data\n",
    "x_train = torch.linspace(0, 1, N, requires_grad=True).unsqueeze(1)\n",
    "q_train = c1.q(x_train.detach())\n",
    "\n",
    "\n",
    "data = TensorDataset(x_train, q_train)\n",
    "\n",
    "model_params_iter = create_subdictionary_iterator(MODEL_PARAMS)\n",
    "model_exp_iter = create_subdictionary_iterator(MODEL_PARAMS_EXPERIMENT, product=False)\n",
    "exp_model_params_iter = add_dictionary_iterators(model_exp_iter, model_params_iter)\n",
    "\n",
    "training_params_iter = create_subdictionary_iterator(TRAINING_PARAMS)\n",
    "training_exp_iter = create_subdictionary_iterator(TRAINING_PARAMS_EXPERIMENT, product=False)\n",
    "exp_training_params_iter = add_dictionary_iterators(training_exp_iter, training_params_iter)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Do the actual training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running model (trial=0, mod=0, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00717711\n",
      "Validation Loss:  0.00717711\n",
      "Final Training Loss:  0.00717711\n",
      "Final Validation Loss:  0.00717711\n",
      "\n",
      "Running model (trial=0, mod=1, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01159034\n",
      "Validation Loss:  0.01159034\n",
      "Final Training Loss:  0.01159034\n",
      "Final Validation Loss:  0.01159034\n",
      "\n",
      "Running model (trial=0, mod=2, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00587765\n",
      "Validation Loss:  0.00587765\n",
      "Final Training Loss:  0.00587765\n",
      "Final Validation Loss:  0.00587765\n",
      "\n",
      "Running model (trial=0, mod=3, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  91.77373505\n",
      "Validation Loss:  91.77371979\n",
      "Final Training Loss:  91.77373505\n",
      "Final Validation Loss:  91.77371979\n",
      "\n",
      "Running model (trial=0, mod=4, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  92.72607422\n",
      "Validation Loss:  92.72605896\n",
      "Final Training Loss:  92.72607422\n",
      "Final Validation Loss:  92.72605896\n",
      "\n",
      "Running model (trial=0, mod=5, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  18.0139389\n",
      "Validation Loss:  18.013937\n",
      "Final Training Loss:  18.0139389\n",
      "Final Validation Loss:  18.013937\n",
      "\n",
      "Running model (trial=0, mod=6, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07535115\n",
      "Validation Loss:  0.07535114\n",
      "Final Training Loss:  0.07535115\n",
      "Final Validation Loss:  0.07535114\n",
      "\n",
      "Running model (trial=0, mod=7, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00599206\n",
      "Validation Loss:  0.00599206\n",
      "Final Training Loss:  0.00599206\n",
      "Final Validation Loss:  0.00599206\n",
      "\n",
      "Running model (trial=0, mod=8, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03278619\n",
      "Validation Loss:  0.03278619\n",
      "Final Training Loss:  0.03278619\n",
      "Final Validation Loss:  0.03278619\n",
      "\n",
      "Running model (trial=0, mod=9, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10437012\n",
      "Validation Loss:  600.10437012\n",
      "Final Training Loss:  600.10437012\n",
      "Final Validation Loss:  600.10437012\n",
      "\n",
      "Running model (trial=0, mod=10, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10449219\n",
      "Validation Loss:  600.10449219\n",
      "Final Training Loss:  600.10449219\n",
      "Final Validation Loss:  600.10449219\n",
      "\n",
      "Running model (trial=0, mod=11, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10394287\n",
      "Validation Loss:  600.10394287\n",
      "Final Training Loss:  600.10394287\n",
      "Final Validation Loss:  600.10394287\n",
      "\n",
      "Running model (trial=0, mod=12, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.19858353\n",
      "Validation Loss:  0.19858353\n",
      "Final Training Loss:  0.19858353\n",
      "Final Validation Loss:  0.19858353\n",
      "\n",
      "Running model (trial=0, mod=13, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0264829\n",
      "Validation Loss:  0.0264829\n",
      "Final Training Loss:  0.0264829\n",
      "Final Validation Loss:  0.0264829\n",
      "\n",
      "Running model (trial=0, mod=14, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.13548797\n",
      "Validation Loss:  0.13548797\n",
      "Final Training Loss:  0.13548797\n",
      "Final Validation Loss:  0.13548797\n",
      "\n",
      "Running model (trial=0, mod=15, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0537931\n",
      "Validation Loss:  0.0537931\n",
      "Final Training Loss:  0.0537931\n",
      "Final Validation Loss:  0.0537931\n",
      "\n",
      "Running model (trial=0, mod=16, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  106.18667603\n",
      "Validation Loss:  106.18668365\n",
      "Final Training Loss:  106.18667603\n",
      "Final Validation Loss:  106.18668365\n",
      "\n",
      "Running model (trial=0, mod=17, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  149.53744507\n",
      "Validation Loss:  149.53744507\n",
      "Final Training Loss:  149.53744507\n",
      "Final Validation Loss:  149.53744507\n",
      "\n",
      "Running model (trial=0, mod=18, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01143382\n",
      "Validation Loss:  0.01143382\n",
      "Final Training Loss:  0.01143382\n",
      "Final Validation Loss:  0.01143382\n",
      "\n",
      "Running model (trial=0, mod=19, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02667802\n",
      "Validation Loss:  0.02667803\n",
      "Final Training Loss:  0.02667802\n",
      "Final Validation Loss:  0.02667803\n",
      "\n",
      "Running model (trial=0, mod=20, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05268594\n",
      "Validation Loss:  0.05268594\n",
      "Final Training Loss:  0.05268594\n",
      "Final Validation Loss:  0.05268594\n",
      "\n",
      "Running model (trial=0, mod=21, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.09851074\n",
      "Validation Loss:  600.09851074\n",
      "Final Training Loss:  600.09851074\n",
      "Final Validation Loss:  600.09851074\n",
      "\n",
      "Running model (trial=0, mod=22, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1026001\n",
      "Validation Loss:  600.1026001\n",
      "Final Training Loss:  600.1026001\n",
      "Final Validation Loss:  600.1026001\n",
      "\n",
      "Running model (trial=0, mod=23, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10412598\n",
      "Validation Loss:  600.10412598\n",
      "Final Training Loss:  600.10412598\n",
      "Final Validation Loss:  600.10412598\n",
      "\n",
      "Running model (trial=0, mod=24, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06299898\n",
      "Validation Loss:  0.06299898\n",
      "Final Training Loss:  0.06299898\n",
      "Final Validation Loss:  0.06299898\n",
      "\n",
      "Running model (trial=0, mod=25, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05253856\n",
      "Validation Loss:  0.05253854\n",
      "Final Training Loss:  0.05253856\n",
      "Final Validation Loss:  0.05253854\n",
      "\n",
      "Running model (trial=0, mod=26, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05997768\n",
      "Validation Loss:  0.05997768\n",
      "Final Training Loss:  0.05997768\n",
      "Final Validation Loss:  0.05997768\n",
      "\n",
      "Running model (trial=0, mod=27, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01415738\n",
      "Validation Loss:  0.01415738\n",
      "Final Training Loss:  0.01415738\n",
      "Final Validation Loss:  0.01415738\n",
      "\n",
      "Running model (trial=0, mod=28, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  392.52282715\n",
      "Validation Loss:  392.52282715\n",
      "Final Training Loss:  392.52282715\n",
      "Final Validation Loss:  392.52282715\n",
      "\n",
      "Running model (trial=0, mod=29, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  416.46789551\n",
      "Validation Loss:  416.46789551\n",
      "Final Training Loss:  416.46789551\n",
      "Final Validation Loss:  416.46789551\n",
      "\n",
      "Running model (trial=0, mod=30, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  6.68900633\n",
      "Validation Loss:  6.68900585\n",
      "Final Training Loss:  6.68900633\n",
      "Final Validation Loss:  6.68900585\n",
      "\n",
      "Running model (trial=0, mod=31, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00728358\n",
      "Validation Loss:  0.00728357\n",
      "Final Training Loss:  0.00728358\n",
      "Final Validation Loss:  0.00728357\n",
      "\n",
      "Running model (trial=0, mod=32, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01930797\n",
      "Validation Loss:  0.01930796\n",
      "Final Training Loss:  0.01930797\n",
      "Final Validation Loss:  0.01930796\n",
      "\n",
      "Running model (trial=0, mod=33, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.05981445\n",
      "Validation Loss:  600.05981445\n",
      "Final Training Loss:  600.05981445\n",
      "Final Validation Loss:  600.05981445\n",
      "\n",
      "Running model (trial=0, mod=34, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10107422\n",
      "Validation Loss:  600.10107422\n",
      "Final Training Loss:  600.10107422\n",
      "Final Validation Loss:  600.10107422\n",
      "\n",
      "Running model (trial=0, mod=35, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10540771\n",
      "Validation Loss:  600.10540771\n",
      "Final Training Loss:  600.10540771\n",
      "Final Validation Loss:  600.10540771\n",
      "\n",
      "Running model (trial=0, mod=36, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11554798\n",
      "Validation Loss:  0.11554798\n",
      "Final Training Loss:  0.11554798\n",
      "Final Validation Loss:  0.11554798\n",
      "\n",
      "Running model (trial=0, mod=37, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  90.0568161\n",
      "Validation Loss:  90.05682373\n",
      "Final Training Loss:  90.0568161\n",
      "Final Validation Loss:  90.05682373\n",
      "\n",
      "Running model (trial=0, mod=38, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01632105\n",
      "Validation Loss:  0.01632105\n",
      "Final Training Loss:  0.01632105\n",
      "Final Validation Loss:  0.01632105\n",
      "\n",
      "Running model (trial=0, mod=39, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00564418\n",
      "Validation Loss:  0.00564418\n",
      "Final Training Loss:  0.00564418\n",
      "Final Validation Loss:  0.00564418\n",
      "\n",
      "Running model (trial=0, mod=40, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  166.34403992\n",
      "Validation Loss:  166.34403992\n",
      "Final Training Loss:  166.34403992\n",
      "Final Validation Loss:  166.34403992\n",
      "\n",
      "Running model (trial=0, mod=41, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  5061.75585938\n",
      "Validation Loss:  5061.75585938\n",
      "Final Training Loss:  5061.75585938\n",
      "Final Validation Loss:  5061.75585938\n",
      "\n",
      "Running model (trial=0, mod=42, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03350904\n",
      "Validation Loss:  0.03350904\n",
      "Final Training Loss:  0.03350904\n",
      "Final Validation Loss:  0.03350904\n",
      "\n",
      "Running model (trial=0, mod=43, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01689725\n",
      "Validation Loss:  0.01689726\n",
      "Final Training Loss:  0.01689725\n",
      "Final Validation Loss:  0.01689726\n",
      "\n",
      "Running model (trial=0, mod=44, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02546747\n",
      "Validation Loss:  0.02546747\n",
      "Final Training Loss:  0.02546747\n",
      "Final Validation Loss:  0.02546747\n",
      "\n",
      "Running model (trial=0, mod=45, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01132415\n",
      "Validation Loss:  0.01132415\n",
      "Final Training Loss:  0.01132415\n",
      "Final Validation Loss:  0.01132415\n",
      "\n",
      "Running model (trial=0, mod=46, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10449219\n",
      "Validation Loss:  600.10449219\n",
      "Final Training Loss:  600.10449219\n",
      "Final Validation Loss:  600.10449219\n",
      "\n",
      "Running model (trial=0, mod=47, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10284424\n",
      "Validation Loss:  600.10284424\n",
      "Final Training Loss:  600.10284424\n",
      "Final Validation Loss:  600.10284424\n",
      "\n",
      "Running model (trial=0, mod=48, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  90.4750824\n",
      "Validation Loss:  90.47507477\n",
      "Final Training Loss:  90.4750824\n",
      "Final Validation Loss:  90.47507477\n",
      "\n",
      "Running model (trial=0, mod=49, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01159301\n",
      "Validation Loss:  0.01159301\n",
      "Final Training Loss:  0.01159301\n",
      "Final Validation Loss:  0.01159301\n",
      "\n",
      "Running model (trial=0, mod=50, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00684018\n",
      "Validation Loss:  0.00684018\n",
      "Final Training Loss:  0.00684018\n",
      "Final Validation Loss:  0.00684018\n",
      "\n",
      "Running model (trial=0, mod=51, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02370575\n",
      "Validation Loss:  0.02370575\n",
      "Final Training Loss:  0.02370575\n",
      "Final Validation Loss:  0.02370575\n",
      "\n",
      "Running model (trial=0, mod=52, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  290.33056641\n",
      "Validation Loss:  290.33056641\n",
      "Final Training Loss:  290.33056641\n",
      "Final Validation Loss:  290.33056641\n",
      "\n",
      "Running model (trial=0, mod=53, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  5617.41015625\n",
      "Validation Loss:  5617.41015625\n",
      "Final Training Loss:  5617.41015625\n",
      "Final Validation Loss:  5617.41015625\n",
      "\n",
      "Running model (trial=0, mod=54, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06914448\n",
      "Validation Loss:  0.06914447\n",
      "Final Training Loss:  0.06914448\n",
      "Final Validation Loss:  0.06914447\n",
      "\n",
      "Running model (trial=0, mod=55, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00329856\n",
      "Validation Loss:  0.00329856\n",
      "Final Training Loss:  0.00329856\n",
      "Final Validation Loss:  0.00329856\n",
      "\n",
      "Running model (trial=0, mod=56, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00235195\n",
      "Validation Loss:  0.00235195\n",
      "Final Training Loss:  0.00235195\n",
      "Final Validation Loss:  0.00235195\n",
      "\n",
      "Running model (trial=0, mod=57, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  90.84541321\n",
      "Validation Loss:  90.84541321\n",
      "Final Training Loss:  90.84541321\n",
      "Final Validation Loss:  90.84541321\n",
      "\n",
      "Running model (trial=0, mod=58, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10345459\n",
      "Validation Loss:  600.10345459\n",
      "Final Training Loss:  600.10345459\n",
      "Final Validation Loss:  600.10345459\n",
      "\n",
      "Running model (trial=0, mod=59, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10284424\n",
      "Validation Loss:  600.10284424\n",
      "Final Training Loss:  600.10284424\n",
      "Final Validation Loss:  600.10284424\n",
      "\n",
      "Running model (trial=0, mod=60, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  35.42670441\n",
      "Validation Loss:  35.42670441\n",
      "Final Training Loss:  35.42670441\n",
      "Final Validation Loss:  35.42670441\n",
      "\n",
      "Running model (trial=0, mod=61, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00454168\n",
      "Validation Loss:  0.00454168\n",
      "Final Training Loss:  0.00454168\n",
      "Final Validation Loss:  0.00454168\n",
      "\n",
      "Running model (trial=0, mod=62, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  87.80000305\n",
      "Validation Loss:  87.80000305\n",
      "Final Training Loss:  87.80000305\n",
      "Final Validation Loss:  87.80000305\n",
      "\n",
      "Running model (trial=0, mod=63, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07982993\n",
      "Validation Loss:  0.07982994\n",
      "Final Training Loss:  0.07982993\n",
      "Final Validation Loss:  0.07982994\n",
      "\n",
      "Running model (trial=0, mod=64, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  322.15112305\n",
      "Validation Loss:  322.15112305\n",
      "Final Training Loss:  322.15112305\n",
      "Final Validation Loss:  322.15112305\n",
      "\n",
      "Running model (trial=0, mod=65, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  21989.94726562\n",
      "Validation Loss:  21989.94726562\n",
      "Final Training Loss:  21989.94726562\n",
      "Final Validation Loss:  21989.94726562\n",
      "\n",
      "Running model (trial=0, mod=66, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06538082\n",
      "Validation Loss:  0.06538081\n",
      "Final Training Loss:  0.06538082\n",
      "Final Validation Loss:  0.06538081\n",
      "\n",
      "Running model (trial=0, mod=67, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.15754348\n",
      "Validation Loss:  0.1575435\n",
      "Final Training Loss:  0.15754348\n",
      "Final Validation Loss:  0.1575435\n",
      "\n",
      "Running model (trial=0, mod=68, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01159321\n",
      "Validation Loss:  0.01159321\n",
      "Final Training Loss:  0.01159321\n",
      "Final Validation Loss:  0.01159321\n",
      "\n",
      "Running model (trial=0, mod=69, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03126765\n",
      "Validation Loss:  0.03126765\n",
      "Final Training Loss:  0.03126765\n",
      "Final Validation Loss:  0.03126765\n",
      "\n",
      "Running model (trial=0, mod=70, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10357666\n",
      "Validation Loss:  600.10357666\n",
      "Final Training Loss:  600.10357666\n",
      "Final Validation Loss:  600.10357666\n",
      "\n",
      "Running model (trial=0, mod=71, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10290527\n",
      "Validation Loss:  600.10290527\n",
      "Final Training Loss:  600.10290527\n",
      "Final Validation Loss:  600.10290527\n",
      "\n",
      "Running model (trial=1, mod=72, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04606553\n",
      "Validation Loss:  0.04606553\n",
      "Final Training Loss:  0.04606553\n",
      "Final Validation Loss:  0.04606553\n",
      "\n",
      "Running model (trial=1, mod=73, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02017591\n",
      "Validation Loss:  0.02017591\n",
      "Final Training Loss:  0.02017591\n",
      "Final Validation Loss:  0.02017591\n",
      "\n",
      "Running model (trial=1, mod=74, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.10172684\n",
      "Validation Loss:  0.10172685\n",
      "Final Training Loss:  0.10172684\n",
      "Final Validation Loss:  0.10172685\n",
      "\n",
      "Running model (trial=1, mod=75, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  96.56794739\n",
      "Validation Loss:  96.56794739\n",
      "Final Training Loss:  96.56794739\n",
      "Final Validation Loss:  96.56794739\n",
      "\n",
      "Running model (trial=1, mod=76, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07758934\n",
      "Validation Loss:  0.07758933\n",
      "Final Training Loss:  0.07758934\n",
      "Final Validation Loss:  0.07758933\n",
      "\n",
      "Running model (trial=1, mod=77, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  289.47784424\n",
      "Validation Loss:  289.47784424\n",
      "Final Training Loss:  289.47784424\n",
      "Final Validation Loss:  289.47784424\n",
      "\n",
      "Running model (trial=1, mod=78, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01205354\n",
      "Validation Loss:  0.01205354\n",
      "Final Training Loss:  0.01205354\n",
      "Final Validation Loss:  0.01205354\n",
      "\n",
      "Running model (trial=1, mod=79, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00514913\n",
      "Validation Loss:  0.00514913\n",
      "Final Training Loss:  0.00514913\n",
      "Final Validation Loss:  0.00514913\n",
      "\n",
      "Running model (trial=1, mod=80, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.2619181\n",
      "Validation Loss:  0.26191807\n",
      "Final Training Loss:  0.2619181\n",
      "Final Validation Loss:  0.26191807\n",
      "\n",
      "Running model (trial=1, mod=81, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.02331543\n",
      "Validation Loss:  600.02331543\n",
      "Final Training Loss:  600.02331543\n",
      "Final Validation Loss:  600.02331543\n",
      "\n",
      "Running model (trial=1, mod=82, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10351562\n",
      "Validation Loss:  600.10351562\n",
      "Final Training Loss:  600.10351562\n",
      "Final Validation Loss:  600.10351562\n",
      "\n",
      "Running model (trial=1, mod=83, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10467529\n",
      "Validation Loss:  600.10467529\n",
      "Final Training Loss:  600.10467529\n",
      "Final Validation Loss:  600.10467529\n",
      "\n",
      "Running model (trial=1, mod=84, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03544589\n",
      "Validation Loss:  0.03544589\n",
      "Final Training Loss:  0.03544589\n",
      "Final Validation Loss:  0.03544589\n",
      "\n",
      "Running model (trial=1, mod=85, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01897321\n",
      "Validation Loss:  0.01897321\n",
      "Final Training Loss:  0.01897321\n",
      "Final Validation Loss:  0.01897321\n",
      "\n",
      "Running model (trial=1, mod=86, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  103.37441254\n",
      "Validation Loss:  103.37440491\n",
      "Final Training Loss:  103.37441254\n",
      "Final Validation Loss:  103.37440491\n",
      "\n",
      "Running model (trial=1, mod=87, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  89.07329559\n",
      "Validation Loss:  89.07328796\n",
      "Final Training Loss:  89.07329559\n",
      "Final Validation Loss:  89.07328796\n",
      "\n",
      "Running model (trial=1, mod=88, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  123.40206909\n",
      "Validation Loss:  123.40206146\n",
      "Final Training Loss:  123.40206909\n",
      "Final Validation Loss:  123.40206146\n",
      "\n",
      "Running model (trial=1, mod=89, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  3119.31884766\n",
      "Validation Loss:  3119.31884766\n",
      "Final Training Loss:  3119.31884766\n",
      "Final Validation Loss:  3119.31884766\n",
      "\n",
      "Running model (trial=1, mod=90, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.62818533\n",
      "Validation Loss:  0.62818533\n",
      "Final Training Loss:  0.62818533\n",
      "Final Validation Loss:  0.62818533\n",
      "\n",
      "Running model (trial=1, mod=91, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03891601\n",
      "Validation Loss:  0.03891601\n",
      "Final Training Loss:  0.03891601\n",
      "Final Validation Loss:  0.03891601\n",
      "\n",
      "Running model (trial=1, mod=92, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00989205\n",
      "Validation Loss:  0.00989205\n",
      "Final Training Loss:  0.00989205\n",
      "Final Validation Loss:  0.00989205\n",
      "\n",
      "Running model (trial=1, mod=93, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.06280518\n",
      "Validation Loss:  600.06286621\n",
      "Final Training Loss:  600.06280518\n",
      "Final Validation Loss:  600.06286621\n",
      "\n",
      "Running model (trial=1, mod=94, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10339355\n",
      "Validation Loss:  600.10339355\n",
      "Final Training Loss:  600.10339355\n",
      "Final Validation Loss:  600.10339355\n",
      "\n",
      "Running model (trial=1, mod=95, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1027832\n",
      "Validation Loss:  600.1027832\n",
      "Final Training Loss:  600.1027832\n",
      "Final Validation Loss:  600.1027832\n",
      "\n",
      "Running model (trial=1, mod=96, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00525053\n",
      "Validation Loss:  0.00525053\n",
      "Final Training Loss:  0.00525053\n",
      "Final Validation Loss:  0.00525053\n",
      "\n",
      "Running model (trial=1, mod=97, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02619808\n",
      "Validation Loss:  0.02619808\n",
      "Final Training Loss:  0.02619808\n",
      "Final Validation Loss:  0.02619808\n",
      "\n",
      "Running model (trial=1, mod=98, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01120584\n",
      "Validation Loss:  0.01120584\n",
      "Final Training Loss:  0.01120584\n",
      "Final Validation Loss:  0.01120584\n",
      "\n",
      "Running model (trial=1, mod=99, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02821818\n",
      "Validation Loss:  0.02821818\n",
      "Final Training Loss:  0.02821818\n",
      "Final Validation Loss:  0.02821818\n",
      "\n",
      "Running model (trial=1, mod=100, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.16390057\n",
      "Validation Loss:  0.16390055\n",
      "Final Training Loss:  0.16390057\n",
      "Final Validation Loss:  0.16390055\n",
      "\n",
      "Running model (trial=1, mod=101, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07554021\n",
      "Validation Loss:  0.07554021\n",
      "Final Training Loss:  0.07554021\n",
      "Final Validation Loss:  0.07554021\n",
      "\n",
      "Running model (trial=1, mod=102, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03002799\n",
      "Validation Loss:  0.03002799\n",
      "Final Training Loss:  0.03002799\n",
      "Final Validation Loss:  0.03002799\n",
      "\n",
      "Running model (trial=1, mod=103, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00452116\n",
      "Validation Loss:  0.00452116\n",
      "Final Training Loss:  0.00452116\n",
      "Final Validation Loss:  0.00452116\n",
      "\n",
      "Running model (trial=1, mod=104, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01081299\n",
      "Validation Loss:  0.01081299\n",
      "Final Training Loss:  0.01081299\n",
      "Final Validation Loss:  0.01081299\n",
      "\n",
      "Running model (trial=1, mod=105, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  599.92822266\n",
      "Validation Loss:  599.92822266\n",
      "Final Training Loss:  599.92822266\n",
      "Final Validation Loss:  599.92822266\n",
      "\n",
      "Running model (trial=1, mod=106, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10510254\n",
      "Validation Loss:  600.1050415\n",
      "Final Training Loss:  600.10510254\n",
      "Final Validation Loss:  600.1050415\n",
      "\n",
      "Running model (trial=1, mod=107, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10229492\n",
      "Validation Loss:  600.10229492\n",
      "Final Training Loss:  600.10229492\n",
      "Final Validation Loss:  600.10229492\n",
      "\n",
      "Running model (trial=1, mod=108, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.30144832\n",
      "Validation Loss:  0.30144835\n",
      "Final Training Loss:  0.30144832\n",
      "Final Validation Loss:  0.30144835\n",
      "\n",
      "Running model (trial=1, mod=109, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01245856\n",
      "Validation Loss:  0.01245856\n",
      "Final Training Loss:  0.01245856\n",
      "Final Validation Loss:  0.01245856\n",
      "\n",
      "Running model (trial=1, mod=110, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02027325\n",
      "Validation Loss:  0.02027325\n",
      "Final Training Loss:  0.02027325\n",
      "Final Validation Loss:  0.02027325\n",
      "\n",
      "Running model (trial=1, mod=111, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0389905\n",
      "Validation Loss:  0.0389905\n",
      "Final Training Loss:  0.0389905\n",
      "Final Validation Loss:  0.0389905\n",
      "\n",
      "Running model (trial=1, mod=112, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  204.06359863\n",
      "Validation Loss:  204.06359863\n",
      "Final Training Loss:  204.06359863\n",
      "Final Validation Loss:  204.06359863\n",
      "\n",
      "Running model (trial=1, mod=113, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  6194.91650391\n",
      "Validation Loss:  6194.91650391\n",
      "Final Training Loss:  6194.91650391\n",
      "Final Validation Loss:  6194.91650391\n",
      "\n",
      "Running model (trial=1, mod=114, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06604683\n",
      "Validation Loss:  0.06604683\n",
      "Final Training Loss:  0.06604683\n",
      "Final Validation Loss:  0.06604683\n",
      "\n",
      "Running model (trial=1, mod=115, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00741139\n",
      "Validation Loss:  0.00741139\n",
      "Final Training Loss:  0.00741139\n",
      "Final Validation Loss:  0.00741139\n",
      "\n",
      "Running model (trial=1, mod=116, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01774281\n",
      "Validation Loss:  0.01774282\n",
      "Final Training Loss:  0.01774281\n",
      "Final Validation Loss:  0.01774282\n",
      "\n",
      "Running model (trial=1, mod=117, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  138.55322266\n",
      "Validation Loss:  138.55322266\n",
      "Final Training Loss:  138.55322266\n",
      "Final Validation Loss:  138.55322266\n",
      "\n",
      "Running model (trial=1, mod=118, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10424805\n",
      "Validation Loss:  600.10424805\n",
      "Final Training Loss:  600.10424805\n",
      "Final Validation Loss:  600.10424805\n",
      "\n",
      "Running model (trial=1, mod=119, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10229492\n",
      "Validation Loss:  600.10229492\n",
      "Final Training Loss:  600.10229492\n",
      "Final Validation Loss:  600.10229492\n",
      "\n",
      "Running model (trial=1, mod=120, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03336043\n",
      "Validation Loss:  0.03336043\n",
      "Final Training Loss:  0.03336043\n",
      "Final Validation Loss:  0.03336043\n",
      "\n",
      "Running model (trial=1, mod=121, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01056214\n",
      "Validation Loss:  0.01056214\n",
      "Final Training Loss:  0.01056214\n",
      "Final Validation Loss:  0.01056214\n",
      "\n",
      "Running model (trial=1, mod=122, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00872317\n",
      "Validation Loss:  0.00872317\n",
      "Final Training Loss:  0.00872317\n",
      "Final Validation Loss:  0.00872317\n",
      "\n",
      "Running model (trial=1, mod=123, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  117.69620514\n",
      "Validation Loss:  117.69620514\n",
      "Final Training Loss:  117.69620514\n",
      "Final Validation Loss:  117.69620514\n",
      "\n",
      "Running model (trial=1, mod=124, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  248.18099976\n",
      "Validation Loss:  248.18099976\n",
      "Final Training Loss:  248.18099976\n",
      "Final Validation Loss:  248.18099976\n",
      "\n",
      "Running model (trial=1, mod=125, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  16764.11132812\n",
      "Validation Loss:  16764.11132812\n",
      "Final Training Loss:  16764.11132812\n",
      "Final Validation Loss:  16764.11132812\n",
      "\n",
      "Running model (trial=1, mod=126, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02861677\n",
      "Validation Loss:  0.02861676\n",
      "Final Training Loss:  0.02861677\n",
      "Final Validation Loss:  0.02861676\n",
      "\n",
      "Running model (trial=1, mod=127, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.1764777\n",
      "Validation Loss:  0.1764777\n",
      "Final Training Loss:  0.1764777\n",
      "Final Validation Loss:  0.1764777\n",
      "\n",
      "Running model (trial=1, mod=128, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01305108\n",
      "Validation Loss:  0.01305108\n",
      "Final Training Loss:  0.01305108\n",
      "Final Validation Loss:  0.01305108\n",
      "\n",
      "Running model (trial=1, mod=129, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  5.84588242\n",
      "Validation Loss:  5.84588289\n",
      "Final Training Loss:  5.84588242\n",
      "Final Validation Loss:  5.84588289\n",
      "\n",
      "Running model (trial=1, mod=130, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10089111\n",
      "Validation Loss:  600.10089111\n",
      "Final Training Loss:  600.10089111\n",
      "Final Validation Loss:  600.10089111\n",
      "\n",
      "Running model (trial=1, mod=131, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10321045\n",
      "Validation Loss:  600.10321045\n",
      "Final Training Loss:  600.10321045\n",
      "Final Validation Loss:  600.10321045\n",
      "\n",
      "Running model (trial=1, mod=132, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  90.30393219\n",
      "Validation Loss:  90.30391693\n",
      "Final Training Loss:  90.30393219\n",
      "Final Validation Loss:  90.30391693\n",
      "\n",
      "Running model (trial=1, mod=133, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.15943618\n",
      "Validation Loss:  0.15943618\n",
      "Final Training Loss:  0.15943618\n",
      "Final Validation Loss:  0.15943618\n",
      "\n",
      "Running model (trial=1, mod=134, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04397921\n",
      "Validation Loss:  0.04397921\n",
      "Final Training Loss:  0.04397921\n",
      "Final Validation Loss:  0.04397921\n",
      "\n",
      "Running model (trial=1, mod=135, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02541487\n",
      "Validation Loss:  0.02541487\n",
      "Final Training Loss:  0.02541487\n",
      "Final Validation Loss:  0.02541487\n",
      "\n",
      "Running model (trial=1, mod=136, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  208.81562805\n",
      "Validation Loss:  208.81562805\n",
      "Final Training Loss:  208.81562805\n",
      "Final Validation Loss:  208.81562805\n",
      "\n",
      "Running model (trial=1, mod=137, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  14775.76464844\n",
      "Validation Loss:  14775.76464844\n",
      "Final Training Loss:  14775.76464844\n",
      "Final Validation Loss:  14775.76464844\n",
      "\n",
      "Running model (trial=1, mod=138, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00671909\n",
      "Validation Loss:  0.00671909\n",
      "Final Training Loss:  0.00671909\n",
      "Final Validation Loss:  0.00671909\n",
      "\n",
      "Running model (trial=1, mod=139, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.29989764\n",
      "Validation Loss:  0.29989764\n",
      "Final Training Loss:  0.29989764\n",
      "Final Validation Loss:  0.29989764\n",
      "\n",
      "Running model (trial=1, mod=140, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01053671\n",
      "Validation Loss:  0.01053671\n",
      "Final Training Loss:  0.01053671\n",
      "Final Validation Loss:  0.01053671\n",
      "\n",
      "Running model (trial=1, mod=141, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  3.80730128\n",
      "Validation Loss:  3.80730128\n",
      "Final Training Loss:  3.80730128\n",
      "Final Validation Loss:  3.80730128\n",
      "\n",
      "Running model (trial=1, mod=142, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10235596\n",
      "Validation Loss:  600.10235596\n",
      "Final Training Loss:  600.10235596\n",
      "Final Validation Loss:  600.10235596\n",
      "\n",
      "Running model (trial=1, mod=143, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10247803\n",
      "Validation Loss:  600.10247803\n",
      "Final Training Loss:  600.10247803\n",
      "Final Validation Loss:  600.10247803\n",
      "\n",
      "Running model (trial=2, mod=144, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01880821\n",
      "Validation Loss:  0.01880821\n",
      "Final Training Loss:  0.01880821\n",
      "Final Validation Loss:  0.01880821\n",
      "\n",
      "Running model (trial=2, mod=145, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02010783\n",
      "Validation Loss:  0.02010782\n",
      "Final Training Loss:  0.02010783\n",
      "Final Validation Loss:  0.02010782\n",
      "\n",
      "Running model (trial=2, mod=146, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00811859\n",
      "Validation Loss:  0.00811859\n",
      "Final Training Loss:  0.00811859\n",
      "Final Validation Loss:  0.00811859\n",
      "\n",
      "Running model (trial=2, mod=147, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  115.34590149\n",
      "Validation Loss:  115.34590912\n",
      "Final Training Loss:  115.34590149\n",
      "Final Validation Loss:  115.34590912\n",
      "\n",
      "Running model (trial=2, mod=148, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04278272\n",
      "Validation Loss:  0.04278273\n",
      "Final Training Loss:  0.04278272\n",
      "Final Validation Loss:  0.04278273\n",
      "\n",
      "Running model (trial=2, mod=149, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  99.95714569\n",
      "Validation Loss:  99.95713806\n",
      "Final Training Loss:  99.95714569\n",
      "Final Validation Loss:  99.95713806\n",
      "\n",
      "Running model (trial=2, mod=150, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01426709\n",
      "Validation Loss:  0.01426709\n",
      "Final Training Loss:  0.01426709\n",
      "Final Validation Loss:  0.01426709\n",
      "\n",
      "Running model (trial=2, mod=151, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01128635\n",
      "Validation Loss:  0.01128635\n",
      "Final Training Loss:  0.01128635\n",
      "Final Validation Loss:  0.01128635\n",
      "\n",
      "Running model (trial=2, mod=152, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00469682\n",
      "Validation Loss:  0.00469682\n",
      "Final Training Loss:  0.00469682\n",
      "Final Validation Loss:  0.00469682\n",
      "\n",
      "Running model (trial=2, mod=153, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  140.72711182\n",
      "Validation Loss:  140.72711182\n",
      "Final Training Loss:  140.72711182\n",
      "Final Validation Loss:  140.72711182\n",
      "\n",
      "Running model (trial=2, mod=154, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10253906\n",
      "Validation Loss:  600.10253906\n",
      "Final Training Loss:  600.10253906\n",
      "Final Validation Loss:  600.10253906\n",
      "\n",
      "Running model (trial=2, mod=155, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1036377\n",
      "Validation Loss:  600.1036377\n",
      "Final Training Loss:  600.1036377\n",
      "Final Validation Loss:  600.1036377\n",
      "\n",
      "Running model (trial=2, mod=156, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.15141091\n",
      "Validation Loss:  0.15141091\n",
      "Final Training Loss:  0.15141091\n",
      "Final Validation Loss:  0.15141091\n",
      "\n",
      "Running model (trial=2, mod=157, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04333963\n",
      "Validation Loss:  0.04333963\n",
      "Final Training Loss:  0.04333963\n",
      "Final Validation Loss:  0.04333963\n",
      "\n",
      "Running model (trial=2, mod=158, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00889897\n",
      "Validation Loss:  0.00889898\n",
      "Final Training Loss:  0.00889897\n",
      "Final Validation Loss:  0.00889898\n",
      "\n",
      "Running model (trial=2, mod=159, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  180.96284485\n",
      "Validation Loss:  180.96284485\n",
      "Final Training Loss:  180.96284485\n",
      "Final Validation Loss:  180.96284485\n",
      "\n",
      "Running model (trial=2, mod=160, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  220.03269958\n",
      "Validation Loss:  220.03268433\n",
      "Final Training Loss:  220.03269958\n",
      "Final Validation Loss:  220.03268433\n",
      "\n",
      "Running model (trial=2, mod=161, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  9406.43261719\n",
      "Validation Loss:  9406.43261719\n",
      "Final Training Loss:  9406.43261719\n",
      "Final Validation Loss:  9406.43261719\n",
      "\n",
      "Running model (trial=2, mod=162, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00711616\n",
      "Validation Loss:  0.00711616\n",
      "Final Training Loss:  0.00711616\n",
      "Final Validation Loss:  0.00711616\n",
      "\n",
      "Running model (trial=2, mod=163, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00709679\n",
      "Validation Loss:  0.00709679\n",
      "Final Training Loss:  0.00709679\n",
      "Final Validation Loss:  0.00709679\n",
      "\n",
      "Running model (trial=2, mod=164, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00359243\n",
      "Validation Loss:  0.00359243\n",
      "Final Training Loss:  0.00359243\n",
      "Final Validation Loss:  0.00359243\n",
      "\n",
      "Running model (trial=2, mod=165, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.09295654\n",
      "Validation Loss:  600.09295654\n",
      "Final Training Loss:  600.09295654\n",
      "Final Validation Loss:  600.09295654\n",
      "\n",
      "Running model (trial=2, mod=166, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10327148\n",
      "Validation Loss:  600.10327148\n",
      "Final Training Loss:  600.10327148\n",
      "Final Validation Loss:  600.10327148\n",
      "\n",
      "Running model (trial=2, mod=167, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10412598\n",
      "Validation Loss:  600.10418701\n",
      "Final Training Loss:  600.10412598\n",
      "Final Validation Loss:  600.10418701\n",
      "\n",
      "Running model (trial=2, mod=168, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  90.06406403\n",
      "Validation Loss:  90.06406403\n",
      "Final Training Loss:  90.06406403\n",
      "Final Validation Loss:  90.06406403\n",
      "\n",
      "Running model (trial=2, mod=169, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02756321\n",
      "Validation Loss:  0.02756321\n",
      "Final Training Loss:  0.02756321\n",
      "Final Validation Loss:  0.02756321\n",
      "\n",
      "Running model (trial=2, mod=170, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00883228\n",
      "Validation Loss:  0.00883228\n",
      "Final Training Loss:  0.00883228\n",
      "Final Validation Loss:  0.00883228\n",
      "\n",
      "Running model (trial=2, mod=171, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03276281\n",
      "Validation Loss:  0.03276281\n",
      "Final Training Loss:  0.03276281\n",
      "Final Validation Loss:  0.03276281\n",
      "\n",
      "Running model (trial=2, mod=172, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05127239\n",
      "Validation Loss:  0.05127239\n",
      "Final Training Loss:  0.05127239\n",
      "Final Validation Loss:  0.05127239\n",
      "\n",
      "Running model (trial=2, mod=173, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  42949.3359375\n",
      "Validation Loss:  42949.3359375\n",
      "Final Training Loss:  42949.3359375\n",
      "Final Validation Loss:  42949.3359375\n",
      "\n",
      "Running model (trial=2, mod=174, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01464767\n",
      "Validation Loss:  0.01464767\n",
      "Final Training Loss:  0.01464767\n",
      "Final Validation Loss:  0.01464767\n",
      "\n",
      "Running model (trial=2, mod=175, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00610265\n",
      "Validation Loss:  0.00610265\n",
      "Final Training Loss:  0.00610265\n",
      "Final Validation Loss:  0.00610265\n",
      "\n",
      "Running model (trial=2, mod=176, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00317464\n",
      "Validation Loss:  0.00317464\n",
      "Final Training Loss:  0.00317464\n",
      "Final Validation Loss:  0.00317464\n",
      "\n",
      "Running model (trial=2, mod=177, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.19976807\n",
      "Validation Loss:  600.19976807\n",
      "Final Training Loss:  600.19976807\n",
      "Final Validation Loss:  600.19976807\n",
      "\n",
      "Running model (trial=2, mod=178, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10168457\n",
      "Validation Loss:  600.10168457\n",
      "Final Training Loss:  600.10168457\n",
      "Final Validation Loss:  600.10168457\n",
      "\n",
      "Running model (trial=2, mod=179, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1027832\n",
      "Validation Loss:  600.1027832\n",
      "Final Training Loss:  600.1027832\n",
      "Final Validation Loss:  600.1027832\n",
      "\n",
      "Running model (trial=2, mod=180, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00702848\n",
      "Validation Loss:  0.00702848\n",
      "Final Training Loss:  0.00702848\n",
      "Final Validation Loss:  0.00702848\n",
      "\n",
      "Running model (trial=2, mod=181, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0272599\n",
      "Validation Loss:  0.0272599\n",
      "Final Training Loss:  0.0272599\n",
      "Final Validation Loss:  0.0272599\n",
      "\n",
      "Running model (trial=2, mod=182, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01589149\n",
      "Validation Loss:  0.01589149\n",
      "Final Training Loss:  0.01589149\n",
      "Final Validation Loss:  0.01589149\n",
      "\n",
      "Running model (trial=2, mod=183, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  nan\n",
      "Validation Loss:  nan\n",
      "Final Training Loss:  nan\n",
      "Final Validation Loss:  nan\n",
      "\n",
      "Running model (trial=2, mod=184, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  162.46405029\n",
      "Validation Loss:  162.46405029\n",
      "Final Training Loss:  162.46405029\n",
      "Final Validation Loss:  162.46405029\n",
      "\n",
      "Running model (trial=2, mod=185, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  25685.23242188\n",
      "Validation Loss:  25685.23242188\n",
      "Final Training Loss:  25685.23242188\n",
      "Final Validation Loss:  25685.23242188\n",
      "\n",
      "Running model (trial=2, mod=186, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.10008852\n",
      "Validation Loss:  0.10008852\n",
      "Final Training Loss:  0.10008852\n",
      "Final Validation Loss:  0.10008852\n",
      "\n",
      "Running model (trial=2, mod=187, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00554381\n",
      "Validation Loss:  0.00554381\n",
      "Final Training Loss:  0.00554381\n",
      "Final Validation Loss:  0.00554381\n",
      "\n",
      "Running model (trial=2, mod=188, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00366616\n",
      "Validation Loss:  0.00366616\n",
      "Final Training Loss:  0.00366616\n",
      "Final Validation Loss:  0.00366616\n",
      "\n",
      "Running model (trial=2, mod=189, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1.95487189\n",
      "Validation Loss:  1.95487189\n",
      "Final Training Loss:  1.95487189\n",
      "Final Validation Loss:  1.95487189\n",
      "\n",
      "Running model (trial=2, mod=190, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10406494\n",
      "Validation Loss:  600.10406494\n",
      "Final Training Loss:  600.10406494\n",
      "Final Validation Loss:  600.10406494\n",
      "\n",
      "Running model (trial=2, mod=191, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10284424\n",
      "Validation Loss:  600.10284424\n",
      "Final Training Loss:  600.10284424\n",
      "Final Validation Loss:  600.10284424\n",
      "\n",
      "Running model (trial=2, mod=192, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.16722713\n",
      "Validation Loss:  0.16722712\n",
      "Final Training Loss:  0.16722713\n",
      "Final Validation Loss:  0.16722712\n",
      "\n",
      "Running model (trial=2, mod=193, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07417215\n",
      "Validation Loss:  0.07417215\n",
      "Final Training Loss:  0.07417215\n",
      "Final Validation Loss:  0.07417215\n",
      "\n",
      "Running model (trial=2, mod=194, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.19383307\n",
      "Validation Loss:  0.19383304\n",
      "Final Training Loss:  0.19383307\n",
      "Final Validation Loss:  0.19383304\n",
      "\n",
      "Running model (trial=2, mod=195, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11884003\n",
      "Validation Loss:  0.11884005\n",
      "Final Training Loss:  0.11884003\n",
      "Final Validation Loss:  0.11884005\n",
      "\n",
      "Running model (trial=2, mod=196, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1265.45715332\n",
      "Validation Loss:  1265.45715332\n",
      "Final Training Loss:  1265.45715332\n",
      "Final Validation Loss:  1265.45715332\n",
      "\n",
      "Running model (trial=2, mod=197, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  38859.203125\n",
      "Validation Loss:  38859.203125\n",
      "Final Training Loss:  38859.203125\n",
      "Final Validation Loss:  38859.203125\n",
      "\n",
      "Running model (trial=2, mod=198, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01084182\n",
      "Validation Loss:  0.01084183\n",
      "Final Training Loss:  0.01084182\n",
      "Final Validation Loss:  0.01084183\n",
      "\n",
      "Running model (trial=2, mod=199, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00490687\n",
      "Validation Loss:  0.00490687\n",
      "Final Training Loss:  0.00490687\n",
      "Final Validation Loss:  0.00490687\n",
      "\n",
      "Running model (trial=2, mod=200, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01196239\n",
      "Validation Loss:  0.01196239\n",
      "Final Training Loss:  0.01196239\n",
      "Final Validation Loss:  0.01196239\n",
      "\n",
      "Running model (trial=2, mod=201, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00692549\n",
      "Validation Loss:  0.00692549\n",
      "Final Training Loss:  0.00692549\n",
      "Final Validation Loss:  0.00692549\n",
      "\n",
      "Running model (trial=2, mod=202, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10217285\n",
      "Validation Loss:  600.10217285\n",
      "Final Training Loss:  600.10217285\n",
      "Final Validation Loss:  600.10217285\n",
      "\n",
      "Running model (trial=2, mod=203, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10266113\n",
      "Validation Loss:  600.10266113\n",
      "Final Training Loss:  600.10266113\n",
      "Final Validation Loss:  600.10266113\n",
      "\n",
      "Running model (trial=2, mod=204, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04862739\n",
      "Validation Loss:  0.04862738\n",
      "Final Training Loss:  0.04862739\n",
      "Final Validation Loss:  0.04862738\n",
      "\n",
      "Running model (trial=2, mod=205, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00943203\n",
      "Validation Loss:  0.00943203\n",
      "Final Training Loss:  0.00943203\n",
      "Final Validation Loss:  0.00943203\n",
      "\n",
      "Running model (trial=2, mod=206, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00962828\n",
      "Validation Loss:  0.00962828\n",
      "Final Training Loss:  0.00962828\n",
      "Final Validation Loss:  0.00962828\n",
      "\n",
      "Running model (trial=2, mod=207, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  122.7629776\n",
      "Validation Loss:  122.7629776\n",
      "Final Training Loss:  122.7629776\n",
      "Final Validation Loss:  122.7629776\n",
      "\n",
      "Running model (trial=2, mod=208, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  652.76928711\n",
      "Validation Loss:  652.76928711\n",
      "Final Training Loss:  652.76928711\n",
      "Final Validation Loss:  652.76928711\n",
      "\n",
      "Running model (trial=2, mod=209, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  4900.21582031\n",
      "Validation Loss:  4900.21582031\n",
      "Final Training Loss:  4900.21582031\n",
      "Final Validation Loss:  4900.21582031\n",
      "\n",
      "Running model (trial=2, mod=210, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.29173234\n",
      "Validation Loss:  0.29173234\n",
      "Final Training Loss:  0.29173234\n",
      "Final Validation Loss:  0.29173234\n",
      "\n",
      "Running model (trial=2, mod=211, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01806015\n",
      "Validation Loss:  0.01806015\n",
      "Final Training Loss:  0.01806015\n",
      "Final Validation Loss:  0.01806015\n",
      "\n",
      "Running model (trial=2, mod=212, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02046858\n",
      "Validation Loss:  0.02046858\n",
      "Final Training Loss:  0.02046858\n",
      "Final Validation Loss:  0.02046858\n",
      "\n",
      "Running model (trial=2, mod=213, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  2.17714739\n",
      "Validation Loss:  2.17714739\n",
      "Final Training Loss:  2.17714739\n",
      "Final Validation Loss:  2.17714739\n",
      "\n",
      "Running model (trial=2, mod=214, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10412598\n",
      "Validation Loss:  600.10412598\n",
      "Final Training Loss:  600.10412598\n",
      "Final Validation Loss:  600.10412598\n",
      "\n",
      "Running model (trial=2, mod=215, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10266113\n",
      "Validation Loss:  600.10266113\n",
      "Final Training Loss:  600.10266113\n",
      "Final Validation Loss:  600.10266113\n",
      "\n",
      "Running model (trial=3, mod=216, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.14324704\n",
      "Validation Loss:  0.14324704\n",
      "Final Training Loss:  0.14324704\n",
      "Final Validation Loss:  0.14324704\n",
      "\n",
      "Running model (trial=3, mod=217, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02870912\n",
      "Validation Loss:  0.02870912\n",
      "Final Training Loss:  0.02870912\n",
      "Final Validation Loss:  0.02870912\n",
      "\n",
      "Running model (trial=3, mod=218, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01645991\n",
      "Validation Loss:  0.01645991\n",
      "Final Training Loss:  0.01645991\n",
      "Final Validation Loss:  0.01645991\n",
      "\n",
      "Running model (trial=3, mod=219, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  107.23591614\n",
      "Validation Loss:  107.23592377\n",
      "Final Training Loss:  107.23591614\n",
      "Final Validation Loss:  107.23592377\n",
      "\n",
      "Running model (trial=3, mod=220, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  35.65116501\n",
      "Validation Loss:  35.65116119\n",
      "Final Training Loss:  35.65116501\n",
      "Final Validation Loss:  35.65116119\n",
      "\n",
      "Running model (trial=3, mod=221, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  373.21340942\n",
      "Validation Loss:  373.21340942\n",
      "Final Training Loss:  373.21340942\n",
      "Final Validation Loss:  373.21340942\n",
      "\n",
      "Running model (trial=3, mod=222, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1.24524474\n",
      "Validation Loss:  1.24524474\n",
      "Final Training Loss:  1.24524474\n",
      "Final Validation Loss:  1.24524474\n",
      "\n",
      "Running model (trial=3, mod=223, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00660319\n",
      "Validation Loss:  0.00660319\n",
      "Final Training Loss:  0.00660319\n",
      "Final Validation Loss:  0.00660319\n",
      "\n",
      "Running model (trial=3, mod=224, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00824469\n",
      "Validation Loss:  0.00824469\n",
      "Final Training Loss:  0.00824469\n",
      "Final Validation Loss:  0.00824469\n",
      "\n",
      "Running model (trial=3, mod=225, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.09222412\n",
      "Validation Loss:  600.09222412\n",
      "Final Training Loss:  600.09222412\n",
      "Final Validation Loss:  600.09222412\n",
      "\n",
      "Running model (trial=3, mod=226, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10302734\n",
      "Validation Loss:  600.10302734\n",
      "Final Training Loss:  600.10302734\n",
      "Final Validation Loss:  600.10302734\n",
      "\n",
      "Running model (trial=3, mod=227, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10351562\n",
      "Validation Loss:  600.10351562\n",
      "Final Training Loss:  600.10351562\n",
      "Final Validation Loss:  600.10351562\n",
      "\n",
      "Running model (trial=3, mod=228, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00669406\n",
      "Validation Loss:  0.00669406\n",
      "Final Training Loss:  0.00669406\n",
      "Final Validation Loss:  0.00669406\n",
      "\n",
      "Running model (trial=3, mod=229, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03911708\n",
      "Validation Loss:  0.03911708\n",
      "Final Training Loss:  0.03911708\n",
      "Final Validation Loss:  0.03911708\n",
      "\n",
      "Running model (trial=3, mod=230, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00994607\n",
      "Validation Loss:  0.00994607\n",
      "Final Training Loss:  0.00994607\n",
      "Final Validation Loss:  0.00994607\n",
      "\n",
      "Running model (trial=3, mod=231, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04212466\n",
      "Validation Loss:  0.04212466\n",
      "Final Training Loss:  0.04212466\n",
      "Final Validation Loss:  0.04212466\n",
      "\n",
      "Running model (trial=3, mod=232, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.09207031\n",
      "Validation Loss:  0.09207029\n",
      "Final Training Loss:  0.09207031\n",
      "Final Validation Loss:  0.09207029\n",
      "\n",
      "Running model (trial=3, mod=233, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  3023332.25\n",
      "Validation Loss:  3023332.5\n",
      "Final Training Loss:  3023332.25\n",
      "Final Validation Loss:  3023332.5\n",
      "\n",
      "Running model (trial=3, mod=234, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.10710225\n",
      "Validation Loss:  0.10710225\n",
      "Final Training Loss:  0.10710225\n",
      "Final Validation Loss:  0.10710225\n",
      "\n",
      "Running model (trial=3, mod=235, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01370817\n",
      "Validation Loss:  0.01370817\n",
      "Final Training Loss:  0.01370817\n",
      "Final Validation Loss:  0.01370817\n",
      "\n",
      "Running model (trial=3, mod=236, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00195268\n",
      "Validation Loss:  0.00195268\n",
      "Final Training Loss:  0.00195268\n",
      "Final Validation Loss:  0.00195268\n",
      "\n",
      "Running model (trial=3, mod=237, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.08380127\n",
      "Validation Loss:  600.08380127\n",
      "Final Training Loss:  600.08380127\n",
      "Final Validation Loss:  600.08380127\n",
      "\n",
      "Running model (trial=3, mod=238, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10571289\n",
      "Validation Loss:  600.10571289\n",
      "Final Training Loss:  600.10571289\n",
      "Final Validation Loss:  600.10571289\n",
      "\n",
      "Running model (trial=3, mod=239, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1015625\n",
      "Validation Loss:  600.1015625\n",
      "Final Training Loss:  600.1015625\n",
      "Final Validation Loss:  600.1015625\n",
      "\n",
      "Running model (trial=3, mod=240, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.39772886\n",
      "Validation Loss:  0.39772886\n",
      "Final Training Loss:  0.39772886\n",
      "Final Validation Loss:  0.39772886\n",
      "\n",
      "Running model (trial=3, mod=241, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00345887\n",
      "Validation Loss:  0.00345887\n",
      "Final Training Loss:  0.00345887\n",
      "Final Validation Loss:  0.00345887\n",
      "\n",
      "Running model (trial=3, mod=242, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00307078\n",
      "Validation Loss:  0.00307078\n",
      "Final Training Loss:  0.00307078\n",
      "Final Validation Loss:  0.00307078\n",
      "\n",
      "Running model (trial=3, mod=243, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01122614\n",
      "Validation Loss:  0.01122615\n",
      "Final Training Loss:  0.01122614\n",
      "Final Validation Loss:  0.01122615\n",
      "\n",
      "Running model (trial=3, mod=244, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  124.29723358\n",
      "Validation Loss:  124.29724121\n",
      "Final Training Loss:  124.29723358\n",
      "Final Validation Loss:  124.29724121\n",
      "\n",
      "Running model (trial=3, mod=245, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  18818.82421875\n",
      "Validation Loss:  18818.82421875\n",
      "Final Training Loss:  18818.82421875\n",
      "Final Validation Loss:  18818.82421875\n",
      "\n",
      "Running model (trial=3, mod=246, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0099986\n",
      "Validation Loss:  0.0099986\n",
      "Final Training Loss:  0.0099986\n",
      "Final Validation Loss:  0.0099986\n",
      "\n",
      "Running model (trial=3, mod=247, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00116327\n",
      "Validation Loss:  0.00116327\n",
      "Final Training Loss:  0.00116327\n",
      "Final Validation Loss:  0.00116327\n",
      "\n",
      "Running model (trial=3, mod=248, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02340652\n",
      "Validation Loss:  0.02340652\n",
      "Final Training Loss:  0.02340652\n",
      "Final Validation Loss:  0.02340652\n",
      "\n",
      "Running model (trial=3, mod=249, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.05615234\n",
      "Validation Loss:  600.05615234\n",
      "Final Training Loss:  600.05615234\n",
      "Final Validation Loss:  600.05615234\n",
      "\n",
      "Running model (trial=3, mod=250, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10571289\n",
      "Validation Loss:  600.10571289\n",
      "Final Training Loss:  600.10571289\n",
      "Final Validation Loss:  600.10571289\n",
      "\n",
      "Running model (trial=3, mod=251, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10681152\n",
      "Validation Loss:  600.10681152\n",
      "Final Training Loss:  600.10681152\n",
      "Final Validation Loss:  600.10681152\n",
      "\n",
      "Running model (trial=3, mod=252, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07794941\n",
      "Validation Loss:  0.07794942\n",
      "Final Training Loss:  0.07794941\n",
      "Final Validation Loss:  0.07794942\n",
      "\n",
      "Running model (trial=3, mod=253, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  90.68387604\n",
      "Validation Loss:  90.68387604\n",
      "Final Training Loss:  90.68387604\n",
      "Final Validation Loss:  90.68387604\n",
      "\n",
      "Running model (trial=3, mod=254, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04226491\n",
      "Validation Loss:  0.04226491\n",
      "Final Training Loss:  0.04226491\n",
      "Final Validation Loss:  0.04226491\n",
      "\n",
      "Running model (trial=3, mod=255, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01051102\n",
      "Validation Loss:  0.01051103\n",
      "Final Training Loss:  0.01051102\n",
      "Final Validation Loss:  0.01051103\n",
      "\n",
      "Running model (trial=3, mod=256, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  149.60119629\n",
      "Validation Loss:  149.60119629\n",
      "Final Training Loss:  149.60119629\n",
      "Final Validation Loss:  149.60119629\n",
      "\n",
      "Running model (trial=3, mod=257, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1868.17175293\n",
      "Validation Loss:  1868.17175293\n",
      "Final Training Loss:  1868.17175293\n",
      "Final Validation Loss:  1868.17175293\n",
      "\n",
      "Running model (trial=3, mod=258, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05391693\n",
      "Validation Loss:  0.05391692\n",
      "Final Training Loss:  0.05391693\n",
      "Final Validation Loss:  0.05391692\n",
      "\n",
      "Running model (trial=3, mod=259, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05238862\n",
      "Validation Loss:  0.05238861\n",
      "Final Training Loss:  0.05238862\n",
      "Final Validation Loss:  0.05238861\n",
      "\n",
      "Running model (trial=3, mod=260, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00568576\n",
      "Validation Loss:  0.00568576\n",
      "Final Training Loss:  0.00568576\n",
      "Final Validation Loss:  0.00568576\n",
      "\n",
      "Running model (trial=3, mod=261, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.14434814\n",
      "Validation Loss:  600.14434814\n",
      "Final Training Loss:  600.14434814\n",
      "Final Validation Loss:  600.14434814\n",
      "\n",
      "Running model (trial=3, mod=262, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10223389\n",
      "Validation Loss:  600.10223389\n",
      "Final Training Loss:  600.10223389\n",
      "Final Validation Loss:  600.10223389\n",
      "\n",
      "Running model (trial=3, mod=263, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10656738\n",
      "Validation Loss:  600.10656738\n",
      "Final Training Loss:  600.10656738\n",
      "Final Validation Loss:  600.10656738\n",
      "\n",
      "Running model (trial=3, mod=264, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.17246497\n",
      "Validation Loss:  0.17246495\n",
      "Final Training Loss:  0.17246497\n",
      "Final Validation Loss:  0.17246495\n",
      "\n",
      "Running model (trial=3, mod=265, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02502994\n",
      "Validation Loss:  0.02502994\n",
      "Final Training Loss:  0.02502994\n",
      "Final Validation Loss:  0.02502994\n",
      "\n",
      "Running model (trial=3, mod=266, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06336445\n",
      "Validation Loss:  0.06336445\n",
      "Final Training Loss:  0.06336445\n",
      "Final Validation Loss:  0.06336445\n",
      "\n",
      "Running model (trial=3, mod=267, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04235249\n",
      "Validation Loss:  0.04235249\n",
      "Final Training Loss:  0.04235249\n",
      "Final Validation Loss:  0.04235249\n",
      "\n",
      "Running model (trial=3, mod=268, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  66.43888855\n",
      "Validation Loss:  66.43888855\n",
      "Final Training Loss:  66.43888855\n",
      "Final Validation Loss:  66.43888855\n",
      "\n",
      "Running model (trial=3, mod=269, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  4894.06982422\n",
      "Validation Loss:  4894.06982422\n",
      "Final Training Loss:  4894.06982422\n",
      "Final Validation Loss:  4894.06982422\n",
      "\n",
      "Running model (trial=3, mod=270, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00596884\n",
      "Validation Loss:  0.00596884\n",
      "Final Training Loss:  0.00596884\n",
      "Final Validation Loss:  0.00596884\n",
      "\n",
      "Running model (trial=3, mod=271, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00558887\n",
      "Validation Loss:  0.00558887\n",
      "Final Training Loss:  0.00558887\n",
      "Final Validation Loss:  0.00558887\n",
      "\n",
      "Running model (trial=3, mod=272, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00713345\n",
      "Validation Loss:  0.00713345\n",
      "Final Training Loss:  0.00713345\n",
      "Final Validation Loss:  0.00713345\n",
      "\n",
      "Running model (trial=3, mod=273, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01562255\n",
      "Validation Loss:  0.01562255\n",
      "Final Training Loss:  0.01562255\n",
      "Final Validation Loss:  0.01562255\n",
      "\n",
      "Running model (trial=3, mod=274, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10583496\n",
      "Validation Loss:  600.10583496\n",
      "Final Training Loss:  600.10583496\n",
      "Final Validation Loss:  600.10583496\n",
      "\n",
      "Running model (trial=3, mod=275, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10186768\n",
      "Validation Loss:  600.10186768\n",
      "Final Training Loss:  600.10186768\n",
      "Final Validation Loss:  600.10186768\n",
      "\n",
      "Running model (trial=3, mod=276, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03178663\n",
      "Validation Loss:  0.03178662\n",
      "Final Training Loss:  0.03178663\n",
      "Final Validation Loss:  0.03178662\n",
      "\n",
      "Running model (trial=3, mod=277, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  89.22057343\n",
      "Validation Loss:  89.2205658\n",
      "Final Training Loss:  89.22057343\n",
      "Final Validation Loss:  89.2205658\n",
      "\n",
      "Running model (trial=3, mod=278, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03538385\n",
      "Validation Loss:  0.03538385\n",
      "Final Training Loss:  0.03538385\n",
      "Final Validation Loss:  0.03538385\n",
      "\n",
      "Running model (trial=3, mod=279, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  7.19338036\n",
      "Validation Loss:  7.19338131\n",
      "Final Training Loss:  7.19338036\n",
      "Final Validation Loss:  7.19338131\n",
      "\n",
      "Running model (trial=3, mod=280, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  588.22521973\n",
      "Validation Loss:  588.22521973\n",
      "Final Training Loss:  588.22521973\n",
      "Final Validation Loss:  588.22521973\n",
      "\n",
      "Running model (trial=3, mod=281, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  28612.6640625\n",
      "Validation Loss:  28612.6640625\n",
      "Final Training Loss:  28612.6640625\n",
      "Final Validation Loss:  28612.6640625\n",
      "\n",
      "Running model (trial=3, mod=282, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  5.68712616\n",
      "Validation Loss:  5.68712616\n",
      "Final Training Loss:  5.68712616\n",
      "Final Validation Loss:  5.68712616\n",
      "\n",
      "Running model (trial=3, mod=283, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.26407111\n",
      "Validation Loss:  0.26407114\n",
      "Final Training Loss:  0.26407111\n",
      "Final Validation Loss:  0.26407114\n",
      "\n",
      "Running model (trial=3, mod=284, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01783231\n",
      "Validation Loss:  0.01783231\n",
      "Final Training Loss:  0.01783231\n",
      "Final Validation Loss:  0.01783231\n",
      "\n",
      "Running model (trial=3, mod=285, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  195.20097351\n",
      "Validation Loss:  195.20097351\n",
      "Final Training Loss:  195.20097351\n",
      "Final Validation Loss:  195.20097351\n",
      "\n",
      "Running model (trial=3, mod=286, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10296631\n",
      "Validation Loss:  600.10296631\n",
      "Final Training Loss:  600.10296631\n",
      "Final Validation Loss:  600.10296631\n",
      "\n",
      "Running model (trial=3, mod=287, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10253906\n",
      "Validation Loss:  600.10253906\n",
      "Final Training Loss:  600.10253906\n",
      "Final Validation Loss:  600.10253906\n",
      "\n",
      "Running model (trial=4, mod=288, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.50722426\n",
      "Validation Loss:  0.50722426\n",
      "Final Training Loss:  0.50722426\n",
      "Final Validation Loss:  0.50722426\n",
      "\n",
      "Running model (trial=4, mod=289, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.4727293\n",
      "Validation Loss:  0.4727293\n",
      "Final Training Loss:  0.4727293\n",
      "Final Validation Loss:  0.4727293\n",
      "\n",
      "Running model (trial=4, mod=290, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00494175\n",
      "Validation Loss:  0.00494175\n",
      "Final Training Loss:  0.00494175\n",
      "Final Validation Loss:  0.00494175\n",
      "\n",
      "Running model (trial=4, mod=291, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  58.0865593\n",
      "Validation Loss:  58.0865593\n",
      "Final Training Loss:  58.0865593\n",
      "Final Validation Loss:  58.0865593\n",
      "\n",
      "Running model (trial=4, mod=292, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  81.89391327\n",
      "Validation Loss:  81.89390564\n",
      "Final Training Loss:  81.89391327\n",
      "Final Validation Loss:  81.89390564\n",
      "\n",
      "Running model (trial=4, mod=293, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.10954296\n",
      "Validation Loss:  0.10954295\n",
      "Final Training Loss:  0.10954296\n",
      "Final Validation Loss:  0.10954295\n",
      "\n",
      "Running model (trial=4, mod=294, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05886891\n",
      "Validation Loss:  0.05886892\n",
      "Final Training Loss:  0.05886891\n",
      "Final Validation Loss:  0.05886892\n",
      "\n",
      "Running model (trial=4, mod=295, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00866081\n",
      "Validation Loss:  0.00866081\n",
      "Final Training Loss:  0.00866081\n",
      "Final Validation Loss:  0.00866081\n",
      "\n",
      "Running model (trial=4, mod=296, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00253565\n",
      "Validation Loss:  0.00253565\n",
      "Final Training Loss:  0.00253565\n",
      "Final Validation Loss:  0.00253565\n",
      "\n",
      "Running model (trial=4, mod=297, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10687256\n",
      "Validation Loss:  600.10687256\n",
      "Final Training Loss:  600.10687256\n",
      "Final Validation Loss:  600.10687256\n",
      "\n",
      "Running model (trial=4, mod=298, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10284424\n",
      "Validation Loss:  600.10284424\n",
      "Final Training Loss:  600.10284424\n",
      "Final Validation Loss:  600.10284424\n",
      "\n",
      "Running model (trial=4, mod=299, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10345459\n",
      "Validation Loss:  600.10345459\n",
      "Final Training Loss:  600.10345459\n",
      "Final Validation Loss:  600.10345459\n",
      "\n",
      "Running model (trial=4, mod=300, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11425558\n",
      "Validation Loss:  0.11425558\n",
      "Final Training Loss:  0.11425558\n",
      "Final Validation Loss:  0.11425558\n",
      "\n",
      "Running model (trial=4, mod=301, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04388537\n",
      "Validation Loss:  0.04388537\n",
      "Final Training Loss:  0.04388537\n",
      "Final Validation Loss:  0.04388537\n",
      "\n",
      "Running model (trial=4, mod=302, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01498391\n",
      "Validation Loss:  0.01498391\n",
      "Final Training Loss:  0.01498391\n",
      "Final Validation Loss:  0.01498391\n",
      "\n",
      "Running model (trial=4, mod=303, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  187.11204529\n",
      "Validation Loss:  187.11204529\n",
      "Final Training Loss:  187.11204529\n",
      "Final Validation Loss:  187.11204529\n",
      "\n",
      "Running model (trial=4, mod=304, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  89.18275452\n",
      "Validation Loss:  89.18274689\n",
      "Final Training Loss:  89.18275452\n",
      "Final Validation Loss:  89.18274689\n",
      "\n",
      "Running model (trial=4, mod=305, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  160.19540405\n",
      "Validation Loss:  160.19541931\n",
      "Final Training Loss:  160.19540405\n",
      "Final Validation Loss:  160.19541931\n",
      "\n",
      "Running model (trial=4, mod=306, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01507107\n",
      "Validation Loss:  0.01507107\n",
      "Final Training Loss:  0.01507107\n",
      "Final Validation Loss:  0.01507107\n",
      "\n",
      "Running model (trial=4, mod=307, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00477451\n",
      "Validation Loss:  0.00477451\n",
      "Final Training Loss:  0.00477451\n",
      "Final Validation Loss:  0.00477451\n",
      "\n",
      "Running model (trial=4, mod=308, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0007255\n",
      "Validation Loss:  0.0007255\n",
      "Final Training Loss:  0.0007255\n",
      "Final Validation Loss:  0.0007255\n",
      "\n",
      "Running model (trial=4, mod=309, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.12017822\n",
      "Validation Loss:  600.12011719\n",
      "Final Training Loss:  600.12017822\n",
      "Final Validation Loss:  600.12011719\n",
      "\n",
      "Running model (trial=4, mod=310, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10266113\n",
      "Validation Loss:  600.10266113\n",
      "Final Training Loss:  600.10266113\n",
      "Final Validation Loss:  600.10266113\n",
      "\n",
      "Running model (trial=4, mod=311, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10351562\n",
      "Validation Loss:  600.10351562\n",
      "Final Training Loss:  600.10351562\n",
      "Final Validation Loss:  600.10351562\n",
      "\n",
      "Running model (trial=4, mod=312, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00496787\n",
      "Validation Loss:  0.00496787\n",
      "Final Training Loss:  0.00496787\n",
      "Final Validation Loss:  0.00496787\n",
      "\n",
      "Running model (trial=4, mod=313, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01603779\n",
      "Validation Loss:  0.01603779\n",
      "Final Training Loss:  0.01603779\n",
      "Final Validation Loss:  0.01603779\n",
      "\n",
      "Running model (trial=4, mod=314, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00193731\n",
      "Validation Loss:  0.00193731\n",
      "Final Training Loss:  0.00193731\n",
      "Final Validation Loss:  0.00193731\n",
      "\n",
      "Running model (trial=4, mod=315, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01517411\n",
      "Validation Loss:  0.01517411\n",
      "Final Training Loss:  0.01517411\n",
      "Final Validation Loss:  0.01517411\n",
      "\n",
      "Running model (trial=4, mod=316, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.11489087\n",
      "Validation Loss:  0.11489087\n",
      "Final Training Loss:  0.11489087\n",
      "Final Validation Loss:  0.11489087\n",
      "\n",
      "Running model (trial=4, mod=317, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  113.38102722\n",
      "Validation Loss:  113.38102722\n",
      "Final Training Loss:  113.38102722\n",
      "Final Validation Loss:  113.38102722\n",
      "\n",
      "Running model (trial=4, mod=318, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.04112125\n",
      "Validation Loss:  0.04112125\n",
      "Final Training Loss:  0.04112125\n",
      "Final Validation Loss:  0.04112125\n",
      "\n",
      "Running model (trial=4, mod=319, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00248029\n",
      "Validation Loss:  0.00248029\n",
      "Final Training Loss:  0.00248029\n",
      "Final Validation Loss:  0.00248029\n",
      "\n",
      "Running model (trial=4, mod=320, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03047313\n",
      "Validation Loss:  0.03047314\n",
      "Final Training Loss:  0.03047313\n",
      "Final Validation Loss:  0.03047314\n",
      "\n",
      "Running model (trial=4, mod=321, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.21722412\n",
      "Validation Loss:  600.21722412\n",
      "Final Training Loss:  600.21722412\n",
      "Final Validation Loss:  600.21722412\n",
      "\n",
      "Running model (trial=4, mod=322, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10198975\n",
      "Validation Loss:  600.10192871\n",
      "Final Training Loss:  600.10198975\n",
      "Final Validation Loss:  600.10192871\n",
      "\n",
      "Running model (trial=4, mod=323, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1015625\n",
      "Validation Loss:  600.1015625\n",
      "Final Training Loss:  600.1015625\n",
      "Final Validation Loss:  600.1015625\n",
      "\n",
      "Running model (trial=4, mod=324, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.07337503\n",
      "Validation Loss:  0.07337504\n",
      "Final Training Loss:  0.07337503\n",
      "Final Validation Loss:  0.07337504\n",
      "\n",
      "Running model (trial=4, mod=325, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02682383\n",
      "Validation Loss:  0.02682383\n",
      "Final Training Loss:  0.02682383\n",
      "Final Validation Loss:  0.02682383\n",
      "\n",
      "Running model (trial=4, mod=326, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.05984462\n",
      "Validation Loss:  0.05984461\n",
      "Final Training Loss:  0.05984462\n",
      "Final Validation Loss:  0.05984461\n",
      "\n",
      "Running model (trial=4, mod=327, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03828961\n",
      "Validation Loss:  0.03828961\n",
      "Final Training Loss:  0.03828961\n",
      "Final Validation Loss:  0.03828961\n",
      "\n",
      "Running model (trial=4, mod=328, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  1448.29040527\n",
      "Validation Loss:  1448.29040527\n",
      "Final Training Loss:  1448.29040527\n",
      "Final Validation Loss:  1448.29040527\n",
      "\n",
      "Running model (trial=4, mod=329, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  644.76965332\n",
      "Validation Loss:  644.76965332\n",
      "Final Training Loss:  644.76965332\n",
      "Final Validation Loss:  644.76965332\n",
      "\n",
      "Running model (trial=4, mod=330, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02423885\n",
      "Validation Loss:  0.02423885\n",
      "Final Training Loss:  0.02423885\n",
      "Final Validation Loss:  0.02423885\n",
      "\n",
      "Running model (trial=4, mod=331, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00315071\n",
      "Validation Loss:  0.00315071\n",
      "Final Training Loss:  0.00315071\n",
      "Final Validation Loss:  0.00315071\n",
      "\n",
      "Running model (trial=4, mod=332, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.02696709\n",
      "Validation Loss:  0.02696709\n",
      "Final Training Loss:  0.02696709\n",
      "Final Validation Loss:  0.02696709\n",
      "\n",
      "Running model (trial=4, mod=333, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.06140137\n",
      "Validation Loss:  600.06140137\n",
      "Final Training Loss:  600.06140137\n",
      "Final Validation Loss:  600.06140137\n",
      "\n",
      "Running model (trial=4, mod=334, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10168457\n",
      "Validation Loss:  600.10168457\n",
      "Final Training Loss:  600.10168457\n",
      "Final Validation Loss:  600.10168457\n",
      "\n",
      "Running model (trial=4, mod=335, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10162354\n",
      "Validation Loss:  600.10162354\n",
      "Final Training Loss:  600.10162354\n",
      "Final Validation Loss:  600.10162354\n",
      "\n",
      "Running model (trial=4, mod=336, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00478311\n",
      "Validation Loss:  0.00478311\n",
      "Final Training Loss:  0.00478311\n",
      "Final Validation Loss:  0.00478311\n",
      "\n",
      "Running model (trial=4, mod=337, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  5.1606369\n",
      "Validation Loss:  5.1606369\n",
      "Final Training Loss:  5.1606369\n",
      "Final Validation Loss:  5.1606369\n",
      "\n",
      "Running model (trial=4, mod=338, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00854413\n",
      "Validation Loss:  0.00854413\n",
      "Final Training Loss:  0.00854413\n",
      "Final Validation Loss:  0.00854413\n",
      "\n",
      "Running model (trial=4, mod=339, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  109.56937408\n",
      "Validation Loss:  109.56937408\n",
      "Final Training Loss:  109.56937408\n",
      "Final Validation Loss:  109.56937408\n",
      "\n",
      "Running model (trial=4, mod=340, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  326.5670166\n",
      "Validation Loss:  326.5670166\n",
      "Final Training Loss:  326.5670166\n",
      "Final Validation Loss:  326.5670166\n",
      "\n",
      "Running model (trial=4, mod=341, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  3306.04345703\n",
      "Validation Loss:  3306.04345703\n",
      "Final Training Loss:  3306.04345703\n",
      "Final Validation Loss:  3306.04345703\n",
      "\n",
      "Running model (trial=4, mod=342, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01439004\n",
      "Validation Loss:  0.01439004\n",
      "Final Training Loss:  0.01439004\n",
      "Final Validation Loss:  0.01439004\n",
      "\n",
      "Running model (trial=4, mod=343, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03992989\n",
      "Validation Loss:  0.03992989\n",
      "Final Training Loss:  0.03992989\n",
      "Final Validation Loss:  0.03992989\n",
      "\n",
      "Running model (trial=4, mod=344, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03680268\n",
      "Validation Loss:  0.03680267\n",
      "Final Training Loss:  0.03680268\n",
      "Final Validation Loss:  0.03680267\n",
      "\n",
      "Running model (trial=4, mod=345, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  140.67192078\n",
      "Validation Loss:  140.67190552\n",
      "Final Training Loss:  140.67192078\n",
      "Final Validation Loss:  140.67190552\n",
      "\n",
      "Running model (trial=4, mod=346, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10498047\n",
      "Validation Loss:  600.10498047\n",
      "Final Training Loss:  600.10498047\n",
      "Final Validation Loss:  600.10498047\n",
      "\n",
      "Running model (trial=4, mod=347, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.1038208\n",
      "Validation Loss:  600.1038208\n",
      "Final Training Loss:  600.1038208\n",
      "Final Validation Loss:  600.1038208\n",
      "\n",
      "Running model (trial=4, mod=348, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  90.73820496\n",
      "Validation Loss:  90.73819733\n",
      "Final Training Loss:  90.73820496\n",
      "Final Validation Loss:  90.73819733\n",
      "\n",
      "Running model (trial=4, mod=349, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.06024884\n",
      "Validation Loss:  0.06024884\n",
      "Final Training Loss:  0.06024884\n",
      "Final Validation Loss:  0.06024884\n",
      "\n",
      "Running model (trial=4, mod=350, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.03128527\n",
      "Validation Loss:  0.03128527\n",
      "Final Training Loss:  0.03128527\n",
      "Final Validation Loss:  0.03128527\n",
      "\n",
      "Running model (trial=4, mod=351, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.1091681\n",
      "Validation Loss:  0.1091681\n",
      "Final Training Loss:  0.1091681\n",
      "Final Validation Loss:  0.1091681\n",
      "\n",
      "Running model (trial=4, mod=352, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  224.70040894\n",
      "Validation Loss:  224.70040894\n",
      "Final Training Loss:  224.70040894\n",
      "Final Validation Loss:  224.70040894\n",
      "\n",
      "Running model (trial=4, mod=353, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  13358.35839844\n",
      "Validation Loss:  13358.35839844\n",
      "Final Training Loss:  13358.35839844\n",
      "Final Validation Loss:  13358.35839844\n",
      "\n",
      "Running model (trial=4, mod=354, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01623676\n",
      "Validation Loss:  0.01623676\n",
      "Final Training Loss:  0.01623676\n",
      "Final Validation Loss:  0.01623676\n",
      "\n",
      "Running model (trial=4, mod=355, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.01307637\n",
      "Validation Loss:  0.01307637\n",
      "Final Training Loss:  0.01307637\n",
      "Final Validation Loss:  0.01307637\n",
      "\n",
      "Running model (trial=4, mod=356, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.00516273\n",
      "Validation Loss:  0.00516273\n",
      "Final Training Loss:  0.00516273\n",
      "Final Validation Loss:  0.00516273\n",
      "\n",
      "Running model (trial=4, mod=357, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  0.0496827\n",
      "Validation Loss:  0.0496827\n",
      "Final Training Loss:  0.0496827\n",
      "Final Validation Loss:  0.0496827\n",
      "\n",
      "Running model (trial=4, mod=358, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10412598\n",
      "Validation Loss:  600.10412598\n",
      "Final Training Loss:  600.10412598\n",
      "Final Validation Loss:  600.10412598\n",
      "\n",
      "Running model (trial=4, mod=359, k=0):\n",
      "################################  0  ################################\n",
      "Training Loss:  600.10229492\n",
      "Validation Loss:  600.10229492\n",
      "Final Training Loss:  600.10229492\n",
      "Final Validation Loss:  600.10229492\n"
     ]
    }
   ],
   "source": [
    "cv_results = k_fold_cv_grid(\n",
    "    model_params=exp_model_params_iter,\n",
    "    fit=fit_FFNN,\n",
    "    training_params=exp_training_params_iter,\n",
    "    data=data,\n",
    "    folds=FOLDS,\n",
    "    verbose=True,\n",
    "    trials=5\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../figures/curve_1/history_curve_1_exp_1_0.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-0d84761eed95>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m     \u001B[0;34m\"y_axis\"\u001B[0m\u001B[0;34m:\u001B[0m \u001B[0;34m\"$\\\\xi(t)$\"\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m }\n\u001B[0;32m---> 12\u001B[0;31m plot_result(\n\u001B[0m\u001B[1;32m     13\u001B[0m     \u001B[0mpath_figures\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mPATH_FIGURES\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m     \u001B[0mplot_name\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mSET_NAME\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git_repos/git_skole/deepthermal/deepthermal/plotting.py\u001B[0m in \u001B[0;36mplot_result\u001B[0;34m(models, loss_history_trains, loss_history_vals, path_figures, plot_name, rel_val_errors, plot_function, function_kwargs, model_list, history, **kwargs)\u001B[0m\n\u001B[1;32m    148\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mi\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodel_list\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    149\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhistory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 150\u001B[0;31m             plot_model_history(\n\u001B[0m\u001B[1;32m    151\u001B[0m                 \u001B[0mmodels\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    152\u001B[0m                 \u001B[0mloss_history_trains\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/git_repos/git_skole/deepthermal/deepthermal/plotting.py\u001B[0m in \u001B[0;36mplot_model_history\u001B[0;34m(models, loss_history_trains, loss_history_vals, plot_name, path_figures)\u001B[0m\n\u001B[1;32m     42\u001B[0m         \u001B[0maxis\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_ylabel\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Loss\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0maxis\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlegend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m     \u001B[0mhistfig\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0msavefig\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34mf\"{path_figures}/history_{plot_name}.pdf\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m     \u001B[0mplt\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mclose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mhistfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/figure.py\u001B[0m in \u001B[0;36msavefig\u001B[0;34m(self, fname, transparent, **kwargs)\u001B[0m\n\u001B[1;32m   2957\u001B[0m                 \u001B[0mpatch\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mset_edgecolor\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'none'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2958\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2959\u001B[0;31m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcanvas\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mprint_figure\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2960\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2961\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mtransparent\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/backend_bases.py\u001B[0m in \u001B[0;36mprint_figure\u001B[0;34m(self, filename, dpi, facecolor, edgecolor, orientation, format, bbox_inches, pad_inches, bbox_extra_artists, backend, **kwargs)\u001B[0m\n\u001B[1;32m   2253\u001B[0m                 \u001B[0;31m# force the figure dpi to 72), so we need to set it again here.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2254\u001B[0m                 \u001B[0;32mwith\u001B[0m \u001B[0mcbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_setattr_cm\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfigure\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdpi\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdpi\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2255\u001B[0;31m                     result = print_method(\n\u001B[0m\u001B[1;32m   2256\u001B[0m                         \u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2257\u001B[0m                         \u001B[0mfacecolor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfacecolor\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/backend_bases.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m   1667\u001B[0m             \u001B[0mkwargs\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpop\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1668\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1669\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1670\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1671\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/_api/deprecation.py\u001B[0m in \u001B[0;36mwrapper\u001B[0;34m(*inner_args, **inner_kwargs)\u001B[0m\n\u001B[1;32m    429\u001B[0m                          \u001B[0;32melse\u001B[0m \u001B[0mdeprecation_addendum\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    430\u001B[0m                 **kwargs)\n\u001B[0;32m--> 431\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mfunc\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0minner_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0minner_kwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    432\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    433\u001B[0m     \u001B[0;32mreturn\u001B[0m \u001B[0mwrapper\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/backends/backend_pdf.py\u001B[0m in \u001B[0;36mprint_pdf\u001B[0;34m(self, filename, dpi, bbox_inches_restore, metadata)\u001B[0m\n\u001B[1;32m   2716\u001B[0m             \u001B[0mfile\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mfilename\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_file\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2717\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2718\u001B[0;31m             \u001B[0mfile\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mPdfFile\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmetadata\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2719\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2720\u001B[0m             \u001B[0mfile\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnewPage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mwidth\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mheight\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/backends/backend_pdf.py\u001B[0m in \u001B[0;36m__init__\u001B[0;34m(self, filename, metadata)\u001B[0m\n\u001B[1;32m    635\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0moriginal_file_like\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    636\u001B[0m         \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtell_base\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m0\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 637\u001B[0;31m         \u001B[0mfh\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mopened\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mcbook\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mto_filehandle\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfilename\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m\"wb\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mreturn_opened\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    638\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mopened\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    639\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.9/site-packages/matplotlib/cbook/__init__.py\u001B[0m in \u001B[0;36mto_filehandle\u001B[0;34m(fname, flag, return_opened, encoding)\u001B[0m\n\u001B[1;32m    460\u001B[0m             \u001B[0mfh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mbz2\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mBZ2File\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflag\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    461\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 462\u001B[0;31m             \u001B[0mfh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mopen\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mflag\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mencoding\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mencoding\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    463\u001B[0m         \u001B[0mopened\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    464\u001B[0m     \u001B[0;32melif\u001B[0m \u001B[0mhasattr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mfname\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'seek'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: [Errno 2] No such file or directory: '../figures/curve_1/history_curve_1_exp_1_0.pdf'"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 576x432 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plotting\n",
    "x_train_ = x_train.detach()\n",
    "x_sorted, indices = torch.sort(x_train_, dim=0)\n",
    "\n",
    "plot_kwargs = {\n",
    "    \"x_test\": x_sorted,\n",
    "    \"x_train\": x_sorted,\n",
    "    \"y_train\": c1.ksi(x_sorted),\n",
    "    \"x_axis\": \"t\",\n",
    "    \"y_axis\": \"$\\\\xi(t)$\",\n",
    "}\n",
    "plot_result(\n",
    "    path_figures=PATH_FIGURES,\n",
    "    plot_name=SET_NAME,\n",
    "    **cv_results,\n",
    "    plot_function=plot_model_1d,\n",
    "    function_kwargs=plot_kwargs,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "models = cv_results[\"models\"]\n",
    "\n",
    "parameters = np.vectorize(lambda model: sum(p.numel() for p in model.parameters()))(models)\n",
    "model_type = np.vectorize(lambda model: 0 if isinstance(model,ResNET) else 1)(models)\n",
    "layers = np.vectorize(lambda model: model.n_hidden_layers)(models)\n",
    "neurons = np.vectorize(lambda model: model.neurons)(models)\n",
    "loss_array = np.vectorize(lambda model: loss_func(model, x_train, q_train).detach())(models)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(layers,loss_array)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"Layers\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(parameters,loss_array)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"Number of parameters\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(neurons,loss_array)\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_ylabel(\"Error\")\n",
    "ax.set_xlabel(\"Neurons per layer\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "fmri = sns.load_dataset(\"fmri\")\n",
    "sns.relplot(x=parameters.ravel(), y=loss_array.ravel(), kind=\"line\");\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}